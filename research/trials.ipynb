{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7634aac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begins\n"
     ]
    }
   ],
   "source": [
    "print(\"begins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7f2fa23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19fac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a015c711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab644a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\GenAiPed_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders  import PyMuPDFLoader,DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f07a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "\n",
    "def load_pdf_data(folder_path):\n",
    "    loader = DirectoryLoader(\n",
    "        folder_path,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyMuPDFLoader\n",
    "    )\n",
    "    return loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7d5b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf_data(\"D:\\XXX.BADI BAAT CHEET.XXX\\GenAiPedia\\GenAIPed\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "497b12bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 0}, page_content='The\\nHundred-\\nPage\\nMachine\\nLearning\\nBook\\nAndriy Burkov'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 1}, page_content='“All models are wrong, but some are useful.”\\n— George Box\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 2}, page_content='Preface\\nLet’s start by telling the truth: machines don’t learn. What a typical “learning machine”\\ndoes, is ﬁnding a mathematical formula, which, when applied to a collection of inputs (called\\n“training data”), produces the desired outputs. This mathematical formula also generates the\\ncorrect outputs for most other inputs (distinct from the training data) on the condition that\\nthose inputs come from the same or a similar statistical distribution as the one the training\\ndata was drawn from.\\nWhy isn’t that learning? Because if you slightly distort the inputs, the output is very likely\\nto become completely wrong. It’s not how learning in animals works. If you learned to play\\na video game by looking straight at the screen, you would still be a good player if someone\\nrotates the screen slightly. A machine learning algorithm, if it was trained by “looking”\\nstraight at the screen, unless it was also trained to recognize rotation, will fail to play the\\ngame on a rotated screen.\\nSo why the name “machine learning” then? The reason, as is often the case, is marketing:\\nArthur Samuel, an American pioneer in the ﬁeld of computer gaming and artiﬁcial intelligence,\\ncoined the term in 1959 while at IBM. Similarly to how in the 2010s IBM tried to market\\nthe term “cognitive computing” to stand out from competition, in the 1960s, IBM used the\\nnew cool term “machine learning” to attract both clients and talented employees.\\nAs you can see, just like artiﬁcial intelligence is not intelligence, machine learning is not\\nlearning. However, machine learning is a universally recognized term that usually refers\\nto the science and engineering of building machines capable of doing various useful things\\nwithout being explicitly programmed to do so. So, the word “learning” in the term is used\\nby analogy with the learning in animals rather than literally.\\nWho This Book is For\\nThis book contains only those parts of the vast body of material on machine learning developed\\nsince the 1960s that have proven to have a signiﬁcant practical value. A beginner in machine\\nlearning will ﬁnd in this book just enough details to get a comfortable level of understanding\\nof the ﬁeld and start asking the right questions.\\nPractitioners with experience can use this book as a collection of directions for further\\nself-improvement. The book also comes in handy when brainstorming at the beginning of a\\nproject, when you try to answer the question whether a given technical or business problem\\nis “machine-learnable” and, if yes, which techniques you should try to solve it.\\nHow to Use This Book\\nIf you are about to start learning machine learning, you should read this book from the\\nbeginning to the end. (It’s just a hundred pages, not a big deal.) If you are interested\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 3}, page_content='in a speciﬁc topic covered in the book and want to know more, most sections have a QR\\ncode. By scanning one of those QR codes with your phone, you will get a link to a page on\\nthe book’s companion wiki theMLbook.com with additional materials: recommended reads,\\nvideos, Q&As, code snippets, tutorials, and other bonuses.\\nThe book’s wiki is continuously updated with contributions from the book’s author himself\\nas well as volunteers from all over the world. So this book, like a good wine, keeps getting\\nbetter after you buy it.\\nScan the QR code below with your phone to get to the book’s wiki:\\nSome sections don’t have a QR code, but they still most likely have a wiki page. You can\\nﬁnd it by submitting the section’s title to the wiki’s search engine.\\nShould You Buy This Book?\\nThis book is distributed on the “read ﬁrst, buy later” principle. I ﬁrmly believe that paying\\nfor the content before consuming it is buying a pig in a poke. You can see and try a car in a\\ndealership before you buy it. You can try on a shirt or a dress in a department store. You\\nhave to be able to read a book before paying for it.\\nThe read ﬁrst, buy later principle implies that you can freely download the book, read it and\\nshare it with your friends and colleagues. If you liked the book, only then you have to buy it.\\nNow you are all set. Enjoy your reading!\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n4'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 4}, page_content='The\\nHundred-\\nPage\\nMachine\\nLearning\\nBook\\nAndriy Burkov'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 5}, page_content='“All models are wrong, but some are useful.”\\n— George Box\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 6}, page_content='1\\nIntroduction\\n1.1\\nWhat is Machine Learning\\nMachine learning is a subﬁeld of computer science that is concerned with building algorithms\\nwhich, to be useful, rely on a collection of examples of some phenomenon. These examples\\ncan come from nature, be handcrafted by humans or generated by another algorithm.\\nMachine learning can also be deﬁned as the process of solving a practical problem by 1)\\ngathering a dataset, and 2) algorithmically building a statistical model based on that dataset.\\nThat statistical model is assumed to be used somehow to solve the practical problem.\\nTo save keystrokes, I use the terms “learning” and “machine learning” interchangeably.\\n1.2\\nTypes of Learning\\nLearning can be supervised, semi-supervised, unsupervised and reinforcement.\\n1.2.1\\nSupervised Learning\\nIn supervised learning1, the dataset is the collection of labeled examples {(xi, yi)}N\\ni=1.\\nEach element xi among N is called a feature vector. A feature vector is a vector in which\\neach dimension j = 1, . . . , D contains a value that describes the example somehow. That\\nvalue is called a feature and is denoted as x(j). For instance, if each example x in our\\ncollection represents a person, then the ﬁrst feature, x(1), could contain height in cm, the\\nsecond feature, x(2), could contain weight in kg, x(3) could contain gender, and so on. For all\\nexamples in the dataset, the feature at position j in the feature vector always contains the\\nsame kind of information. It means that if x(2)\\ni\\ncontains weight in kg in some example xi,\\nthen x(2)\\nk\\nwill also contain weight in kg in every example xk, k = 1, . . . , N. The label yi can\\nbe either an element belonging to a ﬁnite set of classes {1, 2, . . . , C}, or a real number, or a\\nmore complex structure, like a vector, a matrix, a tree, or a graph. Unless otherwise stated,\\nin this book yi is either one of a ﬁnite set of classes or a real number. You can see a class as\\na category to which an example belongs. For instance, if your examples are email messages\\nand your problem is spam detection, then you have two classes {spam, not_spam}.\\nThe goal of a supervised learning algorithm is to use the dataset to produce a model\\nthat takes a feature vector x as input and outputs information that allows deducing the label\\nfor this feature vector. For instance, the model created using the dataset of people could\\ntake as input a feature vector describing a person and output a probability that the person\\nhas cancer.\\n1In this book, if a term is in bold, that means that this term can be found in the index at the end of the\\nbook.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 7}, page_content='1.2.2\\nUnsupervised Learning\\nIn unsupervised learning, the dataset is a collection of unlabeled examples {xi}N\\ni=1.\\nAgain, x is a feature vector, and the goal of an unsupervised learning algorithm is\\nto create a model that takes a feature vector x as input and either transforms it into\\nanother vector or into a value that can be used to solve a practical problem. For example,\\nin clustering, the model returns the id of the cluster for each feature vector in the dataset.\\nIn dimensionality reduction, the output of the model is a feature vector that has fewer\\nfeatures than the input x; in outlier detection, the output is a real number that indicates\\nhow x is diﬀerent from a “typical” example in the dataset.\\n1.2.3\\nSemi-Supervised Learning\\nIn semi-supervised learning, the dataset contains both labeled and unlabeled examples.\\nUsually, the quantity of unlabeled examples is much higher than the number of labeled\\nexamples. The goal of a semi-supervised learning algorithm is the same as the goal of\\nthe supervised learning algorithm. The hope here is that using many unlabeled examples can\\nhelp the learning algorithm to ﬁnd (we might say “produce” or “compute”) a better model2.\\n1.2.4\\nReinforcement Learning\\nReinforcement learning is a subﬁeld of machine learning where the machine “lives” in an\\nenvironment and is capable of perceiving the state of that environment as a vector of\\nfeatures. The machine can execute actions in every state. Diﬀerent actions bring diﬀerent\\nrewards and could also move the machine to another state of the environment. The goal\\nof a reinforcement learning algorithm is to learn a policy. A policy is a function f (similar\\nto the model in supervised learning) that takes the feature vector of a state as input and\\noutputs an optimal action to execute in that state. The action is optimal if it maximizes the\\nexpected average reward.\\nReinforcement learning solves a particular kind of problems where\\ndecision making is sequential, and the goal is long-term, such as game\\nplaying, robotics, resource management, or logistics. In this book, I\\nput emphasis on one-shot decision making where input examples are\\nindependent of one another and the predictions made in the past. I\\nleave reinforcement learning out of the scope of this book.\\n2It could look counter-intuitive that learning could beneﬁt from adding more unlabeled examples. It seems\\nlike we add more uncertainty to the problem. However, when you add unlabeled examples, you add more\\ninformation about your problem: a larger sample reﬂects better the probability distribution the data we\\nlabeled came from. Theoretically, a learning algorithm should be able to leverage this additional information.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n4'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 8}, page_content='1.3\\nHow Supervised Learning Works\\nIn this section, I brieﬂy explain how supervised learning works so that you have the picture\\nof the whole process before we go into detail. I decided to use supervised learning as an\\nexample because it’s the type of machine learning most frequently used in practice.\\nThe supervised learning process starts with gathering the data. The data for supervised\\nlearning is a collection of pairs (input, output). Input could be anything, for example, email\\nmessages, pictures, or sensor measurements. Outputs are usually real numbers, or labels (e.g.\\n“spam”, “not_spam”, “cat”, “dog”, “mouse”, etc). In some cases, outputs are vectors (e.g.,\\nfour coordinates of the rectangle around a person on the picture), sequences (e.g. [“adjective”,\\n“adjective”, “noun”] for the input “big beautiful car”), or have some other structure.\\nLet’s say the problem that you want to solve using supervised learning is spam detection.\\nYou gather the data, for example, 10,000 email messages, each with a label either “spam” or\\n“not_spam” (you could add those labels manually or pay someone to do that for us). Now,\\nyou have to convert each email message into a feature vector.\\nThe data analyst decides, based on their experience, how to convert a real-world entity, such\\nas an email message, into a feature vector. One common way to convert a text into a feature\\nvector, called bag of words, is to take a dictionary of English words (let’s say it contains\\n20,000 alphabetically sorted words) and stipulate that in our feature vector:\\n• the ﬁrst feature is equal to 1 if the email message contains the word “a”; otherwise,\\nthis feature is 0;\\n• the second feature is equal to 1 if the email message contains the word “aaron”; otherwise,\\nthis feature equals 0;\\n• . . .\\n• the feature at position 20,000 is equal to 1 if the email message contains the word\\n“zulu”; otherwise, this feature is equal to 0.\\nYou repeat the above procedure for every email message in our collection, which gives\\nus 10,000 feature vectors (each vector having the dimensionality of 20,000) and a label\\n(“spam”/“not_spam”).\\nNow you have a machine-readable input data, but the output labels are still in the form of\\nhuman-readable text. Some learning algorithms require transforming labels into numbers.\\nFor example, some algorithms require numbers like 0 (to represent the label “not_spam”)\\nand 1 (to represent the label “spam”). The algorithm I use to illustrate supervised learning is\\ncalled Support Vector Machine (SVM). This algorithm requires that the positive label (in\\nour case it’s “spam”) has the numeric value of +1 (one), and the negative label (“not_spam”)\\nhas the value of ≠1 (minus one).\\nAt this point, you have a dataset and a learning algorithm, so you are ready to apply\\nthe learning algorithm to the dataset to get the model.\\nSVM sees every feature vector as a point in a high-dimensional space (in our case, space\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 9}, page_content='is 20,000-dimensional). The algorithm puts all feature vectors on an imaginary 20,000-\\ndimensional plot and draws an imaginary 20,000-dimensional line (a hyperplane) that separates\\nexamples with positive labels from examples with negative labels. In machine learning, the\\nboundary separating the examples of diﬀerent classes is called the decision boundary.\\nThe equation of the hyperplane is given by two parameters, a real-valued vector w of the\\nsame dimensionality as our input feature vector x, and a real number b like this:\\nwx ≠b = 0,\\nwhere the expression wx means w(1)x(1) + w(2)x(2) + . . . + w(D)x(D), and D is the number\\nof dimensions of the feature vector x.\\n(If some equations aren’t clear to you right now, in Chapter 2 we revisit the math and\\nstatistical concepts necessary to understand them. For the moment, try to get an intuition of\\nwhat’s happening here. It all becomes more clear after you read the next chapter.)\\nNow, the predicted label for some input feature vector x is given like this:\\ny = sign(wx ≠b),\\nwhere sign is a mathematical operator that takes any value as input and returns +1 if the\\ninput is a positive number or ≠1 if the input is a negative number.\\nThe goal of the learning algorithm — SVM in this case — is to leverage the dataset and ﬁnd\\nthe optimal values wú and bú for parameters w and b. Once the learning algorithm identiﬁes\\nthese optimal values, the model f(x) is then deﬁned as:\\nf(x) = sign(wúx ≠bú)\\nTherefore, to predict whether an email message is spam or not spam using an SVM model,\\nyou have to take a text of the message, convert it into a feature vector, then multiply this\\nvector by wú, subtract bú and take the sign of the result. This will give us the prediction (+1\\nmeans “spam”, ≠1 means “not_spam”).\\nNow, how does the machine ﬁnd wú and bú? It solves an optimization problem. Machines\\nare good at optimizing functions under constraints.\\nSo what are the constraints we want to satisfy here? First of all, we want the model to predict\\nthe labels of our 10,000 examples correctly. Remember that each example i = 1, . . . , 10000 is\\ngiven by a pair (xi, yi), where xi is the feature vector of example i and yi is its label that\\ntakes values either ≠1 or +1. So the constraints are naturally:\\n• wxi ≠b Ø 1 if yi = +1, and\\n• wxi ≠b Æ ≠1 if yi = ≠1\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n6'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 10}, page_content='x(2)\\nx(1)\\nwx\\xa0—\\xa0b\\xa0=\\xa00\\nwx\\xa0—\\xa0b\\xa0=\\xa01\\nwx\\xa0—\\xa0b\\xa0=\\xa0—1\\nb\\xa0\\n||w||\\xa0\\n2\\xa0\\n||w||\\xa0\\nFigure 1: An example of an SVM model for two-dimensional feature vectors.\\nWe would also prefer that the hyperplane separates positive examples from negative ones with\\nthe largest margin. The margin is the distance between the closest examples of two classes,\\nas deﬁned by the decision boundary. A large margin contributes to a better generalization,\\nthat is how well the model will classify new examples in the future. To achieve that, we need\\nto minimize the Euclidean norm of w denoted by ÎwÎ and given by\\nÒqD\\nj=1(w(j))2.\\nSo, the optimization problem that we want the machine to solve looks like this:\\nMinimize ÎwÎ subject to yi(wxi ≠b) Ø 1 for i = 1, . . . , N. The expression yi(wxi ≠b) Ø 1\\nis just a compact way to write the above two constraints.\\nThe solution of this optimization problem, given by wú and bú, is called the statistical\\nmodel, or, simply, the model. The process of building the model is called training.\\nFor two-dimensional feature vectors, the problem and the solution can be visualized as shown\\nin ﬁg. 1. The blue and orange circles represent, respectively, positive and negative examples,\\nand the line given by wx ≠b = 0 is the decision boundary.\\nWhy, by minimizing the norm of w, do we ﬁnd the highest margin between the two classes?\\nGeometrically, the equations wx ≠b = 1 and wx ≠b = ≠1 deﬁne two parallel hyperplanes,\\nas you see in ﬁg. 1. The distance between these hyperplanes is given by\\n2\\nÎwÎ, so the smaller\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n7'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 11}, page_content='the norm ÎwÎ, the larger the distance between these two hyperplanes.\\nThat’s how Support Vector Machines work. This particular version of the algorithm builds\\nthe so-called linear model. It’s called linear because the decision boundary is a straight line\\n(or a plane, or a hyperplane). SVM can also incorporate kernels that can make the decision\\nboundary arbitrarily non-linear. In some cases, it could be impossible to perfectly separate\\nthe two groups of points because of noise in the data, errors of labeling, or outliers (examples\\nvery diﬀerent from a “typical” example in the dataset). Another version of SVM can also\\nincorporate a penalty hyperparameter for misclassiﬁcation of training examples of speciﬁc\\nclasses. We study the SVM algorithm in more detail in Chapter 3.\\nAt this point, you should retain the following: any classiﬁcation learning algorithm that\\nbuilds a model implicitly or explicitly creates a decision boundary. The decision boundary\\ncan be straight, or curved, or it can have a complex form, or it can be a superposition of\\nsome geometrical ﬁgures. The form of the decision boundary determines the accuracy of\\nthe model (that is the ratio of examples whose labels are predicted correctly). The form of\\nthe decision boundary, the way it is algorithmically or mathematically computed based on\\nthe training data, diﬀerentiates one learning algorithm from another.\\nIn practice, there are two other essential diﬀerentiators of learning algorithms to consider:\\nspeed of model building and prediction processing time. In many practical cases, you would\\nprefer a learning algorithm that builds a less accurate model fast. Additionally, you might\\nprefer a less accurate model that is much quicker at making predictions.\\n1.4\\nWhy the Model Works on New Data\\nWhy is a machine-learned model capable of predicting correctly the labels of new, previously\\nunseen examples? To understand that, look at the plot in ﬁg. 1. If two classes are separable\\nfrom one another by a decision boundary, then, obviously, examples that belong to each class\\nare located in two diﬀerent subspaces which the decision boundary creates.\\nIf the examples used for training were selected randomly, independently of one another, and\\nfollowing the same procedure, then, statistically, it is more likely that the new negative\\nexample will be located on the plot somewhere not too far from other negative examples.\\nThe same concerns the new positive example: it will likely come from the surroundings of\\nother positive examples. In such a case, our decision boundary will still, with high probability,\\nseparate well new positive and negative examples from one another. For other, less likely\\nsituations, our model will make errors, but because such situations are less likely, the number\\nof errors will likely be smaller than the number of correct predictions.\\nIntuitively, the larger is the set of training examples, the more unlikely that the new examples\\nwill be dissimilar to (and lie on the plot far from) the examples used for training. To minimize\\nthe probability of making errors on new examples, the SVM algorithm, by looking for the\\nlargest margin, explicitly tries to draw the decision boundary in such a way that it lies as far\\nas possible from examples of both classes.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n8'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 12}, page_content='The reader interested in knowing more about the learnability and un-\\nderstanding the close relationship between the model error, the size of\\nthe training set, the form of the mathematical equation that deﬁnes\\nthe model, and the time it takes to build the model is encouraged to\\nread about the PAC learning. The PAC (for “probably approximately\\ncorrect”) learning theory helps to analyze whether and under what\\nconditions a learning algorithm will probably output an approximately\\ncorrect classiﬁer.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n9'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 13}, page_content='The\\nHundred-\\nPage\\nMachine\\nLearning\\nBook\\nAndriy Burkov'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 14}, page_content='“All models are wrong, but some are useful.”\\n— George Box\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 15}, page_content='2\\nNotation and Deﬁnitions\\n2.1\\nNotation\\nLet’s start by revisiting the mathematical notation we all learned at school, but some likely\\nforgot right after the prom.\\n2.1.1\\nScalars, Vectors, and Sets\\nA scalar is a simple numerical value, like 15 or ≠3.25. Variables or constants that take scalar\\nvalues are denoted by an italic letter, like x or a.\\nFigure 1: Three vectors visualized as directions and as points.\\nA vector is an ordered list of scalar values, called attributes. We denote a vector as a bold\\ncharacter, for example, x or w. Vectors can be visualized as arrows that point to some\\ndirections as well as points in a multi-dimensional space. Illustrations of three two-dimensional\\nvectors, a = [2, 3], b = [≠2, 5], and c = [1, 0] is given in ﬁg. 1. We denote an attribute of a\\nvector as an italic value with an index, like this: w(j) or x(j). The index j denotes a speciﬁc\\ndimension of the vector, the position of an attribute in the list. For instance, in the vector a\\nshown in red in ﬁg. 1, a(1) = 2 and a(2) = 3.\\nThe notation x(j) should not be confused with the power operator, like this x2 (squared) or\\nx3 (cubed). If we want to apply a power operator, say square, to an indexed attribute of a\\nvector, we write like this: (x(j))2.\\nA variable can have two or more indices, like this: x(j)\\ni\\nor like this x(k)\\ni,j . For example, in\\nneural networks, we denote as x(j)\\nl,u the input feature j of unit u in layer l.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 16}, page_content='A set is an unordered collection of unique elements. We denote a set as a calligraphic\\ncapital character, for example, S. A set of numbers can be ﬁnite (include a ﬁxed amount\\nof values). In this case, it is denoted using accolades, for example, {1, 3, 18, 23, 235} or\\n{x1, x2, x3, x4, . . . , xn}. A set can be inﬁnite and include all values in some interval. If a set\\nincludes all values between a and b, including a and b, it is denoted using brackets as [a, b].\\nIf the set doesn’t include the values a and b, such a set is denoted using parentheses like this:\\n(a, b). For example, the set [0, 1] includes such values as 0, 0.0001, 0.25, 0.784, 0.9995, and\\n1.0. A special set denoted R includes all numbers from minus inﬁnity to plus inﬁnity.\\nWhen an element x belongs to a set S, we write x œ S. We can obtain a new set S3 as\\nan intersection of two sets S1 and S2. In this case, we write S3 Ω S1 ﬂS2. For example\\n{1, 3, 5, 8} ﬂ{1, 8, 4} gives the new set {1, 8}.\\nWe can obtain a new set S3 as a union of two sets S1 and S2. In this case, we write\\nS3 Ω S1 ﬁS2. For example {1, 3, 5, 8} ﬁ{1, 8, 4} gives the new set {1, 3, 4, 5, 8}.\\n2.1.2\\nCapital Sigma Notation\\nThe summation over a collection X = {x1, x2, . . . , xn≠1, xn} or over the attributes of a vector\\nx = [x(1), x(2), . . . , x(m≠1), x(m)] is denoted like this:\\nn\\nÿ\\ni=1\\nxi\\ndef\\n= x1 + x2 + . . . + xn≠1 + xn, or else:\\nm\\nÿ\\nj=1\\nx(j) def\\n= x(1) + x(2) + . . . + x(m≠1) + x(m).\\nThe notation\\ndef\\n= means “is deﬁned as”.\\n2.1.3\\nCapital Pi Notation\\nA notation analogous to capital sigma is the capital pi notation. It denotes a product of\\nelements in a collection or attributes of a vector:\\nn\\nŸ\\ni=1\\nxi\\ndef\\n= x1 · x2 · . . . · xn≠1 · xn,\\nwhere a · b means a multiplied by b. Where possible, we omit · to simplify the notation, so ab\\nalso means a multiplied by b.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n4'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 17}, page_content='2.1.4\\nOperations on Sets\\nA derived set creation operator looks like this: SÕ Ω {x2 | x œ S, x > 3}. This notation means\\nthat we create a new set SÕ by putting into it x squared such that that x is in S, and x is\\ngreater than 3.\\nThe cardinality operator |S| returns the number of elements in set S.\\n2.1.5\\nOperations on Vectors\\nThe sum of two vectors x + z is deﬁned as the vector [x(1) + z(1), x(2) + z(2), . . . , x(m) + z(m)].\\nThe diﬀerence of two vectors x ≠z is deﬁned as the vector [x(1) ≠z(1), x(2) ≠z(2), . . . , x(m) ≠\\nz(m)].\\nA vector multiplied by a scalar is a vector. For example xc\\ndef\\n= [cx(1), cx(2), . . . , cx(m)].\\nA dot-product of two vectors is a scalar. For example, wx\\ndef\\n= qm\\ni=1 w(i)x(i). In some books,\\nthe dot-product is denoted as w · x. The two vectors must be of the same dimensionality.\\nOtherwise, the dot-product is undeﬁned.\\nThe multiplication of a matrix W by a vector x gives another vector as a result. Let our\\nmatrix be,\\nW =\\n5w(1,1)\\nw(1,2)\\nw(1,3)\\nw(2,1)\\nw(2,2)\\nw(2,3)\\n6\\n.\\nWhen vectors participate in operations on matrices, a vector is by default represented as a\\nmatrix with one column. When the vector is on the right of the matrix, it remains a column\\nvector. We can only multiply a matrix by vector if the vector has the same number of rows\\nas the number of columns in the matrix. Let our vector be x\\ndef\\n= [x(1), x(2), x(3)]. Then Wx\\nis a two-dimensional vector deﬁned as,\\nWx =\\n5w(1,1)\\nw(1,2)\\nw(1,3)\\nw(2,1)\\nw(2,2)\\nw(2,3)\\n6 S\\nU\\nx(1)\\nx(2)\\nx(3)\\nT\\nV\\ndef\\n=\\n5w(1,1)x(1) + w(1,2)x(2) + w(1,3)x(3)\\nw(2,1)x(1) + w(2,2)x(2) + w(2,3)x(3)\\n6\\n=\\n5w(1)x\\nw(2)x\\n6\\nIf our matrix had, say, ﬁve rows, the result of the above product would be a ﬁve-dimensional\\nvector.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 18}, page_content='When the vector is on the left side of the matrix in the multiplication, then it has to be\\ntransposed before we multiply it by the matrix. The transpose of the vector x denoted as x€\\nmakes a row vector out of a column vector. Let’s say,\\nx =\\n5x(1)\\nx(2)\\n6\\n,\\nthen,\\nx€ def\\n=\\nË\\nx(1), x(2)È\\n.\\nThe multiplication of the vector x by the matrix W is given by x€W,\\nx€W =\\nË\\nx(1), x(2)È 5w(1,1)\\nw(1,2)\\nw(1,3)\\nw(2,1)\\nw(2,2)\\nw(2,3)\\n6\\ndef\\n=\\n#\\nw(1,1)x(1) + w(2,1)x(2), w(1,2)x(1) + w(2,2)x(2), w(1,3)x(1) + w(2,3)x(2)$\\nAs you can see, we can only multiply a vector by a matrix if the vector has the same number\\nof dimensions as the number of rows in the matrix.\\n2.1.6\\nFunctions\\nA function is a relation that associates each element x of a set X, the domain of the function,\\nto a single element y of another set Y, the codomain of the function. A function usually has a\\nname. If the function is called f, this relation is denoted y = f(x) (read f of x), the element\\nx is the argument or input of the function, and y is the value of the function or the output.\\nThe symbol that is used for representing the input is the variable of the function (we often\\nsay that f is a function of the variable x).\\nWe say that f(x) has a local minimum at x = c if f(x) Ø f(c) for every x in some open\\ninterval around x = c. An interval is a set of real numbers with the property that any number\\nthat lies between two numbers in the set is also included in the set. An open interval does\\nnot include its endpoints and is denoted using parentheses. For example, (0, 1) means greater\\nthan 0 and less than 1. The minimal value among all the local minima is called the global\\nminimum. See illustration in ﬁg. 2.\\nA vector function, denoted as y = f(x) is a function that returns a vector y. It can have a\\nvector or a scalar argument.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n6'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 19}, page_content='global  minimum\\nlocal  minimum\\n6\\n4\\n2\\n0\\n–2 \\n–4 \\n–6 \\nf(x)\\nFigure 2: A local and a global minima of a function.\\n2.1.7\\nMax and Arg Max\\nGiven a set of values A = {a1, a2, . . . , an}, the operator,\\nmax\\naœA f(a)\\nreturns the highest value f(a) for all elements in the set A. On the other hand, the operator,\\narg max\\naœA\\nf(a)\\nreturns the element of the set A that maximizes f(a).\\nSometimes, when the set is implicit or inﬁnite, we can write maxa f(a) or arg max\\na\\nf(a).\\nOperators min and arg min operate in a similar manner.\\n2.1.8\\nAssignment Operator\\nThe expression a Ω f(x) means that the variable a gets the new value: the result of f(x).\\nWe say that the variable a gets assigned a new value. Similarly, a Ω [a1, a2] means that the\\ntwo-dimensional vector a gets the value [a1, a2].\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n7'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 20}, page_content='2.1.9\\nDerivative and Gradient\\nA derivative f Õ of a function f is a function or a value that describes how fast f grows (or\\ndecreases). If the derivative is a constant value, like 5 or ≠3, then the function grows (or\\ndecreases) constantly at any point x of its domain. If the derivative f Õ is a function, then the\\nfunction f can grow at a diﬀerent pace in diﬀerent regions of its domain. If the derivative f Õ\\nis positive at some point x, then the function f grows at this point. If the derivative of f is\\nnegative at some x, then the function decreases at this point. The derivative of zero at x\\nmeans that the function’s slope at x is horizontal.\\nThe process of ﬁnding a derivative is called diﬀerentiation.\\nDerivatives for basic functions are known. For example if f(x) = x2, then f Õ(x) = 2x; if\\nf(x) = 2x then f Õ(x) = 2; if f(x) = 2 then f Õ(x) = 0 (the derivative of any function f(x) = c,\\nwhere c is a constant value, is zero).\\nIf the function we want to diﬀerentiate is not basic, we can ﬁnd its derivative using the\\nchain rule. For example if F(x) = f(g(x)), where f and g are some functions, then F Õ(x) =\\nf Õ(g(x))gÕ(x). For example if F(x) = (5x + 1)2 then g(x) = 5x + 1 and f(g(x)) = (g(x))2.\\nBy applying the chain rule, we ﬁnd F Õ(x) = 2(5x + 1)gÕ(x) = 2(5x + 1)5 = 50x + 10.\\nGradient is the generalization of derivative for functions that take several inputs (or one\\ninput in the form of a vector or some other complex structure). A gradient of a function\\nis a vector of partial derivatives. You can look at ﬁnding a partial derivative of a function\\nas the process of ﬁnding the derivative by focusing on one of the function’s inputs and by\\nconsidering all other inputs as constant values.\\nFor example, if our function is deﬁned as f([x(1), x(2)]) = ax(1) + bx(2) + c, then the partial\\nderivative of function f with respect to x(1), denoted as\\nˆf\\nˆx(1) , is given by,\\nˆf\\nˆx(1) = a + 0 + 0 = a,\\nwhere a is the derivative of the function ax(1); the two zeroes are respectively derivatives of\\nbx(2) and c, because x(2) is considered constant when we compute the derivative with respect\\nto x(1), and the derivative of any constant is zero.\\nSimilarly, the partial derivative of function f with respect to x(2),\\nˆf\\nˆx(2) , is given by,\\nˆf\\nˆx(2) = 0 + b + 0 = b.\\nThe gradient of function f, denoted as Òf is given by the vector [\\nˆf\\nˆx(1) ,\\nˆf\\nˆx(2) ].\\nThe chain rule works with partial derivatives too, as I illustrate in Chapter 4.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n8'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 21}, page_content='(a)\\n(b)\\nFigure 3: A probability mass function and a probability density function.\\n2.2\\nRandom Variable\\nA random variable, usually written as an italic capital letter, like X, is a variable whose\\npossible values are numerical outcomes of a random phenomenon. There are two types of\\nrandom variables: discrete and continuous.\\nA discrete random variable takes on only a countable number of distinct values such as red,\\nyellow, blue or 1, 2, 3, . . ..\\nThe probability distribution of a discrete random variable is described by a list of probabilities\\nassociated with each of its possible values. This list of probabilities is called probability mass\\nfunction (pmf). For example: Pr(X = red) = 0.3, Pr(X = yellow) = 0.45, Pr(X = blue) =\\n0.25. Each probability in a probability mass function is a value greater than or equal to 0.\\nThe sum of probabilities equals 1 (ﬁg. 3a).\\nA continuous random variable takes an inﬁnite number of possible values in some interval.\\nExamples include height, weight, and time. Because the number of values of a continuous\\nrandom variable X is inﬁnite, the probability Pr(X = c) for any c is 0. Therefore, instead\\nof the list of probabilities, the probability distribution of a continuous random variable (a\\ncontinuous probability distribution) is described by a probability density function (pdf). The\\npdf is a function whose codomain is nonnegative and the area under the curve is equal to 1\\n(ﬁg. 3b).\\nLet a discrete random variable X have k possible values {xi}k\\ni=1. The expectation of X\\ndenoted as E[X] is given by,\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n9'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 22}, page_content='E[X]\\ndef\\n=\\nk\\nÿ\\ni=1\\nxi Pr(X = xi) = x1 Pr(X = x1) + x2 Pr(X = x2) + · · · + xk Pr(X = xk),\\n(1)\\nwhere Pr(X = xi) is the probability that X has the value xi according to the pmf. The\\nexpectation of a random variable is also called the mean, average or expected value and is\\nfrequently denoted with the letter µ. The expectation is one of the most important statistics\\nof a random variable. Another important statistic is the standard deviation. For a discrete\\nrandom variable, the standard deviation usually denoted as ‡ is given by:\\n‡\\ndef\\n=\\n\\uf8ff\\nE[(X ≠µ)2] =\\n\\uf8ff\\nPr(X = x1)(x1 ≠µ)2 + Pr(X = x2)(x2 ≠µ)2 + · · · + Pr(X = xk)(xk ≠µ)2,\\nwhere µ = E[X].\\nThe expectation of a continuous random variable X is given by,\\nE[X]\\ndef\\n=\\n⁄\\nR\\nxfX(x) dx,\\n(2)\\nwhere fX is the pdf of the variable X and\\ns\\nR is the integral of function xfX.\\nIntegral is an equivalent of the summation over all values of the function when the function\\nhas a continuous domain. It equals the area under the curve of the function. The property of\\nthe pdf that the area under its curve is 1 mathematically means that\\ns\\nR fX(x) dx = 1.\\nMost of the time we don’t know fX, but we can observe some values of X. In machine\\nlearning, we call these values examples, and the collection of these examples is called a\\nsample or a dataset.\\n2.3\\nUnbiased Estimators\\nBecause fX is usually unknown, but we have a sample SX = {xi}N\\ni=1, we often content\\nourselves not with the true values of statistics of the probability distribution, such as\\nexpectation, but with their unbiased estimators.\\nWe say that ˆ◊(SX) is an unbiased estimator of some statistic ◊calculated using a sample SX\\ndrawn from an unknown probability distribution if ˆ◊(SX) has the following property:\\nE\\nË\\nˆ◊(SX)\\nÈ\\n= ◊,\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n10'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 23}, page_content='where ˆ◊is a sample statistic, obtained using a sample SX and not the real statistic ◊that\\ncan be obtained only knowing X; the expectation is taken over all possible samples drawn\\nfrom X. Intuitively, this means that if you can have an unlimited number of such samples\\nas SX, and you compute some unbiased estimator, such as ˆµ, using each sample, then the\\naverage of all these ˆµ equals the real statistic µ that you would get computed on X.\\nIt can be shown that an unbiased estimator of an unknown E[X] (given by either eq. 1 or\\neq. 2) is given by\\n1\\nN\\nqN\\ni=1 xi (called in statistics the sample mean).\\n2.4\\nBayes’ Rule\\nThe conditional probability Pr(X = x|Y = y) is the probability of the random variable X to\\nhave a speciﬁc value x given that another random variable Y has a speciﬁc value of y. The\\nBayes’ Rule (also known as the Bayes’ Theorem) stipulates that:\\nPr(X = x|Y = y) = Pr(Y = y|X = x) Pr(X = x)\\nPr(Y = y)\\n.\\n2.5\\nParameter Estimation\\nBayes’ Rule comes in handy when we have a model of X’s distribution, and this model f◊is a\\nfunction that has some parameters in the form of a vector ◊. An example of such a function\\ncould be the Gaussian function that has two parameters, µ and ‡, and is deﬁned as:\\nf◊(x) =\\n1\\nÔ\\n2ﬁ‡2 e≠(x≠µ)2\\n2‡2 ,\\nwhere ◊\\ndef\\n= [µ, ‡].\\nThis function has all the properties of a pdf. Therefore, we can use it as a model of an\\nunknown distribution of X. We can update the values of parameters in the vector ◊from the\\ndata using the Bayes’ Rule:\\nPr(◊= ˆ◊|X = x) Ω Pr(X = x|◊= ˆ◊) Pr(◊= ˆ◊)\\nPr(X = x)\\n= Pr(X = x|◊= ˆ◊) Pr(◊= ˆ◊)\\nq\\n˜◊Pr(X = x|◊= ˜◊)\\n.\\n(3)\\nwhere Pr(X = x|◊= ˆ◊)\\ndef\\n= fˆ◊.\\nIf we have a sample S of X and the set of possible values for ◊is ﬁnite, we can easily estimate\\nPr(◊= ˆ◊) by applying Bayes’ Rule iteratively, one example x œ S at a time. The initial value\\nPr(◊= ˆ◊) can be guessed such that q\\nˆ◊Pr(◊= ˆ◊) = 1. This guess of the probabilities for\\ndiﬀerent ˆ◊is called the prior.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n11'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 24}, page_content='First, we compute Pr(◊= ˆ◊|X = x1) for all possible values ˆ◊.\\nThen, before updating\\nPr(◊= ˆ◊|X = x) once again, this time for x = x2 œ S using eq. 3, we replace the prior\\nPr(◊= ˆ◊) in eq. 3 by the new estimate Pr(◊= ˆ◊) Ω 1\\nN\\nq\\nxœS Pr(◊= ˆ◊|X = x).\\nThe best value of the parameters ◊ú given one example is obtained using the principle of\\nmaximum-likelihood:\\n◊ú = arg max\\n◊\\nN\\nŸ\\ni=1\\nPr(◊= ˆ◊|X = xi).\\n(4)\\nIf the set of possible values for ◊isn’t ﬁnite, then we need to optimize eq. 4 directly using a\\nnumerical optimization routine, such as gradient descent, which we consider in Chapter 4.\\nUsually, we optimize the natural logarithm of the right-hand side expression in eq. 4 because\\nthe logarithm of a product becomes the sum of logarithms and it’s easier for the machine to\\nwork with the sum than with a product1.\\n2.6\\nClassiﬁcation vs. Regression\\nClassiﬁcation is a problem of automatically assigning a label to an unlabeled example.\\nSpam detection is a famous example of classiﬁcation.\\nIn machine learning, the classiﬁcation problem is solved by a classiﬁcation learning algorithm\\nthat takes a collection of labeled examples as inputs and produces a model that can take\\nan unlabeled example as input and either directly output a label or output a number that\\ncan be used by the data analyst to deduce the label easily. An example of such a number is\\na probability.\\nIn a classiﬁcation problem, a label is a member of a ﬁnite set of classes. If the size of\\nthe set of classes is two (“sick”/“healthy”, “spam”/“not_spam”), we talk about binary\\nclassiﬁcation (also called binomial in some books).\\nMulticlass classiﬁcation (also called multinomial) is a classiﬁcation problem with three\\nor more classes2.\\nWhile some learning algorithms naturally allow for more than two classes, others are by nature\\nbinary classiﬁcation algorithms. There are strategies allowing to turn a binary classiﬁcation\\nlearning algorithm into a multiclass one. I talk about one of them in Chapter 7.\\nRegression is a problem of predicting a real-valued label (often called a target) given an\\nunlabeled example. Estimating house price valuation based on house features, such as area,\\nthe number of bedrooms, location and so on is a famous example of regression.\\n1Multiplication of many numbers can give either a very small result or a very large one. It often results in\\nthe problem of numerical overﬂow when the machine cannot store such extreme numbers in memory.\\n2There’s still one label per example though.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n12'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 25}, page_content='The regression problem is solved by a regression learning algorithm that takes a collection\\nof labeled examples as inputs and produces a model that can take an unlabeled example as\\ninput and output a target.\\n2.7\\nModel-Based vs. Instance-Based Learning\\nMost supervised learning algorithms are model-based.\\nWe have already seen one such\\nalgorithm: SVM. Model-based learning algorithms use the training data to create a model\\nthat has parameters learned from the training data. In SVM, the two parameters we saw\\nwere wú and bú. After the model was built, the training data can be discarded.\\nInstance-based learning algorithms use the whole dataset as the model. One instance-based\\nalgorithm frequently used in practice is k-Nearest Neighbors (kNN). In classiﬁcation, to\\npredict a label for an input example the kNN algorithm looks at the close neighborhood of\\nthe input example in the space of feature vectors and outputs the label that it saw the most\\noften in this close neighborhood.\\n2.8\\nShallow vs. Deep Learning\\nA shallow learning algorithm learns the parameters of the model directly from the features\\nof the training examples. Most supervised learning algorithms are shallow. The notorious\\nexceptions are neural network learning algorithms, speciﬁcally those that build neural\\nnetworks with more than one layer between input and output. Such neural networks are\\ncalled deep neural networks. In deep neural network learning (or, simply, deep learning),\\ncontrary to shallow learning, most model parameters are learned not directly from the features\\nof the training examples, but from the outputs of the preceding layers.\\nDon’t worry if you don’t understand what that means right now. We look at neural networks\\nmore closely in Chapter 6.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n13'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 26}, page_content='The\\nHundred-\\nPage\\nMachine\\nLearning\\nBook\\nAndriy Burkov'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 27}, page_content='“All models are wrong, but some are useful.”\\n— George Box\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 28}, page_content='3\\nFundamental Algorithms\\nIn this chapter, I describe ﬁve algorithms which are not just the most known but also either\\nvery eﬀective on their own or are used as building blocks for the most eﬀective learning\\nalgorithms out there.\\n3.1\\nLinear Regression\\nLinear regression is a popular regression learning algorithm that learns a model which is a\\nlinear combination of features of the input example.\\n3.1.1\\nProblem Statement\\nWe have a collection of labeled examples {(xi, yi)}N\\ni=1, where N is the size of the collection,\\nxi is the D-dimensional feature vector of example i = 1, . . . , N, yi is a real-valued1 target\\nand every feature x(j)\\ni , j = 1, . . . , D, is also a real number.\\nWe want to build a model fw,b(x) as a linear combination of features of example x:\\nfw,b(x) = wx + b,\\n(1)\\nwhere w is a D-dimensional vector of parameters and b is a real number. The notation fw,b\\nmeans that the model f is parametrized by two values: w and b.\\nWe will use the model to predict the unknown y for a given x like this: y Ω fw,b(x). Two\\nmodels parametrized by two diﬀerent pairs (w, b) will likely produce two diﬀerent predictions\\nwhen applied to the same example. We want to ﬁnd the optimal values (wú, bú). Obviously,\\nthe optimal values of parameters deﬁne the model that makes the most accurate predictions.\\nYou could have noticed that the form of our linear model in eq. 1 is very similar to the form\\nof the SVM model. The only diﬀerence is the missing sign operator. The two models are\\nindeed similar. However, the hyperplane in the SVM plays the role of the decision boundary:\\nit’s used to separate two groups of examples from one another. As such, it has to be as far\\nfrom each group as possible.\\nOn the other hand, the hyperplane in linear regression is chosen to be as close to all training\\nexamples as possible.\\nYou can see why this latter requirement is essential by looking at the illustration in ﬁg. 1. It\\ndisplays the regression line (in light-blue) for one-dimensional examples (dark-blue dots). We\\ncan use this line to predict the value of the target ynew for a new unlabeled input example\\nxnew. If our examples are D-dimensional feature vectors (for D > 1), the only diﬀerence\\n1To say that yi is real-valued, we write yi œ R, where R denotes the set of all real numbers, an inﬁnite set\\nof numbers from minus inﬁnity to plus inﬁnity.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 29}, page_content='Figure 1: Linear Regression for one-dimensional examples.\\nwith the one-dimensional case is that the regression model is not a line but a plane (for two\\ndimensions) or a hyperplane (for D > 2).\\nNow you see why it’s essential to have the requirement that the regression hyperplane lies as\\nclose to the training examples as possible: if the blue line in ﬁg. 1 was far from the blue dots,\\nthe prediction ynew would have fewer chances to be correct.\\n3.1.2\\nSolution\\nTo get this latter requirement satisﬁed, the optimization procedure which we use to ﬁnd the\\noptimal values for wú and bú tries to minimize the following expression:\\n1\\nN\\nÿ\\ni=1...N\\n(fw,b(xi) ≠yi)2.\\n(2)\\nIn mathematics, the expression we minimize or maximize is called an objective function, or,\\nsimply, an objective. The expression (f(xi) ≠yi)2 in the above objective is called the loss\\nfunction. It’s a measure of penalty for misclassiﬁcation of example i. This particular choice\\nof the loss function is called squared error loss. All model-based learning algorithms have\\na loss function and what we do to ﬁnd the best model is we try to minimize the objective\\nknown as the cost function. In linear regression, the cost function is given by the average\\nloss, also called the empirical risk. The average loss, or empirical risk, for a model, is the\\naverage of all penalties obtained by applying the model to the training data.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n4'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 30}, page_content='Why is the loss in linear regression a quadratic function? Why couldn’t we get the absolute\\nvalue of the diﬀerence between the true target yi and the predicted value f(xi) and use that\\nas a penalty? We could. Moreover, we also could use a cube instead of a square.\\nNow you probably start realizing how many seemingly arbitrary decisions are made when we\\ndesign a machine learning algorithm: we decided to use the linear combination of features to\\npredict the target. However, we could use a square or some other polynomial to combine the\\nvalues of features. We could also use some other loss function that makes sense: the absolute\\ndiﬀerence between f(xi) and yi makes sense, the cube of the diﬀerence too; the binary loss\\n(1 when f(xi) and yi are diﬀerent and 0 when they are the same) also makes sense, right?\\nIf we made diﬀerent decisions about the form of the model, the form of the loss function,\\nand about the choice of the algorithm that minimizes the average loss to ﬁnd the best values\\nof parameters, we would end up inventing a diﬀerent machine learning algorithm. Sounds\\neasy, doesn’t it? However, do not rush to invent a new learning algorithm. The fact that it’s\\ndiﬀerent doesn’t mean that it will work better in practice.\\nPeople invent new learning algorithms for one of the two main reasons:\\n1. The new algorithm solves a speciﬁc practical problem better than the existing algorithms.\\n2. The new algorithm has better theoretical guarantees on the quality of the model it\\nproduces.\\nOne practical justiﬁcation of the choice of the linear form for the model is that it’s simple.\\nWhy use a complex model when you can use a simple one? Another consideration is that\\nlinear models rarely overﬁt. Overﬁtting is the property of a model such that the model\\npredicts very well labels of the examples used during training but frequently makes errors\\nwhen applied to examples that weren’t seen by the learning algorithm during training.\\nAn example of overﬁtting in regression is shown in ﬁg. 2. The data used to build the red\\nregression line is the same as in ﬁg. 1. The diﬀerence is that this time, this is the polynomial\\nregression with a polynomial of degree 10. The regression line predicts almost perfectly the\\ntargets almost all training examples, but will likely make signiﬁcant errors on new data, as\\nyou can see in ﬁg. 1 for xnew. We talk more about overﬁtting and how to avoid it Chapter 5.\\nNow you know why linear regression can be useful: it doesn’t overﬁt much. But what\\nabout the squared loss? Why did we decide that it should be squared? In 1705, the French\\nmathematician Adrien-Marie Legendre, who ﬁrst published the sum of squares method for\\ngauging the quality of the model stated that squaring the error before summing is convenient.\\nWhy did he say that? The absolute value is not convenient, because it doesn’t have a\\ncontinuous derivative, which makes the function not smooth. Functions that are not smooth\\ncreate unnecessary diﬃculties when employing linear algebra to ﬁnd closed form solutions\\nto optimization problems. Closed form solutions to ﬁnding an optimum of a function are\\nsimple algebraic expressions and are often preferable to using complex numerical optimization\\nmethods, such as gradient descent (used, among others, to train neural networks).\\nIntuitively, squared penalties are also advantageous because they exaggerate the diﬀerence\\nbetween the true target and the predicted one according to the value of this diﬀerence. We\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 31}, page_content='y\\nx\\nnew\\nnew\\nFigure 2: Overﬁtting.\\nmight also use the powers 3 or 4, but their derivatives are more complicated to work with.\\nFinally, why do we care about the derivative of the average loss? Remember from algebra\\nthat if we can calculate the gradient of the function in eq. 2, we can then set this gradient to\\nzero2 and ﬁnd the solution to a system of equations that gives us the optimal values wú and\\nbú. You can spend several minutes and check it yourself.\\n3.2\\nLogistic Regression\\nThe ﬁrst thing to say is that logistic regression is not a regression, but a classiﬁcation learning\\nalgorithm. The name comes from statistics and is due to the fact that the mathematical\\nformulation of logistic regression is similar to that of linear regression.\\nI explain logistic regression on the case of binary classiﬁcation. However, it can naturally be\\nextended to multiclass classiﬁcation.\\n3.2.1\\nProblem Statement\\nIn logistic regression, we still want to model yi as a linear function of xi, however, with a\\nbinary yi this is not straightforward. The linear combination of features such as wxi + b is a\\nfunction that spans from minus inﬁnity to plus inﬁnity, while yi has only two possible values.\\n2To ﬁnd the minimum or the maximum of a function, we set the gradient to zero because the value of the\\ngradient at extrema of a function is always zero. In 2D, the gradient at an extremum is a horizontal line.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n6'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 32}, page_content='Figure 3: Standard logistic function.\\nAt the time where the absence of computers required scientists to perform manual calculations,\\nthey were eager to ﬁnd a linear classiﬁcation model. They ﬁgured out that if we deﬁne a\\nnegative label as 0 and the positive label as 1, we would just need to ﬁnd a simple continuous\\nfunction whose codomain is (0, 1). In such a case, if the value returned by the model for\\ninput x is closer to 0, then we assign a negative label to x; otherwise, the example is labeled\\nas positive. One function that has such a property is the standard logistic function (also\\nknown as the sigmoid function):\\nf(x) =\\n1\\n1 + e≠x ,\\nwhere e is the base of the natural logarithm (also called Euler’s number; ex is also known as\\nthe exp(x) function in Excel and many programming languages). Its graph is depicted in ﬁg.\\n3.\\nBy looking at the graph of the standard logistic function, we can see how well it ﬁts our\\nclassiﬁcation purpose: if we optimize the values of x and b appropriately, we could interpret\\nthe output of f(x) as the probability of yi being positive. For example, if it’s higher than or\\nequal to the threshold 0.5 we would say that the class of x is positive; otherwise, it’s negative.\\nIn practice, the choice of the threshold could be diﬀerent depending on the problem. We\\nreturn to this discussion in Chapter 5 when we talk about model performance assessment.\\nSo our logistic regression model looks like this:\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n7'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 33}, page_content='fw,b(x)\\ndef\\n=\\n1\\n1 + e≠(wx+b) .\\n(3)\\nYou can see the familiar term wx + b from linear regression. Now, how do we ﬁnd the best\\nvalues wú and bú for our model? In linear regression, we minimized the empirical risk which\\nwas deﬁned as the average squared error loss, also known as the mean squared error or\\nMSE.\\n3.2.2\\nSolution\\nIn logistic regression, instead of using a squared loss and trying to minimize the empirical\\nrisk, we maximize the likelihood of our training set according to the model. In statistics, the\\nlikelihood function deﬁnes how likely the observation (an example) is according to our model.\\nFor instance, assume that we have a labeled example (xi, yi) in our training data. Assume\\nalso that we have found (guessed) some speciﬁc values ˆw and ˆb of our parameters. If we now\\napply our model fˆw,ˆb to xi using eq. 3 we will get some value 0 < p < 1 as output. If yi is\\nthe positive class, the likelihood of yi being the positive class, according to our model, is\\ngiven by p. Similarly, if yi is the negative class, the likelihood of it being the negative class is\\ngiven by 1 ≠p.\\nThe optimization criterion in logistic regression is called maximum likelihood. Instead of\\nminimizing the average loss, like in linear regression, we now maximize the likelihood of the\\ntraining data according to our model:\\nLw,b\\ndef\\n=\\nŸ\\ni=1...N\\nfw,b(xi)yi(1 ≠fw,b(xi))(1≠yi).\\n(4)\\nThe expression fw,b(x)yi(1 ≠fw,b(x))(1≠yi) may look scary but it’s just a fancy mathematical\\nway of saying: “fw,b(x) when yi = 1 and (1 ≠fw,b(x)) otherwise”. Indeed, if yi = 1, then\\n(1 ≠fw,b(x))(1≠yi) equals 1 because (1 ≠yi) = 0 and we know that anything power 0 equals\\n1. On the other hand, if yi = 0, then fw,b(x)yi equals 1 for the same reason.\\nYou may have noticed that we used the product operator r in the objective function instead\\nof the sum operator q which was used in linear regression. It’s because the likelihood of\\nobserving N labels for N examples is the product of likelihoods of each observation (assuming\\nthat all observations are independent of one another, which is the case). You can draw\\na parallel with the multiplication of probabilities of outcomes in a series of independent\\nexperiments in the probability theory.\\nBecause of the exp function used in the model, in practice, it’s more convenient to maximize\\nthe log-likelihood instead of likelihood. The log-likelihood is deﬁned like follows:\\nLogLw,b\\ndef\\n= ln(L(w,b(x)) =\\nN\\nÿ\\ni=1\\nyi ln fw,b(x) + (1 ≠yi) ln (1 ≠fw,b(x)).\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n8'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 34}, page_content='Because ln is a strictly increasing function, maximizing this function is the same as maximizing\\nits argument, and the solution to this new optimization problem is the same as the solution\\nto the original problem.\\nContrary to linear regression, there’s no closed form solution to the above optimization\\nproblem. A typical numerical optimization procedure used in such cases is gradient descent.\\nI talk about it in the next chapter.\\n3.3\\nDecision Tree Learning\\nA decision tree is an acyclic graph that can be used to make decisions. In each branching\\nnode of the graph, a speciﬁc feature j of the feature vector is examined. If the value of the\\nfeature is below a speciﬁc threshold, then the left branch is followed; otherwise, the right\\nbranch is followed. As the leaf node is reached, the decision is made about the class to which\\nthe example belongs.\\nAs the title of the section suggests, a decision tree can be learned from data.\\n3.3.1\\nProblem Statement\\nLike previously, we have a collection of labeled examples; labels belong to the set {0, 1}. We\\nwant to build a decision tree that would allow us to predict the class of an example given a\\nfeature vector.\\n3.3.2\\nSolution\\nThere are various formulations of the decision tree learning algorithm. In this book, we\\nconsider just one, called ID3.\\nThe optimization criterion, in this case, is the average log-likelihood:\\n1\\nN\\nN\\nÿ\\ni=1\\nyi ln fID3(xi) + (1 ≠yi) ln (1 ≠fID3(xi)),\\n(5)\\nwhere fID3 is a decision tree.\\nBy now, it looks very similar to logistic regression. However, contrary to the logistic regression\\nlearning algorithm which builds a parametric model fwú,bú by ﬁnding an optimal solution\\nto the optimization criterion, the ID3 algorithm optimizes it approximately by constructing a\\nnon-parametric model fID3(x)\\ndef\\n= Pr(y = 1|x).\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n9'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 35}, page_content='S={(x1,\\xa0y1),\\xa0(x2,\\xa0y2),\\xa0(x3,\\xa0y3),\\n(x4,\\xa0y4),\\xa0(x5,\\xa0y5),\\xa0(x6,\\xa0y6),\\n(x7,\\xa0y7),\\xa0(x8,\\xa0y8),\\xa0(x9,\\xa0y9),\\n(x10,\\xa0y10),\\xa0(x11,\\xa0y11),\\xa0(x12,\\xa0y12)}\\nx\\nPr(y\\xa0=\\xa01|x)\\xa0=\\xa0(y1+y2+y3+y4+y5\\xa0\\n+y6+y7+y8+y9+y10+y11+y12)/12\\nPr(y\\xa0=\\xa01|x)\\n(a)\\nx\\nPr(y\\xa0=\\xa01|x)\\xa0=\\xa0(y1+y2+y4\\xa0\\n+y6+y7+y8+y9)/7\\nPr(y\\xa0=\\xa01|x)\\nx(3)\\xa0<\\xa018.3?\\nS\\xad\\xa0=\\xa0{(x1,\\xa0y1),\\xa0(x2,\\xa0y2),\\n(x4,\\xa0y4),\\xa0(x6,\\xa0y6),\\xa0(x7,\\xa0y7),\\n(x8,\\xa0y8),\\xa0(x9,\\xa0y9)}\\xa0\\nPr(y\\xa0=\\xa01|x)\\xa0=\\n(y3+y5+y10+y11+y12)/5\\nPr(y\\xa0=\\xa01|x)\\nS+\\xa0=\\xa0{(x3,\\xa0y3),\\xa0(x5,\\xa0y5),\\xa0(x10,\\xa0y10),\\n(x11,\\xa0y11),\\xa0(x12,\\xa0y12)}\\xa0\\nYes\\nNo\\n(b)\\nFigure 4: An illustration of a decision tree building algorithm. The set S contains 12 labeled\\nexamples. (a) In the beginning, the decision tree only contains the start node; it makes the\\nsame prediction for any input. (b) The decision tree after the ﬁrst split; it tests whether\\nfeature 3 is less than 18.3 and, depending on the result, the prediction is made in one of the\\ntwo leaf nodes.\\nThe ID3 learning algorithm works as follows. Let S denote a set of labeled examples. In the\\nbeginning, the decision tree only has a start node that contains all examples: S\\ndef\\n= {(xi, yi)}N\\ni=1.\\nStart with a constant model f S\\nID3:\\nf S\\nID3 = 1\\n|S|\\nÿ\\n(x,y)œS\\ny.\\n(6)\\nThe prediction given by the above model, f S\\nID3(x), would be the same for any input x. The\\ncorresponding decision tree is shown in ﬁg 4a.\\nThen we search through all features j = 1, . . . , D and all thresholds t, and split the set S\\ninto two subsets: S≠\\ndef\\n= {(x, y) | (x, y) œ S, x(j) < t} and S+ = {(x, y) | (x, y) œ S, x(j) Ø t}.\\nThe two new subsets would go to two new leaf nodes, and we evaluate, for all possible pairs\\n(j, t) how good the split with pieces S≠and S+ is. Finally, we pick the best such values (j, t),\\nsplit S into S+ and S≠, form two new leaf nodes, and continue recursively on S+ and S≠(or\\nquit if no split produces a model that’s suﬃciently better than the current one). A decision\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n10'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 36}, page_content='tree after one split is illustrated in ﬁg 4b.\\nNow you should wonder what do the words “evaluate how good the split is” mean. In ID3, the\\ngoodness of a split is estimated by using the criterion called entropy. Entropy is a measure of\\nuncertainty about a random variable. It reaches its maximum when all values of the random\\nvariables are equiprobable. Entropy reaches its minimum when the random variable can have\\nonly one value. The entropy of a set of examples S is given by:\\nH(S) = ≠f S\\nID3 ln f S\\nID3 ≠(1 ≠f S\\nID3) ln(1 ≠f S\\nID3).\\nWhen we split a set of examples by a certain feature j and a threshold t, the entropy of a\\nsplit, H(S≠, S+), is simply a weighted sum of two entropies:\\nH(S≠, S+) = |S≠|\\n|S| H(S≠) + |S+|\\n|S| H(S+).\\n(7)\\nSo, in ID3, at each step, at each leaf node, we ﬁnd a split that minimizes the entropy given\\nby eq. 7 or we stop at this leaf node.\\nThe algorithm stops at a leaf node in any of the below situations:\\n• All examples in the leaf node are classiﬁed correctly by the one-piece model (eq. 6).\\n• We cannot ﬁnd an attribute to split upon.\\n• The split reduces the entropy less than some ‘ (the value for which has to be found\\nexperimentally3).\\n• The tree reaches some maximum depth d (also has to be found experimentally).\\nBecause in ID3, the decision to split the dataset on each iteration is local (doesn’t depend\\non future splits), the algorithm doesn’t guarantee an optimal solution. The model can be\\nimproved by using techniques like backtracking during the search for the optimal decision\\ntree at the cost of possibly taking longer to build a model.\\nThe entropy-based split criterion intuitively makes sense:\\nentropy\\nreaches its minimum of 0 when all examples in S have the same label;\\non the other hand, the entropy is at its maximum of 1 when exactly\\none-half of examples in S is labeled with 1, making such a leaf useless\\nfor classiﬁcation. The only remaining question is how this algorithm\\napproximately maximizes the average log-likelihood criterion. I leave it\\nfor further reading.\\n3In Chapter 5, we will see how to do that when we talk about hyperparameter tuning.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n11'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 37}, page_content='3.4\\nSupport Vector Machine\\nWe already considered SVM in the introduction, so this section only ﬁlls a couple of blanks.\\nTwo critical questions need to be answered:\\n1. What if there’s noise in the data and no hyperplane can perfectly separate positive\\nexamples from negative ones?\\n2. What if the data cannot be separated using a plane, but could be separated by a\\nhigher-order polynomial?\\nFigure 5: Linearly non-separable cases.\\nLeft: the presence of noise.\\nRight: inherent\\nnonlinearity.\\nYou can see both situations depicted in ﬁg 5. In the left case, the data could be separated by\\na straight line if not for the noise (outliers or examples with wrong labels). In the right case,\\nthe decision boundary is a circle and not a straight line.\\nRemember that in SVM, we want to satisfy the following constraints:\\na) wxi ≠b Ø 1 if yi = +1, and\\nb) wxi ≠b Æ ≠1 if yi = ≠1\\nWe also want to minimize ÎwÎ so that the hyperplane was equally distant from the closest\\nexamples of each class. Minimizing ÎwÎ is equivalent to minimizing 1\\n2||w||2, and the use of\\nthis term makes it possible to perform quadratic programming optimization later on. The\\noptimization problem for SVM, therefore, looks like this:\\nmin 1\\n2||w||2, such that yi(xiw ≠b) ≠1 Ø 0, i = 1, . . . , N.\\n(8)\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n12'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 38}, page_content='3.4.1\\nDealing with Noise\\nTo extend SVM to cases in which the data is not linearly separable, we introduce the hinge\\nloss function: max (0, 1 ≠yi(wxi ≠b)).\\nThe hinge loss function is zero if the constraints a) and b) are satisﬁed, in other words, if wxi\\nlies on the correct side of the decision boundary. For data on the wrong side of the decision\\nboundary, the function’s value is proportional to the distance from the decision boundary.\\nWe then wish to minimize the following cost function,\\nCÎwÎ2 + 1\\nN\\nN\\nÿ\\ni=1\\nmax (0, 1 ≠yi(wxi ≠b)) ,\\nwhere the hyperparameter C determines the tradeoﬀbetween increasing the size of the\\ndecision boundary and ensuring that each xi lies on the correct side of the decision boundary.\\nThe value of C is usually chosen experimentally, just like ID3’s hyperparameters ‘ and d.\\nSVMs that optimize hinge loss are called soft-margin SVMs, while the original formulation is\\nreferred to as a hard-margin SVM.\\nAs you can see, for suﬃciently high values of C, the second term in the cost function will\\nbecome negligible, so the SVM algorithm will try to ﬁnd the highest margin by completely\\nignoring misclassiﬁcation. As we decrease the value of C, making classiﬁcation errors is\\nbecoming more costly, so the SVM algorithm will try to make fewer mistakes by sacriﬁcing\\nthe margin size. As we have already discussed, a larger margin is better for generalization.\\nTherefore, C regulates the tradeoﬀbetween classifying the training data well (minimizing\\nempirical risk) and classifying future examples well (generalization).\\n3.4.2\\nDealing with Inherent Non-Linearity\\nSVM can be adapted to work with datasets that cannot be separated by a hyperplane in\\nits original space. However, if we manage to transform the original space into a space of\\nhigher dimensionality, we could hope that the examples will become linearly separable in this\\ntransformed space. In SVMs, using a function to implicitly transform the original space into\\na higher dimensional space during the cost function optimization is called the kernel trick.\\nThe eﬀect of applying the kernel trick is illustrated in ﬁg. 6. As you can see, it’s possible\\nto transform a two-dimensional non-linearly-separable data into a linearly-separable three-\\ndimensional data using a speciﬁc mapping „ : x ‘æ „(x), where „(x) is a vector of higher\\ndimensionality than x. For the example of 2D data in ﬁg. 5 (right), the mapping „ for\\nexample x = [q, p] that projects this example into a 3D space (ﬁg. 6) would look like this\\n„([q, p])\\ndef\\n= (q2,\\nÔ\\n2qp, p2), where q2 means q squared. You see now that the data becomes\\nlinearly separable in the transformed space.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n13'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 39}, page_content='Figure 6: The data from ﬁg. 5 (right) becomes linearly separable after a transformation into\\na three-dimensional space.\\nHowever, we don’t know a priori which mapping „ would work for our data. If we ﬁrst\\ntransform all our input examples using some mapping into very high dimensional vectors and\\nthen apply SVM to this data, and we try all possible mapping functions, the computation\\ncould become very ineﬃcient, and we would never solve our classiﬁcation problem.\\nFortunately, scientists ﬁgured out how to use kernel functions (or, simply, kernels) to\\neﬃciently work in higher-dimensional spaces without doing this transformation explicitly. To\\nunderstand how kernels work, we have to see ﬁrst how the optimization algorithm for SVM\\nﬁnds the optimal values for w and b.\\nThe method traditionally used to solve the optimization problem in eq. 8 is the method of\\nLagrange multipliers. Instead of solving the original problem from eq. 8, it is convenient to\\nsolve an equivalent problem formulated like this:\\nmax\\n–1...–N\\nN\\nÿ\\ni=1\\n–i ≠1\\n2\\nN\\nÿ\\ni=1\\nN\\nÿ\\nk=1\\nyi–i(xixk)yk–k subject to\\nN\\nÿ\\ni=1\\n–iyi = 0 and –i Ø 0, i = 1, . . . , N,\\nwhere –i are called Lagrange multipliers.\\nWhen formulated like this, the optimization\\nproblem becomes a convex quadratic optimization problem, eﬃciently solvable by quadratic\\nprogramming algorithms.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n14'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 40}, page_content='Now, you could have noticed that in the above formulation, there is a term xixk, and this is\\nthe only place where the feature vectors are used. If we want to transform our vector space\\ninto higher dimensional space, we need to transform xi into „(xi) and xj into „(xj) and\\nthen multiply „(xi) and „(xj). It would be very costly to do so.\\nOn the other hand, we are only interested in the result of the dot-product xixk, which, as\\nwe know, is a real number. We don’t care how this number was obtained as long as it’s\\ncorrect. By using the kernel trick, we can get rid of a costly transformation of original\\nfeature vectors into higher-dimensional vectors and avoid computing their dot-product. We\\nreplace that by a simple operation on the original feature vectors that gives the same\\nresult. For example, instead of transforming (q1, p1) into (q2\\n1,\\nÔ\\n2q1p1, p2\\n1) and (q2, p2) into\\n(q2\\n2,\\nÔ\\n2q2p2, p2\\n2) and then computing the dot-product of (q2\\n1,\\nÔ\\n2q1p1, p2\\n1) and (q2\\n2,\\nÔ\\n2q2p2, p2\\n2)\\nto obtain (q2\\n1q2\\n2 +2q1q2p1p2 +p2\\n1p2\\n2) we could ﬁnd the dot-product between (q1, p1) and (q2, p2)\\nto get (q1q2 +p1p2) and then square it to get exactly the same result (q2\\n1q2\\n2 +2q1q2p1p2 +p2\\n1p2\\n2).\\nThat was an example of the kernel trick, and we used the quadratic kernel k(xi, xk)\\ndef\\n= (xixk)2.\\nMultiple kernel functions exist, the most widely used of which is the RBF kernel:\\nk(x, xÕ) = exp\\n3\\n≠Îx ≠xÕÎ2\\n2‡2\\n4\\n,\\nwhere Îx ≠xÕÎ2 is the squared Euclidean distance between two feature vectors. The\\nEuclidean distance is given by the following equation:\\nd(xi, xk)\\ndef\\n=\\nÚ1\\nx(1)\\ni\\n≠x(1)\\nk\\n22\\n+\\n1\\nx(2)\\ni\\n≠x(2)\\nk\\n22\\n+ · · · +\\n1\\nx(N)\\ni\\n≠x(N)\\nk\\n22\\n=\\nˆ\\nı\\nı\\nÙ\\nD\\nÿ\\nj=1\\n1\\nx(j)\\ni\\n≠x(j)\\nk\\n22\\n.\\nIt can be shown that the feature space of the RBF (for “radial basis function”) kernel has\\nan inﬁnite number of dimensions. By varying the hyperparameter ‡, the data analyst can\\nchoose between getting a smooth or curvy decision boundary in the original space.\\n3.5\\nk-Nearest Neighbors\\nk-Nearest Neighbors (kNN) is a non-parametric learning algorithm. Contrary to other\\nlearning algorithms that allow discarding the training data after the model is built, kNN\\nkeeps all training examples in memory. Once a new, previously unseen example x comes in,\\nthe kNN algorithm ﬁnds k training examples closest to x and returns the majority label (in\\ncase of classiﬁcation) or the average label (in case of regression).\\nThe closeness of two points is given by a distance function. For example, Euclidean distance\\nseen above is frequently used in practice. Another popular choice of the distance function is\\nthe negative cosine similarity. Cosine similarity deﬁned as,\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n15'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 41}, page_content='s(xi, xk)\\ndef\\n= cos(\\\\(xi, xk)) =\\nqD\\nj=1 x(j)\\ni x(j)\\nk\\nÚ\\nqD\\nj=1\\n1\\nx(j)\\ni\\n22Ú\\nqD\\nj=1\\n1\\nx(j)\\nk\\n22 ,\\nis a measure of similarity of the directions of two vectors. If the angle between two vectors\\nis 0 degrees, then two vectors point to the same direction, and cosine similarity is equal to\\n1. If the vectors are orthogonal, the cosine similarity is 0. For vectors pointing in opposite\\ndirections, the cosine similarity is ≠1. If we want to use cosine similarity as a distance metric,\\nwe need to multiply it by ≠1. Other popular distance metrics include Chebychev distance,\\nMahalanobis distance, and Hamming distance. The choice of the distance metric, as well as\\nthe value for k, are the choices the analyst makes before running the algorithm. So these\\nare hyperparameters. The distance metric could also be learned from data (as opposed to\\nguessing it). We talk about that in Chapter 10.\\nNow you know how the model building algorithm works and how the prediction is made. A\\nreasonable question is what is the cost function here? Surprisingly, this question has not\\nbeen well studied in the literature, despite the algorithm’s popularity since the earlier 1960s.\\nThe only attempt to analyze the cost function of kNN I’m aware of was undertaken by Li\\nand Yang in 20034. Below, I outline their considerations.\\nFor simplicity, let’s make our derivation under the assumptions of binary classiﬁcation\\n(y œ {0, 1}) with cosine similarity and normalized feature vectors5. Under these assumptions,\\nkNN does a locally linear classiﬁcation with the vector of coeﬃcients,\\nwx =\\nÿ\\n(xÕ,yÕ)œRk(x)\\nyÕxÕ,\\n(9)\\nwhere Rk(x) is the set of k nearest neighbors to the input example x. The above equation\\nsays that we take the sum of all nearest neighbor feature vectors to some input vector x\\nby ignoring those that have label 0. The classiﬁcation decision is obtained by deﬁning a\\nthreshold on the dot-product wxx which, in the case of normalized feature vectors, is equal\\nto the cosine similarity between wx and x.\\nNow, deﬁning the cost function like this:\\nL = ≠\\nÿ\\n(xÕ,yÕ)œRk(x)\\nyÕxÕwx + 1\\n2||w||2\\nand setting the ﬁrst order derivative of the right-hand side to zero yields the formula for the\\ncoeﬃcient vector in eq. 9.\\n4F. Li and Y. Yang, “A loss function analysis for classiﬁcation methods in text categorization,” in ICML\\n2003, pp. 472–479, 2003.\\n5We discuss normalization later; for the moment assume that all features of feature vectors were squeezed\\ninto the range [0, 1].\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n16'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 42}, page_content='The\\nHundred-\\nPage\\nMachine\\nLearning\\nBook\\nAndriy Burkov'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 43}, page_content='“All models are wrong, but some are useful.”\\n— George Box\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 44}, page_content='4\\nAnatomy of a Learning Algorithm\\n4.1\\nBuilding Blocks of a Learning Algorithm\\nYou may have noticed by reading the previous chapter that each learning algorithm we saw\\nconsisted of three parts:\\n1) a loss function;\\n2) an optimization criterion based on the loss function (a cost function, for example); and\\n3) an optimization routine that leverages training data to ﬁnd a solution to the optimization\\ncriterion.\\nThese are the building blocks of any learning algorithm. You saw in the previous chapter\\nthat some algorithms were designed to explicitly optimize a speciﬁc criterion (both linear and\\nlogistic regressions, SVM). Some others, including decision tree learning and kNN, optimize\\nthe criterion implicitly. Decision tree learning and kNN are among the oldest machine\\nlearning algorithms and were invented experimentally based on intuition, without a speciﬁc\\nglobal optimization criterion in mind, and (like it often happens in scientiﬁc history) the\\noptimization criteria were developed later to explain why those algorithms work.\\nBy reading modern literature on machine learning, you often encounter references to gradient\\ndescent or stochastic gradient descent. These are two most frequently used optimization\\nalgorithms used in cases where the optimization criterion is diﬀerentiable.\\nGradient descent is an iterative optimization algorithm for ﬁnding the minimum of a function.\\nTo ﬁnd a local minimum of a function using gradient descent, one starts at some random\\npoint and takes steps proportional to the negative of the gradient (or approximate gradient)\\nof the function at the current point.\\nGradient descent can be used to ﬁnd optimal parameters for linear and logistic regression,\\nSVM and also neural networks which we consider later. For many models, such as logistic\\nregression or SVM, the optimization criterion is convex. Convex functions have only one\\nminimum, which is global. Optimization criteria for neural networks are not convex, but in\\npractice even ﬁnding a local minimum suﬃces.\\nLet’s see how gradient descent works.\\n4.2\\nGradient Descent\\nIn this section, I demonstrate how gradient descent ﬁnds the solution to a linear regression\\nproblem1. I illustrate my description with Python source code as well as with plots that\\nshow how the solution improves after some iterations of the gradient descent algorithm.\\n1As you know, linear regression has a closed form solution. That means that gradient descent is not\\nneeded to solve this speciﬁc type of problem. However, for illustration purposes, linear regression is a perfect\\nproblem to explain gradient descent.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 45}, page_content='I use a dataset with only one feature. However, the optimization criterion will have two\\nparameters: w and b. The extension to multi-dimensional training data is straightforward:\\nyou have variables w(1), w(2), and b for two-dimensional data, w(1), w(2), w(3), and b for\\nthree-dimensional data and so on.\\nFigure 1: The original data. The Y-axis corresponds to the sales in units (the quantity we\\nwant to predict), the X-axis corresponds to our feature: the spendings on radio ads in M$.\\nTo give a practical example, I use the real dataset with the following columns: the Spendings\\nof various companies on radio advertising each year and their annual Sales in terms of units\\nsold. We want to build a regression model that we can use to predict units sold based on\\nhow much a company spends on radio advertising. Each row in the dataset represents one\\nspeciﬁc company:\\nCompany\\nSpendings, M$\\nSales, Units\\n1\\n37.8\\n22.1\\n2\\n39.3\\n10.4\\n3\\n45.9\\n9.3\\n4\\n41.3\\n18.5\\n..\\n..\\n..\\nWe have data for 200 companies, so we have 200 training examples. Fig. 1 shows all examples\\non a 2D plot.\\nRemember that the linear regression model looks like this: f(x) = wx + b. We don’t know\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n4'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 46}, page_content='what the optimal values for w and b are and we want to learn them from data. To do that,\\nwe look for such values for w and b that minimize the mean squared error:\\nl = 1\\nN\\nN\\nÿ\\ni=1\\n(yi ≠(wxi + b))2.\\nGradient descent starts with calculating the partial derivative for every parameter:\\nˆl\\nˆw = 1\\nN\\nN\\nÿ\\ni=1\\n≠2xi(yi ≠(wxi + b));\\nˆl\\nˆb = 1\\nN\\nN\\nÿ\\ni=1\\n≠2(yi ≠(wxi + b)).\\n(1)\\nTo ﬁnd the partial derivative of the term (yi ≠(wx + b))2 with respect to w we applied the\\nchain rule. Here, we have the chain f = f2(f1) where f1 = yi ≠(wx + b) and f2 = f 2\\n1 . To ﬁnd\\na partial derivative of f with respect to w we have to ﬁrst ﬁnd the partial derivative of f with\\nrespect to f2 which is equal to 2(yi ≠(wx + b)) (from calculus, we know that the derivative\\nˆf\\nˆxx2 = 2x) and then we have to multiply it by the partial derivative of yi ≠(wx + b) with\\nrespect to w which is equal to ≠x. So overall\\nˆl\\nˆw = 1\\nN\\nqN\\ni=1 ≠2xi(yi ≠(wxi +b)). In a similar\\nway, the partial derivative of l with respect to b, ˆl\\nˆb, was calculated.\\nWe initialize2 w0 = 0 and b0 = 0 and then iterate through our training examples, each\\nexample having the form of (xi, yi) = (Spendingsi, Salesi). For each training example, we\\nupdate w and b using our partial derivatives. The learning rate – controls the size of an\\nupdate:\\nwi Ω –≠2xi(yi ≠(wi≠1xi + bi≠1))\\nN\\n;\\nbi Ω –≠2(yi ≠(wi≠1xi + bi≠1))\\nN\\n,\\n(2)\\nwhere wi and bi denote the values of w and b after using the example (xi, yi) for the update.\\nOne pass through all training examples is called an epoch. Typically, we need multiple\\nepochs until we start seeing that the values for w and b don’t change much; then we stop.\\n2In complex models, such as neural networks, which have thousands of parameters, the initialization of\\nparameters may signiﬁcantly aﬀect the solution found using gradient descent. There are diﬀerent initialization\\nmethods (at random, with all zeroes, with small values around zero, and others) and it is an important choice\\nthe data analyst has to make.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 47}, page_content='It’s hard to imagine a machine learning engineer who doesn’t use Python. So, if you waited\\nfor the right moment to start learning Python, this is that moment. Below we show how to\\nprogram gradient descent in Python.\\nThe function that updates the parameters w and b during one epoch is shown below:\\ndef update_w_and_b(spendings, sales, w, b, alpha):\\ndl_dw = 0.0\\ndl_db = 0.0\\nN = len(spendings)\\nfor i in range(N):\\ndl_dw += -2*spendings[i]*(sales[i] - (w*spendings[i] + b))\\ndl_db += -2*(sales[i] - (w*spendings[i] + b))\\n# update w and b\\nw = w - (1/float(N))*dl_dw*alpha\\nb = b - (1/float(N))*dl_db*alpha\\nreturn w, b\\nThe function that loops over multiple epochs is shown below:\\ndef train(spendings, sales, w, b, alpha, epochs):\\nfor e in range(epochs):\\nw, b = update_w_and_b(spendings, sales, w, b, alpha)\\n# log the progress\\nif e % 400 == 0:\\nprint(\"epoch:\", e, \"loss: \", avg_loss(spendings, sales, w, b))\\nreturn w, b\\nThe function avg_loss in the above code snippet is a function that computes the mean\\nsquared error. It is deﬁned as:\\ndef avg_loss(spendings, sales, w, b):\\nN = len(spendings)\\ntotal_error = 0.0\\nfor i in range(N):\\ntotal_error += (sales[i] - (w*spendings[i] + b))**2\\nreturn total_error / float(N)\\nIf we run the train function for – = 0.001, w = 0.0, b = 0.0, and 15000 epochs, we will see\\nthe following output (shown partially):\\nepoch:\\n0 loss: 92.32078294903626\\nepoch:\\n400 loss: 33.79131790081576\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n6'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 48}, page_content='epoch:\\n800 loss: 27.9918542960729\\nepoch:\\n1200 loss: 24.33481690722147\\nepoch:\\n1600 loss: 22.028754937538633\\n...\\nepoch:\\n2800 loss: 19.07940244306619\\nEpoch 0\\nEpoch 400\\nEpoch 800\\nEpoch 1200\\nEpoch 1600\\nEpoch 3000\\nFigure 2: The evolution of the regression line through gradient descent epochs.\\nYou can see that the average loss decreases as the train function loops through epochs. Fig.\\n2 shows the evolution of the regression line through epochs.\\nFinally, once we have found the optimal values of parameters w and b, the only missing piece\\nis a function that makes predictions:\\ndef predict(x, w, b):\\nreturn w*x + b\\nTry to execute the following code:\\nw, b = train(x, y, 0.0, 0.0, 0.001, 15000)\\nx_new = 23.0\\ny_new = predict(x_new, w, b)\\nprint(y_new)\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n7'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 49}, page_content='The output is 13.97.\\nThe gradient descent algorithm is sensitive to the choice of the step –. It is also slow for large\\ndatasets. Fortunately, several signiﬁcant improvements to this algorithm have been proposed.\\nStochastic gradient descent (SGD) is a version of the algorithm that speeds up the\\ncomputation by approximating the gradient using smaller batches (subsets) of the training\\ndata. SGD itself has various “upgrades”. Adagrad is a version of SGD that scales – for\\neach parameter according to the history of gradients. As a result, – is reduced for very large\\ngradients and vice-versa. Momentum is a method that helps accelerate SGD by orienting\\nthe gradient descent in the relevant direction and reducing oscillations. In neural network\\ntraining, variants of SGD such as RMSprop and Adam, are most frequently used.\\nNotice that gradient descent and its variants are not machine learning algorithms. They are\\nsolvers of minimization problems in which the function to minimize has a gradient in most\\npoints of its domain.\\n4.3\\nHow Machine Learning Engineers Work\\nUnless you are a research scientist or work for a huge corporation with a large R&D budget,\\nyou usually don’t implement machine learning algorithms yourself. You don’t implement\\ngradient descent or some other solver either. You use libraries, most of which are open\\nsource. A library is a collection of algorithms and supporting tools implemented with stability\\nand eﬃciency in mind. The most frequently used in practice open-source machine learning\\nlibrary is scikit-learn. It’s written in Python and C. Here’s how you do linear regression in\\nscikit-learn:\\ndef train(x, y):\\nfrom sklearn.linear_model import LinearRegression\\nmodel = LinearRegression().fit(x,y)\\nreturn model\\nmodel = train(x,y)\\nx_new = 23.0\\ny_new = model.predict(x_new)\\nprint(y_new)\\nThe output will, again, be 13.97. Easy, right? You can replace LinearRegression with some\\nother type of regression learning algorithm without modifying anything else. It just works.\\nThe same can be said about classiﬁcation. You can easily replace LogisticRegression algorithm\\nwith SVC algorithm (this is scikit-learn’s name for the Support Vector Machine algorithm),\\nDecisionTreeClassiﬁer, NearestNeighbors or many other classiﬁcation learning algorithms\\nimplemented in scikit-learn.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n8'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 50}, page_content='4.4\\nLearning Algorithms’ Particularities\\nHere we outline some practical particularities that can diﬀerentiate one learning algorithm\\nfrom another.\\nYou already know that diﬀerent learning algorithms can have diﬀerent\\nhyperparameters (C in SVM, ‘ and d in ID3). Solvers such as gradient descent can also have\\nhyperparameters, like – for example.\\nSome algorithms, like decision tree learning, can accept categorical features. For example, if\\nyou have a feature “color” that can take values “red”, “yellow”, or “green”, you can keep\\nthis feature as is. SVM, logistic and linear regression, as well as kNN (with cosine similarity\\nor Euclidean distance metrics), expect numerical values for all features. All algorithms\\nimplemented in scikit-learn expect numerical features. I show in the next chapter how to\\nconvert categorical features into numerical ones.\\nSome algorithms, like SVM, allow the data analyst to provide weightings for each class.\\nThese weightings inﬂuence how the decision boundary is drawn. If the weight of some class\\nis high, the learning algorithm tries to not make errors in predicting training examples of\\nthis class (typically, for the cost of making an error elsewhere). That could be important if\\ninstances of some class are in the minority in your training data, but you would like to avoid\\nmisclassifying examples of that class as much as possible.\\nSome classiﬁcation models, like SVM and kNN, given a feature vector only output the class.\\nOthers, like logistic regression or decision trees, can also return the score between 0 and 1\\nwhich can be interpreted as either how conﬁdent the model is about the prediction or as the\\nprobability that the input example belongs to a certain class3.\\nSome classiﬁcation algorithms (like decision tree learning, logistic regression, or SVM) build the\\nmodel using the whole dataset at once. If you have got additional labeled examples, you have\\nto rebuild the model from scratch. Other algorithms (such as Naïve Bayes, multilayer percep-\\ntron, SGDClassiﬁer/SGDRegressor, PassiveAggressiveClassiﬁer/PassiveAggressiveRegressor\\nin scikit-learn) can be trained iteratively, one batch at a time. Once new training examples\\nare available, you can update the model using only the new data.\\nFinally, some algorithms, like decision tree learning, SVM, and kNN can be used for both clas-\\nsiﬁcation and regression, while others can only solve one type of problem: either classiﬁcation\\nor regression, but not both.\\nUsually, each library provides the documentation that explains what kind of problem each\\nalgorithm solves, what input values are allowed and what kind of output the model produces.\\nThe documentation also provides information on hyperparameters.\\n3If it’s really necessary, the score for SVM and kNN predictions could be synthetically created using some\\nsimple techniques.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n9'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 51}, page_content=\"Andriy Burkov's\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 52}, page_content='“All models are wrong, but some are useful.”\\n— George Box\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 53}, page_content='5\\nBasic Practice\\nUntil now, I only mentioned in passing some problems a data analyst can encounter when\\nworking on a machine learning problem: feature engineering, overﬁtting, and hyperparameter\\ntuning. In this chapter, we talk about these and other challenges that have to be addressed\\nbefore you can type model = LogisticRegresion().ﬁt(x,y) in scikit-learn.\\n5.1\\nFeature Engineering\\nWhen a product manager tells you “We need to be able to predict whether a particular\\ncustomer will stay with us. Here are the logs of customers’ interactions with our product for\\nﬁve years.” you cannot just grab the data, load it into a library and get a prediction. You\\nneed to build a dataset ﬁrst.\\nRemember from the ﬁrst chapter that the dataset is the collection of labeled examples\\n{(xi, yi)}N\\ni=1. Each element xi among N is called a feature vector. A feature vector is a\\nvector in which each dimension j = 1, . . . , D contains a value that describes the example\\nsomehow. That value is called a feature and is denoted as x(j).\\nThe problem of transforming raw data into a dataset is called feature engineering. For\\nmost practical problems, feature engineering is a labor-intensive process that demands from\\nthe data analyst a lot of creativity and, preferably, domain knowledge.\\nFor example, to transform the logs of user interaction with a computer system, one could\\ncreate features that contain information about the user and various statistics extracted from\\nthe logs. For each user, one feature would contain the price of the subscription; other features\\nwould contain the frequency of connections per day, week and year. Another feature would\\ncontain the average session duration in seconds or the average response time for one request,\\nand so on. Everything measurable can be used as a feature. The role of the data analyst is to\\ncreate informative features: those would allow the learning algorithm to build a model that\\npredicts well labels of the data used for training. Highly informative features are also called\\nfeatures with high predictive power. For example, the average duration of a user’s session\\nhas high predictive power for the problem of predicting whether the user will keep using the\\napplication in the future.\\nWe say that a model has a low bias when it predicts well the training data. That is, the\\nmodel makes few mistakes when we try to predict labels of the examples used to build the\\nmodel.\\n5.1.1\\nOne-Hot Encoding\\nSome learning algorithms only work with numerical feature vectors. When some feature in\\nyour dataset is categorical, like “colors” or “days of the week,” you can transform such a\\ncategorical feature into several binary ones.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 54}, page_content='If your example has a categorical feature “colors” and this feature has three possible values:\\n“red,” “yellow,” “green,” you can transform this feature into a vector of three numerical\\nvalues:\\nred = [1, 0, 0]\\nyellow = [0, 1, 0]\\ngreen = [0, 0, 1]\\n(1)\\nBy doing so, you increase the dimensionality of your feature vectors. You should not transform\\nred into 1, yellow into 2, and green into 3 to avoid increasing the dimensionality because that\\nwould imply that there’s an order among the values in this category and this speciﬁc order is\\nimportant for the decision making. If the order of a feature’s values is not important, using\\nordered numbers as values is likely to confuse the learning algorithm,1 because the algorithm\\nwill try to ﬁnd a regularity where there’s no one, which may potentially lead to overﬁtting.\\n5.1.2\\nBinning\\nAn opposite situation, occurring less frequently in practice, is when you have a numerical\\nfeature but you want to convert it into a categorical one. Binning (also called bucketing)\\nis the process of converting a continuous feature into multiple binary features called bins or\\nbuckets, typically based on value range. For example, instead of representing age as a single\\nreal-valued feature, the analyst could chop ranges of age into discrete bins: all ages between\\n0 and 5 years-old could be put into one bin, 6 to 10 years-old could be in the second bin, 11\\nto 15 years-old could be in the third bin, and so on.\\nFor example, suppose in our feature j = 18 represents age. By applying binning, we replace\\nthis feature with the corresponding bins. Let the three new bins, “age_bin1”, “age_bin2”\\nand “age_bin3” be added with indexes j = 123, j = 124 and j = 125 respectively. Now if\\nx(18)\\ni\\n= 7 for some example xi, then we set feature x(124)\\ni\\nto 1; if x(18)\\ni\\n= 13, then we set\\nfeature x(125)\\ni\\nto 1, and so on.\\nIn some cases, a carefully designed binning can help the learning algorithm to learn using\\nfewer examples. It happens because we give a “hint” to the learning algorithm that if the\\nvalue of a feature falls within a speciﬁc range, the exact value of the feature doesn’t matter.\\n1When the ordering of values of some categorical variable matters, we can replace those values by numbers\\nby keeping only one variable. For example, if our variable represents the quality of an article, and the\\nvalues are {poor, decent, good, excellent}, then we could replace those categories by numbers, for example,\\n{1, 2, 3, 4}.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n4'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 55}, page_content='5.1.3\\nNormalization\\nNormalization is the process of converting an actual range of values which a numerical\\nfeature can take, into a standard range of values, typically in the interval [≠1, 1] or [0, 1].\\nFor example, suppose the natural range of a particular feature is 350 to 1450. By subtracting\\n350 from every value of the feature, and dividing the result by 1100, one can normalize those\\nvalues into the range [0, 1].\\nMore generally, the normalization formula looks like this:\\n¯x(j) =\\nx(j) ≠min(j)\\nmax(j) ≠min(j) ,\\nwhere min(j) and max(j) are, respectively, the minimum and the maximum value of the\\nfeature j in the dataset.\\nWhy do we normalize? Normalizing the data is not a strict requirement. However, in practice,\\nit can lead to an increased speed of learning. Remember the gradient descent example from\\nthe previous chapter. Imagine you have a two-dimensional feature vector. When you update\\nthe parameters of w(1) and w(2), you use partial derivatives of the average squared error with\\nrespect to w(1) and w(2). If x(1) is in the range [0, 1000] and x(2) the range [0, 0.0001], then\\nthe derivative with respect to a larger feature will dominate the update.\\nAdditionally, it’s useful to ensure that our inputs are roughly in the same relatively small\\nrange to avoid problems which computers have when working with very small or very big\\nnumbers (known as numerical overﬂow).\\n5.1.4\\nStandardization\\nStandardization (or z-score normalization) is the procedure during which the feature\\nvalues are rescaled so that they have the properties of a standard normal distribution with\\nµ = 0 and ‡ = 1, where µ is the mean (the average value of the feature, averaged over all\\nexamples in the dataset) and ‡ is the standard deviation from the mean.\\nStandard scores (or z-scores) of features are calculated as follows:\\nˆx(j) = x(j) ≠µ(j)\\n‡(j)\\n.\\nYou may ask when you should use normalization and when standardization. There’s no\\ndeﬁnitive answer to this question. Usually, if your dataset is not too big and you have time,\\nyou can try both and see which one performs better for your task.\\nIf you don’t have time to run multiple experiments, as a rule of thumb:\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 56}, page_content='• unsupervised learning algorithms, in practice, more often beneﬁt from standardization\\nthan from normalization;\\n• standardization is also preferred for a feature if the values this feature takes are\\ndistributed close to a normal distribution (so-called bell curve);\\n• again, standardization is preferred for a feature if it can sometimes have extremely high\\nor low values (outliers); this is because normalization will “squeeze” the normal values\\ninto a very small range;\\n• in all other cases, normalization is preferable.\\nModern implementations of the learning algorithms, which you can ﬁnd in popular libraries,\\nare robust to features lying in diﬀerent ranges. Feature rescaling is usually beneﬁcial to most\\nlearning algorithms, but in many cases, the model will still be good when trained from the\\noriginal features.\\n5.1.5\\nDealing with Missing Features\\nIn some cases, the data comes to the analyst in the form of a dataset with features already\\ndeﬁned. In some examples, values of some features can be missing. That often happens when\\nthe dataset was handcrafted, and the person working on it forgot to ﬁll some values or didn’t\\nget them measured at all.\\nThe typical approaches of dealing with missing values for a feature include:\\n• Removing the examples with missing features from the dataset. That can be done if\\nyour dataset is big enough so you can sacriﬁce some training examples.\\n• Using a learning algorithm that can deal with missing feature values (depends on the\\nlibrary and a speciﬁc implementation of the algorithm).\\n• Using a data imputation technique.\\n5.1.6\\nData Imputation Techniques\\nOne technique consists in replacing the missing value of a feature by an average value of this\\nfeature in the dataset:\\nˆx(j) = 1\\nN x(j).\\nAnother technique is to replace the missing value by the same value outside the normal range\\nof values. For example, if the normal range is [0, 1], then you can set the missing value equal\\nto 2 or ≠1. The idea is that the learning algorithm will learn what is it better to do when the\\nfeature has a value signiﬁcantly diﬀerent from other values. Alternatively, you can replace the\\nmissing value by a value in the middle of the range. For example, if the range for a feature is\\n[≠1, 1], you can set the missing value to be equal to 0. Here, the idea is that if we use the\\nvalue in the middle of the range to replace missing features, such value will not signiﬁcantly\\naﬀect the prediction.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n6'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 57}, page_content='A more advanced technique is to use the missing value as the target variable for a regression\\nproblem. You can use all remaining features [x(1)\\ni , x(2)\\ni , . . . , x(j≠1)\\ni\\n, x(j+1)\\ni\\n, . . . , x(D)\\ni\\n] to form\\na feature vector ˆxi, set ˆyi = x(j), where j is the feature with a missing value. Now we can\\nbuild a regression model to predict ˆy from the feature vectors ˆx. Of course, to build training\\nexamples (ˆx, ˆy), you only use those examples from the original dataset, in which the value of\\nfeature j is present.\\nFinally, if you have a signiﬁcantly large dataset and just a few features with missing values,\\nyou can increase the dimensionality of your feature vectors by adding a binary indicator\\nfeature for each feature with missing values. Let’s say feature j = 12 in your D-dimensional\\ndataset has missing values. For each feature vector x, you then add the feature j = D + 1\\nwhich is equal to 1 if the value of feature 12 is present in x and 0 otherwise. The missing\\nfeature value then can be replaced by 0 or any number of your choice.\\nAt prediction time, if your example is not complete, you should use the same data imputation\\ntechnique to ﬁll the missing features as the technique you used to complete the training data.\\nBefore you start working on the learning problem, you cannot tell which data imputation\\ntechnique will work the best. Try several techniques, build several models and select the one\\nthat works the best.\\n5.2\\nLearning Algorithm Selection\\nChoosing a machine learning algorithm can be a diﬃcult task. If you have much time, you\\ncan try all of them. However, usually the time you have to solve a problem is limited. You\\ncan ask yourself several questions before starting to work on the problem. Depending on\\nyour answers, you can shortlist some algorithms and try them on your data.\\n• Explainability\\nDoes your model have to be explainable to a non-technical audience? Most very accurate\\nlearning algorithms are so-called “black boxes.” They learn models that make very few errors,\\nbut why a model made a speciﬁc prediction could be very hard to understand and even\\nharder to explain. Examples of such models are neural networks or ensemble models.\\nOn the other hand, kNN, linear regression, or decision tree learning algorithms produce\\nmodels that are not always the most accurate, however, the way they make their prediction\\nis very straightforward.\\n• In-memory vs. out-of-memory\\nCan your dataset be fully loaded into the RAM of your server or personal computer? If\\nyes, then you can choose from a wide variety of algorithms. Otherwise, you would prefer\\nincremental learning algorithms that can improve the model by adding more data\\ngradually.\\n• Number of features and examples\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n7'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 58}, page_content='How many training examples do you have in your dataset? How many features does each\\nexample have? Some algorithms, including neural networks and gradient boosting (we\\nconsider both later), can handle a huge number of examples and millions of features. Others,\\nlike SVM, can be very modest in their capacity.\\n• Categorical vs. numerical features\\nIs your data composed of categorical only, or numerical only features, or a mix of both?\\nDepending on your answer, some algorithms cannot handle your dataset directly, and you\\nwould need to convert your categorical features into numerical ones by using some techniques\\nlike one-hot encoding.\\n• Nonlinearity of the data\\nIs your data linearly separable or can it be modeled using a linear model? If yes, SVM with\\nthe linear kernel, logistic regression or linear regression can be a good choice. Otherwise,\\ndeep neural networks or ensemble algorithms, discussed in Chapters 6 and 7, might work\\nbetter for your data.\\n• Training speed\\nHow much time is a learning algorithm allowed to use to build a model? Neural networks\\nare known to be slow to train. Simple algorithms like logistic and linear regression as well\\nas decision tree learning are much faster. Some specialized libraries contain very eﬃcient\\nimplementations of some algorithms; you may prefer to do research online to ﬁnd such\\nlibraries. Some algorithms, such as random forests, beneﬁt from the availability of multiple\\nCPU cores, so their model building time can be signiﬁcantly reduced on a machine with\\ndozens of CPU cores.\\n• Prediction speed\\nHow fast does the model have to be when generating predictions? Will your model be used in\\nproduction where very high throughput is required? Some algorithms, like SVMs, linear and\\nlogistic regression, or some types of neural networks, are extremely fast at the prediction time.\\nSome others, like kNN, ensemble algorithms, and very deep or recurrent neural networks,\\ncan be slower2.\\nIf you don’t want to guess the best algorithm for your data, a popular way to choose one is\\nby testing it on the validation set. We talk about that below.\\nAlternatively, if you use scikit-learn, you could try their algorithm selection diagram shown\\nin ﬁg. 1.\\n2The prediction speeds of kNN and ensemble methods implemented in the modern libraries are still very\\nfast. Don’t be afraid of using these algorithms in your practice.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n8'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 59}, page_content='Figure 1: Machine learning algorithm selection diagram for scikit-learn.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n9'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 60}, page_content='5.3\\nThree Sets\\nUntil now, I used the expressions “dataset” and “training set” interchangeably. However, in\\npractice data analysts work with three sets of labeled examples:\\n1) training set,\\n2) validation set, and\\n3) test set.\\nOnce you have got your annotated dataset, the ﬁrst thing you do is you shuﬄe the examples\\nand split the dataset into three subsets: training, validation, and test. The training set is\\nusually the biggest one, and you use it to build the model. The validation and test sets are\\nroughly the same sizes, much smaller than the size of the training set. The learning algorithm\\ncannot use examples from these two subsets to build a model. That is why those two sets are\\noften called hold-out sets.\\nThere’s no optimal proportion to split the dataset into these three subsets. In the past, the\\nrule of thumb was to use 70% of the dataset for training, 15% for validation and 15% for\\ntesting. However, in the age of big data, datasets often have millions of examples. In such\\ncases, it could be reasonable to keep 95% for training and 2.5%/2.5% for validation/testing.\\nYou may wonder, what is the reason to have three sets and not one. The answer is simple:\\nwhen we build a model, what we do not want is for the model to only do well at predicting\\nlabels of examples the learning algorithms has already seen. A trivial algorithm that simply\\nmemorizes all training examples and then uses the memory to “predict” their labels will make\\nno mistakes when asked to predict the labels of the training examples, but such an algorithm\\nwould be useless in practice. What we really want is that our model predicts well examples\\nthat the learning algorithm didn’t see. So we want good performance on a hold-out set.\\nWhy do we need two hold-out sets and not one? We use the validation set to 1) choose the\\nlearning algorithm and 2) ﬁnd the best values of hyperparameters. We use the test set to\\nassess the model before delivering it to the client or putting it in production.\\n5.4\\nUnderﬁtting and Overﬁtting\\nI mentioned above the notion of bias. I said that a model has a low bias if it predicts well\\nthe labels of the training data. If the model makes many mistakes on the training data,\\nwe say that the model has a high bias or that the model underﬁts. So, underﬁtting is the\\ninability of the model to predict well the labels of the data it was trained on. There could be\\nseveral reasons for underﬁtting, the most important of which are:\\n• your model is too simple for the data (for example a linear model can often underﬁt);\\n• the features you engineered are not informative enough.\\nThe ﬁrst reason is easy to illustrate in the case of one-dimensional regression: the dataset can\\nresemble a curved line, but our model is a straight line. The second reason can be illustrated\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n10'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 61}, page_content='Underﬁtting\\nGood ﬁt\\nOverﬁtting\\nFigure 2: Examples of underﬁtting (linear model), good ﬁt (quadratic model), and overﬁtting\\n(polynomial of degree 15).\\nlike this: let’s say you want to predict whether a patient has cancer, and the features you\\nhave are height, blood pressure, and heart rate. These three features are clearly not good\\npredictors for cancer so our model will not be able to learn a meaningful relationship between\\nthese features and the label.\\nThe solution to the problem of underﬁtting is to try a more complex model or to engineer\\nfeatures with higher predictive power.\\nOverﬁtting is another problem a model can exhibit. The model that overﬁts predicts very\\nwell the training data but poorly the data from at least one of the two hold-out sets. I already\\ngave an illustration of overﬁtting in Chapter 3. Several reasons can lead to overﬁtting, the\\nmost important of which are:\\n• your model is too complex for the data (for example a very tall decision tree or a very\\ndeep or wide neural network often overﬁt);\\n• you have too many features but a small number of training examples.\\nIn the literature, you can ﬁnd another name for the problem of overﬁtting: the problem of\\nhigh variance. This term comes from statistics. The variance is an error of the model due to\\nits sensitivity to small ﬂuctuations in the training set. It means that if your training data\\nwas sampled diﬀerently, the learning would result in a signiﬁcantly diﬀerent model. Which\\nis why the model that overﬁts performs poorly on the test data: test and training data are\\nsampled from the dataset independently of one another.\\nSeveral solutions to the problem of overﬁtting are possible:\\n1. Try a simpler model (linear instead of polynomial regression, or SVM with a linear\\nkernel instead of RBF, a neural network with fewer layers/units).\\n2. Reduce the dimensionality of examples in the dataset (for example, by using one of the\\ndimensionality reduction techniques discussed in Chapter 9).\\n3. Add more training data, if possible.\\n4. Regularize the model.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n11'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 62}, page_content='Figure 2 illustrates a one-dimensional dataset for which a regression model underﬁts, ﬁts well\\nand overﬁts the data.\\nRegularization is the most widely used approach to prevent overﬁtting.\\n5.5\\nRegularization\\nEven the simplest model, such as linear, can overﬁt the data. That usually happens when the\\ndata is high-dimensional, but the number of training examples is relatively low. In fact, when\\nfeature vectors are very high-dimensional, the linear learning algorithm can build a model\\nthat assigns non-zero values to most dimensions w(j) in the parameter vector w, trying to\\nﬁnd very complex relationships between all available features to predict labels of training\\nexamples perfectly.\\nSuch a complex model will most likely predict poorly the labels of the hold-out examples.\\nThis is because by trying to perfectly predict labels of all training examples, the model will\\nalso learn the idiosyncrasies of the training set: the noise in the values of features of the\\ntraining examples, the sampling imperfection due to the small dataset size, and other artifacts\\nextrinsic to the decision problem in hand but present in the training set.\\nRegularization is an umbrella-term that encompasses methods that force the learning\\nalgorithm to build a less complex model. In practice, that often leads to slightly higher\\nbias but signiﬁcantly reduces the variance. This problem is known in the literature as the\\nbias-variance tradeoﬀ.\\nThe two most widely used types of regularization are called L1 regularization and L2\\nregularization. The idea is quite simple. To create a regularized model, we modify the\\nobjective function by adding a penalizing term whose value is higher when the model is more\\ncomplex.\\nFor simplicity, I illustrate regularization using the example of linear regression. The same\\nprinciple can be applied to a wide variety of models.\\nRecall the linear regression objective we want to minimize:\\nmin\\nw,b\\n1\\nN\\nN\\nÿ\\ni=1\\n(fw,b(xi) ≠yi)2.\\n(2)\\nAn L1-regularized objective looks like this:\\nmin\\nw,b C|w| + 1\\nN\\nN\\nÿ\\ni=1\\n(fw,b(xi) ≠yi)2,\\n(3)\\nwhere |w|\\ndef\\n= qD\\nj=1 |w(j)| and C is a hyperparameter that controls the importance of regular-\\nization. If we set C to zero, the model becomes a standard non-regularized linear regression\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n12'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 63}, page_content='model. On the other hand, if we set to C to a high value, the learning algorithm will try to\\nset most w(j) to a very small value or zero to minimize the objective, the model will become\\nvery simple which can lead to underﬁtting. Your role as the data analyst is to ﬁnd such\\na value of the hyperparameter C that doesn’t increase the bias too much but reduces the\\noverﬁtting to a level reasonable for the problem in hand. In the next section, I will show how\\nto do that.\\nAn L2-regularized objective will look like this:\\nmin\\nw,b CÎwÎ2 + 1\\nN\\nN\\nÿ\\ni=1\\n(fw,b(xi) ≠yi)2, where ÎwÎ2 def\\n=\\nD\\nÿ\\nj=1\\n(w(j))2.\\n(4)\\nIn practice, L1 regularization produces a sparse model, a model that has most of its\\nparameters (in case of linear models, most of w(j)) equal to zero (provided the hyperparameter\\nC is large enough). So L1 makes feature selection by deciding which features are essential\\nfor prediction and which are not. That can be useful in case you want to increase model\\nexplainability. However, if your only goal is to maximize the performance of the model on\\nthe hold-out data, then L2 usually gives better results. L2 also has the advantage of being\\ndiﬀerentiable, so gradient descent can be used for optimizing the objective function.\\nL1 and L2 regularization methods are also combined in what is called elastic net regular-\\nization with L1 and L2 regularizations being special cases. You can ﬁnd in the literature\\nthe name ridge regularization for L2 and lasso for L1.\\nIn addition to being widely used with linear models, L1 and L2 regularization are also\\nfrequently used with neural networks and many other types of models, which directly\\nminimize an objective function.\\nNeural networks also beneﬁt from two other regularization techniques: dropout and batch-\\nnormalization. There are also non-mathematical methods that have a regularization eﬀect:\\ndata augmentation and early stopping. We talk about these techniques in Chapter 8.\\n5.6\\nModel Performance Assessment\\nOnce you have a model which our learning algorithm has built using the training set, how\\ncan you say how good the model is? You use the test set to assess the model.\\nThe test set contains the examples that the learning algorithm has never seen before, so if\\nour model performs well on predicting the labels of the examples from the test set, we say\\nthat our model generalizes well or, simply, that it’s good.\\nTo be more rigorous, machine learning specialists use various formal metrics and tools to\\nassess the model performance. For regression, the assessment of the model is quite simple. A\\nwell-ﬁtting regression model results in predicted values close to the observed data values. The\\nmean model, which always predicts the average of the labels in the training data, generally\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n13'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 64}, page_content='would be used if there were no informative features. The ﬁt of a regression model being\\nassessed should, therefore, be better than the ﬁt of the mean model. If this is the case, then\\nthe next step is to compare the performances of the model on the training and the test data.\\nTo do that, we compute the mean squared error3 (MSE) for the training, and, separately,\\nfor the test data. If the MSE of the model on the test data is substantially higher than\\nthe MSE obtained on the training data, this is a sign of overﬁtting. Regularization or a\\nbetter hyperparameter tuning could solve the problem. The meaning of “substantially higher”\\ndepends on the problem in hand and has to be decided by the data analyst jointly with the\\ndecision maker/product owner who ordered the model.\\nFor classiﬁcation, things are a little bit more complicated. The most widely used metrics and\\ntools to assess the classiﬁcation model are:\\n• confusion matrix,\\n• accuracy,\\n• cost-sensitive accuracy,\\n• precision/recall, and\\n• area under the ROC curve.\\nTo simplify the illustration, I use a binary classiﬁcation problem. Where necessary, I show\\nhow to extend the approach to the multiclass case.\\n5.6.1\\nConfusion Matrix\\nThe confusion matrix is a table that summarizes how successful the classiﬁcation model\\nis at predicting examples belonging to various classes. One axis of the confusion matrix\\nis the label that the model predicted, and the other axis is the actual label. In a binary\\nclassiﬁcation problem, there are two classes. Let’s say, the model predicts two classes: “spam”\\nand “not_spam”:\\nspam (predicted)\\nnot spam (predicted)\\nspam (actual)\\n23 (TP)\\n1 (FN)\\nnot spam (actual)\\n12 (FP)\\n556 (TN)\\nThe above confusion matrix shows that of the 24 examples that actually were spam, the\\nmodel correctly classiﬁed 23 as spam. In this case, we say that we have 23 true positives\\nor TP = 23. The model incorrectly classiﬁed 1 example as not spam. In this case, we have 1\\nfalse negative, or FN = 1. Similarly, of 568 examples that actually were not spam, 556 were\\ncorrectly classiﬁed (556 true negatives or TN = 556), and 12 were incorrectly classiﬁed (12\\nfalse positives, FP = 12).\\nThe confusion matrix for multiclass classiﬁcation has as many rows and columns as there are\\n3Or any other type of loss function you used to build your optimization problem.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n14'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 65}, page_content='diﬀerent classes. It can help you to determine mistake patterns. For example, a confusion\\nmatrix could reveal that a model trained to recognize diﬀerent species of animals tends to\\nmistakenly predict “cat” instead of “panther,” or “mouse” instead of “rat.” In this case, you\\ncan decide to add more labeled examples of these species to help the learning algorithm to\\n“see” the diﬀerence between them. Alternatively, you can decide to add additional features\\nthe learning algorithm can use to build a model that would better distinguish between these\\nspecies.\\nConfusion matrices can be used to calculate two important performance metrics: precision\\nand recall.\\n5.6.2\\nPrecision/Recall\\nThe two most frequently used metrics to assess the model are precision and recall. Precision\\nis the ratio of correct positive predictions to the overall number of positive predictions:\\nPrecision =\\nTP\\nTP + FP.\\nRecall is the ratio of correct positive predictions to the overall number of positive examples\\nin the test set:\\nRecall =\\nTP\\nTP + FN.\\nTo understand the meaning and importance of precision and recall for the model assessment it\\nis often useful to think about the prediction problem as the problem of research of documents\\nin the database using a query. The precision is the proportion of relevant documents in the\\nlist of all returned documents. The recall is the ratio of the relevant documents returned\\nby the search engine to the total number of the relevant documents that could have been\\nreturned.\\nIn the case of the spam detection problem, we want to have high precision (we want to avoid\\nmaking mistakes by detecting that a legitimate message is spam) and we are ready to tolerate\\nlower recall (we tolerate some spam messages in our inbox).\\nAlmost always, in practice, we have to choose between a high precision or a high recall. It’s\\nusually impossible to have both. We can achieve either of the two by various means:\\n• by assigning a higher weighting to messages with spam (the SVM algorithm accepts\\nweightings of classes as input);\\n• by tuning hyperparameters such that the precision or recall on the validation set are\\nmaximized;\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n15'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 66}, page_content='• by varying the decision threshold for algorithms that return probabilities of classes;\\nfor instance, if we use logistic regression or decision tree, to increase precision (at the\\ncost of a lower recall), we can decide that the prediction will be positive only if the\\nprobability returned by the model is higher than 0.9.\\nEven if precision and recall are deﬁned for the binary classiﬁcation case, you can always use\\nit to assess a multiclass classiﬁcation model. To do that, ﬁrst select a class for which you\\nwant to assess these metrics. Then you consider all examples of the selected class as positives\\nand all examples of the remaining classes as negatives.\\n5.6.3\\nAccuracy\\nAccuracy is given by the number of correctly classiﬁed examples divided by the total number\\nof classiﬁed examples. In terms of the confusion matrix, it is given by:\\nAccuracy =\\nTP + TN\\nTP + TN + FP + FN.\\n(5)\\nAccuracy is a useful metric when errors in predicting all classes are equally important. In\\ncase of the spam/not spam, this may not be the case. For example, you would tolerate false\\npositives less than false negatives. A false positive in spam detection is the situation in which\\nyour friend sends you an email, but the model labels it as spam and doesn’t show you. On\\nthe other hand, the false negative is less of a problem: if your model doesn’t detect a small\\npercentage of spam messages, it’s not a big deal.\\n5.6.4\\nCost-Sensitive Accuracy\\nFor dealing with the situation in which diﬀerent classes have diﬀerent importance, a useful\\nmetric is cost-sensitive accuracy. To compute a cost-sensitive accuracy, you ﬁrst assign a\\ncost (a positive number) to both types of mistakes: FP and FN. You then compute the counts\\nTP, TN, FP, FN as usual and multiply the counts for FP and FN by the corresponding cost\\nbefore calculating the accuracy using eq. 5.\\n5.6.5\\nArea under the ROC Curve (AUC)\\nThe ROC curve (ROC stands for “receiver operating characteristic,” the term comes from\\nradar engineering) is a commonly used method to assess the performance of classiﬁcation\\nmodels. ROC curves use a combination the true positive rate (the proportion of positive\\nexamples predicted correctly, deﬁned exactly as recall) and false positive rate (the proportion\\nof negative examples predicted incorrectly) to build up a summary picture of the classiﬁcation\\nperformance.\\nThe true positive rate and the false positive rate are respectively deﬁned as,\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n16'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 67}, page_content='TPR =\\nTP\\n(TP + FN) and FPR =\\nFP\\n(FP + TN).\\nROC curves can only be used to assess classiﬁers that return some conﬁdence score (or a\\nprobability) of prediction. For example, logistic regression, neural networks, and decision\\ntrees (and ensemble models based on decision trees) can be assessed using ROC curves.\\nTo draw a ROC curve, we ﬁrst discretize the range of the conﬁdence score. If this range for\\nour model is [0, 1], then we can discretize it like this: [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1].\\nThen, we use each discrete value as the prediction threshold and predict the labels of examples\\nin our dataset using our model and this threshold. For example, if we want to compute TPR\\nand FPR for the threshold equal to 0.7, we apply the model to each example, get the score,\\nand, if the score if higher than or equal to 0.7, we predict the positive class; otherwise, we\\npredict the negative class.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n17'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 68}, page_content='0\\n1\\n1\\nAUC\\xa0=\\xa00.4\\nFalse\\xa0positive\\xa0rate\\nTrue\\xa0positive\\xa0rate\\n0\\n1\\n1\\nAUC\\xa0=\\xa00.5\\nFalse\\xa0positive\\xa0rate\\nTrue\\xa0positive\\xa0rate\\n0\\n1\\n1\\nAUC\\xa0=\\xa00.6\\nFalse\\xa0positive\\xa0rate\\nTrue\\xa0positive\\xa0rate\\n0\\n1\\n1\\nAUC\\xa0=\\xa00.85\\nFalse\\xa0positive\\xa0rate\\nTrue\\xa0positive\\xa0rate\\nFigure 3: Area under the ROC curve.\\nLook at the illustration in Figure 3. It’s easy to see that if the threshold is 0, all our\\npredictions will be positive, so both TPR and FPR will be 1 (the upper right corner). On\\nthe other hand, if the threshold is 1, then no positive prediction will be made, both TPR\\nand FPR will be 0 which corresponds to the lower left corner.\\nThe higher the area under the ROC curve (AUC), the better the classiﬁer. A classiﬁer\\nwith an AUC higher than 0.5 is better than a random classiﬁer. If AUC is lower than 0.5,\\nthen something is wrong with your model. A perfect classiﬁer would have an AUC of 1.\\nUsually, if our model behaves well, we obtain a good classiﬁer by selecting the value of the\\nthreshold that gives TPR close to 1 while keeping FPR near 0.\\nROC curves are widely used because they are relatively simple to understand, they capture\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n18'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 69}, page_content='more than one aspect of the classiﬁcation (by taking both false positives and false negatives\\ninto account) and allow visually and with low eﬀort comparing the performance of diﬀerent\\nmodels.\\n5.7\\nHyperparameter Tuning\\nWhen I presented learning algorithms, I mentioned that you as a data analyst have to select\\ngood values for the algorithm’s hyperparameters, such as ‘ and d for ID3, C for SVM, or –\\nfor gradient descent. But what does that exactly mean? Which value is the best and how to\\nﬁnd it? In this section, I answer these essential questions.\\nAs you already know, hyperparameters aren’t optimized by the learning algorithm itself. The\\ndata analyst has to “tune” hyperparameters by experimentally ﬁnding the best combination\\nof values, one per hyperparameter.\\nOne typical way to do that, when you have enough data to have a decent validation set (in\\nwhich each class is represented by at least a couple of dozen examples) and the number of\\nhyperparameters and their range is not too large is to use grid search.\\nGrid search is the most simple hyperparameter tuning strategy. Let’s say you train an SVM\\nand you have two hyperparameters to tune: the penalty parameter C (a positive real number)\\nand the kernel (either “linear” or “rbf”).\\nIf it’s the ﬁrst time you are working with this dataset, you don’t know what is the possible\\nrange of values for C. The most common trick is to use a logarithmic scale. For example, for\\nC you can try the following values: [0.001, 0.01, 0.1, 1.0, 10, 100, 1000]. In this case you have\\n14 combinations of hyperparameters to try: [(0.001, “linear”), (0.01, “linear”), (0.1, “linear”),\\n(1.0, “linear”), (10, “linear”), (100, “linear”), (1000, “linear”), (0.001, “rbf”), (0.01, “rbf”),\\n(0.1, “rbf”), (1.0, “rbf”), (10, “rbf”), (100, “rbf”), (1000, “rbf”)].\\nYou use the training set and train 14 models, one for each combination of hyperparameters.\\nThen you assess the performance of each model on the validation data using one of the\\nmetrics we discussed in the previous section (or some other metric that matters to you).\\nFinally, you keep the model that performs the best according to the metric.\\nOnce you have found the best pair of hyperparameters, you can try to explore the values\\nclose to the best ones in some region around them. Sometimes, this can result in an even\\nbetter model.\\nFinally, you assess the selected model using the test set.\\nAs you could notice, trying all combinations of hyperparameters, especially if there are\\nmore than a couple of them, could be time-consuming, especially for large datasets. There\\nare more eﬃcient techniques, such as random search and Bayesian hyperparameter\\noptimization.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n19'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 70}, page_content='Random search diﬀers from grid search in that you no longer provide a discrete set of\\nvalues to explore for each hyperparameter; instead, you provide a statistical distribution for\\neach hyperparameter from which values are randomly sampled and set the total number of\\ncombinations you want to try.\\nBayesian techniques diﬀer from random or grid search in that they use past evaluation results\\nto choose the next values to evaluate. The idea is to limit expensive optimization of the\\nobjective function by choosing the next hyperparameter values based on those that have done\\nwell in the past.\\nThere are also gradient-based techniques, evolutionary optimiza-\\ntion techniques, and other algorithmic hyperparameter tuning tech-\\nniques. Most modern machine learning libraries implement one or more\\nsuch techniques. There are also hyperparameter tuning libraries that can\\nhelp you to tune hyperparameters of virtually any learning algorithm,\\nincluding ones you programmed yourself.\\n5.7.1\\nCross-Validation\\nWhen you don’t have a decent validation set to tune your hyperparameters on, the common\\ntechnique that can help you is called cross-validation. When you have few training examples,\\nit could be prohibitive to have both validation and test set. You would prefer to use more\\ndata to train the model. In such a case, you only split your data into a training and a test\\nset. Then you use cross-validation to on the training set to simulate a validation set.\\nCross-validation works like follows. First, you ﬁx the values of the hyperparameters you want\\nto evaluate. Then you split your training set into several subsets of the same size. Each\\nsubset is called a fold. Typically, ﬁve-fold cross-validation is used in practice. With ﬁve-fold\\ncross-validation, you randomly split your training data into ﬁve folds: {F1, F2, . . . , F5}. Each\\nFk, k = 1, . . . , 5 contains 20% of your training data. Then you train ﬁve models as follows.\\nTo train the ﬁrst model, f1, you use all examples from folds F2, F3, F4, and F5 as the training\\nset and the examples from F1 as the validation set. To train the second model, f2, you\\nuse the examples from folds F1, F3, F4, and F5 to train and the examples from F2 as the\\nvalidation set. You continue building models iteratively like this and compute the value of\\nthe metric of interest on each validation set, from F1 to F5. Then you average the ﬁve values\\nof the metric to get the ﬁnal value.\\nYou can use grid search with cross-validation to ﬁnd the best values of hyperparameters for\\nyour model. Once you have found these values, you use the entire training set to build the\\nmodel with these best values of hyperparameters you have found via cross-validation. Finally,\\nyou assess the model using the test set.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n20'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 71}, page_content='The\\nHundred-\\nPage\\nMachine\\nLearning\\nBook\\nAndriy Burkov'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 72}, page_content='“All models are wrong, but some are useful.”\\n— George Box\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 73}, page_content='6\\nNeural Networks and Deep Learning\\nFirst of all, you already know what a neural network is, and you already know how to build\\nsuch a model. Yes, it’s logistic regression! As a matter of fact, the logistic regression model,\\nor rather its generalization for multiclass classiﬁcation, called the softmax regression model,\\nis a standard unit in a neural network.\\n6.1\\nNeural Networks\\nIf you understood linear regression, logistic regression, and gradient descent, understanding\\nneural networks would not be a problem.\\nA neural network (NN), just like a regression or an SVM model, is a mathematical function:\\ny = fNN(x).\\nThe function fNN has a particular form: it’s a nested function. You have probably already\\nheard of neural network layers. So, for a 3-layer neural network that returns a scalar, fNN\\nlooks like this:\\ny = fNN(x) = f3(f2(f1(x))).\\nIn the above equation, f2, f3 are vector functions of the following form:\\nfl(z)\\ndef\\n= gl(Wlz + bl),\\n(1)\\nwhere l is called the layer index and can span from 1 to any number of layers. The function\\ngl is called an activation function. It is a ﬁxed, usually nonlinear function chosen by the\\ndata analyst before the learning is started. The parameters Wl (a matrix) and bl (a vector)\\nfor each layer are learned using the familiar gradient descent by optimizing, depending on the\\ntask, a particular cost function (such as MSE). Compare eq. 1 with the equation for logistic\\nregression, where you replace gl by the sigmoid function, and you will not see any diﬀerence.\\nThe function f1 is a scalar function for the regression task, but can also be a vector function\\ndepending on your problem.\\nYou may probably wonder why a matrix Wl is used and not a vector wl. The reason is\\nthat gl is a vector function. Each row wl,u (u for unit) of the matrix Wl is a vector of\\nthe same dimensionality as z. Let al,u = wl,uz + bl,u. The output of fl(z) is a vector\\n[gl(al,1), gl(al,2), . . . , gl(al,sizel)], where gl is some scalar function1, and sizel is the number of\\nunits in layer l. To make it more concrete, let’s consider one architecture of neural networks\\ncalled multilayer perceptron and often referred to as a vanilla neural network.\\n1A scalar function outputs a scalar, that is a simple number and not a vector.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 74}, page_content='6.1.1\\nMultilayer Perceptron Example\\nWe have a closer look at one particular conﬁguration of neural networks called feed-forward\\nneural networks (FFNN), and more speciﬁcally the architecture called a multilayer\\nperceptron (MLP). As an illustration, we consider an MLP with three layers. Our network\\ntakes a two-dimensional feature vector as input and outputs a number. This FFNN can be a\\nregression or a classiﬁcation model, depending on the activation function used in the third,\\noutput layer.\\nOur MLP is depicted in ﬁg. 1. The neural network is represented graphically as a connected\\ncombination of units logically organized into one or more layers. Each unit is represented by\\neither a circle or a rectangle. The inbound arrow represents an input of a unit and indicates\\nwhere this input came from. The outbound arrow indicates the output of a unit.\\nThe output of each unit is the result of the mathematical operation written inside the circle\\nor a rectangle. Circle units don’t do anything with the input; they just send their input\\ndirectly to the output.\\nThe following happens in each rectangle unit. Firstly, all inputs of the unit are joined together\\nto form an input vector. Then the unit applies a linear transformation to the input vector,\\nexactly like linear regression model does with its input feature vector. Finally, the unit\\napplies an activation function g to the result of the linear transformation and obtains the\\noutput value, a real number. In a vanilla FFNN, the output value of a unit of some layer\\nbecomes an input value of each of the units of the subsequent layer.\\nIn ﬁg. 1, the activation function gl has one index: l, the index of the layer the unit belongs to.\\nUsually, all units of a layer use the same activation function, but it’s not strictly necessary.\\nEach layer can have a diﬀerent number of units. Each unit has its own parameters wl,u\\nand bl,u, where u is the index of the unit, and l is the index of the layer. The vector yl≠1\\nin each unit is deﬁned as [y(1)\\nl≠1, y(2)\\nl≠1, y(3)\\nl≠1, y(4)\\nl≠1]. The vector x in the ﬁrst layer is deﬁned as\\n[x(1), . . . , x(D)].\\nAs you can see in ﬁg. 1, in multilayer perceptron all outputs of one layer are connected to\\neach input of the succeeding layer. This architecture is called fully-connected. A neural\\nnetwork can contain fully-connected layers. Those are the layers whose units receive as\\ninputs the outputs of each of the units of the previous layer.\\n6.1.2\\nFeed-Forward Neural Network Architecture\\nIf we want to solve a regression or a classiﬁcation problem discussed in previous chapters, the\\nlast (the rightmost) layer of a neural network usually contains only one unit. If the activation\\nfunction glast of the last unit is linear, then the neural network is a regression model. If the\\nglast is a logistic function, the neural network is a binary classiﬁcation model.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n4'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 75}, page_content='x(1)\\nx(2)\\ny1(1)\\xa0←\\xa0g1(w1,1x\\xa0+\\xa0b1,1)\\nx(1)\\nx(2)\\ny1(2)\\xa0←\\xa0g1(w1,2x\\xa0+\\xa0b1,2)\\ny1(3)\\xa0←\\xa0g1(w1,3x\\xa0+\\xa0b1,3)\\ny1(4)\\xa0←\\xa0g1(w1,4x\\xa0+\\xa0b1,4)\\ny2(1)\\xa0←\\xa0g2(w2,1y1\\xa0+\\xa0b2,1)\\ny2(2)\\xa0←\\xa0g2(w2,2y1\\xa0+\\xa0b2,2)\\ny2(3)\\xa0←\\xa0g2(w2,3y1\\xa0+\\xa0b2,3)\\ny2(4)\\xa0←\\xa0g2(w2,4y1\\xa0+\\xa0b2,4)\\ny\\xa0←\\xa0g3(w3,1y2\\xa0+\\xa0b3,1)\\ny\\nlayer\\xa03\\xa0(f3)\\xa0\\nlayer\\xa02\\xa0(f2)\\xa0\\nlayer\\xa01\\xa0(f1)\\xa0\\ny1(1)\\ny1(4)\\ny2(4)\\ny2(3)\\ny2(2)\\ny2(1)\\nFigure 1: A multilayer perceptron with two-dimensional input, two layers with four units and one output layer with one\\nunit.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 76}, page_content='The data analyst is free to choose any mathematical function as gl,u, assuming it’s diﬀeren-\\ntiable2. The latter property is essential for gradient descent, which is used to ﬁnd the values\\nof the parameters wl,u and bl,u for all l and u. The primary purpose of having nonlinear\\ncomponents in the function fNN is to allow the neural network to approximate nonlinear\\nfunctions. Without nonlinearities, fNN would be linear, no matter how many layers it has.\\nThe reason is that Wlz + bl is a linear function and a linear function of a linear function is\\nalso a linear function.\\nPopular choices of activation functions are the logistic function, already known to you, as well\\nas TanH and ReLU. The former is the hyperbolic tangent function, similar to the logistic\\nfunction but ranging from ≠1 to 1 (without reaching them). The latter is the rectiﬁed linear\\nunit function, which equals to zero when its input z is negative and to z otherwise:\\ntanh(z) = ez ≠e≠z\\nez + e≠z ,\\nrelu(z) =\\nI\\n0\\nif z < 0\\nz\\notherwise .\\nAs I said above, Wl in the expression Wlz + bl, is a matrix, while bl is a vector. That looks\\ndiﬀerent from linear regression’s wz + b. In matrix Wl, each row u corresponds to a vector of\\nparameters wl,u. The dimensionality of the vector wl,u equals to the number of units in the\\nlayer l ≠1. The operation Wlz results in a vector al\\ndef\\n= [wl,1z, wl,2z, . . . , wl,sizelz]. Then\\nthe sum al + bl gives a sizel-dimensional vector cl. Finally, the function gl(cl) produces the\\nvector yl\\ndef\\n= [y(1)\\nl\\n, y(2)\\nl\\n, . . . , y(sizel)\\nl\\n] as output.\\n6.2\\nDeep Learning\\nDeep learning refers to training neural networks with more than two non-output layers. In the\\npast, it became more diﬃcult to train such networks as the number of layers grew. The two\\nbiggest challenges were referred to as the problems of exploding gradient and vanishing\\ngradient as gradient descent was used to train the network parameters.\\nWhile the problem of exploding gradient was easier to deal with by applying simple techniques\\nlike gradient clipping and L1 or L2 regularization, the problem of vanishing gradient\\nremained intractable for decades.\\nWhat is vanishing gradient and why does it arise? To update the values of parameters in\\nneural networks the algorithm called backpropagation is typically used. Backpropagation\\nis an eﬃcient algorithm for computing gradients on neural networks using the chain rule. In\\nChapter 4, we have already seen how the chain rule is used to calculate partial derivatives of\\n2The function has to be diﬀerentiable across its whole domain or in the majority of the points of its\\ndomain. For example, ReLU is not diﬀerentiable at 0.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n6'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 77}, page_content='a complex function. During gradient descent, the neural network’s parameters receive an\\nupdate proportional to the partial derivative of the cost function with respect to the current\\nparameter in each iteration of training. The problem is that in some cases, the gradient will\\nbe vanishingly small, eﬀectively preventing some parameters from changing their value. In\\nthe worst case, this may completely stop the neural network from further training.\\nTraditional activation functions, such as the hyperbolic tangent function I mentioned above,\\nhave gradients in the range (0, 1), and backpropagation computes gradients by the chain rule.\\nThat has the eﬀect of multiplying n of these small numbers to compute gradients of the earlier\\n(leftmost) layers in an n-layer network, meaning that the gradient decreases exponentially\\nwith n. That results in the eﬀect that the earlier layers train very slowly, if at all.\\nHowever, the modern implementations of neural network learning algorithms allow you to\\neﬀectively train very deep neural networks (up to hundreds of layers). The ReLU activation\\nfunction suﬀers much less from the problem of vanishing gradient. Also, long short-term\\nmemory (LSTM) networks, which we consider below, as well as such techniques as skip\\nconnections used in residual neural networks allow you to train even deeper neural\\nnetworks, with thousands of layers.\\nTherefore, today, since the problems of vanishing and exploding gradient are mostly solved\\n(or their eﬀect diminished) to a great extent, the term “deep learning” refers to training\\nneural networks using the modern algorithmic and mathematical toolkit independently of\\nhow deep the neural network is. In practice, many business problems can be solved with\\nneural networks having 2-3 layers between the input and output layers. The layers that are\\nneither input nor output are often called hidden layers.\\n6.2.1\\nConvolutional Neural Network\\nYou may have noticed that the number of parameters an MLP can have grows very fast as you\\nmake your network bigger. More speciﬁcally, as you add one layer, you add sizel(sizel≠1 + 1)\\nparameters (our matrix Wl plus the vector bl). That means that if you add another 1000-unit\\nlayer to an existing neural network, then you add more than 1 million additional parameters\\nto your model. Optimizing such big models is a very computationally intensive problem.\\nWhen our training examples are images, the input is very high-dimensional. If you want\\nto learn to classify images using an MLP, the optimization problem is likely to become\\nintractable.\\nA convolutional neural network (CNN) is a special kind of FFNN that signiﬁcantly\\nreduces the number of parameters in a deep neural network with many units without losing\\ntoo much in the quality of the model. CNNs have found applications in image and text\\nprocessing where they beat many previously established benchmarks.\\nBecause CNNs were invented with image processing in mind, I explain them on the image\\nclassiﬁcation example.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n7'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 78}, page_content='You may have noticed that in images, pixels that are close to one another usually represent\\nthe same type of information: sky, water, leaves, fur, bricks, and so on. The exception from\\nthe rule are the edges: the parts of an image where two diﬀerent objects “touch” one another.\\nSo, if we can train the neural network to recognize regions of the same information as well\\nas the edges, then this knowledge would allow the neural network to predict the object\\nrepresented in the image. For example, if the neural network detected multiple skin regions\\nand edges that look like parts of an oval with skin-like tone on the inside and bluish tone on\\nthe outside, then it is very likely that there’s a face on the sky background. If our goal is to\\ndetect people on pictures, the neural network will most likely succeed in predicting a person\\nin this picture.\\nHaving in mind that the most important information in the image is local, we can split the\\nimage into square patches using a moving window approach3. We can then train multiple\\nsmaller regression models at once, each small regression model receiving a square patch as\\ninput. The goal of each small regression model is to learn to detect a speciﬁc kind of pattern\\nin the input patch. For example, one small regression model will learn to detect the sky;\\nanother one will detect the grass, the third one will detect edges of a building, and so on.\\nIn CNNs, a small regression model looks like the one in ﬁg. 1, but it only has the layer 1 and\\ndoesn’t have layers 2 and 3. To detect some pattern, a small regression model has to learn\\nthe parameters of a matrix F (for “ﬁlter”) of size p ◊p, where p is the size of a patch. Let’s\\nassume, for simplicity, that the input image is back and white, with 1 representing black and\\n0 representing white pixels. Assume also that our patches are 3 by 3 pixels (p = 3). Some\\npatch could then look like the following matrix P (for “patch”):\\nP =\\nS\\nU\\n0\\n1\\n0\\n1\\n1\\n1\\n0\\n1\\n0\\nT\\nV .\\nThe above patch represents a pattern that looks like a cross. The small regression model that\\nwill detect such patterns (and only them) would need to learn a 3 by 3 parameter matrix F\\nwhere parameters at positions corresponding to the 1s in the input patch would be positive\\nnumbers, while the parameters in positions corresponding to 0s would be close to zero. If\\nwe calculate the dot-product between matrices P and F and then sum all values from the\\nresulting vector, the value we obtain is higher the more similar F is to P. For instance,\\nassume that F looks like this:\\nF =\\nS\\nU\\n0\\n2\\n3\\n2\\n4\\n1\\n0\\n3\\n0\\nT\\nV .\\nThen,\\nP · F = [0 · 0 + 2 · 1 + 0 · 0, 2 · 1 + 4 · 1 + 3 · 1, 3 · 0 + 1 · 1 + 0 · 1] = [2, 9, 1].\\n3Consider this as if you looked at a dollar bill in a microscope. To see the whole bill you have to gradually\\nmove your bill from left to right and from top to bottom. At each moment in time, you see only a part of the\\nbill of ﬁxed dimensions. This approach is called moving window.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n8'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 79}, page_content='Then the sum of all elements of the above vector is 2 + 9 + 1 = 12. This operation — the dot\\nproduct between a patch and a ﬁlter and then summing the values — is called convolution.\\nIf our input patch P had a diﬀerent patten, for example, that of a letter T,\\nP =\\nS\\nU\\n1\\n1\\n1\\n0\\n1\\n0\\n0\\n1\\n0\\nT\\nV ,\\nthen the convolution would give a lower result: 0 + 9 + 0 = 9. So, you can see the more\\nthe patch “looks” like the ﬁlter, the higher the value of the convolution operation is. For\\nconvenience, there’s also a bias parameter b associated with each ﬁlter F which is added to\\nthe result of a convolution before applying the nonlinearity.\\nOne layer of a CNN consists of multiple convolution ﬁlters (each with its own bias parameter),\\njust like one layer in a vanilla FFNN consists of multiple units. Each ﬁlter of the ﬁrst\\n(leftmost) layer slides — or convolves — across the input image, left to right, top to bottom,\\nand convolution is computed at each iteration.\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n-1 \\n2 \\n4 \\n-2 \\nImage\\nFilter \\n4 \\nOutput\\xa0before\\xa0nonlinearity \\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n-1 \\n2 \\n4 \\n-2 \\n4 \\n-1 \\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n-1 \\n2 \\n4 \\n-2 \\n4 \\n-1 \\n7 \\nConv\\xa01\\nConv\\xa02\\nConv\\xa03\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n-1 \\n2 \\n4 \\n-2 \\n4 \\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n-1 \\n2 \\n4 \\n-2 \\n4 \\n-1 \\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n0 \\n-1 \\n2 \\n4 \\n-2 \\n4 \\n-1 \\n7 \\n-1 \\n7 \\n7 \\n2 \\nConv\\xa04\\n2 \\n7 \\n2 \\n7 \\n0 \\nConv\\xa05\\nConv\\xa06\\n1\\nBias\\n1\\n1\\n1\\n1\\n1\\nFigure 2: A ﬁlter convolving across an image.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n9'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 80}, page_content='An illustration of the process is given in ﬁg. 2 where 6 steps of a ﬁlter convolving across an\\nimage are shown.\\nThe numbers in the ﬁlter matrix, for each ﬁlter F in each layer, as well as the value of\\nthe bias term b, are found by the gradient descent with backpropagation, based on data by\\nminimizing the cost function.\\nA nonlinearity is applied to the sum of the convolution and the bias term. Typically, the\\nReLU activation function is used in all hidden layers. The activation function of the output\\nlayer depends on the task.\\nSince we can have sizel ﬁlters in each layer l, the output of the convolution layer l would\\nconsist of sizel matrices, one for each ﬁlter.\\nIf the CNN has one convolution layer following another convolution layer, then the subsequent\\nlayer l + 1 treats the output of the preceding layer l as a collection of sizel image matrices.\\nSuch a collection is called a volume. Each ﬁlter of layer l +1 convolves the whole volume. The\\nconvolution of a patch of a volume is simply the sum of convolutions of the corresponding\\npatches of individual matrices the volume consists of.\\n3\\n4\\n2\\n-2\\n2\\n0\\n4\\n2\\n1 \\n5 \\n0 \\n-1 \\n1 \\n1 \\n-2 \\n1 \\n-2 \\n3 \\n5 \\n-1 \\nFilter \\n-3 \\nOutput\\xa0before\\xa0nonlinearity \\n2\\n-3\\n-1\\n0\\n2\\n0\\n-2\\n-5\\n-3 \\n1 \\n2 \\n2 \\n1 \\n1 \\n3 \\n-1 \\n1\\n2\\n1\\n-3\\n0\\n-1\\n2\\n-2\\n4 \\n0 \\n3 \\n1 \\n1 \\n-1 \\n1 \\n-1 \\n-2\\nBias\\nVolume\\nFigure 3: Convolution of a volume consisting of three matrices.\\nAn example of a convolution of a patch of a volume consisting of three matrices is shown in\\nﬁg. 3. The value of the convolution, ≠3, was obtained as (≠2 · 3 + 3 · 1 + 5 · 4 + ≠1 · 1) +\\n(≠2 · 2 + 3 · (≠1) + 5 · (≠3) + ≠1 · 1) + (≠2 · 1 + 3 · (≠1) + 5 · 2 + ≠1 · (≠1)) + (≠2).\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n10'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 81}, page_content='In computer vision, CNNs often get volumes as input, since an image is usually represented\\nby three channels: R, G, and B, each channel being a monochrome picture.\\nBy now, you should have a good high-level understanding of the CNN\\narchitecture. We didn’t discuss some essential features of CNNs though,\\nsuch as strides, padding, and pooling. Strides and padding are two\\nimportant hyperparameters of the convolution ﬁlter and the sliding\\nwindow, while pooling is a technique that works very well in practice\\nby reducing the number of parameters of a CNN even more.\\n6.2.2\\nRecurrent Neural Network\\nRecurrent neural networks (RNNs) are used to label, classify, or generate sequences. A\\nsequence is a matrix, each row of which is a feature vector and the order of rows matters.\\nLabeling a sequence means predicting a class to each feature vector in a sequence. Classifying\\na sequence means predicting a class for the entire sequence. Generating a sequence means\\nto output another sequence (of a possibly diﬀerent length) somehow relevant to the input\\nsequence.\\nRNNs are often used in text processing because sentences and texts are naturally sequences\\nof either words/punctuation marks or sequences of characters. For the same reason, recurrent\\nneural networks are also used in speech processing.\\nA recurrent neural network is not feed-forward, because it contains loops. The idea is that\\neach unit u of recurrent layer l has a real-valued state hl,u. The state can be seen as the\\nmemory of the unit. In RNN, each unit u in each recurrent layer l receives two inputs: a\\nvector of outputs from the previous layer l ≠1 and the vector of states from this same layer l\\nfrom the previous time step.\\nTo illustrate the idea, let’s consider the ﬁrst and the second recurrent layers of an RNN. The\\nﬁrst (leftmost) layer receives a feature vector as input. The second layer receives the output\\nof the ﬁrst layer as input.\\nThis situation is schematically depicted in ﬁg. 4. As I said above, each training example is\\na matrix in which each row is a feature vector. For simplicity, let’s illustrate this matrix\\nas a sequence of vectors X = [x1, x2, . . . , xt≠1, xt, xt+1, . . . , xlengthX], where lengthX is the\\nlength of the input sequence. If our input example X is a text sentence, then feature vector\\nxt for each t = 1, . . . , lengthX represents a word in the sentence at position t.\\nAs depicted in ﬁg. 4, in an RNN, the input example is “read” by the neural network one\\nfeature vector at a timestep. The index t denotes a timestep. To update the state ht\\nl,u at each\\ntimestep t in each unit u of each layer l we ﬁrst calculate a linear combination of the input\\nfeature vector with the state vector ht≠1\\nl,u of this same layer from the previous timestep, t ≠1.\\nThe linear combination of two vectors is calculated using two parameter vectors wl,u, ul,u\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n11'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 82}, page_content='xt\\xa0←\\xa0[x(1),t,x(2),t]\\nx(2),t\\nht\\xad1\\n1,1\\nht\\xad1\\nl,2\\nht\\n1,1\\nht1,1\\xa0←\\xa0g1(w1,1xt\\xa0+\\xa0u1,1ht\\xad11\\xa0+\\xa0b1,1)\\nyt\\n1\\xa0←\\xa0g2(V1ht\\n1\\xa0+\\xa0c1)\\nx(1),t\\nx(2),t\\nht\\xad11,1\\nht\\xad11,2\\nht1,2\\nht1,2\\xa0←\\xa0g1(w1,2xt\\xa0+\\xa0u1,2ht\\xad1\\n1\\xa0+\\xa0b1,2)\\nx(1),t\\nxt\\xa0←\\xa0[x(1),t,x(2),t]\\nlayer\\xa01\\nht1\\xa0←\\xa0[ht1,1,ht1,2]\\nht\\xad1\\n2,1\\nht\\xad1\\n2,2\\nht\\n2,1\\nht2,1\\xa0←\\xa0g1(w2,1h1t\\xa0+\\xa0u2,1ht\\xad12\\xa0+\\xa0b2,1)\\nht\\xad12,1\\nht\\xad12,2\\nht2,2\\nlayer\\xa02\\nht1\\xa0←\\xa0[ht1,1,ht1,2]\\nyt2\\xa0←\\xa0g2(V2ht2\\xa0+\\xa0c2)\\nht2\\xa0←[ht2,1,ht2,2]\\nyt1\\nyt2\\nht\\n1\\xa0←\\xa0[ht\\n1,1,ht1,2]\\nht\\n2,1\\xa0←\\xa0g1(w2,2ht\\n1\\xa0+\\xa0u2,2ht\\xad12\\xa0+\\xa0b2,2)\\nFigure 4: The ﬁrst two layers of an RNN. The input feature vector is two-dimensional; each layer has two units.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n12'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 83}, page_content='and a parameter bl,u. The value of ht\\nl,u is then obtained by applying an activation function\\ng1 to the result of the linear combination. A typical choice for function g1 is tanh. The\\noutput yt\\nl is typically a vector calculated for the whole layer l at once. To obtain yt\\nl, we use\\nan activation function g2 that takes a vector as input and returns a diﬀerent vector of the\\nsame dimensionality. The function g2 is applied to a linear combination of the state vector\\nvalues ht\\nl,u calculated using a parameter matrix Vl and a parameter vector cl,u. A typical\\nchoice for g2 is the softmax function:\\n‡(z)\\ndef\\n= [‡(1), . . . , ‡(D)], where ‡(j) def\\n=\\nexp\\n!\\nz(j)\"\\nqD\\nk=1 exp\\n!\\nz(k)\".\\nThe softmax function is a generalization of the sigmoid function to multidimensional data. It\\nhas the property that qD\\nj=1 ‡(j) = 1 and ‡(j) > 0 for all j.\\nThe dimensionality of Vl is chosen by the data analyst such that multiplication of matrix Vl\\nby the vector ht\\nl results in a vector of the same dimensionality as that of the vector cl. This\\nchoice depends on the dimensionality for the output label y in your training data. (Until\\nnow we only saw one-dimensional labels, but we will see in the future chapters that labels\\ncan be multidimensional as well.)\\nThe values of wl,u, ul,u, bl,u, Vl,u, and cl,u are computed from the training data using gradient\\ndescent with backpropagation. To train RNN models, a special version of backpropagation is\\nused called backpropagation through time.\\nBoth tanh and softmax suﬀer from the vanishing gradient problem. Even if our RNN has just\\none or two recurrent layers, because of the sequential nature of the input, backpropagation\\nhas to “unfold” the network over time. From the point of view of the gradient calculation, in\\npractice this means that the longer is the input sequence, the deeper is the unfolded network.\\nAnother problem RNNs have is that of handling long-term dependencies. As the length of\\nthe input sequence grows, the feature vectors from the beginning of the sequence tend to\\nbe “forgotten,” because the state of each unit, which serves as network’s memory, becomes\\nsigniﬁcantly aﬀected by the feature vectors read more recently. Therefore, in text or speech\\nprocessing, the cause-eﬀect link between distant words in a long sentence can be lost.\\nThe most eﬀective recurrent neural network models used in practice are gated RNNs. These\\ninclude the long short-term memory (LSTM) networks and networks based on the gated\\nrecurrent unit (GRU).\\nThe beauty of using gated units in RNNs is that such networks can store information in their\\nunits for future use, much like bits in a computer’s memory. The diﬀerence with the real\\nmemory is that reading, writing, and erasure of information stored in each unit is controlled\\nby activation functions that take values in the range (0, 1). The trained neural network can\\n“read” the input sequence of feature vectors and decide at some early time step t to keep\\nspeciﬁc information about the feature vectors. That information about the earlier feature\\nvectors can later be used by the model to process the feature vectors from near the end of\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n13'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 84}, page_content='the input sequence. For example, if the input text starts with the word she, a language\\nprocessing RNN model could decide to store the information about the gender to interpret\\ncorrectly the word her seen later in the sentence.\\nUnits make decisions about what information to store, and when to allow reads, writes, and\\nerasures. These decisions are learned from data and implemented through the concept of\\ngates. There are several architectures of gated units. The simplest one (working well in\\npractice) is called the minimal gated GRU and is composed of a memory cell, and a forget\\ngate.\\nLet’s look at the math of a GRU unit on an example of the ﬁrst layer of the RNN (the one\\nthat takes the sequence of feature vectors as input). A minimal gated GRU unit u in layer\\nl takes two inputs: the vector of the memory cell values from all units in the same layer\\nfrom the previous timestep, ht≠1\\nl\\n, and a feature vector xt. It then uses these two vectors like\\nfollows (all operations in the below sequence are executed in the unit one after another):\\n˜ht\\nl,u Ω g1(wl,uxt + ul,uht≠1\\nl\\n+ bl,u),\\nΓt\\nl,u Ω g2(ml,uxt + ol,uht≠1 + al,u),\\nht\\nl,u Ω Γt\\nl,u˜ht\\nl + (1 ≠Γt\\nl,u)ht≠1\\nl\\n,\\nht\\nl Ω [ht\\nl,1, . . . , ht\\nl,sizel]\\nyt\\nl Ω g3(Vlht\\nl + cl,u),\\nwhere g1 is the tanh activation function, g2 is called the gate function and is implemented as\\nthe sigmoid function. The sigmoid function takes values in the range of (0, 1). If the gate\\nΓl,u is close to 0, then the memory cell keeps its value from the previous time step, ht≠1\\nl\\n. On\\nthe other hand, if the gate Γl,u is close to 1, the value of the memory cell is overwritten by a\\nnew value ˜ht\\nl,u (this happens in the third assignment from the top). Just like in standard\\nRNNs, g3 is usually softmax.\\nA gated unit takes an input and stores it for some time. This is equivalent to applying the\\nidentity function (f(x) = x) to the input. Because the derivative of the identity function is\\nconstant, when a network with gated units is trained with backpropagation through time,\\nthe gradient does not vanish.\\nOther important extensions to RNNs include bi-directional RNNs,\\nRNNs with attention and sequence-to-sequence RNN models.\\nSequence-to-sequence RNNs, in particular, are frequently used to build\\nneural machine translation models and other models for text to text\\ntransformations.\\nA generalization of RNNs is a recursive neural\\nnetwork model.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n14'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 85}, page_content='The\\nHundred-\\nPage\\nMachine\\nLearning\\nBook\\nAndriy Burkov'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 86}, page_content='“All models are wrong, but some are useful.”\\n— George Box\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 87}, page_content='7\\nProblems and Solutions\\n7.1\\nKernel Regression\\nWe talked about linear regression, but what if our data doesn’t have the form of a straight\\nline? Polynomial regression could help. Let’s say we have a one-dimensional data {(xi, yi)}N\\ni=1.\\nWe could try to ﬁt a quadratic line y = w1xi + w2x2\\ni + b to our data. By deﬁning the mean\\nsquared error cost function, we could apply gradient descent and ﬁnd the values of parameters\\nw1, w2, and b that minimize this cost function. In one- or two-dimensional space, we can\\neasily see whether the function ﬁts the data. However, if our input is a D-dimensional feature\\nvector, with D > 3, ﬁnding the right polynomial would be hard.\\nKernel regression is a non-parametric method. That means that there are no parameters to\\nlearn. The model is based on the data itself (like in kNN). In its simplest form, in kernel\\nregression we look for a model like this:\\nf(x) = 1\\nN\\nN\\nÿ\\ni=1\\nwiyi, where wi =\\nNk( xi≠x\\nb\\n)\\nqN\\nk=1 k( xk≠x\\nb\\n)\\n.\\n(1)\\nThe function k(·) is a kernel. It can have diﬀerent forms, the most frequently used one is the\\nGaussian kernel:\\nk(z) =\\n1\\nÔ\\n2ﬁexp\\n3≠z2\\n2\\n4\\n.\\nGood ﬁt\\nSlight overﬁt\\nStrong overﬁt\\nFigure 1: Example of kernel regression line with a Gaussian kernel for three values of b.\\nThe value b is a hyperparameter that we tune using the validation set (by running the model\\nbuilt with a speciﬁc value of b on the validation set examples and calculating the mean\\nsquared error). You can see an illustration of the inﬂuence b has on the shape of the regression\\nline in ﬁg. 1.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 88}, page_content='If your inputs are multi-dimensional feature vectors, the terms xi ≠x and xk ≠x in eq. 1\\nhave to be replaced by Euclidean distance Îxi ≠xÎ and Îxk ≠xÎ respectively.\\n7.2\\nMulticlass Classiﬁcation\\nIn multiclass classiﬁcation, the label can be one of the C classes: y œ {1, . . . , C}. Many\\nmachine learning algorithms are binary; SVM is an example. Some algorithms can naturally\\nbe extended to handle multiclass problems. ID3 and other decision tree learning algorithms\\ncan be simply changed like this:\\nf S\\nID3\\ndef\\n= Pr(yi = c|x) = 1\\n|S|\\nÿ\\n{y | (x,y)œS,y=c}\\ny,\\nfor all c œ {1, . . . , C}.\\nLogistic regression can be naturally extended to multiclass learning problems by replacing\\nthe sigmoid function with the softmax function which we already saw in Chapter 6.\\nThe kNN algorithm is also straightforward to extend to the multiclass case: when we ﬁnd\\nthe k closest examples for the input x and examine them, we return the class that we saw\\nthe most among the k examples.\\nSVM cannot be naturally extended to multiclass problems. Some algorithms can be imple-\\nmented more eﬃciently in the binary case. What should you do if you have a multiclass\\nproblem but a binary classiﬁcation learning algorithm? One common strategy is called one\\nversus rest. The idea is to transform a multiclass problem into C binary classiﬁcation\\nproblems and build C binary classiﬁers. For example, if we have three classes, y œ {1, 2, 3},\\nwe create copies of the original datasets and modify them. In the ﬁrst copy, we replace all\\nlabels not equal to 1 by 0. In the second copy, we replace all labels not equal to 2 by 0. In the\\nthird copy, we replace all labels not equal to 3 by 0. Now we have three binary classiﬁcation\\nproblems where we have to learn to distinguish between labels 1 and 0, 2 and 0, and between\\nlabels 3 and 0.\\nOnce we have the three models and we need to classify the new input feature vector x,\\nwe apply the three models to the input, and we get three predictions. We then pick the\\nprediction of a non-zero class which is the most certain. Remember that in logistic regression,\\nthe model returns not a label but a score (0, 1) that can be interpreted as the probability\\nthat the label is positive. We can also interpret this score as the certainty of prediction. In\\nSVM, the analog of certainty is the distance from the input x to the decision boundary. This\\ndistance is given by,\\nd = wúx + bú\\nÎwÎ\\n.\\nThe larger the distance, the more certain is the prediction. Most learning algorithm either\\ncan be naturally converted to a multiclass case, or they return a score we can use in the one\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n4'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 89}, page_content='versus rest strategy.\\n7.3\\nOne-Class Classiﬁcation\\nOne-class classiﬁcation, also known as unary classiﬁcation or class modeling, tries to\\nidentify objects of a speciﬁc class among all objects, by learning from a training set containing\\nonly the objects of that class. That is diﬀerent from and more diﬃcult than the traditional\\nclassiﬁcation problem, which tries to distinguish between two or more classes with the\\ntraining set containing objects from all classes. A typical one-class classiﬁcation problem is\\nthe classiﬁcation of the traﬃc in a secure network as normal. In this scenario, there are few,\\nif any, examples of the traﬃc under an attack or during an intrusion. However, the examples\\nof normal traﬃc are often in abundance. One-class classiﬁcation learning algorithms are used\\nfor outlier detection, anomaly detection, and novelty detection.\\nThere are several one-class learning algorithms.\\nThe most widely used in practice are\\none-class Gaussian, one-class kmeans, one-class kNN, and one-class SVM.\\nThe idea behind the one-class gaussian is that we model our data as if it came from a Gaussian\\ndistribution, more precisely multivariate normal distribution (MND). The probability density\\nfunction (pdf) for MND is given by the following equation:\\nfµ,Σ(x) = exp\\n!\\n≠1\\n2(x ≠µ)TΣ≠1(x ≠µ)\\n\"\\n\\uf8ff\\n(2ﬁ)D|Σ|\\n,\\nwhere fµ,Σ(x) returns the probability density corresponding to the input feature vector x.\\nProbability density can be interpreted as the likelihood that example x was drawn from the\\nprobability distribution we model as an MND. Values µ (a vector) and Σ (a matrix) are\\nthe parameters we have to learn. The maximum likelihood criterion (similarly to how\\nwe solved the logistic regression learning problem) is optimized to ﬁnd the optimal values\\nfor these two parameters. |Σ| = det Σ is the determinant of the matrix Σ; the notation aT\\nmeans the transpose of the vector a, and Σ≠1 is the inverse of the matrix Σ.\\nIf the terms determinant, transpose, and inverse are new to you, don’t worry. These are\\nstandard operations on vector and matrices from the branch of mathematics called matrix\\ntheory. If you feel the need to know what they are, Wikipedia explains these concepts very\\nwell.\\nIn practice, the numbers in the vector µ determine the place where the curve of our Gaussian\\ndistribution is centered, while the numbers in Σ determine the shape of the curve. For\\na training set consisting of two-dimensional feature vectors, an example of the one-class\\nGaussian model is given in ﬁg 2.\\nOnce we have our model parametrized by µ and Σ learned from the data, we predict the\\nlikelihood of every input x by using fµ,Σ(x). Only if the likelihood is above a certain\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 90}, page_content='Figure 2: One-class classiﬁcation solved using the one-class gaussian method. Left: two-\\ndimensional feature vectors. Right: the MND curve that maximizes the likelihood of the\\nexamples on the left.\\nthreshold, we predict that the example belongs to our class; otherwise, it is classiﬁed as the\\noutlier. The value of the threshold is found experimentally or using an “educated guess.”\\nWhen the data has a more complex shape, a more advanced algorithm can use a combination\\nof several Gaussians (called a mixture of Gaussians). In this case, there are more parameters\\nto learn from data: one µ and one Σ for each Gaussian as well as the parameters that allow\\ncombining multiple Gaussians to form one pdf. In Chapter 9, we consider a mixture of\\nGaussians with an application to clustering.\\nOne-class kmeans and one-class kNN are based on a similar principle as that of one-class\\nGaussian: build some model of the data and then deﬁne a threshold to decide whether our\\nnew feature vector looks similar to other examples according to the model. In the former,\\nall training examples are clustered using the kmeans clustering algorithm and, when a new\\nexample x is observed, the distance d(x) is calculated as the minimum distance between x\\nand the center of each cluster. If d(x) is less than a particular threshold, then x belongs to\\nthe class.\\nOne-class SVM, depending on formulation, tries either 1) to separate all\\ntraining examples from the origin (in the feature space) and maximize\\nthe distance from the hyperplane to the origin, or 2) to obtain a spherical\\nboundary around the data by minimizing the volume of this hypersphere.\\nI leave the description of the one-class kNN algorithm, as well as the\\ndetails of the one-class kmeans and one-class SVM for the complementary\\nreading.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n6'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 91}, page_content='Figure 3: A picture labeled as “people”, “concert”, and “nature”.\\n7.4\\nMulti-Label Classiﬁcation\\nIn multi-label classiﬁcation, each training example doesn’t just have one label, but several\\nof them. For instance, if we want to describe an image, we could assign several labels to it:\\n“people,” “concert,” “nature,” all three at the same time (ﬁg. 3).\\nIf the number of possible values for labels is high, but they are all of the same nature, like\\ntags, we can transform each labeled example into several labeled examples, one per label.\\nThese new examples all have the same feature vector and only one label. That becomes a\\nmulticlass classiﬁcation problem. We can solve it using the one versus rest strategy. The\\nonly diﬀerence with the usual multiclass problem is that now we have a new hyperparameter:\\nthreshold. If the prediction score for some label is above the threshold, this label is predicted\\nfor the input feature vector. In this scenario, multiple labels can be predicted for one feature\\nvector. The value of the threshold is chosen using the validation set.\\nAnalogously, algorithms that naturally can be made multiclass (decision trees, logistic\\nregression and neural networks among others) can be applied to multi-label classiﬁcation\\nproblems. Because they return the score for each class, we can deﬁne a threshold and then\\nassign multiple labels to one feature vector if the threshold is above some value chosen\\nexperimentally using the validation set.\\nNeural networks algorithms can naturally train multi-label classiﬁcation models by using the\\nbinary cross-entropy cost function. The output layer of the neural network, in this case,\\nhas one unit per label. Each unit of the output layer has the sigmoid activation function.\\nAccordingly, each label l is binary (yi,l œ {0, 1}), where l = 1, . . . , L and i = 1, . . . , N. The\\nbinary cross-entropy of predicting the probability ˆyi,l that example xi has label l is deﬁned\\nas ≠(yi,l ln(ˆyi,l) + (1 ≠yi,l) ln(1 ≠ˆyi,l)). The minimization criterion is simply the average of\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n7'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 92}, page_content='all binary cross-entropy terms across all training examples and all labels of those examples.\\nIn cases where the number of possible values each label can take is small, one can convert\\nmultilabel into a multiclass problem using a diﬀerent approach. Imagine the following problem.\\nWe want to label images and labels can be of two types. The ﬁrst type of label can have\\ntwo possible values: {photo, painting}; the label of the second type can have three possible\\nvalues {portrait, paysage, other}. We can create a new fake class for each combination of\\nthe two original classes, like this:\\nFake Class\\nReal Class 1\\nReal Class 2\\n1\\nphoto\\nportrait\\n2\\nphoto\\npaysage\\n3\\nphoto\\nother\\n4\\npainting\\nportrait\\n5\\npainting\\npaysage\\n6\\npainting\\nother\\nNow we have the same labeled examples, but we replace real multi-labels with one fake label\\nwith values from 1 to 6. This approach works well in practice when there are not too many\\npossible combinations of classes. Otherwise, you need to use much more training data to\\ncompensate for an increased set of classes.\\nThe primary advantage of this latter approach is that you keep your labels correlated,\\ncontrary to the previously seen methods that predict each label independently of one another.\\nCorrelation between labels can be an essential property in many problems. For example, if\\nyou want to predict for an email message whether it’s spam or not_spam at the same time\\nas you predict whether it’s ordinary or priority email. You would like to avoid predictions\\nlike [spam, priority].\\n7.5\\nEnsemble Learning\\nEnsemble learning is a learning paradigm that, instead of trying to learn one super-accurate\\nmodel, focuses on training a large number of low-accuracy models and then combining the\\npredictions given by those weak models to obtain a high-accuracy meta-model.\\nLow-accuracy models are usually learned by weak learners, that is learning algorithms that\\ncannot learn complex models, and thus are typically fast at the training and at the prediction\\ntime. The most frequently used weak learner is a decision tree learning algorithm in which\\nwe often stop splitting the training set after just a few iterations. The obtained trees are\\nshallow and not particularly accurate, but the idea behind ensemble learning is that if the\\ntrees are not identical and each tree is at least slightly better than random guessing, then we\\ncan obtain high accuracy by combining a large number of such trees.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n8'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 93}, page_content='To obtain the prediction for input x, the predictions of each weak model are combined using\\nsome sort of weighted voting. The speciﬁc form of vote weighting depends on the algorithm,\\nbut, independently of the algorithm, the idea is the same: if the council of weak models\\npredicts that the message is spam, then we assign the label spam to x.\\nTwo most widely used and eﬀective ensemble learning algorithms are random forest and\\ngradient boosting.\\n7.5.1\\nRandom Forest\\nThere are two ensemble learning paradigms: bagging and boosting. Bagging consists of\\ncreating many “copies” of the training data (each copy is slightly diﬀerent from another) and\\nthen apply the weak learner to each copy to obtain multiple weak models and then combine\\nthem. The bagging paradigm is behind the random forest learning algorithm.\\nThe “vanilla” bagging algorithm works like follows. Given a training set, we create B random\\nsamples Sb (for each b = 1, . . . , B) of the training set and build a decision tree model fb\\nusing each sample Sb as the training set. To sample Sb for some b, we do the sampling with\\nreplacement. This means that we start with an empty set, and then pick at random an\\nexample from the training set and put its exact copy to Sb by keeping the original example\\nin the original training set. We keep picking examples at random until the |Sb| = N.\\nAfter training, we have B decision trees. The prediction for a new example x is obtained as\\nthe average of B predictions:\\ny Ω ˆf(x)\\ndef\\n= 1\\nB\\nB\\nÿ\\nb=1\\nfb(x),\\nin the case of regression, or by taking the majority vote in the case of classiﬁcation.\\nThe random forest algorithm is diﬀerent from the vanilla bagging in just one way. It uses\\na modiﬁed tree learning algorithm that inspects, at each split in the learning process, a\\nrandom subset of the features. The reason for doing this is to avoid the correlation of the\\ntrees: if one or a few features are very strong predictors for the target, these features will\\nbe selected to split examples in many trees. This would result in many correlated trees in\\nour “forest.” Correlated predictors cannot help in improving the accuracy of prediction. The\\nmain reason behind a better performance of model ensembling is that models that are good\\nwill likely agree on the same prediction, while bad models will likely disagree on diﬀerent\\nones. Correlation will make bad models more likely to agree, which will hamper the majority\\nvote or the average.\\nThe most important hyperparameters to tune are the number of trees, B, and the size of the\\nrandom subset of the features to consider at each split.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n9'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 94}, page_content='Random forest is one of the most widely used ensemble learning algorithms. Why is it so\\neﬀective? The reason is that by using multiple samples of the original dataset, we reduce\\nthe variance of the ﬁnal model. Remember that the low variance means low overﬁtting.\\nOverﬁtting happens when our model tries to explain small variations in the dataset because our\\ndataset is just a small sample of the population of all possible examples of the phenomenon we\\ntry to model. If we were unlucky with how our training set was sampled, then it could contain\\nsome undesirable (but unavoidable) artifacts: noise, outliers and over- or underrepresented\\nexamples. By creating multiple random samples with replacement of our training set, we\\nreduce the eﬀect of these artifacts.\\n7.5.2\\nGradient Boosting\\nAnother eﬀective ensemble learning algorithm is gradient boosting. Let’s ﬁrst look at gradient\\nboosting for regression. To build a strong regressor, we start with a constant model f = f0\\n(just like we did in ID3):\\nf = f0(x)\\ndef\\n= 1\\nN\\nN\\nÿ\\ni=1\\nyi.\\nThen we modify labels of each example i = 1, . . . , N in our training set like follows:\\nˆyi Ω yi ≠f(xi),\\n(2)\\nwhere ˆyi, called the residual, is the new label for example xi.\\nNow we use the modiﬁed training set, with residuals instead of original labels, to build a new\\ndecision tree model, f1. The boosting model is now deﬁned as f\\ndef\\n= f0 + –f1, where – is the\\nlearning rate (a hyperparameter).\\nThen we recompute the residuals using eq. 2 and replace the labels in the training data once\\nagain, train the new decision tree model f2, redeﬁne the boosting model as f\\ndef\\n= f0+–f1+–f2\\nand the process continues until the maximum of M (another hyperparameter) trees are\\ncombined.\\nIntuitively, what’s happening here? By computing the residuals, we ﬁnd how well (or poorly)\\nthe target of each training example is predicted by the current model f. We then train\\nanother tree to ﬁx the errors of the current model (this is why we use residuals instead if\\nreal labels) and add this new tree to the existing model with some weight –. Therefore, each\\nadditional tree added to the model partially ﬁxes the errors made by the previous trees until\\nthe maximum number of trees are combined.\\nNow you should reasonably ask why the algorithm is called gradient boosting? In gradient\\nboosting, we don’t calculate any gradient contrary to what we did in Chapter 4 for linear\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n10'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 95}, page_content='regression. To see the similarity between gradient boosting and gradient descent remember\\nwhy we calculated the gradient in linear regression: we did that to get an idea on where we\\nshould move the values of our parameters so that the MSE cost function reaches its minimum.\\nThe gradient showed the direction, but we didn’t know how far we should go in this direction,\\nso we used a small step – at each iteration and then reevaluated our direction. The same\\nhappens in gradient boosting. However, instead of getting the gradient directly, we use its\\nproxy in the form of residuals: they show us how the model has to be adjusted so that the\\nerror (the residual) is reduced.\\nThe three principal hyperparameters to tune in gradient boosting are the number of trees,\\nthe learning rate, and the depth of trees — all three aﬀect model accuracy. The depth of\\ntrees also aﬀects the speed of training and prediction: the shorter, the faster.\\nIt can be shown that training on residuals optimizes the overall model f for the mean squared\\nerror criterion. You can see the diﬀerence with bagging here: boosting reduces the bias (or\\nunderﬁtting) instead of the variance. As such, boosting can overﬁt. However, by tuning the\\ndepth and the number of trees, overﬁtting can be largely avoided.\\nThe gradient boosting algorithm for classiﬁcation is similar, but the steps are slightly diﬀerent.\\nLet’s consider the binary case. Assume we have M regression decision trees. Similarly to\\nlogistic regression, the prediction of the ensemble of decision trees is modeled using the\\nsigmoid function:\\nPr(y = 1|x, f)\\ndef\\n=\\n1\\n1 + e≠f(x) ,\\nwhere f(x) = qM\\nm=1 fm(x) and fm is a regression tree.\\nAgain, like in logistic regression, we apply the maximum likelihood principle by trying to\\nﬁnd such an f that maximizes Lf = qN\\ni=1 ln(Pr(yi = 1|xi, f)). Again, to avoid numerical\\noverﬂow, we maximize the sum of log-likelihoods rather than the product of likelihoods.\\nThe algorithm starts with the initial constant model f = f0 =\\np\\n1≠p, where p =\\n1\\nN\\nqN\\ni=1 yi.\\n(It can be shown that such initialization is optimal for the sigmoid function.) Then at each\\niteration m, a new tree fm is added to the model. To ﬁnd the best fm, ﬁrst the partial\\nderivative gi of the current model is calculated for each i = 1, . . . , N:\\ngi = dLf\\ndf ,\\nwhere f is the ensemble classiﬁer model built at the previous iteration m ≠1. To calculate gi\\nwe need to ﬁnd the derivatives of ln(Pr(yi = 1|xi, f)) with respect to f for all i. Notice that\\nln(Pr(yi = 1|xi, f))\\ndef\\n= ln(\\n1\\n1+e≠f(xi) ). The derivative of the right-hand term in the previous\\nequation with respect to f equals to\\n1\\nef(xi)+1.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n11'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 96}, page_content='We then transform our training set by replacing the original label yi with the corresponding\\npartial derivative gi, and we build a new tree fm using the transformed training set. Then\\nwe ﬁnd the optimal update step ﬂm as:\\nﬂm = arg max\\nﬂ\\nLf+ﬂfm.\\nAt the end of iteration m, we update the ensemble model f by adding the new tree fm:\\nf Ω f + –ﬂmfm.\\nWe iterate until m = M, then we stop and return the ensemble model f.\\nGradient boosting is one of the most powerful machines learning algorithms. Not just because\\nit creates very accurate models, but also because it is capable of handling huge datasets with\\nmillions of examples and features. It usually outperforms random forest in accuracy but,\\nbecause of its sequential nature, can be signiﬁcantly slower in training.\\n7.6\\nLearning to Label Sequences\\nA sequence is one the most frequently observed types of structured data. We communicate\\nusing sequences of words and sentences, we execute tasks in sequences, our genes, the music\\nwe listen and videos we watch, our observations of a continuous process, such as a moving\\ncar or the price of a stock are all sequential.\\nIn sequence labeling, a labeled sequential example is a pair of lists (X, Y ), where X is a list\\nof feature vectors, one per time step, Y is a list of the same length of labels. For example, X\\ncould represent words in a sentence such as [“big”, “beautiful”, “car”], and Y would be the\\nlist of the corresponding parts of speech, such as [“adjective”, “adjective”, “noun”]). More\\nformally, in an example i, Xi = [x1\\ni , x2\\ni , . . . , xsizei\\ni\\n], where sizei is the length of the sequence\\nof the example i, Yi = [y1\\ni , y2\\ni , . . . , ysizei\\ni\\n] and yi œ {1, 2, . . . , C}.\\nYou have already seen that an RNN can be used to annotate a sequence. At each time step t,\\nit reads an input feature vector x(t)\\ni , and the last recurrent layer outputs a label y(t)\\nlast (in the\\ncase of binary labeling) or y(t)\\nlast (in the case of multiclass or multilabel labeling).\\nHowever, RNN is not the only possible model for sequence labeling. The model called\\nConditional Random Fields (CRF) is a very eﬀective alternative that often performs well\\nin practice for the feature vectors that have many informative features. For example, imagine\\nwe have the task of named entity extraction and we want to build a model that would\\nlabel each word in the sentence such as “I go to San Francisco” with one of the following\\nclasses: {location, name, company_name, other}. If our feature vectors (which represent\\nwords) contain such binary features as “whether or not the word starts with a capital letter”\\nand “whether or not the word can be found in the list of locations,” such features would\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n12'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 97}, page_content='be very informative and help to classify the words San and Francisco as location. Building\\nhandcrafted features is known to be a labor-intensive process that requires a signiﬁcant level\\nof domain expertise.\\nCRF is an interesting model and can be seen as a generalization of\\nlogistic regression to sequences.\\nHowever, in practice, it has been\\noutperformed by bidirectional deep gated RNN for sequence labeling\\ntasks. CRFs are also signiﬁcantly slower in training which makes them\\ndiﬃcult to apply to large training sets (with hundreds of thousands of\\nexamples). Additionally, a large training set is where a deep neural\\nnetwork thrives.\\n7.7\\nSequence-to-Sequence Learning\\nSequence-to-sequence learning (often abbreviated as seq2seq learning) is a generalization\\nof the sequence labeling problem. In seq2seq, Xi and Yi can have diﬀerent length. seq2seq\\nmodels have found application in machine translation (where, for example, the input is\\nan English sentence, and the output is the corresponding French sentence), conversational\\ninterfaces (where the input is a question typed by the user, and the output is the answer\\nfrom the machine), text summarization, spelling correction, and many others.\\nMany but not most sequence-to-sequence learning problems are currently best solved by\\nneural networks. Machine translation is a notorious example. There are multiple neural\\nnetwork architectures for seq2seq which perform better than others depending on the task.\\nAll those network architectures have one property in common: they have two parts, an\\nencoder and a decoder (for this reason they are also known as encoder-decoder neural\\nnetworks).\\nIn seq2seq learning, the encoder is a neural network that accepts sequential input. It can\\nbe an RNN, but also a CNN or some other architecture. The role of the encoder is to read\\nthe input and generate some sort of state (similar to the state in RNN) that can be seen\\nas a numerical representation of the meaning of the input the machine can work with. The\\nmeaning of some entity, whether it be an image, a text or a video, is usually a vector or a\\nmatrix that contains real numbers. This vector (or matrix) is called in the machine learning\\njargon the embedding of the input.\\nThe decoder in seq2seq learning is another neural network that takes an embedding as input\\nand is capable of generating a sequence of outputs. As you could have already guessed, that\\nembedding comes from the encoder. To produce a sequence of outputs, the decoder takes a\\nstart of sequence input feature vector x(0) (typically all zeroes), produces the ﬁrst output\\ny(1), updates its state by combining the embedding and the input x(0), and then uses the\\noutput y(1) as its next input x(1). For simplicity, the dimensionality of y(t) can be the same\\nas that of x(t); however, it is not strictly necessary. As we saw in Chapter 6, each layer of an\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n13'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 98}, page_content='RNN can produce many simultaneous outputs: one can be used to generate the label y(t),\\nwhile another one, of diﬀerent dimensionality, can be used as the x(t).\\nEncoder\\nDecoder\\nThe\\nweather\\nis\\nfine\\n<\\xa0start\\xa0>\\nIl\\nfait\\nbeau\\nt\\xa0=\\n1\\n2\\n3\\n4\\n1\\n2\\n3\\nFigure 4: A traditional seq2seq architecture.\\nBoth encoder and decoder are trained simultaneously using the training data. The errors at\\nthe decoder output are propagated to the encoder via backpropagation.\\nA traditional seq2seq architecture is illustrated in ﬁg. 4. More accurate predictions can be\\nobtained using an architecture with attention. Attention mechanism is implemented by an\\nadditional set of parameters that combine some information from the encoder (in RNNs,\\nthis information is the list of state vectors of the last recurrent layer from all encoder time\\nsteps) and the current state of the decoder to generate the label. That allows for even better\\nretention of long-term dependencies than provided by gated units and bidirectional RNN. A\\nseq2seq architecture with attention is illustrated in ﬁg. 5.\\nSequence-to-sequence learning is a relatively new research domain. Novel\\nnetwork architectures are regularly discovered and published. Training\\nsuch architectures can be challenging as the number of hyperparame-\\nters to tune and other architectural decisions can be overwhelming. I\\nrecommend consulting the book’s wiki for the state of the art material,\\ntutorials and code samples.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n14'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 99}, page_content='The\\nweather\\nis\\nfine\\n<\\xa0start\\xa0>\\nIl\\nfait\\nbeau\\nt\\xa0=\\n1\\n2\\n3\\n4\\n1\\n2\\n3\\nAttention\\nFigure 5: A seq2seq architecture with attention.\\n7.8\\nActive Learning\\nActive learning is an interesting supervised learning paradigm. It is usually applied when\\nobtaining labeled examples is costly. That is often the case in the medical or ﬁnancial\\ndomains, where the opinion of an expert may be required to annotate patients’ or customers’\\ndata. The idea is that we start the learning with relatively few labeled examples, and a large\\nnumber of unlabeled ones, and then add labels only to those examples that contribute the\\nmost to the model quality.\\nThere are multiple strategies of active learning. Here, we discuss only the following two:\\n1) data density and uncertainty based, and\\n2) support vector-based.\\nThe former strategy applies the current model f, trained using the existing labeled examples,\\nto each of the remaining unlabelled examples (or, to save the computing time, to some\\nrandom sample of them). For each unlabeled example x, the following importance score is\\ncomputed: density(x) · uncertaintyf(x). Density reﬂects how many examples surround x in\\nits close neighborhood, while uncertaintyf(x) reﬂects how uncertain the prediction of the\\nmodel f is for x. In binary classiﬁcation with sigmoid, the closer the prediction score is to\\n0.5, the more uncertain is the prediction. In SVM, the closer the example is to the decision\\nboundary, the most uncertain is the prediction.\\nIn multiclass classiﬁcation, entropy can be used as a typical measure of uncertainty:\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n15'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 100}, page_content='Hf(x) = ≠\\nC\\nÿ\\nc=1\\nPr(y(c); f(x)) ln Pr(y(c); f(x)),\\nwhere Pr(y(c); f(x)) is the probability score the model f assigns to class y(c) when classifying\\nx. You can see that if for each y(c), f(y(c)) = 1\\nC then the model is the most uncertain and\\nthe entropy is at its maximum of 1; on the other hand, if for some y(c), f(y(c)) = 1, then the\\nmodel is certain about the class y(c) and the entropy is at its minimum of 0.\\nDensity for the example x can be obtained by taking the average of the distance from x to\\neach of its k nearest neighbors (with k being a hyperparameter).\\nOnce we know the importance score of each unlabeled example, we pick the one with the\\nhighest importance score and ask the expert to annotate it. Then we add the new annotated\\nexample to the training set, rebuild the model and continue the process until some stopping\\ncriterion is satisﬁed. A stopping criterion can be chosen in advance (the maximum number\\nof requests to the expert based on the available budget) or depend on how well our model\\nperforms according to some metric.\\nThe support vector-based active learning strategy consists in building an SVM model using\\nthe labeled data. We then ask our expert to annotate the unlabeled example that lies the\\nclosest to the hyperplane that separates the two classes. The idea is that if the example lies\\nclosest to the hyperplane, then it is the least certain and would contribute the most to the\\nreduction of possible places where the true (the one we look for) hyperplane could lie.\\nSome active learning strategies can incorporate the cost of asking an\\nexpert for a label. Others learn to ask expert’s opinion. The “query by\\ncommittee” strategy consists of training multiple models using diﬀerent\\nmethods and then asking an expert to label example on which those\\nmodels disagree the most. Some strategies try to select examples to\\nlabel so that the variance or the bias of the model are reduced the most.\\n7.9\\nSemi-Supervised Learning\\nIn semi-supervised learning (SSL) we also have labeled a small fraction of the dataset;\\nmost of the remaining examples are unlabeled. Our goal is to leverage a large number of\\nunlabeled examples to improve the model performance without asking an expert for additional\\nlabeled examples.\\nHistorically, there were multiple attempts at solving this problem. None of them could be\\ncalled universally acclaimed and frequently used in practice. For example, one frequently\\ncited SSL method is called “self-learning.” In self-learning, we use a learning algorithm to\\nbuild the initial model using the labeled examples. Then we apply the model to all unlabeled\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n16'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 101}, page_content='examples and label them using the model. If the conﬁdence score of prediction for some\\nunlabeled example x is higher than some threshold (chosen experimentally), then we add this\\nlabeled example to our training set, retrain the model and continue like this until a stopping\\ncriterion is satisﬁed. We could stop, for example, if the accuracy of the model has not been\\nimproved during the last m iterations.\\nThe above method can bring some improvement to the model compared to just using the\\ninitially labeled dataset, but the increase in performance usually is not very impressive.\\nFurthermore, in practice, the quality of the model could even decrease. That depends on the\\nproperties of the statistical distribution the data was drawn from, which we usually do not\\nknow.\\nOn the other hand, the recent advancements in neural network learning brought some\\nimpressive results. For example, it was shown that for some datasets, such as MNIST (a\\nfrequent testbench in computer vision that consists of labeled images of handwritten digits\\nfrom 0 to 9) the model trained in a semi-supervised way has an almost perfect performance\\nwith just 10 labeled examples per class (100 labeled examples overall). For comparison,\\nMNIST contains 70,000 labeled examples (60,000 for training and 10,000 for test). The\\nneural network architecture that attained such a remarkable performance is called a ladder\\nnetwork. To understand ladder networks you have to understand what an autoencoder is.\\nAn autoencoder is a feed-forward neural network with an encoder-decoder architecture. It\\nis trained to reconstruct its input. So the training example is a pair (x, x). We want the\\noutput ˆx of the model f(x) to be as similar to the input x as possible.\\nEmbedding\\nx\\nx̂  \\nEncoder\\nDecoder\\nFigure 6: Autoencoder.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n17'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 102}, page_content='An important detail here is that an autoencoder’s network looks like an hourglass with a\\nbottleneck layer in the middle that contains the embedding of the D-dimensional input\\nvector; the embedding layer usually has much fewer units than D. The goal of the decoder is\\nto reconstruct the input feature vector from this embedding. Theoretically, it is suﬃcient\\nto have 10 units in the bottleneck layer to successfully encode MNIST images. In a typical\\nautoencoder schematically depicted in ﬁg. 6, the cost function is usually either the mean\\nsquared error (when features can be any number) or the negative log-likelihood (when features\\nare binary and the units of the last layer of the decoder have the sigmoid activation function).\\nIf the cost is the mean squared error, then it is given by:\\n1\\nN\\nN\\nÿ\\ni=1\\nÎxi ≠f(xi)Î2,\\nwhere Îxi ≠f(xi)Î is the Euclidean distance between two vectors.\\nA denoising autoencoder corrupts the left-hand side x in the training example (x, x) by\\nadding some random perturbation to the features. If our examples are grayscale images with\\npixels represented as values between 0 and 1, usually a normal Gaussian noise is added to\\neach feature. For each feature j of the input feature vector x the noise value n(j) is sampled\\nfrom the following distribution:\\nn(j) ≥\\n1\\n‡\\nÔ\\n2ﬁexp\\n3\\n≠(≠µ)2\\n2‡2\\n4\\n,\\nwhere the notation ≥means “sampled from,” ﬁis the constant 3.14159 . . . and µ is a\\nhyperparameter that has to be tuned. The new, corrupted value of the feature x(j) is given\\nby x(j) + n(j).\\nA ladder network is a denoising autoencoder with an upgrade. The encoder and the decoder\\nhave the same number of layers. The bottleneck layer is used directly to predict the label\\n(using the softmax activation function). The network has several cost functions. For each\\nlayer l of the encoder and the corresponding layer l of the decoder, one cost Cl\\nd penalizes\\nthe diﬀerence between the outputs of the two layers (using the squared Euclidean distance).\\nWhen a labeled example is used during training, another cost function, Cc, penalizes the error\\nin prediction of the label (the negative log-likelihood cost function is used). The combined\\ncost function, Cc + qL\\nl=1 ⁄lCl\\nd (averaged over all examples in the batch), is optimized by the\\nstochastic gradient descent with backpropagation. The hyperparameters ⁄l for each layer l\\ndetermine the tradeoﬀbetween the classiﬁcation and encoding-decoding cost.\\nIn the ladder network, not just the input is corrupted with the noise, but also the output of\\neach encoder layer (during training). When we apply the trained model to the new input x\\nto predict its label, we do not corrupt the input.\\nOther semi-supervised learning techniques, not related to training neural networks, exist.\\nOne of them implies building the model using the labeled data and then cluster the unlabeled\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n18'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 103}, page_content='and labeled examples together using any clustering technique (we consider some of them in\\nChapter 9).\\nFor each new example, we then output as a prediction the majority label\\nin the cluster it belongs to. Another technique, called S3VM, is based\\non using SVM. We build one SVM model for each possible labeling of\\nunlabeled examples and then we pick the model with the largest margin.\\nThe paper on S3VM describes an approach that allows solving this\\nproblem without actually enumerating all possible labelings.\\n7.10\\nOne-Shot Learning\\nThis chapter would be incomplete without mentioning two other important supervised learning\\nparadigms. One of them is one-shot learning. In one-shot learning, typically applied in\\nface recognition, we want to build a model that can recognize that two photos of the same\\nperson represent that same person. If we present to the model two photos of two diﬀerent\\npeople, we expect the model to recognize that the two people are diﬀerent.\\nOne way to build such a model is to train a siamese neural network (SNN). An SNN can\\nbe implemented as any kind of neural network, a CNN, an RNN, or an MLP. What matters\\nis how we train the network.\\nTo train an SNN, we use the triplet loss function. For example, let us have three images of\\na face: the image A (for anchor), the image P (for positive) and the image N (for negative).\\nA and P are two diﬀerent pictures of the same person; N is a picture of another person.\\nEach training example i is now a triplet (Ai, Pi, Ni).\\nLet’s say we have a neural network model f that can take a picture of a face as input and\\noutput an embedding of this picture. The triplet loss for one example is deﬁned as,\\nmax(Îf(Ai) ≠f(Pi)Î2 ≠Îf(Ai) ≠f(Ni)Î2 + –, 0).\\n(3)\\nThe cost function is deﬁned as the average triplet loss:\\n1\\nN\\nN\\nÿ\\ni=1\\nmax(Îf(Ai) ≠f(Pi)Î2 ≠Îf(Ai) ≠f(Ni)Î2 + –, 0),\\nwhere – is a positive hyperparameter. Intuitively, Îf(A) ≠f(P)Î2 is low when our neural\\nnetwork outputs similar embedding vectors for A and P; Îf(Ai) ≠f(Ni)Î2 is high when the\\nembedding for pictures of two diﬀerent people are diﬀerent. If our model works the way\\nwe want, then the term m = Îf(Ai) ≠f(Pi)Î2 ≠Îf(Ai) ≠f(Ni)Î2 will always be negative,\\nbecause we subtract a high value from a small value. By setting – higher, we force the term\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n19'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 104}, page_content='m to be even smaller, to make sure that the model learned to recognize the two same faces\\nand two diﬀerent faces with a high margin. If m is not small enough, then because of – the\\ncost will be positive, and the model parameters will be adjusted in backpropagation.\\nRather than randomly choose an image for N, a better way to create triplets for training is\\nto use the current model after several epochs of learning and ﬁnd candidates for N that are\\nsimilar to A and P according to that model. Using random examples as N would signiﬁcantly\\nslow down the training because the neural network will easily see the diﬀerence between\\npictures of two random people, so the average triplet loss will be low most of the time and\\nthe parameters will not be updated fast enough.\\nTo build an SNN, we ﬁrst decide on the architecture of our neural network. For example,\\nCNN is a typical choice if our inputs are images. Given an example, to calculate the average\\ntriplet loss, we apply, consecutively, the model to A, then to P, then to N, and then we\\ncompute the loss for that example using eq. 3. We repeat that for all triplets in the batch and\\nthen compute the cost; gradient descent with backpropagation propagates the cost through\\nthe network to update its parameters.\\nIt’s a common misconception that for one-shot learning we need only one example of each\\nentity for training. In practice, we need much more than one example of each person for the\\nperson identiﬁcation model to be accurate. It’s called one-shot because of the most frequent\\napplication of such a model: face-based authentication. For example, such a model could be\\nused to unlock your phone. If your model is good, then you only need to have one picture\\nof you on your phone and it will recognize you, and also it will recognize that someone else\\nis not you. When we have the model, to decide whether two pictures A and ˆA belong to\\nthe same person, we check if Îf(A) ≠f( ˆA)Î2 is less than some threshold ·, which is another\\nhyperparameter of the model.\\n7.11\\nZero-Shot Learning\\nWe ﬁnish this chapter with zero-shot learning. It is a relatively new\\nresearch area, so there are no algorithms that proved to have a signiﬁcant\\npractical utility yet. Therefore, I only outline here the basic idea and\\nleave the details of various algorithms for further reading. In zero-shot\\nlearning (ZSL) we want to train a model to assign labels to objects. The\\nmost frequent application is to learn to assign labels to images.\\nHowever, we want the model to be able to predict labels that we didn’t have in the training\\ndata. How is that possible?\\nThe trick is to use embeddings not just to represent the input x but also to represent the\\noutput y. Imagine that we have a model that for any word in English can generate an\\nembedding vector with the following property: if a word yi has a similar meaning to the\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n20'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 105}, page_content='word yk, then the embedding vectors for these two words will be similar. For example, if yi is\\nParis and yk is Rome, then they will have embeddings that are similar; on the other hand, if\\nyk is potato, then the embeddings of yi and yk will be dissimilar. Such embedding vectors are\\ncalled “word embeddings,” and they are usually compared using cosine similarity metrics1.\\nWord embeddings have such a property that each dimension of the embedding represents a\\nspeciﬁc feature of the meaning of the word. For example, if our word embedding has four\\ndimensions (usually they are much wider, between 50 and 300 dimensions), then these four\\ndimensions could represent such features of the meaning as animalness, abstractness, sourness,\\nand yellowness (yes, sounds funny, but it’s just an example). So the word bee would have an\\nembedding like this [1, 0, 0, 1], the word yellow like this [0, 1, 0, 1], the word unicorn like this\\n[1, 1, 0, 0]. The values for each embedding are obtained using a speciﬁc training procedure\\napplied to a vast text corpus.\\nNow, in our classiﬁcation problem, we can replace the label yi for each example i in our\\ntraining set with its word embedding and train a multi-label model that predicts word\\nembeddings. To get the label for a new example x, we apply our model f to x, get the\\nembedding ˆy and then search among all English words those whose embeddings are the most\\nsimilar to ˆy using cosine similarity.\\nWhy does that work? Take a zebra for example. It is white, it is a mammal, and it has stripes.\\nTake a clownﬁsh: it is orange, not a mammal, and has stripes. Now take a tiger: it is orange,\\nit has stripes, and it is a mammal. If these three features are present in word embeddings,\\nthe CNN would learn to detect these same features in pictures. Even if the label tiger was\\nabsent in the training data, but other objects including zebras and clownﬁsh were, then the\\nCNN will most likely learn the notion of mammalness, orangeness, and stripeness to predict\\nlabels of those objects. Once we present the picture of a tiger to the model, those features\\nwill be correctly identiﬁed from the image and most likely the closest word embedding from\\nour English dictionary to the predicted embedding will be that of tiger.\\n1I will show in Chapter 10 how to learn words embeddings from data.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n21'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 106}, page_content=\"Andriy Burkov's\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 107}, page_content='“All models are wrong, but some are useful.”\\n— George Box\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 108}, page_content='8\\nAdvanced Practice\\nThis chapter contains the description of techniques that you could ﬁnd useful in your practice\\nin some contexts. It’s called “Advanced Practice” not because the presented techniques are\\nmore complex, but rather because they are applied in some very speciﬁc contexts. In many\\npractical situations, you will most likely not need to resort to using these techniques, but\\nsometimes they are very helpful.\\n8.1\\nHandling Imbalanced Datasets\\nIn many practical situations, your labeled dataset will have underrepresented the examples\\nof some class. This is the case, for example, when your classiﬁer has to distinguish between\\ngenuine and fraudulent e-commerce transactions: the examples of genuine transactions are\\nmuch more frequent. If you use SVM with soft margin, you can deﬁne a cost for misclassiﬁed\\nexamples. Because noise is always present in the training data, there are high chances that\\nmany examples of genuine transactions would end up on the wrong side of the decision\\nboundary by contributing to the cost.\\nx(2)\\nx(1)\\n(a)\\nx(2)\\nx(1)\\n(b)\\nFigure 1: An illustration of an imbalanced problem. (a) Both classes have the same weight;\\n(b) examples of the minority class have a higher weight.\\nThe SVM algorithm will try to move the hyperplane to avoid as much as possible misclassiﬁed\\nexamples. The “fraudulent” examples, which are in the minority, risk being misclassiﬁed in\\norder to classify more numerous examples of the majority class correctly. This situation is\\nillustrated in Figure 1a. This problem is observed for most learning algorithms applied to\\nimbalanced datasets.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 109}, page_content='If you set the cost of misclassiﬁcation of examples of the minority class higher, then the\\nmodel will try harder to avoid misclassifying those examples, obviously for the cost of\\nmisclassiﬁcation of some examples of the majority class, as illustrated in Figure 1b.\\nSome SVM implementations (including SVC in scikit-learn) allow you to provide weights for\\nevery class. The learning algorithm takes this information into account when looking for the\\nbest hyperplane.\\nIf your learning algorithm doesn’t allow weighting classes, you can try to increase the\\nimportance of examples of some class by making multiple copies of the examples of this class\\n(this is called oversampling).\\nAn opposite approach is to randomly remove from the training set some examples of the\\nmajority class (undersampling).\\nYou might also try to create synthetic examples by randomly sampling feature values of\\nseveral examples of the minority class and combining them to obtain a new example of\\nthat class. There two popular algorithms that oversample the minority class by creating\\nsynthetic examples: the synthetic minority oversampling technique (SMOTE) and the\\nadaptive synthetic sampling method (ADASYN).\\nSMOTE and ADASYN work similarly in many ways. For a given example xi of the minority\\nclass, they pick k nearest neighbors of this example (let’s call this set of k examples Sk) and\\nthen create a synthetic example xnew as xi + ⁄(xzi ≠xi), where xzi is an example of the\\nminority class chosen randomly from Sk. The interpolation hyperparameter ⁄ is a random\\nnumber in the range [0, 1].\\nBoth SMOTE and ADASYN randomly pick all possible xi in the dataset. In ADASYN,\\nthe number of synthetic examples generated for each xi is proportional to the number of\\nexamples in Sk which are not from the minority class. Therefore, more synthetic examples\\nare generated in the area where the examples of the minority class are rare.\\nSome algorithms are less sensitive to the problem of an imbalanced dataset. Decision trees,\\nas well as random forest and gradient boosting, often perform well on imbalanced datasets.\\n8.2\\nCombining Models\\nEnsemble algorithms, like Random Forest, typically combine models of the same nature. They\\nboost performance by combining hundreds of weak models. In practice, we can sometimes\\nget an additional performance gain by combining strong models made with diﬀerent learning\\nalgorithms. In this case, we usually use only two or three models.\\nThere are three typical ways to combine models:\\n1) averaging,\\n2) majority vote, and\\n3) stacking.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n4'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 110}, page_content='Averaging works for regression as well as those classiﬁcation models that return classiﬁcation\\nscores. You simply apply all your models, let’s call them base models, to the input x and\\nthen average the predictions. To see if the averaged model works better than each individual\\nalgorithm, you test it on the validation set using a metric of your choice.\\nMajority vote works for classiﬁcation models. You apply all your base models to the input\\nx and then return the majority class among all predictions. In the case of a tie, you either\\nrandomly pick one of the classes, or, you return an error message (if the fact of misclassifying\\nwould incur a signiﬁcant cost).\\nStacking consists of building a meta-model that takes the output of your base models as\\ninput. Let’s say you want to combine a classiﬁer f1 and a classiﬁer f2, both predicting the\\nsame set of classes. To create a training example (ˆxi, ˆyi) for the stacked model, you set\\nˆxi = [f1(x), f2(x)] and ˆyi = yi.\\nIf some of your base models return not just a class, but also a score for each class, you can\\nuse these values as features too.\\nTo train the stacked model, it is recommended to use examples from the training set and\\ntune the hyperparameters of the stacked model using cross-validation.\\nObviously, you have to make sure that your stacked model performs better on the validation\\nset than each of the base models you stacked.\\nThe reason that combining multiple models can bring better performance overall is the\\nobservation that when several uncorrelated strong models agree they are more likely to agree\\non the correct outcome. The keyword here is “uncorrelated.” Ideally, diﬀerent strong models\\nhave to be obtained using diﬀerent features or using algorithms of a diﬀerent nature — for\\nexample, SVMs and Random Forest. Combining diﬀerent versions of decision tree learning\\nalgorithm, or several SVMs with diﬀerent hyperparameters may not result in a signiﬁcant\\nperformance boosting.\\n8.3\\nTraining Neural Networks\\nIn neural network training, one challenging aspect is to convert your data into the input the\\nnetwork can work with. If your input is images, ﬁrst of all, you have to resize all images so\\nthat they have the same dimensions. After that, pixels are usually ﬁrst standardized and\\nthen normalized to the range [0, 1].\\nTexts have to be tokenized (that is split into pieces, such as words, punctuation marks, and\\nother symbols). For CNN and RNN, each token is converted into a vector using the one-hot\\nencoding, so the text becomes a list of one-hot vectors. Another, often a better way to\\nrepresent tokens is by using word embeddings. For multilayer perceptron, to convert texts\\nto vectors the bag of words approach may work well, especially for larger texts (larger than\\nSMS messages and tweets).\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 111}, page_content='The choice of speciﬁc neural network architecture is a diﬃcult one. For the same problem,\\nlike seq2seq learning, there is a variety of architectures, and new ones are proposed almost\\nevery year. I recommend making the research on the state of the art solutions for your\\nproblem using Google Scholar or Microsoft Academic search engines that allow searching for\\nscientiﬁc publications using keywords and time range. If you don’t mind working with less\\nmodern architecture, I recommend looking for implemented architectures on GitHub and\\nﬁnd one that could be applied to your data with minor modiﬁcations.\\nIn practice, the advantage of modern architecture over an older one becomes less signiﬁcant\\nas you preprocess, clean and normalize your data, and create a larger training set. Many\\nmodern neural network architectures are a result of the collaboration of several scientists\\nfrom several labs and companies; such models could be very complex to implement them on\\nyour own and usually require much computational power to train. The time spent on trying\\nto replicate the results from a recent scientiﬁc paper may not be worth it. This time could\\nbetter be spent on building the solution around a less modern but stable model and getting\\nmore training data.\\nOnce you decided on the architecture of your network, you have to decide on the number\\nof layers, their type, and size. It is recommended to start with one or two layers, train a\\nmodel and see if it ﬁts the training data well (has a low bias). If not, gradually increase the\\nsize of each layer and the number of layers until the model perfectly ﬁts the training data.\\nOnce this is the case, if the model doesn’t perform well on the validation data (has a high\\nvariance), you should add regularization to your model. If, after you added regularization,\\nthe model doesn’t ﬁt the training data anymore, you slightly increase the size of the network\\nonce again, and you continue to work iteratively like this until the model ﬁts both training\\nand validation data well enough according to your metric.\\n8.4\\nAdvanced Regularization\\nIn neural networks, besides L1 and L2 regularization, you can use neural network speciﬁc\\nregularizers: dropout, batch normalization, and early stopping. Batch normalization\\nis technically not a regularization technique, but it often has a regularization eﬀect on the\\nmodel.\\nThe concept of dropout is very simple. Each time you run a training example through the\\nnetwork, you temporarily exclude at random some units from the computation. The higher\\nthe percentage of units excluded the higher the regularization eﬀect. Neural network libraries\\nallow you to add a dropout layer between two successive layers, or you can specify the dropout\\nparameter for the layer. The dropout parameter is in the range [0, 1] and it has to be found\\nexperimentally by tuning it on the validation data.\\nBatch normalization (which rather has to be called batch standardization) is a technique that\\nconsists of standardizing the outputs of each layer before the units of the subsequent layer\\nreceive them as input. In practice, batch normalization results in a faster and more stable\\ntraining, as well as in some regularization eﬀect. So it’s always a good idea to try to use\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n6'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 112}, page_content='batch normalization. In neural network libraries, you can often insert a batch normalization\\nlayer between two layers.\\nEarly stopping is the way to train a neural network by saving the preliminary model after\\nevery epoch and assessing the performance of the preliminary model on the validation set. As\\nyou remember from the section about gradient descent in Chapter 4, as the number of epochs\\nincreases, the cost decreases. The decreased cost means that the model ﬁts the training data\\nwell. However, at some point, after some epoch e, the model can start overﬁtting: the cost\\nkeeps decreasing, but the performance of the model on the validation data deteriorates. If\\nyou keep, in a ﬁle, the version of the model after each epoch, you can stop the training once\\nyou start observing a decreased performance on the validation set. Alternatively, you can\\nkeep running the training process for a ﬁxed number of epochs and then, in the end, you\\npick the best model. Models saved after each epoch are called checkpoints. Some machine\\nlearning practitioners rely on this technique very often; others try to properly regularize the\\nmodel to avoid such undesirable behavior.\\nAnother regularization technique that can be applied not just to neural networks, but to\\nvirtually any learning algorithm, is called data augmentation. This technique is often\\nused to regularize models that work with images. Once you have your original labeled\\ntraining set, you can create a synthetic example from an original example by applying various\\ntransformations to the original image: zooming it slightly, rotating, ﬂipping, darkening, and\\nso on. You keep the original label in these synthetic examples. In practice, this often results\\nin increased performance of the model.\\n8.5\\nHandling Multiple Inputs\\nIn many of your practical problems, you will work with multimodal data. For example, your\\ninput could be an image and text and the binary output could indicate whether the text\\ndescribes this image or not.\\nShallow learning algorithms are not particularly well suited to work with multimodal data.\\nHowever, it doesn’t mean that it is impossible. For example, you can train one model on the\\nimage and another one on the text. Then you can use a model combination technique we\\ndiscussed above.\\nIf you cannot divide your problem into two independent subproblems, you can try to vectorize\\neach input (by applying the corresponding feature engineering method) and then simply\\nconcatenate two feature vectors together to form one wider feature vector. For example,\\nif your image has features [i(1), i(2), i(3)] and your text has features [t(1), t(2), t(3), t(4)] your\\nconcatenated feature vector will be [i(1), i(2), i(3), t(1), t(2), t(3), t(4)].\\nWith neural networks, you have more ﬂexibility. You can build two subnetworks, one for\\neach type of input. For example, a CNN subnetwork would read the image while an RNN\\nsubnetwork would read the text. Both subnetworks have as their last layer an embedding:\\nCNN has an embedding for the image, while RNN has an embedding for the text. You can\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n7'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 113}, page_content='then concatenate two embeddings and then add a classiﬁcation layer, such as softmax or\\nsigmoid, on top of the concatenated embeddings. Neural network libraries provide simple to\\nuse tools that allow concatenating or averaging layers from several subnetworks.\\n8.6\\nHandling Multiple Outputs\\nIn some problems, you would like to predict multiple outputs for one input. We considered\\nmulti-label classiﬁcation in the previous chapter. Some problems with multiple outputs can\\nbe eﬀectively converted into a multi-label classiﬁcation problem. Especially those that have\\nlabels of the same nature (like tags) or fake labels can be created as a full enumeration of\\ncombinations of original labels.\\nHowever, in some cases the outputs are multimodal, and their combinations cannot be\\neﬀectively enumerated. Consider the following example: you want to build a model that\\ndetects an object on an image and returns its coordinates. The same model has to also return\\nthe label of the object, such as “person,” “cat,” or “hamster.” Your training examples will\\nhave an image as input and one vector with coordinates of an object and another vector with\\na one-hot encoded label.\\nTo handle a situation like this, you can create one subnetwork that would work as an\\nencoder. It will read the input image using, for example, one or several convolution layers.\\nThe encoder’s last layer would be the embedding of the image. Then you add two other\\nsubnetworks on top of the embedding layer: one that takes the embedding vector as input\\nand predicts the coordinates of an object. This ﬁrst subnetwork can have a ReLU as the last\\nlayer, which is a good choice for predicting positive real numbers, such as coordinates; this\\nsubnetwork could use the mean squared error cost C1. The second subnetwork will take the\\nsame embedding vector as input and predict the probabilities for each label. This second\\nsubnetwork can have a softmax as the last layer, which is appropriate for the probabilistic\\noutput, and use the averaged negative log-likelihood cost C2 (also called cross-entropy cost).\\nObviously, you are interested in both accurately predicted coordinates and the label. However,\\nit is impossible to optimize the two cost functions at the same time. By trying to optimize one,\\nyou risk hurting the second one and the other way around. What you can do is add another\\nhyperparameter “ in the range (0, 1) and deﬁne the combined cost function as “C1 +(1≠“)C2.\\nThen you tune the value for “ on the validation data just like any other hyperparameter.\\n8.7\\nTransfer Learning\\nTransfer learning is probably where neural networks have a unique advantage over the\\nshallow models. In transfer learning, you pick an existing model trained on some dataset,\\nand you adapt this model to predict examples from another dataset, diﬀerent from the one\\nthe model was built on. This second dataset is not like hold-out sets you use for validation\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n8'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 114}, page_content='and test. It may represent some other phenomenon, or, as machine learning scientists say, it\\nmay come from another statistical distribution.\\nFor example, imagine you have trained your model to recognize (and label) wild animals on a\\nbig labeled dataset. After some time, you have another problem to solve: you need to build a\\nmodel that would recognize domestic animals. With shallow learning algorithms, you do not\\nhave many options: you have to build another big labeled dataset, now for domestic animals.\\nWith neural networks, the situation is much more favorable. Transfer learning in neural\\nnetworks works like this.\\n1. You build a deep model on the original big dataset (wild animals).\\n2. You compile a much smaller labeled dataset for your second model (domestic animals).\\n3. You remove the last one or several layers from the ﬁrst model. Usually, these are layers\\nresponsible for the classiﬁcation or regression; they usually follow the embedding layer.\\n4. You replace the removed layers with new layers adapted for your new problem.\\n5. You “freeze” the parameters of the layers remaining from the ﬁrst model.\\n6. You use your smaller labeled dataset and gradient descent to train the parameters of\\nonly the new layers.\\nUsually, there is an abundance of deep models for visual problems available online. You can\\nﬁnd one that has high chances to be of use for your problem, download that model, remove\\nseveral last layers (the quantity of layers to remove is a hyperparameter), put your own\\nprediction layers and train your model.\\nEven if you don’t have an existing model, transfer learning can still help you in situations when\\nyour problem requires a labeled dataset very costly to obtain, but you can get another dataset\\nfor which labels are more readily available. Let’s say you build a document classiﬁcation\\nmodel. You got the taxonomy of labels from your employer, and it contains a thousand\\ncategories. In this case, you would need to pay someone to a) read, understand and memorize\\nthe diﬀerences between categories and b) read up to a million documents and annotate them.\\nThat doesn’t sound good.\\nTo save on labeling so many examples, you could consider using Wikipedia pages as the dataset\\nto build your ﬁrst model. The labels for a Wikipedia page can be obtained automatically by\\ntaking the category the Wikipedia page belongs to. Once your ﬁrst model has learned to\\npredict Wikipedia categories well, you can transfer this learning to predict the categories of\\nyour employer’s taxonomy. Usually, you will need much fewer annotated examples for your\\nemployer’s problem than you would need if you started solving your original problem from\\nscratch.\\n8.8\\nAlgorithmic Eﬃciency\\nNot all algorithms capable of solving a problem are practical. Some can be fast; some can be\\ntoo slow. Some problems can be solved by a fast algorithm, for others, no fast algorithms\\ncan exist.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n9'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 115}, page_content='The subﬁeld of computer science called analysis of algorithms is concerned with determining\\nand comparing the complexity of algorithms. The big O notation is used to classify algorithms\\naccording to how their running time or space requirements grow as the input size grows.\\nFor example, let’s say we have the problem of ﬁnding the two most distant one-dimensional\\nexamples in the set of examples S of size N. One algorithm we could craft to solve this\\nproblem would look like this (here and below, in Python):\\n1\\ndef find_max_distance(S):\\n2\\nresult = None\\n3\\nmax_distance = 0\\n4\\nfor x1 in S:\\n5\\nfor x2 in S:\\n6\\nif abs(x1 - x2) >= max_distance:\\n7\\nmax_distance = abs(x1 - x2)\\n8\\nresult = (x1, x2)\\n9\\nreturn result\\nIn the above algorithm, we loop over all values in S, and at every iteration of the ﬁrst loop, we\\nloop over all values in S once again. Therefore, the above algorithm makes N 2 comparisons\\nof numbers. If we take as a unit time the time the comparison (once), abs (twice) and\\nassignment (twice) operations take, then the time complexity (or, simply, complexity) of this\\nalgorithm is at most 5N 2. When the complexity of an algorithm is measured in the worst\\ncase, the big O notation is used. For the above algorithm, using the big O notation, we write\\nthat the algorithm’s complexity is O(N 2) (the constants, like 5, are ignored).\\nFor the same problem, we can craft another algorithm like this:\\n1\\ndef find_max_distance(S):\\n2\\nresult = None\\n3\\nmin_x = float(\"inf\")\\n4\\nmax_x = float(\"-inf\")\\n5\\nfor x in S:\\n6\\nif x < min_x:\\n7\\nmin_x = x\\n8\\nelif x > max_x:\\n9\\nmax_x = x\\n10\\nresult = (max_x, min_x)\\n11\\nreturn result\\nIn the above algorithm, we loop over all values in S only once, so the algorithm’s complexity\\nis O(N). In this case, we say that the latter algorithm is more eﬃcient than the former.\\nUsually, an algorithm is called eﬃcient when its complexity in big O notation is polynomial\\nin the size of the input. Therefore both O(N) and O(N 2) are eﬃcient. However, for very\\nlarge inputs an O(N 2) algorithm can still be very slow. In the big data era, scientists often\\nlook for O(logN) algorithms.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n10'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 116}, page_content='From a practical standpoint, when you implement your algorithm, you should avoid using\\nloops whenever possible. For example, you should use operations on matrices and vectors,\\ninstead of loops. In Python, to compute wx, you should write\\n1\\nimport numpy\\n2\\nwx = numpy.dot(w,x)\\nand not\\n1\\nwx = 0\\n2\\nfor i in range(N):\\n3\\nwx += w[i]*x[i]\\nUse appropriate data structures. If the order of elements in a collection doesn’t matter, use\\nset instead of list. In Python, the operation of verifying whether a speciﬁc example x belongs\\nto S is eﬃcient when S is declared as a set and is ineﬃcient when S is declared as a list.\\nAnother important data structure, which you can use to make your Python code more eﬃcient\\nis dict. It is called a dictionary or a hashmap in other languages. It allows you to deﬁne a\\ncollection of key-value pairs with very fast lookups for keys.\\nUnless you know exactly what you do, always prefer using popular libraries to writing your\\nown scientiﬁc code. Scientiﬁc Python packages like numpy, scipy, and scikit-learn were built\\nby experienced scientists and engineers with eﬃciency in mind. They have many methods\\nimplemented in the C programming language for maximum speed.\\nIf you need to iterate over a vast collection of elements, use generators that create a function\\nthat returns one element at a time rather than all the elements at once.\\nUse cProﬁle package in Python to ﬁnd ineﬃciencies in your code.\\nFinally, when nothing can be improved in your code from the algorithmic perspective, you\\ncan further boost the speed of your code by using:\\n• multiprocessing package to run computations in parallel, and\\n• PyPy, Numba or similar tools to compile your Python code into fast, optimized machine\\ncode.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n11'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 117}, page_content='The\\nHundred-\\nPage\\nMachine\\nLearning\\nBook\\nAndriy Burkov'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 118}, page_content='“All models are wrong, but some are useful.”\\n— George Box\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 119}, page_content='9\\nUnsupervised Learning\\nUnsupervised learning deals with problems in which your dataset doesn’t have labels. This\\nproperty is what makes it very problematic for many practical applications. The absence\\nof labels which represent the desired behavior for your model means the absence of a solid\\nreference point to judge the quality of your model. In this book, I only present unsupervised\\nlearning methods that allow building models that can be evaluated based on data as opposed\\nto human judgment.\\n9.1\\nDensity Estimation\\nDensity estimation is a problem of modeling the probability density function (pdf) of the\\nunknown probability distribution from which the dataset has been drawn. It can be useful for\\nmany applications, in particular for novelty or intrusion detection. In Chapter 7, we already\\nestimated the pdf to solve the one-class classiﬁcation problem. To do that, we decided that\\nour model would be parametric, more precisely a multivariate normal distribution (MVN).\\nThis decision was somewhat arbitrary because if the real distribution from which our dataset\\nwas drawn is diﬀerent from the MVN, our model will be very likely far from perfect. We\\nalso know that models can be nonparametric. We used a nonparametric model in kernel\\nregression. It turns out that the same approach can work for density estimation.\\nLet {xi}N\\ni=1 be a one-dimensional dataset (a multi-dimensional case is similar) whose examples\\nwere drawn from a distribution with an unknown pdf f with xi œ R for all i = 1, . . . , N. We\\nare interested in modeling the shape of this function f. Our kernel model of f, let’s denote it\\nas ˆf, is given by,\\nˆfb(x) =\\n1\\nNb\\nN\\nÿ\\ni=1\\nk\\n3x ≠xi\\nb\\n4\\n,\\n(1)\\nwhere b is a hyperparameter that controls the tradeoﬀbetween bias and variance of our\\nmodel and k is a kernel. Again, like in Chapter 7, we use a Gaussian kernel:\\nk(z) =\\n1\\nÔ\\n2ﬁexp\\n3≠z2\\n2\\n4\\n.\\nWe look for such a value of b that minimizes the diﬀerence between the real shape of f and\\nthe shape of our model ˆfb. A reasonable choice of measure of this diﬀerence is called the\\nmean integrated squared error:\\nMISE(b) = E\\n5 ⁄\\nR\\n( ˆfb(x) ≠f(x))2 dx\\n6\\n.\\n(2)\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 120}, page_content='Intuitively, you see in eq. 2 that we square the diﬀerence between the real pdf f and our\\nmodel of it ˆf. The integral\\ns\\nR replaces the summation qN\\ni=1 we employed in the average\\nsquared error, while the expectation operator E replaces the average\\n1\\nN .\\n(a)\\n(b)\\n(c)\\n(d)\\nFigure 1: Kernel density estimation: (a) good ﬁt; (b) overﬁtting; (c) underﬁtting; (d) the\\ncurve of grid search for the best value for b.\\nIndeed, when our loss is a function with a continuous domain, such as ( ˆfb(x) ≠f(x))2, we\\nhave to replace the summation with the integral. The expectation operation E means that\\nwe want b to be optimal for all possible realizations of our training set {xi}N\\ni=1. That is\\nimportant because ˆfb is deﬁned on a ﬁnite sample of some probability distribution, while the\\nreal pdf f is deﬁned on an inﬁnite domain (the set R).\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n4'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 121}, page_content='Now, we can rewrite the right-hand side term in eq. 2 like this:\\nE\\n5⁄\\nR\\nˆf 2\\nb (x)dx\\n6\\n≠2E\\n5⁄\\nR\\nˆfb(x)f(x)dx\\n6\\n+ E\\n5⁄\\nR\\nf(x)2dx\\n6\\n.\\nThe third term in the above summation is independent of b and thus can be ignored. An\\nunbiased estimator of the ﬁrst term is given by\\ns\\nR ˆf 2\\nb (x)dx while the unbiased estimator of\\nthe second term can be approximated by ≠2\\nN\\nqN\\ni=1 ˆf (i)\\nb (xi), where ˆf (i)\\nb\\nis a kernel model of\\nf computed on our training set with the example xi excluded.\\nThe term qN\\ni=1 ˆf (i)\\nb (xi) is known in statistics as the leave one out estimate, a form of cross-\\nvalidation in which each fold consists of one example. You could have noticed that the term\\ns\\nR ˆfb(x)f(x)dx (let’s call it a) is the expected value of the function ˆfb, because f is a pdf. It\\ncan be demonstrated that the leave one out estimate is an unbiased estimator of Ea.\\nNow, to ﬁnd the optimal value bú for b, we want to minimize the cost deﬁned as:\\n⁄\\nR\\nˆf 2\\nb (x)dx ≠2\\nN\\nN\\nÿ\\ni=1\\nˆf (i)\\nb (xi).\\nWe can ﬁnd bú using grid search. For D-dimensional feature vectors x, the error term x ≠xi\\nin eq. 1 can be replaced by the Euclidean distance Îx ≠xiÎ. In ﬁg. 1 you can see the\\nestimates for the same pdf obtained with three diﬀerent values of b from a dataset containing\\n100 examples, as well as the grid search curve. We pick bú at the minimum of the grid search\\ncurve.\\n9.2\\nClustering\\nClustering is a problem of learning to assign a label to examples by leveraging an unlabeled\\ndataset. Because the dataset is completely unlabeled, deciding on whether the learned model\\nis optimal is much more complicated than in supervised learning.\\nThere is a variety of clustering algorithms, and, unfortunately, it’s hard to tell which one is\\nbetter in quality for your dataset. Usually, the performance of each algorithm depends on\\nthe unknown properties of the probability distribution the dataset was drawn from.\\n9.2.1\\nK-Means\\nThe k-means clustering algorithm works as follows. First, the analyst has to choose k — the\\nnumber of classes (or clusters). Then we randomly put k feature vectors, called centroids,\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 122}, page_content='to the feature space1. We then compute the distance from each example x to each centroid c\\nusing some metric, like the Euclidean distance. Then we assign the closest centroid to each\\nexample (like if we labeled each example with a centroid id as the label). For each centroid,\\nwe calculate the average feature vector of the examples labeled with it. These average feature\\nvectors become the new locations of the centroids.\\n(a) original data\\n(b) iteration 1\\n(c) iteration 3\\n(d) iteration 5\\nFigure 2: The progress of the kmeans algorithm for k = 3. The circles are two-dimensional\\nfeature vectors; the squares are moving centroids.\\n1Some variants of k-means compute the initial positions of centroids based on some properties of the\\ndataset.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n6'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 123}, page_content='We recompute the distance from each example to each centroid, modify the assignment and\\nrepeat the procedure until the assignments don’t change after the centroid locations were\\nrecomputed. The model is the list of assignments of centroids IDs to the examples.\\nThe initial position of centroids inﬂuence the ﬁnal positions, so two runs of k-means can\\nresult in two diﬀerent models. One run of the k-means algorithm is illustrated in ﬁg. 2.\\nDiﬀerent background colors represent regions in which all points belong to the same cluster.\\nThe value of k, the number of clusters, is a hyperparameter that has to be tuned by the data\\nanalyst. There are some techniques for selecting k. None of them is proven optimal. Most of\\nthem require from the analyst to make an “educated guess” by looking at some metrics or\\nby examining cluster assignments visually. Later in this chapter, we consider one technique\\nwhich allows choosing a reasonably good value for k without looking at the data and making\\nguesses.\\n9.2.2\\nDBSCAN and HDBSCAN\\nWhile k-means and similar algorithms are centroid-based, DBSCAN is a density-based\\nclustering algorithm. Instead of guessing how many clusters you need, by using DBSCAN,\\nyou deﬁne two hyperparameters: ‘ and n. You start by picking an example x from your\\ndataset at random and assign it to cluster 1. Then you count how many examples have\\nthe distance from x less than or equal to ‘. If this quantity is greater than or equal to n,\\nthen you put all these ‘-neighbors to the same cluster 1. You then examine each member of\\ncluster 1 and ﬁnd their respective ‘-neighbors. If some member of cluster 1 has n or more\\n‘-neighbors, you expand cluster 1 by putting those ‘-neighbors to the cluster. You continue\\nexpanding cluster 1 until there are no more examples to put in it. In the latter case, you pick\\nfrom the dataset another example not belonging to any cluster and put it to cluster 2. You\\ncontinue like this until all examples either belong to some cluster or are marked as outliers.\\nAn outlier is an example whose ‘-neighborhood contains less than n examples.\\nThe advantage of DBSCAN is that it can build clusters that have an arbitrary shape, while k-\\nmeans and other centroid-based algorithms create clusters that have a shape of a hypersphere.\\nAn obvious drawback of DBSCAN is that it has two hyperparameters and choosing good\\nvalues for them (especially ‘) could be challenging. Furthermore, having ‘ ﬁxed, the clustering\\nalgorithm cannot eﬀectively deal with clusters of varying density.\\nHDBSCAN is the clustering algorithm that keeps the advantages of DBSCAN, by removing\\nthe need to decide on the value of ‘.\\nThe algorithm is capable of building clusters of\\nvarying density. HDBSCAN is an ingenious combination of multiple ideas and describing the\\nalgorithm in full is out of the scope of this book.\\nHDBSCAN only has one important hyperparameter: n, that is the minimum number of\\nexamples to put in a cluster. This hyperparameter is relatively simple to choose by intuition.\\nHDBSCAN has very fast implementations: it can deal with millions of examples eﬀectively.\\nModern implementations of k-means are much faster than HDBSCAN though, but the qualities\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n7'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 124}, page_content='of the latter may outweigh its drawbacks for many practical tasks. It is recommended to\\nalways try HDBSCAN on your data ﬁrst.\\n9.2.3\\nDetermining the Number of Clusters\\nThe most important question is how many clusters does your dataset have? When the feature\\nvectors are one-, two- or three-dimensional, you can look at the data and see “clouds” of\\npoints in the feature space. Each cloud is a potential cluster. However, for D-dimensional\\ndata, with D > 3, looking at the data is problematic2.\\nThere’s one practically useful method of determining the reasonable number of clusters based\\non the concept of prediction strength. The idea is to split the data into training and test\\nset, similarly to how we do in supervised learning. Once you have the training and test sets,\\nStr of size Ntr and Ste of size Nte respectively, you ﬁx k, the number of clusters, and run a\\nclustering algorithm C on sets Str and Ste and obtain the clustering results C(Str, k) and\\nC(Ste, k).\\nLet A be the clustering C(Str, k) built using the training set. Note that the clusters in A\\ncan be deﬁned by some regions. If an example falls within one of those regions, then this\\nexample belongs to some speciﬁc cluster. For example, if we apply the k-means algorithm to\\nsome dataset, it results in a partition of the feature space into k polygonal regions, as we saw\\nin ﬁg. 2.\\nDeﬁne the Nte ◊Nte co-membership matrix D[A, Ste] as follows: D[A, Ste](i,iÕ) = 1 if and\\nonly if examples xi and xiÕ from the test set belong to the same cluster according to the\\nclustering A. Otherwise D[A, Ste](i,iÕ) = 0.\\nLet’s take a break and see what we have here. We have built, using the training set of\\nexamples, a clustering A that has k clusters. Then we have built the co-membership matrix\\nthat indicates whether two examples from the test set belong to the same cluster in A.\\nIntuitively, if the quantity k is the reasonable number of clusters, then two examples that\\nbelong to the same cluster in clustering C(Ste, k) will most likely belong to the same cluster\\nin clustering C(Str, k). On the other hand, if k is not reasonable (too high or too low), then\\ntraining data-based and test data-based clusterings will likely be less consistent.\\n2Some analysts look at multiple two-dimensional scatter plots, in which only a pair of features are present\\nat the same time. It might give an intuition about the number of clusters. However, such an approach suﬀers\\nfrom subjectivity, is prone to error and counts as an educated guess rather than a scientiﬁc method.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n8'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 125}, page_content='full dataset\\ntraining set\\ntest set\\n(a)\\n(b)\\n(c)\\nFigure 3: The clustering for k = 4: (a) training data clustering; (b) test data clustering; (c)\\ntest data plotted over the training clustering.\\nThe idea is illustrated in ﬁg. 3. The plots in ﬁg. 3a and 3b show respectively C(Str, 4) and\\nC(Ste, 4) with their respective cluster regions. Fig. 3c shows the test examples plotted over\\nthe training data cluster regions. You can see in 3c that orange test examples don’t belong\\nanymore to the same cluster according to the clustering regions obtained from the training\\ndata. This will result in many zeroes in the matrix D[A, Ste] which, in turn, is an indicator\\nthat k = 4 is likely not the best number of clusters.\\nMore formally, the prediction strength for the number of clusters k is given by,\\nps(k)\\ndef\\n=\\nmin\\nj=1,...,k\\n1\\n|Aj|(|Aj| ≠1)\\nÿ\\ni,iÕœAj\\nD[A, Ste](i,iÕ),\\nwhere A\\ndef\\n= C(Str, k), Aj is jth cluster from the clustering C(Ste, k) and |Aj| is the number\\nof examples in cluster Aj.\\nGiven a clustering C(Str, k), for each test cluster, we compute the proportion of observation\\npairs in that cluster that are also assigned to the same cluster by the training set centroids.\\nThe prediction strength is the minimum of this quantity over the k test clusters.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n9'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 126}, page_content='Figure 4: Predictive strength for diﬀerent values of k for two-, three- and four-cluster data.\\nExperiments suggest that a reasonable number of clusters is the largest k such that ps(k) is\\nabove 0.8. You can see in ﬁg. examples of predictive strength for diﬀerent values of k for\\ntwo-, three- and four-cluster data.\\nFor non-deterministic clustering algorithms, such as k-means, which\\ncan generate diﬀerent clusterings depending on the initial positions\\nof centroids, it is recommended to do multiple runs of the clustering\\nalgorithm for the same k and compute the average prediction strength\\n¯ps(k) over multiple runs. Another eﬀective method to estimate the\\nnumber of clusters is the gap statistic method. Other, less automatic\\nmethods, which some analysts still use, include the elbow method\\nand the average silhouette method.\\n9.2.4\\nOther Clustering Algorithms\\nDBSCAN and k-means compute so-called hard clustering, in which each example can\\nbelong to only one cluster. Gaussian mixture model (GMM) allow each example to be a\\nmember of several clusters with diﬀerent membership score (HDBSCAN allows that too,\\nby the way). Computing a GMM is very similar to doing model-based density estimation.\\nIn GMM, instead of having just one multivariate normal distribution (MND), we have a\\nweighted sum of several MNDs:\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n10'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 127}, page_content='fX =\\nk\\nÿ\\nj=1\\n„jfµj,Σj,\\nwhere fµj,Σj is a MND j, and „j is its weight in the sum. The values of parameters µj, Σj,\\nand „j, for all j = 1, . . . , k are obtained using the expectation maximization algorithm\\n(EM) to optimize the maximum likelihood criterion.\\nAgain, for simplicity, let us look at the one-dimensional data. Also assume that there are two\\nclusters: k = 2. In this case, we have two Gaussian distributions,\\nf(x | µ1, ‡2\\n1) =\\n1\\n\\uf8ff\\n2ﬁ‡2\\n1\\ne\\n≠(x≠µ1)2\\n2‡2\\n1\\nand f(x | µ2, ‡2\\n2) =\\n1\\n\\uf8ff\\n2ﬁ‡2\\n2\\ne\\n≠(x≠µ2)2\\n2‡2\\n2\\n,\\n(3)\\nwhere f(x | µ1, ‡2\\n1) and f(x | µ2, ‡2\\n2) are two parametrized pdf deﬁning the likelihood of\\nX = x.\\nWe use the EM algorithm to estimate µ1, ‡2\\n1, µ2, ‡2\\n2, „1, and „2. The parameters „1 and „2\\nof the GMM are useful for the density estimation task and less useful for the clustering task,\\nas we will see below.\\nEM works like follows. In the beginning, we guess the initial values for µ1, ‡2\\n1, µ2, and ‡2\\n2,\\nand set „1 = „2 = 1\\n2 (in general, it’s 1\\nk for each „k).\\nAt each iteration of EM, the following four steps are executed:\\n1. For all i = 1, . . . , N, calculate the likelihood of each xi using eq. 3:\\nf(xi | µ1, ‡2\\n1) Ω\\n1\\n\\uf8ff\\n2ﬁ‡2\\n1\\ne\\n≠(xi≠µ1)2\\n2‡2\\n1\\nand f(xi | µ2, ‡2) Ω\\n1\\n\\uf8ff\\n2ﬁ‡2\\n2\\ne\\n≠(xi≠µ2)2\\n2‡2\\n2\\n.\\n2. Using Bayes’ Rule, for each example xi, calculate the likelihood b(j)\\ni\\nthat the example\\nbelongs to cluster j œ {1, 2} (in other words, the likelihood that the example was drawn\\nfrom the Gaussian j):\\nb(j)\\ni\\nΩ\\nf(xi | µj, ‡2\\nj )„j\\nf(xi | µ1, ‡2\\n1)„1 + f(xi | µ2, ‡2\\n2)„2\\n.\\nThe parameter „j reﬂects how likely is that our Gaussian distribution j with parameters µj\\nand “j may have produced our dataset. That is why in the beginning we set „1 = „2 = 1\\n2: we\\ndon’t know how each of the two Gaussians is likely, and we reﬂect our ignorance by setting\\nthe likelihood of both to one half.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n11'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 128}, page_content='iteration 1\\niteration 2\\niteration 10\\niteration 40\\nFigure 5: The progress of the Gaussian mixture model estimation using the EM algorithm\\nfor two clusters (k = 2).\\n3. Compute the new values of µj and ‡2\\nj , j œ {1, 2} as,\\nµj Ω\\nqN\\ni=1 b(j)\\ni xi\\nqN\\ni=1 b(j)\\ni\\nand ‡2\\nj Ω\\nqN\\ni=1 b(j)\\ni (xi ≠µj)2\\nqN\\ni=1 b(j)\\ni\\n.\\n(4)\\n4. Update „j, j œ {1, 2} as,\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n12'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 129}, page_content='„j Ω 1\\nN\\nN\\nÿ\\ni=1\\nb(j)\\ni .\\nThe steps 1 ≠4 are executed iteratively until the values µj and ‡2\\nj don’t change much: for\\nexample, the change is below some threshold ‘. Fig. 5 illustrates this process.\\nYou may have noticed that the EM algorithm is very similar to the k-means algorithm: start\\nwith random clusters, then iteratively update each cluster’s parameters by averaging the\\ndata that is assigned to that cluster. The only diﬀerence in the case of the GMM is that the\\nassignment of an example xi to the cluster j is soft: xi belongs to cluster j with probability\\nb(j)\\ni . This is why we calculate the new values for µj and ‡2\\nj in eq. 4 not as an average (used\\nin k-means) but as a weighted average with weights b(j)\\ni .\\nOnce we have learned the parameters µj and ‡2\\nj for each cluster j, the membership score of\\nexample x in cluster j is given by f(x | µj, ‡2\\nj ).\\nThe extension to D-dimensional data (D > 1) is straightforward. The only diﬀerence is\\nthat instead of the variance ‡2, we now have the covariance matrix Σ that parametrizes the\\nmultinomial normal distribution (MND). The advantage of GMM over k-means is that the\\nclusters in GMM can have a form of an ellipse that can have an arbitrary elongation and\\nrotation. The values in the covariance matrix control these properties.\\nHow to choose k in GMM? Unfortunately, there’s no universally recognized method. What\\nis usually recommended to do is to split your dataset into training and test set. Then try\\ndiﬀerent k and build a diﬀerent model f k\\ntr for each k on the training data. Then choose the\\nmodel that maximizes the likelihood of examples in the test set:\\narg max\\nk\\nNte\\nŸ\\ni=1\\nf k\\ntr(xi),\\nwhere Nte is the size of the test set.\\nThere is a variety of clustering algorithms described in the literature.\\nWorth mentioning are spectral clustering and hierarchical cluster-\\ning. For some datasets, you may ﬁnd those more appropriate. However,\\nin most practical cases, kmeans, HDBSCAN and the Gaussian mixture\\nmodel would satisfy your needs.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n13'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 130}, page_content='9.3\\nDimensionality Reduction\\nMany modern machine learning algorithms, such as ensemble algorithms and neural networks\\nhandle well very high-dimensional examples, up to millions of features.\\nWith modern\\ncomputers and graphical processing units (GPUs), dimensionality reduction techniques are\\nused much less in practice than in the past. The most frequent use case for dimensionality\\nreduction is data visualization: humans can only interpret on a plot the maximum of three\\ndimensions.\\nAnother situation in which you could beneﬁt from dimensionality reduction is when you\\nhave to build an interpretable model and to do so you are limited in your choice of learning\\nalgorithms. For example, you can only use decision tree learning or linear regression. By\\nreducing your data to lower dimensionality and by ﬁguring out which quality of the original\\nexample each new feature in the reduced feature space reﬂects, one can use simpler algorithms.\\nDimensionality reduction removes redundant or highly correlated features; it also reduces the\\nnoise in the data — all that contributes to the interpretability of the model.\\nThe three most widely used techniques of dimensionality reduction are principal com-\\nponent analysis (PCA), uniform manifold approximation and projection (UMAP),\\nand autoencoders.\\nI already explained autoencoders in Chapter 7. You can use the low-dimensional output of the\\nbottleneck layer of the autoencoder as the vector of reduced dimensionality that represents\\nthe high-dimensional input feature vector.\\nYou know that this low-dimensional vector\\nrepresents the essential information contained in the input vector because the autoencoder is\\ncapable of reconstructing the input feature vector based on the bottleneck layer output alone.\\n9.3.1\\nPrincipal Component Analysis\\nPrincipal component analysis or PCA is one of the oldest methods. The math behind it\\ninvolves operation on matrices that I didn’t explain in Chapter 2, so I leave the math of\\nPCA for your further reading. Here, I only provide intuition and illustrate the method on an\\nexample.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n14'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 131}, page_content='(a)\\n(b)\\n(c)\\nFigure 6: PCA: (a) the original data; (b) two principal components displayed as vectors; (c)\\nthe data projected on the ﬁrst principal component.\\nConsider a two-dimensional data as shown in ﬁg. 6a. Principal components are vectors that\\ndeﬁne a new coordinate system in which the ﬁrst axis goes in the direction of the highest\\nvariance in the data. The second axis is orthogonal to the ﬁrst one and goes in the direction\\nof the second highest variance in the data. If our data was three-dimensional, the third axis\\nwould be orthogonal to both the ﬁrst and the second axes and go in the direction of the third\\nhighest variance, and so on. In ﬁg. 6b, the two principal components are shown as arrows.\\nThe length of the arrow reﬂects the variance in this direction.\\nNow, if we want to reduce the dimensionality of our data to Dnew < D, we pick Dnew\\nlargest principal components and project our data points on them. For our two-dimensional\\nillustration, we can set Dnew = 1 and project our examples to the ﬁrst principal component\\nto obtain the orange points in ﬁg. 6c.\\nTo describe each orange point, we need only one coordinate instead\\nof two: the coordinate with respect to the ﬁrst principal component.\\nWhen our data is very high-dimensional, it often happens in practice\\nthat the ﬁrst two or three principal components account for most of the\\nvariation in the data, so by displaying the data on a 2D or 3D plot we\\ncan indeed see a very high-dimensional data and its properties.\\n9.3.2\\nUMAP\\nThe idea behind many of the modern dimensionality reduction algorithms, especially those\\ndesigned speciﬁcally for visualization purposes, such as t-SNE and UMAP, is basically\\nthe same. We ﬁrst design a similarity metric for two examples. For visualization purposes,\\nbesides the Euclidean distance between the two examples, this similarity metric often reﬂects\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n15'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 132}, page_content='some local properties of the two examples, such as the density of other examples around\\nthem.\\nIn UMAP, this similarity metric w is deﬁned as follows,\\nw(xi, xj)\\ndef\\n= wi(xi, xj) + wj(xj, xi) ≠wi(xi, xj)wj(xj, xi).\\n(5)\\nThe function wi(xi, xj) is deﬁned as,\\nwi(xi, xj)\\ndef\\n= exp\\n3\\n≠d(xi, xj) ≠ﬂi\\n‡i\\n4\\n,\\nwhere d(xi, xj) is the Euclidean distance between two examples, ﬂi is the distance from xi\\nto its closest neighbor, and ‡i is the distance from xi to its kth closest neighbor (k is a\\nhyperparameter of the algorithm).\\nIt can be shown that the metric in eq. 5 varies in the range from 0 to 1 and is symmetric,\\nwhich means that w(xi, xj) = w(xj, xi).\\nLet w denote the similarity of two examples in the original high dimensional space and let\\nwÕ be the similarity given by the same eq. 5 in the new low-dimensional space. Because the\\nvalues of w and wÕ lie in the range between 0 and 1, we can see them as two probability\\ndistributions. A widely used metric of similarity between two probability distributions is\\ncross-entropy:\\nC(w, wÕ) =\\nN\\nÿ\\ni=1\\nN\\nÿ\\nj=1\\nw(xi, xj) ln\\nA\\nw(xi, xj)\\nwÕ(xÕ\\ni, xÕ\\nj)\\nB\\n+ (1 ≠w(xi, xj)) ln\\nA\\n1 ≠w(xi, xj)\\n1 ≠wÕ(xÕ\\ni, xÕ\\nj)\\nB\\n,\\n(6)\\nwhere xÕ is the low-dimensional “version” of the original high-dimensional example x.\\nAs you can see from eq. 6, when w(xi, xj) is similar to wÕ(xÕ\\ni, xÕ\\nj), for all pairs (i, j), then\\nC(w, wÕ) is minimized. And this is precisely what we want: for any two examples xi and\\nxj, we want their similarity metric in the original and the lower-dimensional spaces to be as\\nsimilar as possible.\\nIn eq. 6 the unknown parameters are xÕ\\ni (for all i = 1, . . . , N) and we can compute them by\\ngradient descent by minimizing C(w, wÕ).\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n16'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 133}, page_content='PCA\\nUMAP\\nAutoencoder\\nFigure 7: Dimensionality reduction of the MNIST dataset using three diﬀerent techniques.\\nIn ﬁg. 7, you can see the result of dimensionality reduction applied to the MNIST dataset of\\nhandwritten digits. MNIST is commonly used for benchmarking various image processing\\nsystems; it contains 70,000 labeled examples. Ten diﬀerent colors on the plot correspond to\\nten classes. Each point on the plot corresponds a speciﬁc example in the dataset. As you can\\nsee, UMAP separates examples visually better (remember, it doesn’t have access to labels).\\nIn practice, UMAP is slightly slower than PCA but faster than autoencoder.\\n9.4\\nOutlier Detection\\nOutlier detection is the problem of detecting in the dataset the examples that are very\\ndiﬀerent from what a typical example in the dataset looks like. We have already seen several\\ntechniques that could help to solve this problem: autoencoder and one-class classiﬁer learning.\\nIf we use autoencoder, we train it on our dataset. Then, if we want to predict whether an\\nexample is an outlier, we can use the autoencoder model to reconstruct the example from\\nthe bottleneck layer. The model will unlikely be capable of reconstructing an outlier.\\nIn one-class classiﬁcation, the model either predicts that the input example belongs to the\\nclass, or it’s an outlier.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n17'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 134}, page_content='The\\nHundred-\\nPage\\nMachine\\nLearning\\nBook\\nAndriy Burkov'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 135}, page_content='“All models are wrong, but some are useful.”\\n— George Box\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 136}, page_content='10\\nOther Forms of Learning\\n10.1\\nMetric Learning\\nI mentioned that the most frequently used metrics of similarity (or dissimilarity) between\\ntwo feature vectors are Euclidean distance and cosine similarity. Such choices of metric\\nseem logical but arbitrary, just like the choice of the squared error in linear regression. The\\nfact that one metric can work better than another depending on the dataset is an indicator\\nthat none of them is perfect.\\nYou can create your metric that would work better for your dataset. It’s then possible to\\nintegrate your metric into any learning algorithm that needs a metric, like k-means or kNN.\\nHow can you know, without trying all possibilities, which equation would be a good metric?\\nYou can train your metric from data.\\nRemember the Euclidean distance between two feature vectors x and xÕ:\\nd(x, xÕ)\\ndef\\n=\\n\\uf8ff\\n(x ≠xÕ)2 =\\n\\uf8ff\\n(x ≠xÕ)(x ≠xÕ).\\nWe can slightly modify this metric to make it parametrizable and then learn these parameters\\nfrom data. Consider the following modiﬁcation:\\ndA(x, xÕ) = Îx ≠xÕÎA\\ndef\\n=\\nÒ\\n(x ≠xÕ)€A(x ≠xÕ),\\nwhere A is a D ◊D matrix. Let’s say D = 3. If we let A be the identity matrix,\\nA\\ndef\\n=\\nS\\nU\\n1\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n1\\nT\\nV ,\\nthen dA becomes the Euclidean distance. If we have a general diagonal matrix, like this,\\nA\\ndef\\n=\\nS\\nU\\n2\\n0\\n0\\n0\\n8\\n0\\n0\\n0\\n1\\nT\\nV ,\\nthen diﬀerent dimensions have diﬀerent importance in the metric. (In the above example,\\nthe second dimension is the most important in the metric calculation.) More generally, to be\\ncalled a metric a function of two variables has to satisfy three conditions:\\n1.\\nd(x, xÕ) Ø 0\\nnonnegativity\\n2.\\nd(x, xÕ) Æ d(x, xÕ) + d(xÕ, z)\\ntriangle inequality\\n3.\\nd(x, xÕ) = d(xÕ, x)\\nsymmetry\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 137}, page_content='To satisfy the ﬁrst two conditions, the matrix A has to be positive semideﬁnite. You can\\nsee a positive semideﬁnite matrix as the generalization of the notion of a nonnegative real\\nnumber to matrices. Any positive semideﬁnite matrix M satisﬁes:\\nz€Mz Ø 0.\\nThe above property follows from the deﬁnition of a positive semideﬁnite matrix. The proof\\nthat the second condition is satisﬁed when the matrix A is positive semideﬁnite can be found\\non the book’s companion website.\\nTo satisfy the third condition, we can simply take (d(x, xÕ) + d(xÕ, x))/2.\\nLet’s say we have an unannotated set X = {xi}N\\ni=1. To build the training data for our\\nmetric learning problem, we manually create two sets. The ﬁrst set S is such that a pair of\\nexamples (xi, xk) belongs to set S if xi and xk are similar (from our subjective perspective).\\nThe second set D is such that a pair of examples (xi, xk) belongs to set D if xi and xk are\\ndissimilar.\\nTo train the matrix of parameters A from the data, we want to ﬁnd a positive semideﬁnite\\nmatrix A that solves the following optimization problem:\\nmin\\nA\\nÿ\\n(xi,xk)œS\\nÎx ≠xÕÎ2\\nA such that\\nÿ\\n(xi,xk)œD\\nÎx ≠xÕÎA Ø c,\\nwhere c is a positive constant (can be any number).\\nThe solution to this optimization problem is found by gradient descent\\nwith a modiﬁcation that ensures that the found matrix A is positive\\nsemideﬁnite. We leave the description of the algorithm out of the scope\\nof this book for further reading. You should know that there are many\\nother ways to learn a metric, including non-linear and kernel-based.\\nHowever, the one presented in this book gives a good result for most\\npractical problems.\\n10.2\\nLearning to Rank\\nLearning to rank is a supervised learning problem. Among others, one frequent problem\\nsolved using learning to rank is the optimization of search results returned by a search engine\\nfor a query. In search result ranking optimization, a labeled example Xi in the training set\\nof size N is a ranked collection of documents of size ri (labels are ranks of documents). A\\nfeature vector represents each document in the collection. The goal of the learning is to ﬁnd\\na ranking function f which outputs values that can be used to rank documents. For each\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n4'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 138}, page_content='training example, an ideal function f would output values that induce the same ranking of\\ndocuments as given by labels.\\nEach example Xi, i = 1, . . . , N, is a collection of feature vectors with labels:\\nXi =\\n{(xi,j, yi,j)}ri\\nj=1. Features in a feature vector xi,j represent the document j = 1, . . . , ri.\\nFor example, x(1)\\ni,j could represent how recent is the document, x(2)\\ni,j would reﬂect whether the\\nwords of the query can be found in the document title, x(3)\\ni,j could represent the size of the\\ndocument, and so on. The label yi,j could be the rank (1, 2, . . . , ri) or a score. For example,\\nthe lower the score, the higher the document should be ranked.\\nThere are three principal approaches to solve such a learning problem: pointwise, pairwise,\\nand listwise.\\nPointwise approach transforms each training example into multiple examples: one example\\nper document. The learning problem becomes a standard supervised learning problem, either\\nregression or logistic regression. In each example (x, y) of the pointwise learning problem,\\nx is the feature vector of some document, and y is the original score (if yi,j is a score) or a\\nsynthetic score obtained from the ranking (the higher the rank, the lower is the synthetic\\nscore). Any supervised learning algorithm can be used in this case. The solution is usually\\nfar from perfect. Principally, this is because each document is considered in isolation, while\\nthe original ranking (given by the labels yi,j of the original training set) could optimize the\\npositions of the whole set of documents. For example, if we have already given a high rank to\\na Wikipedia page in some collection of documents, we would not give a high rank to another\\nWikipedia page for the same query.\\nIn the pairwise approach, the problem also considers documents in isolation, however, in this\\ncase, a pair of documents is considered at once. Given a pair of documents (xi, xk) we want\\nto build a model f that, given (xi, xk) as input, outputs a value close to 1 if xi has to be put\\nhigher than xk in the ranking. Otherwise, f outputs a value close to 0. At the test time,\\ngiven a model, the ﬁnal ranking for an unlabeled example X is obtained by aggregating the\\npredictions for all pairs of documents in X. Such an approach works better than pointwise,\\nbut still far from perfect.\\nThe state of the art rank learning algorithms, such as LambdaMART, implement the\\nlistwise approach. In the listwise approach, we try to optimize the model directly on some\\nmetric that reﬂects the quality of ranking. There are various metrics for assessing search\\nengine result ranking, including precision and recall. One popular metric that combines both\\nprecision and recall is called mean average precision (MAP).\\nTo deﬁne MAP, let us ask judges (Google call those people rankers) to examine a collection\\nof search results for a query and assign relevancy labels to each search result. Labels could\\nbe binary (1 for “relevant” and 0 for “irrelevant”) or on some scale, say from 1 to 5: the\\nhigher the value, the more relevant the document is to the search query. Let our judges build\\nsuch relevancy labeling for a collection of 100 queries. Now, let us test our ranking model on\\nthis collection. The precision of our model for some query is given by:\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 139}, page_content='precision = |{relevant documents} ﬂ{retrieved documents}|\\n|{retrieved documents}|\\n,\\nwhere the notation | · | means “the number of.” The average precision metric, AveP, is\\ndeﬁned for a ranked collection of documents returned by a search engine for a query q as,\\nAveP(q) =\\nqn\\nk=1(P(k) ◊rel(k))\\n|{relevant documents}|,\\nwhere n is the number of retrieved documents, P(k) denotes the precision computed for\\nthe top k search results returned by our ranking model for the query, rel(k) is an indicator\\nfunction equaling 1 if the item at rank k is a relevant document (according to judges) and\\nzero otherwise. Finally, the MAP for a collection of search queries of size Q is given by,\\nMAP =\\nqQ\\nq=1 AveP(q)\\nQ\\n.\\nNow we get back to LambdaMART. This algorithm implements a pairwise approach, and it\\nuses gradient boosting to train the ranking function h(x). Then the binary model f(xi, xk)\\nthat predicts whether the document xi should have a higher rank than the document xk (for\\nthe same search query) is given by a sigmoid with a hyperparameter –,\\nf(xi, xk)\\ndef\\n=\\n1\\n1 + exp((h(xi) ≠h(xk))–.\\nAgain, as with many models that predict probability, the cost function is cross-entropy\\ncomputed using the model f. In our gradient boosting, we combine multiple regression trees\\nto build the function h by trying to minimize the cost. Remember that in gradient boosting\\nwe add a tree to the model to reduce the error that the current model makes on the training\\ndata. For the classiﬁcation problem, we computed the derivative of the cost function to\\nreplace real labels of training examples with these derivatives. LambdaMART works similarly,\\nwith one exception. It replaces the real gradient with a combination of the gradient and\\nanother factor that depends on the metric, such as MAP. This factor modiﬁes the original\\ngradient by increasing or decreasing it so that the metric value is improved.\\nThat is a very bright idea and not many supervised learning algorithms can boast that they\\noptimize a metric directly. Optimizing a metric is what we really want, but what we do in a\\ntypical supervised learning algorithm is we optimize the cost instead of the metric. Usually,\\nin supervised learning, as soon as we have found a model that optimizes the cost function, we\\ntry to tweak hyperparameters to improve the value of the metric. LambdaMART optimizes\\nthe metric directly.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n6'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 140}, page_content='The remaining question is how do we build the ranked list of results based on the predictions\\nof the model f which predicts whether its ﬁrst input has to be ranked higher than the second\\ninput. It’s generally a computationally hard problem, and there are multiple implementations\\nof rankers capable of transforming pairwise comparisons into a ranking list. The most\\nstraightforward approach is to use an existing sorting algorithm.\\nSorting algorithms sort a collection of numbers in increasing or decreas-\\ning order. (The simplest sorting algorithm is called bubble sort. It’s\\nusually taught in engineering schools.) Typically, sorting algorithms\\niteratively compare a pair of numbers in the collection and change their\\npositions in the list based on the result of that comparison. If we plug\\nour function f into a sorting algorithm to execute this comparison, the\\nsorting algorithm will sort documents and not numbers.\\n10.3\\nLearning to Recommend\\nLeaning to recommend is an approach to build recommender systems. Usually, we have a\\nuser who consumes some content. We have the history of consumption, and we want to\\nsuggest this user new content that the user would like. It could be a movie on Netﬂix or a\\nbook on Amazon.\\nTraditionally, two approaches were used to give recommendations: content-based ﬁltering\\nand collaborative ﬁltering.\\nContent-based ﬁltering is based on learning what do users like based on the description of\\nthe content they consume. For example, if the user of a news site often reads news articles on\\nscience and technology, then we would suggest to this user more documents on science and\\ntechnology. More generally, we could create one training set per user and add news articles\\nto this dataset as a feature vector x and whether the user recently read this news article as a\\nlabel y. Then we build the model of each user and can regularly examine each new piece of\\ncontent to determine whether a speciﬁc user would read it or not.\\nThe content-based approach has many limitations. For example, the user can be trapped in\\nthe so-called ﬁlter bubble: the system will always suggest to that user the information that\\nlooks very similar to what user already consumed. That could result in complete isolation of\\nthe user from information that disagrees with their viewpoints or expands them. On a more\\npractical side, the users might just get recommendations of items they already know about,\\nwhich is undesirable.\\nCollaborative ﬁltering has a signiﬁcant advantage over content-based ﬁltering: the recommen-\\ndations to one user are computed based on what other users consume or rate. For instance,\\nif two users gave high ratings to the same ten movies, then it’s more likely that user 1 will\\nappreciate new movies recommended based on the tastes of the user 2 and vice versa. The\\ndrawback of this approach is that the content of the recommended items is ignored.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n7'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 141}, page_content='In collaborative ﬁltering, the information on user preferences is organized in a matrix. Each\\nrow corresponds to a user, and each column corresponds to a piece of content that user rated\\nor consumed. Usually, this matrix is huge and extremely sparse, which means that most of\\nits cells aren’t ﬁlled (or ﬁlled with a zero). The reason for such a sparsity is that most users\\nconsume or rate just a tiny fraction of available content items. It’s is very hard to make\\nmeaningful recommendations based on such sparse data.\\nMost real-world recommender systems use a hybrid approach: they combine recommendations\\nobtained by the content-based and collaborative ﬁltering models.\\nI already mentioned that content-based recommender model could be built using a classiﬁca-\\ntion or regression model that predicts whether a user will like the content based on content’s\\nfeatures. Examples of features could include the words in books or news articles the user\\nliked, the price, the recency of the content, the identity of the content author and so on.\\nTwo eﬀective collaborative-ﬁltering learning algorithms are factorization machines (FM)\\nand denoising autoencoders (DAE).\\n10.3.1\\nFactorization Machines\\nFactorization machines is a relatively new kind of algorithm. It was explicitly designed for\\nsparse datasets. Let’s illustrate the problem.\\nx(1)\\nx(2)\\nx(3)\\nx(4)\\nx(5)\\nx(6)\\n...\\nx(D)\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n...\\n...\\n...\\n...\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\nuser\\nEd\\nAl\\nZak\\n...\\n1\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n...\\n...\\n...\\n...\\n0\\n0\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n0\\n1\\n0\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n0\\n0\\n...\\n0\\n0\\n0\\n0\\n1\\nIt\\nUp Jaws Her\\nmovie\\n0.2\\n0.2\\n0.2\\n0\\n0\\n0\\n1\\n0\\n0\\n...\\n...\\n...\\n...\\n0.8\\n0.4\\n0.4\\n0.8\\n0.8\\xa0 0.4\\n0\\n0.7\\n0\\n0.7\\n0.8\\n0\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n0.6\\n0\\n...\\n0\\n0\\n0.7\\n0.1\\n0.1\\nIt\\nUp Jaws Her\\nrated\\xa0movies\\nx99 x100\\n0.3\\n0.3\\n0.3\\n0.35\\n0.35\\n0.5\\n0.95\\n...\\n0.8\\n0.8\\n0.8\\xa0\\n0.78\\n0.78\\n0.77\\n...\\n0.85\\n1\\n3\\n2\\n3\\n1\\n4\\n5\\n...\\ny \\ny(1)\\ny(2)\\ny(3)\\ny(4)\\ny(5)\\ny(6)\\ny(D)\\n...\\nx1\\nx2\\nx3\\nx21\\nx22\\nx23\\nx24\\n...\\n...\\nx40\\nx41\\nx42 x43\\n...\\nFigure 1: Example for sparse feature vectors x and their respective labels y.\\nIn ﬁg. 1 you see an example of sparse feature vectors with labels. Each feature vector\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n8'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 142}, page_content='represents information about one speciﬁc user and one speciﬁc movie. Features in the blue\\nsection represent a user. Users are encoded as one-hot vectors. Features in the green section\\nrepresent a movie. Movies are also encoded as one-hot vectors. Features in the yellow section\\nrepresent scores the user in green gave to each movie they rated. Feature x99 represents\\nthe ratio of movies with an Oscar among those the speciﬁc user has watched. Feature x100\\nrepresents the percentage of the movie watched by the user in blue before they scored the\\nmovie in green. The target y represents the score given by the user in blue to the movie in\\ngreen.\\nIn real recommender systems, the number of users can count in millions, so the matrix in\\nﬁg. 1 would count hundreds of millions of rows. The number of features could be hundreds\\nof thousands, depending on how reach is the choice of content and how creative you, as a\\ndata analyst, are in feature engineering. Features x99 and x100 were handcrafted during the\\nfeature engineering process, and I only show two features for the illustration purposes.\\nTrying to ﬁt a regression or classiﬁcation model to such an extremely sparse dataset would in\\npractice result in very poor generalization. Factorization machines approach this problem in\\na diﬀerent way.\\nThe factorization machine model is deﬁned as follows:\\nf(x)\\ndef\\n= b +\\nD\\nÿ\\ni=1\\nwixi +\\nD\\nÿ\\ni=1\\nD\\nÿ\\nj=i+1\\n(vivj)xixj,\\nwhere b and wi, i = 1, . . . , D are scalar parameters similar to those used in linear regression.\\nVectors vi, i = 1, . . . , D, are k-dimensional vectors of factors. k is a hyperparameter and\\nis usually much smaller than D. The expression (vivj) is a dot-product of the ith and\\njth vectors of factors. As you can see, instead of trying to ﬁnd just one wide vector of\\nparameters which can reﬂect poorly interactions between features because of sparsity, we\\ncomplete it by additional parameters that apply to pairwise interactions xixj between\\nfeatures. However, instead of having a parameter wi,j for each interaction, which would add\\nan enormous1 quantity of new parameters to the model, we factorize wi,j into vivj by adding\\nonly Dk π D(D ≠1) parameters to the model2.\\nDepending on the problem, the loss function could be squared error loss (for regression) or\\nhinge loss. For classiﬁcation with y œ {≠1, +1}, with hinge loss or logistic loss the prediction\\nis made as y = sign(f(x)). The logistic loss is deﬁned as,\\nloss(f(x), y) =\\n1\\nln 2 ln(1 + e≠yf(x)).\\nGradient descent can be used to optimize the average loss. In the example in ﬁg. 1, the\\nlabels are in {1, 2, 3, 4, 5}, so it’s a multiclass problem. We can use one versus rest strategy\\n1To be more precise we would add D(D ≠1) parameters wi,j.\\n2The notation π means “much less than.”\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n9'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 143}, page_content='to convert this multiclass problem into ﬁve binary classiﬁcation problems.\\n10.3.2\\nDenoising Autoencoders\\nFrom Chapter 7, you know what a denoising autoencoder is: it’s a neural network that\\nreconstructs its input from the bottleneck layer. The fact that the input is corrupted by\\nnoise while the output shouldn’t be, makes denoising autoencoders an ideal tool to build a\\nrecommender model.\\nThe idea is very straightforward: new movies a user could like are seen as if they were\\nremoved from the complete set of preferred movies by some corruption process. The goal of\\nthe denoising autoencoder is to reconstruct those removed items.\\nTo prepare the training set for our denoising autoencoder, remove the blue and green features\\nfrom the training set in ﬁg. 1. Because now some examples become duplicates, keep only the\\nunique ones.\\nAt the training time, randomly replace some of the non-zero yellow features in the input\\nfeature vectors with zeros. Train the autoencoder to reconstruct the uncorrupted input.\\nAt the prediction time, build a feature vector for the user. The feature vector will include\\nuncorrupted yellow features as well as the handcrafted features like x99 and x100. Use the\\ntrained DAE model to reconstruct the uncorrupted input. Recommend to the user movies\\nthat have the highest scores at the model’s output.\\nAnother eﬀective collaborative-ﬁltering model is a feed-forward neural\\nnetwork with two inputs and one output. Remember from Chapter 8\\nthat neural networks are good at handling multiple simultaneous inputs.\\nA training example here is a triplet (u, m, r). The input vector u is a\\none-hot encoding of a user. The second input vector m is a one-hot\\nencoding of a movie. The output layer could be either a sigmoid (in\\nwhich case the label r is in [0, 1]) or ReLU, in which case r can be in\\nsome typical range, [1, 5] for example.\\n10.4\\nSelf-Supervised Learning: Word Embeddings\\nWe have already discussed word embeddings in Chapter 7. Recall that word embeddings are\\nfeature vectors that represent words. They have the property that similar words have similar\\nword vectors. The question that you probably want to ask is where these word embedding\\ncome from. The answer is: they are learned from data.\\nThere are many algorithms to learn word embeddings. Here, we consider in detail only one of\\nthem: word2vec, and only one version of word2vec called skip-gram, which works very well\\nin practice. Pretrained word2vec embeddings for many languages are available to download\\nonline.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n10'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 144}, page_content='In word embedding learning, our goal is to build a model which we can use to convert a\\none-hot encoding of a word into a word embedding. Let our dictionary contain 10000 words.\\nThe one-hot vector for each word is a 10000-dimensional vector of all zeroes except for one\\ndimension that contains a 1. Diﬀerent words have 1 in diﬀerent dimensions.\\nConsider a sentence: “I almost ﬁnished reading the book on machine learning.” Now, consider\\nthe same sentence from which we have removed some word, say “book.” Our sentence becomes:\\n“I almost ﬁnished reading the · on machine learning.” Now let’s only keep the three words\\nbefore the · and three words after it: “ﬁnished reading the · on machine learning.” Looking\\nat this seven-word window around the ·, if I ask you to guess what · stands for, you would\\nprobably say: “book,” “article,” or “paper.” That is how the context words let you predict\\nthe word they surround. It’s also how the machine can discover that words “book,” “paper,”\\nand “article” have a similar meaning: because they share similar contexts in multiple texts.\\nIt turns out that it works the other way around too: a word can predict the context that\\nsurrounds it. The piece “ﬁnished reading the · on machine learning” is called a skip-gram\\nwith window size 7 (3 + 1 + 3). By using the documents available on the Web, we can easily\\ncreate hundreds of millions of skip-grams.\\nLet’s denote words in a skip-gram like this: [x≠3, x≠2, x≠1, x, x+1, x+2, x+3]. In our above\\nexample of the sentence, x≠3 is the one-hot vector for “ﬁnished,” x≠2 corresponds to “reading,”\\nx is the skipped word (·), x+1 is “on” and so on. A skip-gram with window size 5 will look\\nlike this: [x≠2, x≠1, x, x+1, x+2].\\nThe skip-gram model with window size 5 is schematically depicted in ﬁg. 2. It is a fully-\\nconnected network, like the multilayer perceptron. The input word is the one denoted as · in\\nour skip-gram. The neural network has to learn to predict the context words of the skip-gram\\ngiven the central word.\\nYou can see now why the learning of this kind is called self-supervised: the labeled examples\\nget extracted from the unlabeled data such as text.\\nThe activation function used in the output layer is softmax. The cost function is the negative\\nlog-likelihood. The embedding for a word is obtained as the output of the embedding layer\\nwhen the one-hot encoding of this word is given as the input to the model.\\nBecause of the large number of parameters in the word2vec models, two\\ntechniques are used to make the computation more eﬃcient: hierarchical\\nsoftmax (an eﬃcient way of computing softmax that consists in repre-\\nsenting the outputs of softmax as leaves of a binary tree) and negative\\nsampling (essentially, the idea is only to update a random sample of all\\noutputs per iteration of gradient descent). We leave these for further\\nreading.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n11'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 145}, page_content='x\\n1\\n10000\\n0\\n0\\n...\\n1\\n...\\n0\\n0\\n0\\n...\\n...\\n1\\n10000\\n...\\n1\\n300\\n1\\n10000\\n...\\n1\\n10000\\n1\\n10000\\n1\\n10000\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\nx\\xad2\\nx\\xad1\\nx+1\\nx+2\\ninput\\nword\\ninput\\nlayer\\nembedding\\nlayer\\noutput\\nlayer\\nFigure 2: The skip-gram model with window size 5 and the embedding layer of 300 units.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n12'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 146}, page_content='The\\nHundred-\\nPage\\nMachine\\nLearning\\nBook\\nAndriy Burkov'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 147}, page_content='“All models are wrong, but some are useful.”\\n— George Box\\nThe book is distributed on the “read ﬁrst, buy later” principle.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 148}, page_content='11\\nConclusion\\nWow, that was fast! You are really good if you got here and managed to understand most of\\nthe book’s material.\\nIf you look at the number at the bottom of this page, you see that we have overspent paper,\\nwhich means that the title of the book was slightly misleading. I hope that you forgive me\\nthis little marketing trick. After all, if I wanted to make this book exactly a hundred pages, I\\ncould reduce font size, white margins, and line spacing, or remove the section on UMAP and\\nleave you on your own with the original paper. Believe me: you would not want to be left\\nalone with the original paper on UMAP!\\nHowever, by stopping now, I feel conﬁdent that you have got everything you need to become\\na great modern data analyst or machine learning engineer. That doesn’t mean that I covered\\neverything, but what I covered in a hundred pages you would ﬁnd in a bunch of books, each\\nthousand-page thick. Much of what I covered is not in the books at all: typical machine\\nlearning books are conservative and academic, while I emphasize those algorithms and\\nmethods that you will ﬁnd useful in your day to day work.\\nWhat exactly I didn’t cover, but would have covered if it was a thousand-page machine\\nlearning book?\\n11.1\\nTopic Modeling\\nIn text analysis, topic modeling is a prevalent unsupervised learning problem. You have a\\ncollection of text documents, and you would like to discover topics present in each document.\\nLatent Dirichlet Allocation (LDA) is a very eﬀective algorithm of topic discovery. You\\ndecide how many topics are present in your collection of documents and the algorithm assigns\\na topic to each word in this collection. Then, to extract the topics from a document, you\\nsimply count how many words of each topic are present in that document.\\n11.2\\nGaussian Processes\\nGaussian processes (GP) is a supervised learning method that competes with kernel\\nregression. It has some advantages over the latter. For example, it provides conﬁdence\\nintervals for the regression line in each point. I decided not to explain GP because I could\\nnot ﬁgure out a simple way to explain them, but you deﬁnitely could spend some time to\\nlearn about GP. It will be time well spent.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 149}, page_content='11.3\\nGeneralized Linear Models\\nGeneralized linear model (GLM) is a generalization of the linear regression to modeling\\nvarious forms of dependency between the input feature vector and the target. Logistic\\nregression, for instance, is one form of GLMs. If you are interested in regression and you\\nlook for simple and explainable models, you should deﬁnitely read more on GLM.\\n11.4\\nProbabilistic Graphical Models\\nWe have mentioned one example of probabilistic graphical models (PGMs) in Chapter 7:\\nconditional random ﬁelds (CRF). With CRF we can model the input sequence of words\\nand relationships between the features and labels in this sequence as a sequential dependency\\ngraph. More generally, a PGM can be any graph. A graph is a structure consisting of a\\ncollection of nodes and edges that join a pair of nodes. Each node in PGM represents some\\nrandom variable (values of which can be observed or unobserved), and edges represent the\\nconditional dependence of one random variable on another random variable. For example,\\nthe random variable “sidewalk wetness” depends on the random variable “weather condition.”\\nBy observing values of some random variables, an optimization algorithm can learn from\\ndata the dependency between observed and unobserved variables.\\nPGMs allow the data analyst to see how the values of one feature depend on the values of\\nother features. If the edges of the dependency graph are directed, it becomes possible to infer\\ncausality. Unfortunately, constructing such models by hand require a substantial amount of\\ndomain expertise and a strong understanding of probability theory and statistics. The latter\\nis often a problem for many domain experts. Some algorithms allow learning the structure\\nof dependency graphs from data, but the learned models are often hard to interpret by a\\nhuman and thus they aren’t beneﬁcial for understanding complex probabilistic processes that\\ngenerated the data. CRF is by far the most used PGM with applications mostly in text and\\nimage processing. However, in these two domains, they were surpassed by neural networks.\\nAnother graphical model, hidden Markov model or HMM, in the past was frequently used\\nin speech recognition, time series analysis, and other temporal inference tasks, but, again\\nHMM lost to neural networks.\\nPGMs are also known under names of Bayesian networks, belief networks, and probabilistic\\nindependence networks.\\n11.5\\nMarkov Chain Monte Carlo\\nIf you work with graphical models and want to sample examples from a very complex\\ndistribution deﬁned by the dependency graph, you could use Markov chain Monte Carlo\\n(MCMC) algorithms. MCMC is a class of algorithms for sampling from any probability\\ndistribution deﬁned mathematically. Remember that when we talked about the denoising\\nautoencoder, we sampled the noise from the normal distribution. Sampling from standard\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n4'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 150}, page_content='distributions, such as normal or uniform, is relatively easy because their properties are well\\nknown. However, the task of sampling becomes signiﬁcantly more complicated when the\\nprobability distribution can have an arbitrary form deﬁned by a dependency graph learned\\nfrom data.\\n11.6\\nGenetic Algorithms\\nGenetic algorithms (GA) are a numerical optimization technique used to optimize undif-\\nferentiable optimization objective functions. They use concepts from evolutionary biology\\nto search for a global optimum (minimum or maximum) of an optimization problem, by\\nmimicking evolutionary biological processes.\\nGA work by starting with an initial generation of candidate solutions. If we look for optimal\\nvalues of the parameters of our model, we ﬁrst randomly generate multiple combinations of\\nparameter values. We then test each combination of parameter values against the objective\\nfunction. Imagine each combination of parameter values as a point in a multi-dimensional\\nspace. We then generate a subsequent generation of points from the previous generation by\\napplying such concepts as “selection,” “crossover,” and “mutation.”\\nIn a nutshell, this results in each new generation keeping more points similar to those points\\nfrom the previous generation that performed the best against the objective. In the new\\ngeneration, the points that performed the worst in the previous generation are replaced by\\n“mutations” and “crossovers” of the points that performed the best. A mutation of a point is\\nobtained by a random distortion of some attributes of the original point. A crossover is a\\ncertain combination of several points (for example, an average).\\nGenetic algorithms allow ﬁnding solutions to any measurable optimization criteria. For\\nexample, GA can be used to optimize the hyperparameters of a learning algorithm. They are\\ntypically much slower than gradient-based optimization techniques.\\n11.7\\nReinforcement Learning\\nAs we already discussed, reinforcement learning (RL) solves a very speciﬁc kind of problems\\nwhere the decision making is sequential. Usually, there’s an agent acting in an unknown\\nenvironment. Each action brings a reward and moves the agent to another state of the\\nenvironment (usually, as a result of some random process with unknown properties). The\\ngoal of the agent is to optimize its long-term reward.\\nReinforcement learning algorithms, such as Q-learning, as well as its neural network based\\ncounterparts, are used in learning to play video games, robotic navigation and coordination,\\ninventory and supply chain management, optimization of complex electric power systems\\n(power grids), and learning ﬁnancial trading strategies.\\nú ú ú\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2018-12-18T05:07:46+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf', 'total_pages': 152, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-01-22T19:51:34+00:00', 'trapped': '', 'modDate': 'D:20190122195134Z', 'creationDate': 'D:20181218050746Z', 'page': 151}, page_content='The book stops here. Don’t forget to occasionally visit the book’s companion wiki to stay\\nupdated on new developments in each machine learning area considered in the book. As I\\nsaid in Preface, this book, thanks to the constantly updated wiki, like a good wine keeps\\ngetting better after you buy it. Oh, and don’t forget that the book is distributed on the read\\nﬁrst, buy later principle. That means that if while reading these words you look at a digital\\nscreen, you are probably the right person for buying this book.\\nAndriy Burkov\\nThe Hundred-Page Machine Learning Book - Draft\\n6'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 0}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 1}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 2}, page_content='Aurélien Géron\\nHands-on Machine Learning with\\nScikit-Learn, Keras, and\\nTensorFlow\\nConcepts, Tools, and Techniques to\\nBuild Intelligent Systems\\nSECOND EDITION\\nBoston\\nFarnham\\nSebastopol\\nTokyo\\nBeijing\\nBoston\\nFarnham\\nSebastopol\\nTokyo\\nBeijing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 3}, page_content='978-1-492-03264-9\\n[LSI]\\nHands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\\nby Aurélien Géron\\nCopyright © 2019 Aurélien Géron. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com.\\nEditor: Nicole Tache\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Rebecca Demarest\\nJune 2019:\\n Second Edition\\nRevision History for the Early Release\\n2018-11-05: First Release\\n2019-01-24: Second Release\\n2019-03-07: Third Release\\n2019-03-29: Fourth Release\\n2019-04-22: Fifth Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492032649 for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-on Machine Learning with\\nScikit-Learn, Keras, and TensorFlow, the cover image, and related trade dress are trademarks of O’Reilly\\nMedia, Inc.\\nWhile the publisher and the author have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 4}, page_content='Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\\nPart I. \\nThe Fundamentals of Machine Learning\\n1. The Machine Learning Landscape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\\nWhat Is Machine Learning?                                                                                           4\\nWhy Use Machine Learning?                                                                                         4\\nTypes of Machine Learning Systems                                                                             8\\nSupervised/Unsupervised Learning                                                                           8\\nBatch and Online Learning                                                                                       15\\nInstance-Based Versus Model-Based Learning                                                      18\\nMain Challenges of Machine Learning                                                                       24\\nInsufficient Quantity of Training Data                                                                   24\\nNonrepresentative Training Data                                                                            26\\nPoor-Quality Data                                                                                                      27\\nIrrelevant Features                                                                                                     27\\nOverfitting the Training Data                                                                                   28\\nUnderfitting the Training Data                                                                                30\\nStepping Back                                                                                                             30\\nTesting and Validating                                                                                                   31\\nHyperparameter Tuning and Model Selection                                                       32\\nData Mismatch                                                                                                           33\\nExercises                                                                                                                          34\\n2. End-to-End Machine Learning Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\\nWorking with Real Data                                                                                                38\\nLook at the Big Picture                                                                                                  39\\niii'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 5}, page_content='Frame the Problem                                                                                                    39\\nSelect a Performance Measure                                                                                  42\\nCheck the Assumptions                                                                                             45\\nGet the Data                                                                                                                    45\\nCreate the Workspace                                                                                                45\\nDownload the Data                                                                                                    49\\nTake a Quick Look at the Data Structure                                                                50\\nCreate a Test Set                                                                                                          54\\nDiscover and Visualize the Data to Gain Insights                                                     58\\nVisualizing Geographical Data                                                                                 59\\nLooking for Correlations                                                                                           62\\nExperimenting with Attribute Combinations                                                        65\\nPrepare the Data for Machine Learning Algorithms                                                66\\nData Cleaning                                                                                                             67\\nHandling Text and Categorical Attributes                                                              69\\nCustom Transformers                                                                                                71\\nFeature Scaling                                                                                                            72\\nTransformation Pipelines                                                                                          73\\nSelect and Train a Model                                                                                               75\\nTraining and Evaluating on the Training Set                                                         75\\nBetter Evaluation Using Cross-Validation                                                              76\\nFine-Tune Your Model                                                                                                  79\\nGrid Search                                                                                                                 79\\nRandomized Search                                                                                                   81\\nEnsemble Methods                                                                                                     82\\nAnalyze the Best Models and Their Errors                                                             82\\nEvaluate Your System on the Test Set                                                                      83\\nLaunch, Monitor, and Maintain Your System                                                            84\\nTry It Out!                                                                                                                       85\\nExercises                                                                                                                          85\\n3. Classification. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  87\\nMNIST                                                                                                                             87\\nTraining a Binary Classifier                                                                                          90\\nPerformance Measures                                                                                                  90\\nMeasuring Accuracy Using Cross-Validation                                                        91\\nConfusion Matrix                                                                                                       92\\nPrecision and Recall                                                                                                   94\\nPrecision/Recall Tradeoff                                                                                          95\\nThe ROC Curve                                                                                                          99\\nMulticlass Classification                                                                                             102\\nError Analysis                                                                                                              104\\niv \\n| \\nTable of Contents'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 6}, page_content='Multilabel Classification                                                                                             108\\nMultioutput Classification                                                                                          109\\nExercises                                                                                                                        110\\n4. Training Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  113\\nLinear Regression                                                                                                        114\\nThe Normal Equation                                                                                              116\\nComputational Complexity                                                                                    119\\nGradient Descent                                                                                                         119\\nBatch Gradient Descent                                                                                           123\\nStochastic Gradient Descent                                                                                   126\\nMini-batch Gradient Descent                                                                                 129\\nPolynomial Regression                                                                                                130\\nLearning Curves                                                                                                           132\\nRegularized Linear Models                                                                                         136\\nRidge Regression                                                                                                      137\\nLasso Regression                                                                                                      139\\nElastic Net                                                                                                                 142\\nEarly Stopping                                                                                                          142\\nLogistic Regression                                                                                                      144\\nEstimating Probabilities                                                                                          144\\nTraining and Cost Function                                                                                   145\\nDecision Boundaries                                                                                                146\\nSoftmax Regression                                                                                                  149\\nExercises                                                                                                                        153\\n5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  155\\nLinear SVM Classification                                                                                          155\\nSoft Margin Classification                                                                                       156\\nNonlinear SVM Classification                                                                                   159\\nPolynomial Kernel                                                                                                   160\\nAdding Similarity Features                                                                                     161\\nGaussian RBF Kernel                                                                                               162\\nComputational Complexity                                                                                    163\\nSVM Regression                                                                                                           164\\nUnder the Hood                                                                                                           166\\nDecision Function and Predictions                                                                       166\\nTraining Objective                                                                                                   167\\nQuadratic Programming                                                                                         169\\nThe Dual Problem                                                                                                    170\\nKernelized SVM                                                                                                       171\\nOnline SVMs                                                                                                            174\\nTable of Contents \\n| \\nv'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 7}, page_content='Exercises                                                                                                                        175\\n6. Decision Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  177\\nTraining and Visualizing a Decision Tree                                                                177\\nMaking Predictions                                                                                                     179\\nEstimating Class Probabilities                                                                                   181\\nThe CART Training Algorithm                                                                                 182\\nComputational Complexity                                                                                        183\\nGini Impurity or Entropy?                                                                                         183\\nRegularization Hyperparameters                                                                              184\\nRegression                                                                                                                     185\\nInstability                                                                                                                      188\\nExercises                                                                                                                        189\\n7. Ensemble Learning and Random Forests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  191\\nVoting Classifiers                                                                                                         192\\nBagging and Pasting                                                                                                    195\\nBagging and Pasting in Scikit-Learn                                                                     196\\nOut-of-Bag Evaluation                                                                                            197\\nRandom Patches and Random Subspaces                                                                198\\nRandom Forests                                                                                                           199\\nExtra-Trees                                                                                                                200\\nFeature Importance                                                                                                  200\\nBoosting                                                                                                                        201\\nAdaBoost                                                                                                                   202\\nGradient Boosting                                                                                                    205\\nStacking                                                                                                                         210\\nExercises                                                                                                                        213\\n8. Dimensionality Reduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  215\\nThe Curse of Dimensionality                                                                                     216\\nMain Approaches for Dimensionality Reduction                                                   218\\nProjection                                                                                                                  218\\nManifold Learning                                                                                                   220\\nPCA                                                                                                                                222\\nPreserving the Variance                                                                                          222\\nPrincipal Components                                                                                            223\\nProjecting Down to d Dimensions                                                                        224\\nUsing Scikit-Learn                                                                                                    224\\nExplained Variance Ratio                                                                                        225\\nChoosing the Right Number of Dimensions                                                       225\\nPCA for Compression                                                                                             226\\nvi \\n| \\nTable of Contents'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 8}, page_content='Randomized PCA                                                                                                    227\\nIncremental PCA                                                                                                     227\\nKernel PCA                                                                                                                   228\\nSelecting a Kernel and Tuning Hyperparameters                                                229\\nLLE                                                                                                                                 232\\nOther Dimensionality Reduction Techniques                                                         234\\nExercises                                                                                                                        235\\n9. Unsupervised Learning Techniques. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  237\\nClustering                                                                                                                      238\\nK-Means                                                                                                                    240\\nLimits of K-Means                                                                                                   250\\nUsing clustering for image segmentation                                                             251\\nUsing Clustering for Preprocessing                                                                       252\\nUsing Clustering for Semi-Supervised Learning                                                 254\\nDBSCAN                                                                                                                   256\\nOther Clustering Algorithms                                                                                 259\\nGaussian Mixtures                                                                                                       260\\nAnomaly Detection using Gaussian Mixtures                                                     266\\nSelecting the Number of Clusters                                                                          267\\nBayesian Gaussian Mixture Models                                                                      270\\nOther Anomaly Detection and Novelty Detection Algorithms                        274\\nPart II. \\nNeural Networks and Deep Learning\\n10. Introduction to Artificial Neural Networks with Keras. . . . . . . . . . . . . . . . . . . . . . . . . .  277\\nFrom Biological to Artificial Neurons                                                                      278\\nBiological Neurons                                                                                                   279\\nLogical Computations with Neurons                                                                    281\\nThe Perceptron                                                                                                         281\\nMulti-Layer Perceptron and Backpropagation                                                    286\\nRegression MLPs                                                                                                      289\\nClassification MLPs                                                                                                 290\\nImplementing MLPs with Keras                                                                                292\\nInstalling TensorFlow 2                                                                                           293\\nBuilding an Image Classifier Using the Sequential API                                     294\\nBuilding a Regression MLP Using the Sequential API                                       303\\nBuilding Complex Models Using the Functional API                                        304\\nBuilding Dynamic Models Using the Subclassing API                                       309\\nSaving and Restoring a Model                                                                                311\\nUsing Callbacks                                                                                                        311\\nTable of Contents \\n| \\nvii'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 9}, page_content='Visualization Using TensorBoard                                                                          313\\nFine-Tuning Neural Network Hyperparameters                                                     315\\nNumber of Hidden Layers                                                                                      319\\nNumber of Neurons per Hidden Layer                                                                 320\\nLearning Rate, Batch Size and Other Hyperparameters                                     320\\nExercises                                                                                                                        322\\n11. Training Deep Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  325\\nVanishing/Exploding Gradients Problems                                                              326\\nGlorot and He Initialization                                                                                   327\\nNonsaturating Activation Functions                                                                     329\\nBatch Normalization                                                                                                333\\nGradient Clipping                                                                                                    338\\nReusing Pretrained Layers                                                                                          339\\nTransfer Learning With Keras                                                                                341\\nUnsupervised Pretraining                                                                                       343\\nPretraining on an Auxiliary Task                                                                           344\\nFaster Optimizers                                                                                                         344\\nMomentum Optimization                                                                                      345\\nNesterov Accelerated Gradient                                                                              346\\nAdaGrad                                                                                                                    347\\nRMSProp                                                                                                                   349\\nAdam and Nadam Optimization                                                                           349\\nLearning Rate Scheduling                                                                                       352\\nAvoiding Overfitting Through Regularization                                                        356\\nℓ1 and ℓ2 Regularization                                                                                           356\\nDropout                                                                                                                     357\\nMonte-Carlo (MC) Dropout                                                                                  360\\nMax-Norm Regularization                                                                                      362\\nSummary and Practical Guidelines                                                                           363\\nExercises                                                                                                                        364\\n12. Custom Models and Training with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  367\\nA Quick Tour of TensorFlow                                                                                     368\\nUsing TensorFlow like NumPy                                                                                  371\\nTensors and Operations                                                                                          371\\nTensors and NumPy                                                                                                373\\nType Conversions                                                                                                     374\\nVariables                                                                                                                    374\\nOther Data Structures                                                                                             375\\nCustomizing Models and Training Algorithms                                                      376\\nCustom Loss Functions                                                                                           376\\nviii \\n| \\nTable of Contents'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 10}, page_content='Saving and Loading Models That Contain Custom Components                    377\\nCustom Activation Functions, Initializers, Regularizers, and Constraints     379\\nCustom Metrics                                                                                                        380\\nCustom Layers                                                                                                          383\\nCustom Models                                                                                                        386\\nLosses and Metrics Based on Model Internals                                                     388\\nComputing Gradients Using Autodiff                                                                   389\\nCustom Training Loops                                                                                          393\\nTensorFlow Functions and Graphs                                                                           396\\nAutograph and Tracing                                                                                           398\\nTF Function Rules                                                                                                    400\\n13. Loading and Preprocessing Data with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . .  403\\nThe Data API                                                                                                                404\\nChaining Transformations                                                                                      405\\nShuffling the Data                                                                                                    406\\nPreprocessing the Data                                                                                            409\\nPutting Everything Together                                                                                  410\\nPrefetching                                                                                                                411\\nUsing the Dataset With tf.keras                                                                              413\\nThe TFRecord Format                                                                                                414\\nCompressed TFRecord Files                                                                                   415\\nA Brief Introduction to Protocol Buffers                                                              415\\nTensorFlow Protobufs                                                                                             416\\nLoading and Parsing Examples                                                                              418\\nHandling Lists of Lists Using the SequenceExample Protobuf                          419\\nThe Features API                                                                                                         420\\nCategorical Features                                                                                                 421\\nCrossed Categorical Features                                                                                 421\\nEncoding Categorical Features Using One-Hot Vectors                                    422\\nEncoding Categorical Features Using Embeddings                                            423\\nUsing Feature Columns for Parsing                                                                      426\\nUsing Feature Columns in Your Models                                                               426\\nTF Transform                                                                                                               428\\nThe TensorFlow Datasets (TFDS) Project                                                                429\\n14. Deep Computer Vision Using Convolutional Neural Networks. . . . . . . . . . . . . . . . . . .  431\\nThe Architecture of the Visual Cortex                                                                     432\\nConvolutional Layer                                                                                                    434\\nFilters                                                                                                                         436\\nStacking Multiple Feature Maps                                                                             437\\nTensorFlow Implementation                                                                                  439\\nTable of Contents \\n| \\nix'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 11}, page_content='Memory Requirements                                                                                           441\\nPooling Layer                                                                                                                442\\nTensorFlow Implementation                                                                                  444\\nCNN Architectures                                                                                                      446\\nLeNet-5                                                                                                                      449\\nAlexNet                                                                                                                      450\\nGoogLeNet                                                                                                                452\\nVGGNet                                                                                                                     456\\nResNet                                                                                                                        457\\nXception                                                                                                                    459\\nSENet                                                                                                                         461\\nImplementing a ResNet-34 CNN Using Keras                                                        464\\nUsing Pretrained Models From Keras                                                                      465\\nPretrained Models for Transfer Learning                                                                 467\\nClassification and Localization                                                                                  469\\nObject Detection                                                                                                          471\\nFully Convolutional Networks (FCNs)                                                                 473\\nYou Only Look Once (YOLO)                                                                                475\\nSemantic Segmentation                                                                                               478\\nExercises                                                                                                                        482\\nx \\n| \\nTable of Contents'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 12}, page_content='1 Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/.\\n2 Despite the fact that Yann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.\\nPreface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning.” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 13}, page_content='Or maybe your company has tons of data (user logs, financial data, production data,\\nmachine sensor data, hotline stats, HR reports, etc.), and more than likely you could\\nunearth some hidden gems if you just knew where to look; for example:\\n• Segment customers and find the best marketing strategy for each group\\n• Recommend products for each client based on what similar clients bought\\n• Detect which transactions are likely to be fraudulent\\n• Forecast next year’s revenue\\n• And more\\nWhatever the reason, you have decided to learn Machine Learning and implement it\\nin your projects. Great idea!\\nObjective and Approach\\nThis book assumes that you know close to nothing about Machine Learning. Its goal\\nis to give you the concepts, the intuitions, and the tools you need to actually imple‐\\nment programs capable of learning from data.\\nWe will cover a large number of techniques, from the simplest and most commonly\\nused (such as linear regression) to some of the Deep Learning techniques that regu‐\\nlarly win competitions.\\nRather than implementing our own toy versions of each algorithm, we will be using\\nactual production-ready Python frameworks:\\n• Scikit-Learn is very easy to use, yet it implements many Machine Learning algo‐\\nrithms efficiently, so it makes for a great entry point to learn Machine Learning.\\n• TensorFlow is a more complex library for distributed numerical computation. It\\nmakes it possible to train and run very large neural networks efficiently by dis‐\\ntributing the computations across potentially hundreds of multi-GPU servers.\\nTensorFlow was created at Google and supports many of their large-scale\\nMachine Learning applications. It was open sourced in November 2015.\\n• Keras is a high level Deep Learning API that makes it very simple to train and\\nrun neural networks. It can run on top of either TensorFlow, Theano or Micro‐\\nsoft Cognitive Toolkit (formerly known as CNTK). TensorFlow comes with its\\nown implementation of this API, called tf.keras, which provides support for some\\nadvanced TensorFlow features (e.g., to efficiently load data).\\nThe book favors a hands-on approach, growing an intuitive understanding of\\nMachine Learning through concrete working examples and just a little bit of theory.\\nWhile you can read this book without picking up your laptop, we highly recommend\\nxii \\n| \\nPreface'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 14}, page_content='you experiment with the code examples available online as Jupyter notebooks at\\nhttps://github.com/ageron/handson-ml2.\\nPrerequisites\\nThis book assumes that you have some Python programming experience and that you\\nare familiar with Python’s main scientific libraries, in particular NumPy, Pandas, and\\nMatplotlib.\\nAlso, if you care about what’s under the hood you should have a reasonable under‐\\nstanding of college-level math as well (calculus, linear algebra, probabilities, and sta‐\\ntistics).\\nIf you don’t know Python yet, http://learnpython.org/ is a great place to start. The offi‐\\ncial tutorial on python.org is also quite good.\\nIf you have never used Jupyter, Chapter 2 will guide you through installation and the\\nbasics: it is a great tool to have in your toolbox.\\nIf you are not familiar with Python’s scientific libraries, the provided Jupyter note‐\\nbooks include a few tutorials. There is also a quick math tutorial for linear algebra.\\nRoadmap\\nThis book is organized in two parts. Part I, The Fundamentals of Machine Learning,\\ncovers the following topics:\\n• What is Machine Learning? What problems does it try to solve? What are the\\nmain categories and fundamental concepts of Machine Learning systems?\\n• The main steps in a typical Machine Learning project.\\n• Learning by fitting a model to data.\\n• Optimizing a cost function.\\n• Handling, cleaning, and preparing data.\\n• Selecting and engineering features.\\n• Selecting a model and tuning hyperparameters using cross-validation.\\n• The main challenges of Machine Learning, in particular underfitting and overfit‐\\nting (the bias/variance tradeoff).\\n• Reducing the dimensionality of the training data to fight the curse of dimension‐\\nality.\\n• Other unsupervised learning techniques, including clustering, density estimation\\nand anomaly detection.\\nPreface \\n| \\nxiii'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 15}, page_content='• The most common learning algorithms: Linear and Polynomial Regression,\\nLogistic Regression, k-Nearest Neighbors, Support Vector Machines, Decision\\nTrees, Random Forests, and Ensemble methods.\\nxiv \\n| \\nPreface'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 16}, page_content='Part II, Neural Networks and Deep Learning, covers the following topics:\\n• What are neural nets? What are they good for?\\n• Building and training neural nets using TensorFlow and Keras.\\n• The most important neural net architectures: feedforward neural nets, convolu‐\\ntional nets, recurrent nets, long short-term memory (LSTM) nets, autoencoders\\nand generative adversarial networks (GANs).\\n• Techniques for training deep neural nets.\\n• Scaling neural networks for large datasets.\\n• Learning strategies with Reinforcement Learning.\\n• Handling uncertainty with Bayesian Deep Learning.\\nThe first part is based mostly on Scikit-Learn while the second part uses TensorFlow\\nand Keras.\\nDon’t jump into deep waters too hastily: while Deep Learning is no\\ndoubt one of the most exciting areas in Machine Learning, you\\nshould master the fundamentals first. Moreover, most problems\\ncan be solved quite well using simpler techniques such as Random\\nForests and Ensemble methods (discussed in Part I). Deep Learn‐\\ning is best suited for complex problems such as image recognition,\\nspeech recognition, or natural language processing, provided you\\nhave enough data, computing power, and patience.\\nOther Resources\\nMany resources are available to learn about Machine Learning. Andrew Ng’s ML\\ncourse on Coursera and Geoffrey Hinton’s course on neural networks and Deep\\nLearning are amazing, although they both require a significant time investment\\n(think months).\\nThere are also many interesting websites about Machine Learning, including of\\ncourse Scikit-Learn’s exceptional User Guide. You may also enjoy Dataquest, which\\nprovides very nice interactive tutorials, and ML blogs such as those listed on Quora.\\nFinally, the Deep Learning website has a good list of resources to learn more.\\nOf course there are also many other introductory books about Machine Learning, in\\nparticular:\\n• Joel Grus, Data Science from Scratch (O’Reilly). This book presents the funda‐\\nmentals of Machine Learning, and implements some of the main algorithms in\\npure Python (from scratch, as the name suggests).\\nPreface \\n| \\nxv'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 17}, page_content='• Stephen Marsland, Machine Learning: An Algorithmic Perspective (Chapman and\\nHall). This book is a great introduction to Machine Learning, covering a wide\\nrange of topics in depth, with code examples in Python (also from scratch, but\\nusing NumPy).\\n• Sebastian Raschka, Python Machine Learning (Packt Publishing). Also a great\\nintroduction to Machine Learning, this book leverages Python open source libra‐\\nries (Pylearn 2 and Theano).\\n• François Chollet, Deep Learning with Python (Manning). A very practical book\\nthat covers a large range of topics in a clear and concise way, as you might expect\\nfrom the author of the excellent Keras library. It favors code examples over math‐\\nematical theory.\\n• Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, Learning from\\nData (AMLBook). A rather theoretical approach to ML, this book provides deep\\ninsights, in particular on the bias/variance tradeoff (see Chapter 4).\\n• Stuart Russell and Peter Norvig, Artificial Intelligence: A Modern Approach, 3rd\\nEdition (Pearson). This is a great (and huge) book covering an incredible amount\\nof topics, including Machine Learning. It helps put ML into perspective.\\nFinally, a great way to learn is to join ML competition websites such as Kaggle.com\\nthis will allow you to practice your skills on real-world problems, with help and\\ninsights from some of the best ML professionals out there.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nxvi \\n| \\nPreface'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 18}, page_content='This element signifies a tip or suggestion.\\nThis element signifies a general note.\\nThis element indicates a warning or caution.\\nCode Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://github.com/ageron/handson-ml2. It is mostly composed of Jupyter notebooks.\\nSome of the code examples in the book leave out some repetitive sections, or details\\nthat are obvious or unrelated to Machine Learning. This keeps the focus on the\\nimportant parts of the code, and it saves space to cover more topics. However, if you\\nwant the full code examples, they are all available in the Jupyter notebooks.\\nNote that when the code examples display some outputs, then these code examples\\nare shown with Python prompts (>>> and ...), as in a Python shell, to clearly distin‐\\nguish the code from the outputs. For example, this code defines the square() func‐\\ntion then it computes and displays the square of 3:\\n>>> def square(x):\\n...     return x ** 2\\n...\\n>>> result = square(3)\\n>>> result\\n9\\nWhen code does not display anything, prompts are not used. However, the result may\\nsometimes be shown as a comment like this:\\ndef square(x):\\n    return x ** 2\\nresult = square(3)  # result is 9\\nPreface \\n| \\nxvii'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 19}, page_content='Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. You do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9.” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com.\\nO’Reilly Safari\\nSafari (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii \\n| \\nPreface'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 20}, page_content='707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. You can access this page at http://bit.ly/hands-on-machine-learning-\\nwith-scikit-learn-and-tensorflow or https://homl.info/oreilly.\\nTo comment or ask technical questions about this book, send email to bookques‐\\ntions@oreilly.com.\\nFor more information about our books, courses, conferences, and news, see our web‐\\nsite at http://www.oreilly.com.\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on YouTube: http://www.youtube.com/oreillymedia\\nChanges in the Second Edition\\nThis second edition has five main objectives:\\n1. Cover additional topics: additional unsupervised learning techniques (including\\nclustering, anomaly detection, density estimation and mixture models), addi‐\\ntional techniques for training deep nets (including self-normalized networks),\\nadditional computer vision techniques (including the Xception, SENet, object\\ndetection with YOLO, and semantic segmentation using R-CNN), handling\\nsequences using CNNs (including WaveNet), natural language processing using\\nRNNs, CNNs and Transformers, generative adversarial networks, deploying Ten‐\\nsorFlow models, and more.\\n2. Update the book to mention some of the latest results from Deep Learning\\nresearch.\\n3. Migrate all TensorFlow chapters to TensorFlow 2, and use TensorFlow’s imple‐\\nmentation of the Keras API (called tf.keras) whenever possible, to simplify the\\ncode examples.\\n4. Update the code examples to use the latest version of Scikit-Learn, NumPy, Pan‐\\ndas, Matplotlib and other libraries.\\n5. Clarify some sections and fix some errors, thanks to plenty of great feedback\\nfrom readers.\\nSome chapters were added, others were rewritten and a few were reordered. Table P-1\\nshows the mapping between the 1st edition chapters and the 2nd edition chapters:\\nPreface \\n| \\nxix'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 21}, page_content='Table P-1. Chapter mapping between 1st and 2nd edition\\n1st Ed. chapter 2nd Ed. Chapter % Changes\\n2nd Ed. Title\\n1\\n1\\n<10%\\nThe Machine Learning Landscape\\n2\\n2\\n<10%\\nEnd-to-End Machine Learning Project\\n3\\n3\\n<10%\\nClassification\\n4\\n4\\n<10%\\nTraining Models\\n5\\n5\\n<10%\\nSupport Vector Machines\\n6\\n6\\n<10%\\nDecision Trees\\n7\\n7\\n<10%\\nEnsemble Learning and Random Forests\\n8\\n8\\n<10%\\nDimensionality Reduction\\nN/A\\n9\\n100% new\\nUnsupervised Learning Techniques\\n10\\n10\\n~75%\\nIntroduction to Artificial Neural Networks with Keras\\n11\\n11\\n~50%\\nTraining Deep Neural Networks\\n9\\n12\\n100% rewritten\\nCustom Models and Training with TensorFlow\\nPart of 12\\n13\\n100% rewritten\\nLoading and Preprocessing Data with TensorFlow\\n13\\n14\\n~50%\\nDeep Computer Vision Using Convolutional Neural Networks\\nPart of 14\\n15\\n~75%\\nProcessing Sequences Using RNNs and CNNs\\nPart of 14\\n16\\n~90%\\nNatural Language Processing with RNNs and Attention\\n15\\n17\\n~75%\\nAutoencoders and GANs\\n16\\n18\\n~75%\\nReinforcement Learning\\nPart of 12\\n19\\n100% rewritten\\nDeploying your TensorFlow Models\\nMore specifically, here are the main changes for each 2nd edition chapter (other than\\nclarifications, corrections and code updates):\\n• Chapter 1\\n— Added a section on handling mismatch between the training set and the vali‐\\ndation & test sets.\\n• Chapter 2\\n— Added how to compute a confidence interval.\\n— Improved the installation instructions (e.g., for Windows).\\n— Introduced the upgraded OneHotEncoder and the new ColumnTransformer.\\n• Chapter 4\\n— Explained the need for training instances to be Independent and Identically\\nDistributed (IID).\\n• Chapter 7\\n— Added a short section about XGBoost.\\nxx \\n| \\nPreface'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 22}, page_content='• Chapter 9 – new chapter including:\\n— Clustering with K-Means, how to choose the number of clusters, how to use it\\nfor dimensionality reduction, semi-supervised learning, image segmentation,\\nand more.\\n— The DBSCAN clustering algorithm and an overview of other clustering algo‐\\nrithms available in Scikit-Learn.\\n— Gaussian mixture models, the Expectation-Maximization (EM) algorithm,\\nBayesian variational inference, and how mixture models can be used for clus‐\\ntering, density estimation, anomaly detection and novelty detection.\\n— Overview of other anomaly detection and novelty detection algorithms.\\n• Chapter 10 (mostly new)\\n— Added an introduction to the Keras API, including all its APIs (Sequential,\\nFunctional and Subclassing), persistence and callbacks (including the Tensor\\nBoard callback).\\n• Chapter 11 (many changes)\\n— Introduced self-normalizing nets, the SELU activation function and Alpha\\nDropout.\\n— Introduced self-supervised learning.\\n— Added Nadam optimization.\\n— Added Monte-Carlo Dropout.\\n— Added a note about the risks of adaptive optimization methods.\\n— Updated the practical guidelines.\\n• Chapter 12 – completely rewritten chapter, including:\\n— A tour of TensorFlow 2\\n— TensorFlow’s lower-level Python API\\n— Writing custom loss functions, metrics, layers, models\\n— Using auto-differentiation and creating custom training algorithms.\\n— TensorFlow Functions and graphs (including tracing and autograph).\\n• Chapter 13 – new chapter, including:\\n— The Data API\\n— Loading/Storing data efficiently using TFRecords\\n— The Features API (including an introduction to embeddings).\\n— An overview of TF Transform and TF Datasets\\n— Moved the low-level implementation of the neural network to the exercises.\\nPreface \\n| \\nxxi'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 23}, page_content='— Removed details about queues and readers that are now superseded by the\\nData API.\\n• Chapter 14\\n— Added Xception and SENet architectures.\\n— Added a Keras implementation of ResNet-34.\\n— Showed how to use pretrained models using Keras.\\n— Added an end-to-end transfer learning example.\\n— Added classification and localization.\\n— Introduced Fully Convolutional Networks (FCNs).\\n— Introduced object detection using the YOLO architecture.\\n— Introduced semantic segmentation using R-CNN.\\n• Chapter 15\\n— Added an introduction to Wavenet.\\n— Moved the Encoder–Decoder architecture and Bidirectional RNNs to Chapter\\n16.\\n• Chapter 16\\n— Explained how to use the Data API to handle sequential data.\\n— Showed an end-to-end example of text generation using a Character RNN,\\nusing both a stateless and a stateful RNN.\\n— Showed an end-to-end example of sentiment analysis using an LSTM.\\n— Explained masking in Keras.\\n— Showed how to reuse pretrained embeddings using TF Hub.\\n— Showed how to build an Encoder–Decoder for Neural Machine Translation\\nusing TensorFlow Addons/seq2seq.\\n— Introduced beam search.\\n— Explained attention mechanisms.\\n— Added a short overview of visual attention and a note on explainability.\\n— Introduced the fully attention-based Transformer architecture, including posi‐\\ntional embeddings and multi-head attention.\\n— Added an overview of recent language models (2018).\\n• Chapters 17, 18 and 19: coming soon.\\nxxii \\n| \\nPreface'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 24}, page_content='3 “Deep Learning with Python,” François Chollet (2017).\\nAcknowledgments\\nNever in my wildest dreams did I imagine that the first edition of this book would get\\nsuch a large audience. I received so many messages from readers, many asking ques‐\\ntions, some kindly pointing out errata, and most sending me encouraging words. I\\ncannot express how grateful I am to all these readers for their tremendous support.\\nThank you all so very much! Please do not hesitate to file issues on github if you find\\nerrors in the code examples (or just to ask questions), or to submit errata if you find\\nerrors in the text. Some readers also shared how this book helped them get their first\\njob, or how it helped them solve a concrete problem they were working on: I find\\nsuch feedback incredibly motivating. If you find this book helpful, I would love it if\\nyou could share your story with me, either privately (e.g., via LinkedIn) or publicly\\n(e.g., in an Amazon review).\\nI am also incredibly thankful to all the amazing people who took time out of their\\nbusy lives to review my book with such care. In particular, I would like to thank Fran‐\\nçois Chollet for reviewing all the chapters based on Keras & TensorFlow, and giving\\nme some great, in-depth feedback. Since Keras is one of the main additions to this 2nd\\nedition, having its author review the book was invaluable. I highly recommend Fran‐\\nçois’s excellent book Deep Learning with Python3: it has the conciseness, clarity and\\ndepth of the Keras library itself. Big thanks as well to Ankur Patel, who reviewed\\nevery chapter of this 2nd edition and gave me excellent feedback.\\nThis book also benefited from plenty of help from members of the TensorFlow team,\\nin particular Martin Wicke, who tirelessly answered dozens of my questions and dis‐\\npatched the rest to the right people, including Alexandre Passos, Allen Lavoie, André\\nSusano Pinto, Anna Revinskaya, Anthony Platanios, Clemens Mewald, Dan Moldo‐\\nvan, Daniel Dobson, Dustin Tran, Edd Wilder-James, Goldie Gadde, Jiri Simsa, Kar‐\\nmel Allison, Nick Felt, Paige Bailey, Pete Warden (who also reviewed the 1st edition),\\nRyan Sepassi, Sandeep Gupta, Sean Morgan, Todd Wang, Tom O’Malley, William\\nChargin, and Yuefeng Zhou, all of whom were tremendously helpful. A huge thank\\nyou to all of you, and to all other members of the TensorFlow team. Not just for your\\nhelp, but also for making such a great library.\\nBig thanks to Haesun Park, who gave me plenty of excellent feedback and caught sev‐\\neral errors while he was writing the Korean translation of the 1st edition of this book.\\nHe also translated the Jupyter notebooks to Korean, not to mention TensorFlow’s\\ndocumentation. I do not speak Korean, but judging by the quality of his feedback, all\\nhis translations must be truly excellent! Moreover, he kindly contributed some of the\\nsolutions to the exercises in this book.\\nPreface \\n| \\nxxiii'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 25}, page_content='Many thanks as well to O’Reilly’s fantastic staff, in particular Nicole Tache, who gave\\nme insightful feedback, always cheerful, encouraging, and helpful: I could not dream\\nof a better editor. Big thanks to Michele Cronin as well, who was very helpful (and\\npatient) at the start of this 2nd edition. Thanks to Marie Beaugureau, Ben Lorica, Mike\\nLoukides, and Laurel Ruma for believing in this project and helping me define its\\nscope. Thanks to Matt Hacker and all of the Atlas team for answering all my technical\\nquestions regarding formatting, asciidoc, and LaTeX, and thanks to Rachel Mona‐\\nghan, Nick Adams, and all of the production team for their final review and their\\nhundreds of corrections.\\nI would also like to thank my former Google colleagues, in particular the YouTube\\nvideo classification team, for teaching me so much about Machine Learning. I could\\nnever have started the first edition without them. Special thanks to my personal ML\\ngurus: Clément Courbet, Julien Dubois, Mathias Kende, Daniel Kitachewsky, James\\nPack, Alexander Pak, Anosh Raj, Vitor Sessak, Wiktor Tomczak, Ingrid von Glehn,\\nRich Washington, and everyone I worked with at YouTube and in the amazing Goo‐\\ngle research teams in Mountain View. All these people are just as nice and helpful as\\nthey are bright, and that’s saying a lot.\\nI will never forget the kind people who reviewed the 1st edition of this book, including\\nDavid Andrzejewski, Eddy Hung, Grégoire Mesnil, Iain Smears, Ingrid von Glehn,\\nJustin Francis, Karim Matrah, Lukas Biewald, Michel Tessier, Salim Sémaoune, Vin‐\\ncent Guilbeau and of course my dear brother Sylvain.\\nLast but not least, I am infinitely grateful to my beloved wife, Emmanuelle, and to our\\nthree wonderful children, Alexandre, Rémi, and Gabrielle, for encouraging me to\\nwork hard on this book, as well as for their insatiable curiosity: explaining some of\\nthe most difficult concepts in this book to my wife and children helped me clarify my\\nthoughts and directly improved many parts of this book. Plus, they keep bringing me\\ncookies and coffee! What more can one dream of?\\nxxiv \\n| \\nPreface'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 26}, page_content='PART I\\nThe Fundamentals of\\nMachine Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 27}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 28}, page_content='CHAPTER 1\\nThe Machine Learning Landscape\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 1 in the final\\nrelease of the book.\\nWhen most people hear “Machine Learning,” they picture a robot: a dependable but‐\\nler or a deadly Terminator depending on who you ask. But Machine Learning is not\\njust a futuristic fantasy, it’s already here. In fact, it has been around for decades in\\nsome specialized applications, such as Optical Character Recognition (OCR). But the\\nfirst ML application that really became mainstream, improving the lives of hundreds\\nof millions of people, took over the world back in the 1990s: it was the spam filter.\\nNot exactly a self-aware Skynet, but it does technically qualify as Machine Learning\\n(it has actually learned so well that you seldom need to flag an email as spam any‐\\nmore). It was followed by hundreds of ML applications that now quietly power hun‐\\ndreds of products and features that you use regularly, from better recommendations\\nto voice search.\\nWhere does Machine Learning start and where does it end? What exactly does it\\nmean for a machine to learn something? If I download a copy of Wikipedia, has my\\ncomputer really “learned” something? Is it suddenly smarter? In this chapter we will\\nstart by clarifying what Machine Learning is and why you may want to use it.\\nThen, before we set out to explore the Machine Learning continent, we will take a\\nlook at the map and learn about the main regions and the most notable landmarks:\\nsupervised versus unsupervised learning, online versus batch learning, instance-\\nbased versus model-based learning. Then we will look at the workflow of a typical ML\\nproject, discuss the main challenges you may face, and cover how to evaluate and\\nfine-tune a Machine Learning system.\\n3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 29}, page_content='This chapter introduces a lot of fundamental concepts (and jargon) that every data\\nscientist should know by heart. It will be a high-level overview (the only chapter\\nwithout much code), all rather simple, but you should make sure everything is\\ncrystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s\\nget started!\\nIf you already know all the Machine Learning basics, you may want\\nto skip directly to Chapter 2. If you are not sure, try to answer all\\nthe questions listed at the end of the chapter before moving on.\\nWhat Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so they can\\nlearn from data.\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn\\nwithout being explicitly programmed.\\n—Arthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P, if its performance on T, as measured by P, improves\\nwith experience E.\\n—Tom Mitchell, 1997\\nFor example, your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\\n(nonspam, also called “ham”) emails. The examples that the system uses to learn are\\ncalled the training set. Each training example is called a training instance (or sample).\\nIn this case, the task T is to flag spam for new emails, the experience E is the training\\ndata, and the performance measure P needs to be defined; for example, you can use\\nthe ratio of correctly classified emails. This particular performance measure is called\\naccuracy and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is\\nnot suddenly better at any task. Thus, it is not Machine Learning.\\nWhy Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming techni‐\\nques (Figure 1-1):\\n4 \\n| \\nChapter 1: The Machine Learning Landscape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 30}, page_content='1. First you would look at what spam typically looks like. You might notice that\\nsome words or phrases (such as “4U,” “credit card,” “free,” and “amazing”) tend to\\ncome up a lot in the subject. Perhaps you would also notice a few other patterns\\nin the sender’s name, the email’s body, and so on.\\n2. You would write a detection algorithm for each of the patterns that you noticed,\\nand your program would flag emails as spam if a number of these patterns are\\ndetected.\\n3. You would test your program, and repeat steps 1 and 2 until it is good enough.\\nFigure 1-1. The traditional approach\\nSince the problem is not trivial, your program will likely become a long list of com‐\\nplex rules—pretty hard to maintain.\\nIn contrast, a spam filter based on Machine Learning techniques automatically learns\\nwhich words and phrases are good predictors of spam by detecting unusually fre‐\\nquent patterns of words in the spam examples compared to the ham examples\\n(Figure 1-2). The program is much shorter, easier to maintain, and most likely more\\naccurate.\\nWhy Use Machine Learning? \\n| \\n5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 31}, page_content='Figure 1-2. Machine Learning approach\\nMoreover, if spammers notice that all their emails containing “4U” are blocked, they\\nmight start writing “For U” instead. A spam filter using traditional programming\\ntechniques would need to be updated to flag “For U” emails. If spammers keep work‐\\ning around your spam filter, you will need to keep writing new rules forever.\\nIn contrast, a spam filter based on Machine Learning techniques automatically noti‐\\nces that “For U” has become unusually frequent in spam flagged by users, and it starts\\nflagging them without your intervention (Figure 1-3).\\nFigure 1-3. Automatically adapting to change\\nAnother area where Machine Learning shines is for problems that either are too com‐\\nplex for traditional approaches or have no known algorithm. For example, consider \\nspeech recognition: say you want to start simple and write a program capable of dis‐\\ntinguishing the words “one” and “two.” You might notice that the word “two” starts\\nwith a high-pitch sound (“T”), so you could hardcode an algorithm that measures\\nhigh-pitch sound intensity and use that to distinguish ones and twos. Obviously this\\ntechnique will not scale to thousands of words spoken by millions of very different\\n6 \\n| \\nChapter 1: The Machine Learning Landscape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 32}, page_content='people in noisy environments and in dozens of languages. The best solution (at least\\ntoday) is to write an algorithm that learns by itself, given many example recordings\\nfor each word.\\nFinally, Machine Learning can help humans learn (Figure 1-4): ML algorithms can be\\ninspected to see what they have learned (although for some algorithms this can be\\ntricky). For instance, once the spam filter has been trained on enough spam, it can\\neasily be inspected to reveal the list of words and combinations of words that it\\nbelieves are the best predictors of spam. Sometimes this will reveal unsuspected cor‐\\nrelations or new trends, and thereby lead to a better understanding of the problem.\\nApplying ML techniques to dig into large amounts of data can help discover patterns\\nthat were not immediately apparent. This is called data mining.\\nFigure 1-4. Machine Learning can help humans learn\\nTo summarize, Machine Learning is great for:\\n• Problems for which existing solutions require a lot of hand-tuning or long lists of\\nrules: one Machine Learning algorithm can often simplify code and perform bet‐\\nter.\\n• Complex problems for which there is no good solution at all using a traditional\\napproach: the best Machine Learning techniques can find a solution.\\n• Fluctuating environments: a Machine Learning system can adapt to new data.\\n• Getting insights about complex problems and large amounts of data.\\nWhy Use Machine Learning? \\n| \\n7'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 33}, page_content='Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\n• Whether or not they are trained with human supervision (supervised, unsuper‐\\nvised, semisupervised, and Reinforcement Learning)\\n• Whether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\n• Whether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\\ning.\\nSupervised learning\\nIn supervised learning, the training data you feed to the algorithm includes the desired\\nsolutions, called labels (Figure 1-5).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 \\n| \\nChapter 1: The Machine Learning Landscape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 34}, page_content='1 Fun fact: this odd-sounding name is a statistics term introduced by Francis Galton while he was studying the\\nfact that the children of tall people tend to be shorter than their parents. Since children were shorter, he called\\nthis regression to the mean. This name was then applied to the methods he used to analyze correlations\\nbetween variables.\\nA typical supervised learning task is classification. The spam filter is a good example\\nof this: it is trained with many example emails along with their class (spam or ham),\\nand it must learn how to classify new emails.\\nAnother typical task is to predict a target numeric value, such as the price of a car,\\ngiven a set of features (mileage, age, brand, etc.) called predictors. This sort of task is \\ncalled regression (Figure 1-6).1 To train the system, you need to give it many examples\\nof cars, including both their predictors and their labels (i.e., their prices).\\nIn Machine Learning an attribute is a data type (e.g., “Mileage”),\\nwhile a feature has several meanings depending on the context, but\\ngenerally means an attribute plus its value (e.g., “Mileage =\\n15,000”). Many people use the words attribute and feature inter‐\\nchangeably, though.\\nFigure 1-6. Regression\\nNote that some regression algorithms can be used for classification as well, and vice\\nversa. For example, Logistic Regression is commonly used for classification, as it can\\noutput a value that corresponds to the probability of belonging to a given class (e.g.,\\n20% chance of being spam).\\nTypes of Machine Learning Systems \\n| \\n9'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 35}, page_content='2 Some neural network architectures can be unsupervised, such as autoencoders and restricted Boltzmann\\nmachines. They can also be semisupervised, such as in deep belief networks and unsupervised pretraining.\\nHere are some of the most important supervised learning algorithms (covered in this\\nbook):\\n• k-Nearest Neighbors\\n• Linear Regression\\n• Logistic Regression\\n• Support Vector Machines (SVMs)\\n• Decision Trees and Random Forests\\n• Neural networks2\\nUnsupervised learning\\nIn unsupervised learning, as you might guess, the training data is unlabeled\\n(Figure 1-7). The system tries to learn without a teacher.\\nFigure 1-7. An unlabeled training set for unsupervised learning\\nHere are some of the most important unsupervised learning algorithms (most of\\nthese are covered in Chapter 8 and Chapter 9):\\n• Clustering\\n— K-Means\\n— DBSCAN\\n— Hierarchical Cluster Analysis (HCA)\\n• Anomaly detection and novelty detection\\n— One-class SVM\\n— Isolation Forest\\n10 \\n| \\nChapter 1: The Machine Learning Landscape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 36}, page_content='• Visualization and dimensionality reduction\\n— Principal Component Analysis (PCA)\\n— Kernel PCA\\n— Locally-Linear Embedding (LLE)\\n— t-distributed Stochastic Neighbor Embedding (t-SNE)\\n• Association rule learning\\n— Apriori\\n— Eclat\\nFor example, say you have a lot of data about your blog’s visitors. You may want to\\nrun a clustering algorithm to try to detect groups of similar visitors (Figure 1-8). At\\nno point do you tell the algorithm which group a visitor belongs to: it finds those\\nconnections without your help. For example, it might notice that 40% of your visitors\\nare males who love comic books and generally read your blog in the evening, while\\n20% are young sci-fi lovers who visit during the weekends, and so on. If you use a\\nhierarchical clustering algorithm, it may also subdivide each group into smaller\\ngroups. This may help you target your posts for each group.\\nFigure 1-8. Clustering\\nVisualization algorithms are also good examples of unsupervised learning algorithms:\\nyou feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep‐\\nresentation of your data that can easily be plotted (Figure 1-9). These algorithms try\\nto preserve as much structure as they can (e.g., trying to keep separate clusters in the\\ninput space from overlapping in the visualization), so you can understand how the\\ndata is organized and perhaps identify unsuspected patterns.\\nTypes of Machine Learning Systems \\n| \\n11'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 37}, page_content='3 Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds,\\nand so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), “T-SNE visual‐\\nization of the semantic word space.”\\nFigure 1-9. Example of a t-SNE visualization highlighting semantic clusters3\\nA related task is dimensionality reduction, in which the goal is to simplify the data\\nwithout losing too much information. One way to do this is to merge several correla‐\\nted features into one. For example, a car’s mileage may be very correlated with its age,\\nso the dimensionality reduction algorithm will merge them into one feature that rep‐\\nresents the car’s wear and tear. This is called feature extraction.\\nIt is often a good idea to try to reduce the dimension of your train‐\\ning data using a dimensionality reduction algorithm before you\\nfeed it to another Machine Learning algorithm (such as a super‐\\nvised learning algorithm). It will run much faster, the data will take\\nup less disk and memory space, and in some cases it may also per‐\\nform better.\\nYet another important unsupervised task is anomaly detection—for example, detect‐\\ning unusual credit card transactions to prevent fraud, catching manufacturing defects,\\nor automatically removing outliers from a dataset before feeding it to another learn‐\\ning algorithm. The system is shown mostly normal instances during training, so it\\nlearns to recognize them and when it sees a new instance it can tell whether it looks\\n12 \\n| \\nChapter 1: The Machine Learning Landscape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 38}, page_content='4 That’s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes\\nmixes up two people who look alike, so you need to provide a few labels per person and manually clean up\\nsome clusters.\\nlike a normal one or whether it is likely an anomaly (see Figure 1-10). A very similar\\ntask is novelty detection: the difference is that novelty detection algorithms expect to\\nsee only normal data during training, while anomaly detection algorithms are usually\\nmore tolerant, they can often perform well even with a small percentage of outliers in\\nthe training set.\\nFigure 1-10. Anomaly detection\\nFinally, another common unsupervised task is association rule learning, in which the\\ngoal is to dig into large amounts of data and discover interesting relations between\\nattributes. For example, suppose you own a supermarket. Running an association rule\\non your sales logs may reveal that people who purchase barbecue sauce and potato\\nchips also tend to buy steak. Thus, you may want to place these items close to each \\nother.\\nSemisupervised learning\\nSome algorithms can deal with partially labeled training data, usually a lot of unla‐\\nbeled data and a little bit of labeled data. This is called semisupervised learning\\n(Figure 1-11).\\nSome photo-hosting services, such as Google Photos, are good examples of this. Once\\nyou upload all your family photos to the service, it automatically recognizes that the\\nsame person A shows up in photos 1, 5, and 11, while another person B shows up in\\nphotos 2, 5, and 7. This is the unsupervised part of the algorithm (clustering). Now all\\nthe system needs is for you to tell it who these people are. Just one label per person,4\\nand it is able to name everyone in every photo, which is useful for searching photos.\\nTypes of Machine Learning Systems \\n| \\n13'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 39}, page_content='Figure 1-11. Semisupervised learning\\nMost semisupervised learning algorithms are combinations of unsupervised and\\nsupervised algorithms. For example, deep belief networks (DBNs) are based on unsu‐\\npervised components called restricted Boltzmann machines (RBMs) stacked on top of\\none another. RBMs are trained sequentially in an unsupervised manner, and then the\\nwhole system is fine-tuned using supervised learning techniques.\\nReinforcement Learning\\nReinforcement Learning is a very different beast. The learning system, called an agent\\nin this context, can observe the environment, select and perform actions, and get\\nrewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It\\nmust then learn by itself what is the best strategy, called a policy, to get the most\\nreward over time. A policy defines what action the agent should choose when it is in a\\ngiven situation.\\n14 \\n| \\nChapter 1: The Machine Learning Landscape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 40}, page_content='Figure 1-12. Reinforcement Learning\\nFor example, many robots implement Reinforcement Learning algorithms to learn\\nhow to walk. DeepMind’s AlphaGo program is also a good example of Reinforcement\\nLearning: it made the headlines in May 2017 when it beat the world champion Ke Jie\\nat the game of Go. It learned its winning policy by analyzing millions of games, and\\nthen playing many games against itself. Note that learning was turned off during the\\ngames against the champion; AlphaGo was just applying the policy it had learned.\\nBatch and Online Learning\\nAnother criterion used to classify Machine Learning systems is whether or not the\\nsystem can learn incrementally from a stream of incoming data.\\nBatch learning\\nIn batch learning, the system is incapable of learning incrementally: it must be trained\\nusing all the available data. This will generally take a lot of time and computing\\nresources, so it is typically done offline. First the system is trained, and then it is\\nlaunched into production and runs without learning anymore; it just applies what it\\nhas learned. This is called offline learning.\\nIf you want a batch learning system to know about new data (such as a new type of\\nspam), you need to train a new version of the system from scratch on the full dataset\\n(not just the new data, but also the old data), then stop the old system and replace it\\nwith the new one.\\nFortunately, the whole process of training, evaluating, and launching a Machine\\nLearning system can be automated fairly easily (as shown in Figure 1-3), so even a\\nTypes of Machine Learning Systems \\n| \\n15'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 41}, page_content='batch learning system can adapt to change. Simply update the data and train a new\\nversion of the system from scratch as often as needed.\\nThis solution is simple and often works fine, but training using the full set of data can\\ntake many hours, so you would typically train a new system only every 24 hours or\\neven just weekly. If your system needs to adapt to rapidly changing data (e.g., to pre‐\\ndict stock prices), then you need a more reactive solution.\\nAlso, training on the full set of data requires a lot of computing resources (CPU,\\nmemory space, disk space, disk I/O, network I/O, etc.). If you have a lot of data and\\nyou automate your system to train from scratch every day, it will end up costing you a\\nlot of money. If the amount of data is huge, it may even be impossible to use a batch\\nlearning algorithm.\\nFinally, if your system needs to be able to learn autonomously and it has limited\\nresources (e.g., a smartphone application or a rover on Mars), then carrying around\\nlarge amounts of training data and taking up a lot of resources to train for hours\\nevery day is a showstopper.\\nFortunately, a better option in all these cases is to use algorithms that are capable of\\nlearning incrementally.\\nOnline learning\\nIn online learning, you train the system incrementally by feeding it data instances\\nsequentially, either individually or by small groups called mini-batches. Each learning\\nstep is fast and cheap, so the system can learn about new data on the fly, as it arrives\\n(see Figure 1-13).\\nFigure 1-13. Online learning\\nOnline learning is great for systems that receive data as a continuous flow (e.g., stock\\nprices) and need to adapt to change rapidly or autonomously. It is also a good option\\n16 \\n| \\nChapter 1: The Machine Learning Landscape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 42}, page_content='if you have limited computing resources: once an online learning system has learned\\nabout new data instances, it does not need them anymore, so you can discard them\\n(unless you want to be able to roll back to a previous state and “replay” the data). This\\ncan save a huge amount of space.\\nOnline learning algorithms can also be used to train systems on huge datasets that\\ncannot fit in one machine’s main memory (this is called out-of-core learning). The\\nalgorithm loads part of the data, runs a training step on that data, and repeats the\\nprocess until it has run on all of the data (see Figure 1-14).\\nOut-of-core learning is usually done offline (i.e., not on the live\\nsystem), so online learning can be a confusing name. Think of it as\\nincremental learning.\\nFigure 1-14. Using online learning to handle huge datasets\\nOne important parameter of online learning systems is how fast they should adapt to\\nchanging data: this is called the learning rate. If you set a high learning rate, then your\\nsystem will rapidly adapt to new data, but it will also tend to quickly forget the old\\ndata (you don’t want a spam filter to flag only the latest kinds of spam it was shown).\\nConversely, if you set a low learning rate, the system will have more inertia; that is, it\\nwill learn more slowly, but it will also be less sensitive to noise in the new data or to\\nsequences of nonrepresentative data points (outliers).\\nA big challenge with online learning is that if bad data is fed to the system, the sys‐\\ntem’s performance will gradually decline. If we are talking about a live system, your\\nclients will notice. For example, bad data could come from a malfunctioning sensor\\non a robot, or from someone spamming a search engine to try to rank high in search\\nTypes of Machine Learning Systems \\n| \\n17'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 43}, page_content='results. To reduce this risk, you need to monitor your system closely and promptly\\nswitch learning off (and possibly revert to a previously working state) if you detect a\\ndrop in performance. You may also want to monitor the input data and react to\\nabnormal data (e.g., using an anomaly detection algorithm).\\nInstance-Based Versus Model-Based Learning\\nOne more way to categorize Machine Learning systems is by how they generalize.\\nMost Machine Learning tasks are about making predictions. This means that given a\\nnumber of training examples, the system needs to be able to generalize to examples it\\nhas never seen before. Having a good performance measure on the training data is\\ngood, but insufficient; the true goal is to perform well on new instances.\\nThere are two main approaches to generalization: instance-based learning and\\nmodel-based learning.\\nInstance-based learning\\nPossibly the most trivial form of learning is simply to learn by heart. If you were to\\ncreate a spam filter this way, it would just flag all emails that are identical to emails\\nthat have already been flagged by users—not the worst solution, but certainly not the\\nbest.\\nInstead of just flagging emails that are identical to known spam emails, your spam\\nfilter could be programmed to also flag emails that are very similar to known spam\\nemails. This requires a measure of similarity between two emails. A (very basic) simi‐\\nlarity measure between two emails could be to count the number of words they have\\nin common. The system would flag an email as spam if it has many words in com‐\\nmon with a known spam email.\\nThis is called instance-based learning: the system learns the examples by heart, then\\ngeneralizes to new cases by comparing them to the learned examples (or a subset of\\nthem), using a similarity measure. For example, in Figure 1-15 the new instance\\nwould be classified as a triangle because the majority of the most similar instances\\nbelong to that class.\\n18 \\n| \\nChapter 1: The Machine Learning Landscape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 44}, page_content='Figure 1-15. Instance-based learning\\nModel-based learning\\nAnother way to generalize from a set of examples is to build a model of these exam‐\\nples, then use that model to make predictions. This is called model-based learning\\n(Figure 1-16).\\nFigure 1-16. Model-based learning\\nFor example, suppose you want to know if money makes people happy, so you down‐\\nload the Better Life Index data from the OECD’s website as well as stats about GDP\\nper capita from the IMF’s website. Then you join the tables and sort by GDP per cap‐\\nita. Table 1-1 shows an excerpt of what you get.\\nTypes of Machine Learning Systems \\n| \\n19'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 45}, page_content='5 By convention, the Greek letter θ (theta) is frequently used to represent model parameters.\\nTable 1-1. Does money make people happier?\\nCountry\\nGDP per capita (USD)\\nLife satisfaction\\nHungary\\n12,240\\n4.9\\nKorea\\n27,195\\n5.8\\nFrance\\n37,675\\n6.5\\nAustralia\\n50,962\\n7.3\\nUnited States\\n55,805\\n7.2\\nLet’s plot the data for a few random countries (Figure 1-17).\\nFigure 1-17. Do you see a trend here?\\nThere does seem to be a trend here! Although the data is noisy (i.e., partly random), it\\nlooks like life satisfaction goes up more or less linearly as the country’s GDP per cap‐\\nita increases. So you decide to model life satisfaction as a linear function of GDP per\\ncapita. This step is called model selection: you selected a linear model of life satisfac‐\\ntion with just one attribute, GDP per capita (Equation 1-1).\\nEquation 1-1. A simple linear model\\nlife_satisfaction = θ0 + θ1 × GDP_per_capita\\nThis model has two model parameters, θ0 and θ1.5 By tweaking these parameters, you\\ncan make your model represent any linear function, as shown in Figure 1-18.\\n20 \\n| \\nChapter 1: The Machine Learning Landscape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 46}, page_content='Figure 1-18. A few possible linear models\\nBefore you can use your model, you need to define the parameter values θ0 and θ1.\\nHow can you know which values will make your model perform best? To answer this\\nquestion, you need to specify a performance measure. You can either define a utility\\nfunction (or fitness function) that measures how good your model is, or you can define\\na cost function that measures how bad it is. For linear regression problems, people\\ntypically use a cost function that measures the distance between the linear model’s\\npredictions and the training examples; the objective is to minimize this distance.\\nThis is where the Linear Regression algorithm comes in: you feed it your training\\nexamples and it finds the parameters that make the linear model fit best to your data.\\nThis is called training the model. In our case the algorithm finds that the optimal\\nparameter values are θ0 = 4.85 and θ1 = 4.91 × 10–5.\\nNow the model fits the training data as closely as possible (for a linear model), as you\\ncan see in Figure 1-19.\\nFigure 1-19. The linear model that fits the training data best\\nTypes of Machine Learning Systems \\n| \\n21'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 47}, page_content='6 The prepare_country_stats() function’s definition is not shown here (see this chapter’s Jupyter notebook if\\nyou want all the gory details). It’s just boring Pandas code that joins the life satisfaction data from the OECD\\nwith the GDP per capita data from the IMF.\\n7 It’s okay if you don’t understand all the code yet; we will present Scikit-Learn in the following chapters.\\nYou are finally ready to run the model to make predictions. For example, say you\\nwant to know how happy Cypriots are, and the OECD data does not have the answer.\\nFortunately, you can use your model to make a good prediction: you look up Cyprus’s\\nGDP per capita, find $22,587, and then apply your model and find that life satisfac‐\\ntion is likely to be somewhere around 4.85 + 22,587 × 4.91 × 10-5 = 5.96.\\nTo whet your appetite, Example 1-1 shows the Python code that loads the data, pre‐\\npares it,6 creates a scatterplot for visualization, and then trains a linear model and\\nmakes a prediction.7\\nExample 1-1. Training and running a linear model using Scikit-Learn\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nimport sklearn.linear_model\\n# Load the data\\noecd_bli = pd.read_csv(\"oecd_bli_2015.csv\", thousands=\\',\\')\\ngdp_per_capita = pd.read_csv(\"gdp_per_capita.csv\",thousands=\\',\\',delimiter=\\'\\\\t\\',\\n                             encoding=\\'latin1\\', na_values=\"n/a\")\\n# Prepare the data\\ncountry_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\\nX = np.c_[country_stats[\"GDP per capita\"]]\\ny = np.c_[country_stats[\"Life satisfaction\"]]\\n# Visualize the data\\ncountry_stats.plot(kind=\\'scatter\\', x=\"GDP per capita\", y=\\'Life satisfaction\\')\\nplt.show()\\n# Select a linear model\\nmodel = sklearn.linear_model.LinearRegression()\\n# Train the model\\nmodel.fit(X, y)\\n# Make a prediction for Cyprus\\nX_new = [[22587]]  # Cyprus\\' GDP per capita\\nprint(model.predict(X_new)) # outputs [[ 5.96242338]]\\n22 \\n| \\nChapter 1: The Machine Learning Landscape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 48}, page_content='If you had used an instance-based learning algorithm instead, you\\nwould have found that Slovenia has the closest GDP per capita to\\nthat of Cyprus ($20,732), and since the OECD data tells us that\\nSlovenians’ life satisfaction is 5.7, you would have predicted a life\\nsatisfaction of 5.7 for Cyprus. If you zoom out a bit and look at the\\ntwo next closest countries, you will find Portugal and Spain with\\nlife satisfactions of 5.1 and 6.5, respectively. Averaging these three\\nvalues, you get 5.77, which is pretty close to your model-based pre‐\\ndiction. This simple algorithm is called k-Nearest Neighbors regres‐\\nsion (in this example, k = 3).\\nReplacing the Linear Regression model with k-Nearest Neighbors\\nregression in the previous code is as simple as replacing these two\\nlines:\\nimport sklearn.linear_model\\nmodel = sklearn.linear_model.LinearRegression()\\nwith these two:\\nimport sklearn.neighbors\\nmodel = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)\\nIf all went well, your model will make good predictions. If not, you may need to use\\nmore attributes (employment rate, health, air pollution, etc.), get more or better qual‐\\nity training data, or perhaps select a more powerful model (e.g., a Polynomial Regres‐\\nsion model).\\nIn summary:\\n• You studied the data.\\n• You selected a model.\\n• You trained it on the training data (i.e., the learning algorithm searched for the\\nmodel parameter values that minimize a cost function).\\n• Finally, you applied the model to make predictions on new cases (this is called\\ninference), hoping that this model will generalize well.\\nThis is what a typical Machine Learning project looks like. In Chapter 2 you will\\nexperience this first-hand by going through an end-to-end project.\\nWe have covered a lot of ground so far: you now know what Machine Learning is\\nreally about, why it is useful, what some of the most common categories of ML sys‐\\ntems are, and what a typical project workflow looks like. Now let’s look at what can go\\nwrong in learning and prevent you from making accurate predictions.\\nTypes of Machine Learning Systems \\n| \\n23'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 49}, page_content='Main Challenges of Machine Learning\\nIn short, since your main task is to select a learning algorithm and train it on some\\ndata, the two things that can go wrong are “bad algorithm” and “bad data.” Let’s start\\nwith examples of bad data.\\nInsufficient Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\\nsay “apple” (possibly repeating this procedure a few times). Now the child is able to\\nrecognize apples in all sorts of colors and shapes. Genius.\\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\\ning algorithms to work properly. Even for very simple problems you typically need\\nthousands of examples, and for complex problems such as image or speech recogni‐\\ntion you may need millions of examples (unless you can reuse parts of an existing\\nmodel).\\n24 \\n| \\nChapter 1: The Machine Learning Landscape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 50}, page_content='8 For example, knowing whether to write “to,” “two,” or “too” depending on the context.\\n9 Figure reproduced with permission from Banko and Brill (2001), “Learning Curves for Confusion Set Disam‐\\nbiguation.”\\n10 “The Unreasonable Effectiveness of Data,” Peter Norvig et al. (2009).\\nThe Unreasonable Effectiveness of Data\\nIn a famous paper published in 2001, Microsoft researchers Michele Banko and Eric\\nBrill showed that very different Machine Learning algorithms, including fairly simple\\nones, performed almost identically well on a complex problem of natural language\\ndisambiguation8 once they were given enough data (as you can see in Figure 1-20).\\nFigure 1-20. The importance of data versus algorithms9\\nAs the authors put it: “these results suggest that we may want to reconsider the trade-\\noff between spending time and money on algorithm development versus spending it\\non corpus development.”\\nThe idea that data matters more than algorithms for complex problems was further\\npopularized by Peter Norvig et al. in a paper titled “The Unreasonable Effectiveness\\nof Data” published in 2009.10 It should be noted, however, that small- and medium-\\nsized datasets are still very common, and it is not always easy or cheap to get extra\\ntraining data, so don’t abandon algorithms just yet.\\nMain Challenges of Machine Learning \\n| \\n25'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 51}, page_content='Nonrepresentative Training Data\\nIn order to generalize well, it is crucial that your training data be representative of the\\nnew cases you want to generalize to. This is true whether you use instance-based\\nlearning or model-based learning.\\nFor example, the set of countries we used earlier for training the linear model was not\\nperfectly representative; a few countries were missing. Figure 1-21 shows what the\\ndata looks like when you add the missing countries.\\nFigure 1-21. A more representative training sample\\nIf you train a linear model on this data, you get the solid line, while the old model is\\nrepresented by the dotted line. As you can see, not only does adding a few missing\\ncountries significantly alter the model, but it makes it clear that such a simple linear\\nmodel is probably never going to work well. It seems that very rich countries are not\\nhappier than moderately rich countries (in fact they seem unhappier), and conversely\\nsome poor countries seem happier than many rich countries.\\nBy using a nonrepresentative training set, we trained a model that is unlikely to make\\naccurate predictions, especially for very poor and very rich countries.\\nIt is crucial to use a training set that is representative of the cases you want to general‐\\nize to. This is often harder than it sounds: if the sample is too small, you will have\\nsampling noise (i.e., nonrepresentative data as a result of chance), but even very large\\nsamples can be nonrepresentative if the sampling method is flawed. This is called\\nsampling bias.\\nA Famous Example of Sampling Bias\\nPerhaps the most famous example of sampling bias happened during the US presi‐\\ndential election in 1936, which pitted Landon against Roosevelt: the Literary Digest\\nconducted a very large poll, sending mail to about 10 million people. It got 2.4 million\\nanswers, and predicted with high confidence that Landon would get 57% of the votes.\\n26 \\n| \\nChapter 1: The Machine Learning Landscape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 52}, page_content='Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest’s\\nsampling method:\\n• First, to obtain the addresses to send the polls to, the Literary Digest used tele‐\\nphone directories, lists of magazine subscribers, club membership lists, and the\\nlike. All of these lists tend to favor wealthier people, who are more likely to vote\\nRepublican (hence Landon).\\n• Second, less than 25% of the people who received the poll answered. Again, this\\nintroduces a sampling bias, by ruling out people who don’t care much about poli‐\\ntics, people who don’t like the Literary Digest, and other key groups. This is a spe‐\\ncial type of sampling bias called nonresponse bias.\\nHere is another example: say you want to build a system to recognize funk music vid‐\\neos. One way to build your training set is to search “funk music” on YouTube and use\\nthe resulting videos. But this assumes that YouTube’s search engine returns a set of\\nvideos that are representative of all the funk music videos on YouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you live in Brazil\\nyou will get a lot of “funk carioca” videos, which sound nothing like James Brown).\\nOn the other hand, how else can you get a large training set?\\nPoor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\\nquality measurements), it will make it harder for the system to detect the underlying\\npatterns, so your system is less likely to perform well. It is often well worth the effort\\nto spend time cleaning up your training data. The truth is, most data scientists spend\\na significant part of their time doing just that. For example:\\n• If some instances are clearly outliers, it may help to simply discard them or try to\\nfix the errors manually.\\n• If some instances are missing a few features (e.g., 5% of your customers did not\\nspecify their age), you must decide whether you want to ignore this attribute alto‐\\ngether, ignore these instances, fill in the missing values (e.g., with the median\\nage), or train one model with the feature and one model without it, and so on.\\nIrrelevant Features\\nAs the saying goes: garbage in, garbage out. Your system will only be capable of learn‐\\ning if the training data contains enough relevant features and not too many irrelevant\\nones. A critical part of the success of a Machine Learning project is coming up with a\\ngood set of features to train on. This process, called feature engineering, involves:\\nMain Challenges of Machine Learning \\n| \\n27'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 53}, page_content='• Feature selection: selecting the most useful features to train on among existing\\nfeatures.\\n• Feature extraction: combining existing features to produce a more useful one (as\\nwe saw earlier, dimensionality reduction algorithms can help).\\n• Creating new features by gathering new data.\\nNow that we have looked at many examples of bad data, let’s look at a couple of exam‐\\nples of bad algorithms.\\nOverfitting the Training Data\\nSay you are visiting a foreign country and the taxi driver rips you off. You might be\\ntempted to say that all taxi drivers in that country are thieves. Overgeneralizing is\\nsomething that we humans do all too often, and unfortunately machines can fall into\\nthe same trap if we are not careful. In Machine Learning this is called overfitting: it\\nmeans that the model performs well on the training data, but it does not generalize\\nwell.\\nFigure 1-22 shows an example of a high-degree polynomial life satisfaction model\\nthat strongly overfits the training data. Even though it performs much better on the\\ntraining data than the simple linear model, would you really trust its predictions?\\nFigure 1-22. Overfitting the training data\\nComplex models such as deep neural networks can detect subtle patterns in the data,\\nbut if the training set is noisy, or if it is too small (which introduces sampling noise),\\nthen the model is likely to detect patterns in the noise itself. Obviously these patterns\\nwill not generalize to new instances. For example, say you feed your life satisfaction\\nmodel many more attributes, including uninformative ones such as the country’s\\nname. In that case, a complex model may detect patterns like the fact that all coun‐\\ntries in the training data with a w in their name have a life satisfaction greater than 7:\\nNew Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident\\n28 \\n| \\nChapter 1: The Machine Learning Landscape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 54}, page_content='are you that the W-satisfaction rule generalizes to Rwanda or Zimbabwe? Obviously\\nthis pattern occurred in the training data by pure chance, but the model has no way\\nto tell whether a pattern is real or simply the result of noise in the data.\\nOverfitting happens when the model is too complex relative to the\\namount and noisiness of the training data. The possible solutions\\nare:\\n• To simplify the model by selecting one with fewer parameters\\n(e.g., a linear model rather than a high-degree polynomial\\nmodel), by reducing the number of attributes in the training\\ndata or by constraining the model\\n• To gather more training data\\n• To reduce the noise in the training data (e.g., fix data errors\\nand remove outliers)\\nConstraining a model to make it simpler and reduce the risk of overfitting is called\\nregularization. For example, the linear model we defined earlier has two parameters,\\nθ0 and θ1. This gives the learning algorithm two degrees of freedom to adapt the model\\nto the training data: it can tweak both the height (θ0) and the slope (θ1) of the line. If\\nwe forced θ1 = 0, the algorithm would have only one degree of freedom and would\\nhave a much harder time fitting the data properly: all it could do is move the line up\\nor down to get as close as possible to the training instances, so it would end up\\naround the mean. A very simple model indeed! If we allow the algorithm to modify θ1\\nbut we force it to keep it small, then the learning algorithm will effectively have some‐\\nwhere in between one and two degrees of freedom. It will produce a simpler model\\nthan with two degrees of freedom, but more complex than with just one. You want to\\nfind the right balance between fitting the training data perfectly and keeping the\\nmodel simple enough to ensure that it will generalize well.\\nFigure 1-23 shows three models: the dotted line represents the original model that\\nwas trained with a few countries missing, the dashed line is our second model trained\\nwith all countries, and the solid line is a linear model trained with the same data as\\nthe first model but with a regularization constraint. You can see that regularization\\nforced the model to have a smaller slope, which fits a bit less the training data that the\\nmodel was trained on, but actually allows it to generalize better to new examples.\\nMain Challenges of Machine Learning \\n| \\n29'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 55}, page_content='Figure 1-23. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a hyper‐\\nparameter. A hyperparameter is a parameter of a learning algorithm (not of the\\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\\nto training and remains constant during training. If you set the regularization hyper‐\\nparameter to a very large value, you will get an almost flat model (a slope close to\\nzero); the learning algorithm will almost certainly not overfit the training data, but it\\nwill be less likely to find a good solution. Tuning hyperparameters is an important\\npart of building a Machine Learning system (you will see a detailed example in the\\nnext chapter).\\nUnderfitting the Training Data\\nAs you might guess, underfitting is the opposite of overfitting: it occurs when your\\nmodel is too simple to learn the underlying structure of the data. For example, a lin‐\\near model of life satisfaction is prone to underfit; reality is just more complex than\\nthe model, so its predictions are bound to be inaccurate, even on the training exam‐\\nples.\\nThe main options to fix this problem are:\\n• Selecting a more powerful model, with more parameters\\n• Feeding better features to the learning algorithm (feature engineering)\\n• Reducing the constraints on the model (e.g., reducing the regularization hyper‐\\nparameter)\\nStepping Back\\nBy now you already know a lot about Machine Learning. However, we went through\\nso many concepts that you may be feeling a little lost, so let’s step back and look at the\\nbig picture:\\n30 \\n| \\nChapter 1: The Machine Learning Landscape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 56}, page_content='• Machine Learning is about making machines get better at some task by learning\\nfrom data, instead of having to explicitly code rules.\\n• There are many different types of ML systems: supervised or not, batch or online,\\ninstance-based or model-based, and so on.\\n• In a ML project you gather data in a training set, and you feed the training set to\\na learning algorithm. If the algorithm is model-based it tunes some parameters to\\nfit the model to the training set (i.e., to make good predictions on the training set\\nitself), and then hopefully it will be able to make good predictions on new cases\\nas well. If the algorithm is instance-based, it just learns the examples by heart and\\ngeneralizes to new instances by comparing them to the learned instances using a\\nsimilarity measure.\\n• The system will not perform well if your training set is too small, or if the data is\\nnot representative, noisy, or polluted with irrelevant features (garbage in, garbage\\nout). Lastly, your model needs to be neither too simple (in which case it will\\nunderfit) nor too complex (in which case it will overfit).\\nThere’s just one last important topic to cover: once you have trained a model, you\\ndon’t want to just “hope” it generalizes to new cases. You want to evaluate it, and fine-\\ntune it if necessary. Let’s see how.\\nTesting and Validating\\nThe only way to know how well a model will generalize to new cases is to actually try\\nit out on new cases. One way to do that is to put your model in production and moni‐\\ntor how well it performs. This works well, but if your model is horribly bad, your\\nusers will complain—not the best idea.\\nA better option is to split your data into two sets: the training set and the test set. As\\nthese names imply, you train your model using the training set, and you test it using\\nthe test set. The error rate on new cases is called the generalization error (or out-of-\\nsample error), and by evaluating your model on the test set, you get an estimate of this\\nerror. This value tells you how well your model will perform on instances it has never\\nseen before.\\nIf the training error is low (i.e., your model makes few mistakes on the training set)\\nbut the generalization error is high, it means that your model is overfitting the train‐\\ning data.\\nIt is common to use 80% of the data for training and hold out 20%\\nfor testing. However, this depends on the size of the dataset: if it\\ncontains 10 million instances, then holding out 1% means your test\\nset will contain 100,000 instances: that’s probably more than\\nenough to get a good estimate of the generalization error.\\nTesting and Validating \\n| \\n31'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 57}, page_content='Hyperparameter Tuning and Model Selection\\nSo evaluating a model is simple enough: just use a test set. Now suppose you are hesi‐\\ntating between two models (say a linear model and a polynomial model): how can\\nyou decide? One option is to train both and compare how well they generalize using\\nthe test set.\\nNow suppose that the linear model generalizes better, but you want to apply some \\nregularization to avoid overfitting. The question is: how do you choose the value of\\nthe regularization hyperparameter? One option is to train 100 different models using\\n100 different values for this hyperparameter. Suppose you find the best hyperparame‐\\nter value that produces a model with the lowest generalization error, say just 5% error.\\nSo you launch this model into production, but unfortunately it does not perform as\\nwell as expected and produces 15% errors. What just happened?\\nThe problem is that you measured the generalization error multiple times on the test\\nset, and you adapted the model and hyperparameters to produce the best model for\\nthat particular set. This means that the model is unlikely to perform as well on new\\ndata.\\nA common solution to this problem is called holdout validation: you simply hold out\\npart of the training set to evaluate several candidate models and select the best one.\\nThe new heldout set is called the validation set (or sometimes the development set, or\\ndev set). More specifically, you train multiple models with various hyperparameters\\non the reduced training set (i.e., the full training set minus the validation set), and\\nyou select the model that performs best on the validation set. After this holdout vali‐\\ndation process, you train the best model on the full training set (including the valida‐\\ntion set), and this gives you the final model. Lastly, you evaluate this final model on\\nthe test set to get an estimate of the generalization error.\\nThis solution usually works quite well. However, if the validation set is too small, then\\nmodel evaluations will be imprecise: you may end up selecting a suboptimal model by\\nmistake. Conversely, if the validation set is too large, then the remaining training set\\nwill be much smaller than the full training set. Why is this bad? Well, since the final\\nmodel will be trained on the full training set, it is not ideal to compare candidate\\nmodels trained on a much smaller training set. It would be like selecting the fastest\\nsprinter to participate in a marathon. One way to solve this problem is to perform\\nrepeated cross-validation, using many small validation sets. Each model is evaluated\\nonce per validation set, after it is trained on the rest of the data. By averaging out all\\nthe evaluations of a model, we get a much more accurate measure of its performance.\\nHowever, there is a drawback: the training time is multiplied by the number of valida‐\\ntion sets.\\n32 \\n| \\nChapter 1: The Machine Learning Landscape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 58}, page_content='11 “The Lack of A Priori Distinctions Between Learning Algorithms,” D. Wolpert (1996).\\nData Mismatch\\nIn some cases, it is easy to get a large amount of data for training, but it is not per‐\\nfectly representative of the data that will be used in production. For example, suppose\\nyou want to create a mobile app to take pictures of flowers and automatically deter‐\\nmine their species. You can easily download millions of pictures of flowers on the\\nweb, but they won’t be perfectly representative of the pictures that will actually be\\ntaken using the app on a mobile device. Perhaps you only have 10,000 representative\\npictures (i.e., actually taken with the app). In this case, the most important rule to\\nremember is that the validation set and the test must be as representative as possible\\nof the data you expect to use in production, so they should be composed exclusively\\nof representative pictures: you can shuffle them and put half in the validation set, and\\nhalf in the test set (making sure that no duplicates or near-duplicates end up in both\\nsets). After training your model on the web pictures, if you observe that the perfor‐\\nmance of your model on the validation set is disappointing, you will not know\\nwhether this is because your model has overfit the training set, or whether this is just\\ndue to the mismatch between the web pictures and the mobile app pictures. One sol‐\\nution is to hold out part of the training pictures (from the web) in yet another set that\\nAndrew Ng calls the train-dev set. After the model is trained (on the training set, not\\non the train-dev set), you can evaluate it on the train-dev set: if it performs well, then\\nthe model is not overfitting the training set, so if performs poorly on the validation\\nset, the problem must come from the data mismatch. You can try to tackle this prob‐\\nlem by preprocessing the web images to make them look more like the pictures that\\nwill be taken by the mobile app, and then retraining the model. Conversely, if the\\nmodel performs poorly on the train-dev set, then the model must have overfit the\\ntraining set, so you should try to simplify or regularize the model, get more training\\ndata and clean up the training data, as discussed earlier.\\nNo Free Lunch Theorem\\nA model is a simplified version of the observations. The simplifications are meant to\\ndiscard the superfluous details that are unlikely to generalize to new instances. How‐\\never, to decide what data to discard and what data to keep, you must make assump‐\\ntions. For example, a linear model makes the assumption that the data is\\nfundamentally linear and that the distance between the instances and the straight line\\nis just noise, which can safely be ignored.\\nIn a famous 1996 paper,11 David Wolpert demonstrated that if you make absolutely\\nno assumption about the data, then there is no reason to prefer one model over any\\nother. This is called the No Free Lunch (NFL) theorem. For some datasets the best\\nTesting and Validating \\n| \\n33'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 59}, page_content='model is a linear model, while for other datasets it is a neural network. There is no\\nmodel that is a priori guaranteed to work better (hence the name of the theorem). The\\nonly way to know for sure which model is best is to evaluate them all. Since this is not\\npossible, in practice you make some reasonable assumptions about the data and you\\nevaluate only a few reasonable models. For example, for simple tasks you may evalu‐\\nate linear models with various levels of regularization, and for a complex problem you\\nmay evaluate various neural networks.\\nExercises\\nIn this chapter we have covered some of the most important concepts in Machine\\nLearning. In the next chapters we will dive deeper and write more code, but before we\\ndo, make sure you know how to answer the following questions:\\n1. How would you define Machine Learning?\\n2. Can you name four types of problems where it shines?\\n3. What is a labeled training set?\\n4. What are the two most common supervised tasks?\\n5. Can you name four common unsupervised tasks?\\n6. What type of Machine Learning algorithm would you use to allow a robot to\\nwalk in various unknown terrains?\\n7. What type of algorithm would you use to segment your customers into multiple\\ngroups?\\n8. Would you frame the problem of spam detection as a supervised learning prob‐\\nlem or an unsupervised learning problem?\\n9. What is an online learning system?\\n10. What is out-of-core learning?\\n11. What type of learning algorithm relies on a similarity measure to make predic‐\\ntions?\\n12. What is the difference between a model parameter and a learning algorithm’s\\nhyperparameter?\\n13. What do model-based learning algorithms search for? What is the most common\\nstrategy they use to succeed? How do they make predictions?\\n14. Can you name four of the main challenges in Machine Learning?\\n15. If your model performs great on the training data but generalizes poorly to new\\ninstances, what is happening? Can you name three possible solutions?\\n16. What is a test set and why would you want to use it?\\n34 \\n| \\nChapter 1: The Machine Learning Landscape'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 60}, page_content='17. What is the purpose of a validation set?\\n18. What can go wrong if you tune hyperparameters using the test set?\\n19. What is repeated cross-validation and why would you prefer it to using a single\\nvalidation set?\\nSolutions to these exercises are available in ???.\\nExercises \\n| \\n35'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 61}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 62}, page_content='1 The example project is completely fictitious; the goal is just to illustrate the main steps of a Machine Learning\\nproject, not to learn anything about the real estate business.\\nCHAPTER 2\\nEnd-to-End Machine Learning Project\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 2 in the final\\nrelease of the book.\\nIn this chapter, you will go through an example project end to end, pretending to be a\\nrecently hired data scientist in a real estate company.1 Here are the main steps you will\\ngo through:\\n1. Look at the big picture.\\n2. Get the data.\\n3. Discover and visualize the data to gain insights.\\n4. Prepare the data for Machine Learning algorithms.\\n5. Select a model and train it.\\n6. Fine-tune your model.\\n7. Present your solution.\\n8. Launch, monitor, and maintain your system.\\n37'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 63}, page_content='2 The original dataset appeared in R. Kelley Pace and Ronald Barry, “Sparse Spatial Autoregressions,” Statistics\\n& Probability Letters 33, no. 3 (1997): 291–297.\\nWorking with Real Data\\nWhen you are learning about Machine Learning it is best to actually experiment with\\nreal-world data, not just artificial datasets. Fortunately, there are thousands of open\\ndatasets to choose from, ranging across all sorts of domains. Here are a few places\\nyou can look to get data:\\n• Popular open data repositories:\\n— UC Irvine Machine Learning Repository\\n— Kaggle datasets\\n— Amazon’s AWS datasets\\n• Meta portals (they list open data repositories):\\n— http://dataportals.org/\\n— http://opendatamonitor.eu/\\n— http://quandl.com/\\n• Other pages listing many popular open data repositories:\\n— Wikipedia’s list of Machine Learning datasets\\n— Quora.com question\\n— Datasets subreddit\\nIn this chapter we chose the California Housing Prices dataset from the StatLib repos‐\\nitory2 (see Figure 2-1). This dataset was based on data from the 1990 California cen‐\\nsus. It is not exactly recent (you could still afford a nice house in the Bay Area at the\\ntime), but it has many qualities for learning, so we will pretend it is recent data. We\\nalso added a categorical attribute and removed a few features for teaching purposes.\\n38 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 64}, page_content='Figure 2-1. California housing prices\\nLook at the Big Picture\\nWelcome to Machine Learning Housing Corporation! The first task you are asked to\\nperform is to build a model of housing prices in California using the California cen‐\\nsus data. This data has metrics such as the population, median income, median hous‐\\ning price, and so on for each block group in California. Block groups are the smallest\\ngeographical unit for which the US Census Bureau publishes sample data (a block\\ngroup typically has a population of 600 to 3,000 people). We will just call them “dis‐\\ntricts” for short.\\nYour model should learn from this data and be able to predict the median housing\\nprice in any district, given all the other metrics.\\nSince you are a well-organized data scientist, the first thing you do\\nis to pull out your Machine Learning project checklist. You can\\nstart with the one in ???; it should work reasonably well for most\\nMachine Learning projects but make sure to adapt it to your needs.\\nIn this chapter we will go through many checklist items, but we will\\nalso skip a few, either because they are self-explanatory or because\\nthey will be discussed in later chapters.\\nFrame the Problem\\nThe first question to ask your boss is what exactly is the business objective; building a\\nmodel is probably not the end goal. How does the company expect to use and benefit\\nLook at the Big Picture \\n| \\n39'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 65}, page_content='3 A piece of information fed to a Machine Learning system is often called a signal in reference to Shannon’s\\ninformation theory: you want a high signal/noise ratio.\\nfrom this model? This is important because it will determine how you frame the\\nproblem, what algorithms you will select, what performance measure you will use to\\nevaluate your model, and how much effort you should spend tweaking it.\\nYour boss answers that your model’s output (a prediction of a district’s median hous‐\\ning price) will be fed to another Machine Learning system (see Figure 2-2), along\\nwith many other signals.3 This downstream system will determine whether it is worth\\ninvesting in a given area or not. Getting this right is critical, as it directly affects reve‐\\nnue.\\nFigure 2-2. A Machine Learning pipeline for real estate investments\\nPipelines\\nA sequence of data processing components is called a data pipeline. Pipelines are very\\ncommon in Machine Learning systems, since there is a lot of data to manipulate and\\nmany data transformations to apply.\\nComponents typically run asynchronously. Each component pulls in a large amount\\nof data, processes it, and spits out the result in another data store, and then some time\\nlater the next component in the pipeline pulls this data and spits out its own output,\\nand so on. Each component is fairly self-contained: the interface between components\\nis simply the data store. This makes the system quite simple to grasp (with the help of\\na data flow graph), and different teams can focus on different components. Moreover,\\nif a component breaks down, the downstream components can often continue to run\\nnormally (at least for a while) by just using the last output from the broken compo‐\\nnent. This makes the architecture quite robust.\\n40 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 66}, page_content='On the other hand, a broken component can go unnoticed for some time if proper\\nmonitoring is not implemented. The data gets stale and the overall system’s perfor‐\\nmance drops.\\nThe next question to ask is what the current solution looks like (if any). It will often\\ngive you a reference performance, as well as insights on how to solve the problem.\\nYour boss answers that the district housing prices are currently estimated manually\\nby experts: a team gathers up-to-date information about a district, and when they\\ncannot get the median housing price, they estimate it using complex rules.\\nThis is costly and time-consuming, and their estimates are not great; in cases where\\nthey manage to find out the actual median housing price, they often realize that their\\nestimates were off by more than 20%. This is why the company thinks that it would\\nbe useful to train a model to predict a district’s median housing price given other data\\nabout that district. The census data looks like a great dataset to exploit for this pur‐\\npose, since it includes the median housing prices of thousands of districts, as well as\\nother data.\\nOkay, with all this information you are now ready to start designing your system.\\nFirst, you need to frame the problem: is it supervised, unsupervised, or Reinforce‐\\nment Learning? Is it a classification task, a regression task, or something else? Should\\nyou use batch learning or online learning techniques? Before you read on, pause and\\ntry to answer these questions for yourself.\\nHave you found the answers? Let’s see: it is clearly a typical supervised learning task\\nsince you are given labeled training examples (each instance comes with the expected\\noutput, i.e., the district’s median housing price). Moreover, it is also a typical regres‐\\nsion task, since you are asked to predict a value. More specifically, this is a multiple\\nregression problem since the system will use multiple features to make a prediction (it\\nwill use the district’s population, the median income, etc.). It is also a univariate\\nregression problem since we are only trying to predict a single value for each district.\\nIf we were trying to predict multiple values per district, it would be a multivariate\\nregression problem. Finally, there is no continuous flow of data coming in the system,\\nthere is no particular need to adjust to changing data rapidly, and the data is small\\nenough to fit in memory, so plain batch learning should do just fine.\\nIf the data was huge, you could either split your batch learning\\nwork across multiple servers (using the MapReduce technique), or\\nyou could use an online learning technique instead.\\nLook at the Big Picture \\n| \\n41'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 67}, page_content='Select a Performance Measure\\nYour next step is to select a performance measure. A typical performance measure for\\nregression problems is the Root Mean Square Error (RMSE). It gives an idea of how\\nmuch error the system typically makes in its predictions, with a higher weight for\\nlarge errors. Equation 2-1 shows the mathematical formula to compute the RMSE.\\nEquation 2-1. Root Mean Square Error (RMSE)\\nRMSE X, h =\\n1\\nm ∑\\ni = 1\\nm\\nh x i\\n−y i 2\\n42 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 68}, page_content='4 Recall that the transpose operator flips a column vector into a row vector (and vice versa).\\nNotations\\nThis equation introduces several very common Machine Learning notations that we\\nwill use throughout this book:\\n• m is the number of instances in the dataset you are measuring the RMSE on.\\n— For example, if you are evaluating the RMSE on a validation set of 2,000 dis‐\\ntricts, then m = 2,000.\\n• x(i) is a vector of all the feature values (excluding the label) of the ith instance in\\nthe dataset, and y(i) is its label (the desired output value for that instance).\\n— For example, if the first district in the dataset is located at longitude –118.29°,\\nlatitude 33.91°, and it has 1,416 inhabitants with a median income of $38,372,\\nand the median house value is $156,400 (ignoring the other features for now),\\nthen:\\nx 1 =\\n−118 . 29\\n33 . 91\\n1, 416\\n38, 372\\nand:\\ny 1 = 156, 400\\n• X is a matrix containing all the feature values (excluding labels) of all instances in\\nthe dataset. There is one row per instance and the ith row is equal to the transpose\\nof x(i), noted (x(i))T.4\\n— For example, if the first district is as just described, then the matrix X looks\\nlike this:\\nX =\\nx 1 T\\nx 2 T\\n⋮\\nx 1999 T\\nx 2000 T\\n= −118 . 29 33 . 91 1, 416 38, 372\\n⋮\\n⋮\\n⋮\\n⋮\\nLook at the Big Picture \\n| \\n43'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 69}, page_content='• h is your system’s prediction function, also called a hypothesis. When your system\\nis given an instance’s feature vector x(i), it outputs a predicted value ŷ(i) = h(x(i))\\nfor that instance (ŷ is pronounced “y-hat”).\\n— For example, if your system predicts that the median housing price in the first\\ndistrict is $158,400, then ŷ(1) = h(x(1)) = 158,400. The prediction error for this\\ndistrict is ŷ(1) – y(1) = 2,000.\\n• RMSE(X,h) is the cost function measured on the set of examples using your\\nhypothesis h.\\nWe use lowercase italic font for scalar values (such as m or y(i)) and function names\\n(such as h), lowercase bold font for vectors (such as x(i)), and uppercase bold font for\\nmatrices (such as X).\\nEven though the RMSE is generally the preferred performance measure for regression\\ntasks, in some contexts you may prefer to use another function. For example, suppose\\nthat there are many outlier districts. In that case, you may consider using the Mean\\nAbsolute Error (also called the Average Absolute Deviation; see Equation 2-2):\\nEquation 2-2. Mean Absolute Error\\nMAE X, h = 1\\nm ∑\\ni = 1\\nm\\nh x i\\n−y i\\nBoth the RMSE and the MAE are ways to measure the distance between two vectors:\\nthe vector of predictions and the vector of target values. Various distance measures,\\nor norms, are possible:\\n• Computing the root of a sum of squares (RMSE) corresponds to the Euclidean\\nnorm: it is the notion of distance you are familiar with. It is also called the ℓ2\\nnorm, noted ∥ · ∥2 (or just ∥ · ∥).\\n• Computing the sum of absolutes (MAE) corresponds to the ℓ1 norm, noted ∥ · ∥1.\\nIt is sometimes called the Manhattan norm because it measures the distance\\nbetween two points in a city if you can only travel along orthogonal city blocks.\\n• More generally, the ℓk norm of a vector v containing n elements is defined as\\n∥�∥k =\\nv0\\nk + v1\\nk + ⋯+ vn\\nk\\n1\\nk. ℓ0 just gives the number of non-zero ele‐\\nments in the vector, and ℓ∞ gives the maximum absolute value in the vector.\\n• The higher the norm index, the more it focuses on large values and neglects small\\nones. This is why the RMSE is more sensitive to outliers than the MAE. But when\\n44 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 70}, page_content='5 The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.\\noutliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap,”\\n“medium,” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. You don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! You’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2.\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/.5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nYou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). You can use your sys‐\\nGet the Data \\n| \\n45'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 71}, page_content=\"6 We will show the installation steps using pip in a bash shell on a Linux or MacOS system. You may need to\\nadapt these commands to your own system. On Windows, we recommend installing Anaconda instead.\\n7 If you want to upgrade pip for all users on your machine rather than just your own user, you should remove\\nthe --user option and make sure you have administrator rights (e.g., by adding sudo before the whole com‐\\nmand on Linux or MacOSX).\\n8 Alternative tools include venv (very similar to virtualenv and included in the standard library), virtualenv‐\\nwrapper (provides extra functionalities on top of virtualenv), pyenv (allows easy switching between Python\\nversions), and pipenv (a great packaging tool by the same author as the popular requests library, built on top\\nof pip, virtualenv and more).\\ntem’s packaging system (e.g., apt-get on Ubuntu, or MacPorts or HomeBrew on\\nMacOS), install a Scientific Python distribution such as Anaconda and use its packag‐\\ning system, or just use Python’s own packaging system, pip, which is included by\\ndefault with the Python binary installers (since Python 2.7.9).6 You can check to see if\\npip is installed by typing the following command:\\n$ python3 -m pip --version\\npip 19.0.2 from [...]/lib/python3.6/site-packages (python 3.6)\\nYou should make sure you have a recent version of pip installed. To upgrade the pip\\nmodule, type:7\\n$ python3 -m pip install --user -U pip\\nCollecting pip\\n[...]\\nSuccessfully installed pip-19.0.2\\nCreating an Isolated Environment\\nIf you would like to work in an isolated environment (which is strongly recom‐\\nmended so you can work on different projects without having conflicting library ver‐\\nsions), install virtualenv8 by running the following pip command (again, if you want\\nvirtualenv to be installed for all users on your machine, remove --user and run this\\ncommand with administrator rights):\\n$ python3 -m pip install --user -U virtualenv\\nCollecting virtualenv\\n[...]\\nSuccessfully installed virtualenv\\nNow you can create an isolated Python environment by typing:\\n$ cd $ML_PATH\\n$ virtualenv env\\nUsing base prefix '[...]'\\nNew python executable in [...]/ml/env/bin/python3.6\\nAlso creating executable in [...]/ml/env/bin/python\\nInstalling setuptools, pip, wheel...done.\\n46 \\n| \\nChapter 2: End-to-End Machine Learning Project\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 72}, page_content='9 Note that Jupyter can handle multiple versions of Python, and even many other languages such as R or\\nOctave.\\nNow every time you want to activate this environment, just open a terminal and type:\\n$ cd $ML_PATH\\n$ source env/bin/activate # on Linux or MacOSX\\n$ .\\\\env\\\\Scripts\\\\activate  # on Windows\\nTo deactivate this environment, just type deactivate. While the environment is\\nactive, any package you install using pip will be installed in this isolated environment,\\nand Python will only have access to these packages (if you also want access to the sys‐\\ntem’s packages, you should create the environment using virtualenv’s --system-site-\\npackages option). Check out virtualenv’s documentation for more information.\\nNow you can install all the required modules and their dependencies using this sim‐\\nple pip command (if you are not using a virtualenv, you will need the --user option\\nor administrator rights):\\n$ python3 -m pip install -U jupyter matplotlib numpy pandas scipy scikit-learn\\nCollecting jupyter\\n  Downloading jupyter-1.0.0-py2.py3-none-any.whl\\nCollecting matplotlib\\n  [...]\\nTo check your installation, try to import every module like this:\\n$ python3 -c \"import jupyter, matplotlib, numpy, pandas, scipy, sklearn\"\\nThere should be no output and no error. Now you can fire up Jupyter by typing:\\n$ jupyter notebook\\n[I 15:24 NotebookApp] Serving notebooks from local directory: [...]/ml\\n[I 15:24 NotebookApp] 0 active kernels\\n[I 15:24 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/\\n[I 15:24 NotebookApp] Use Control-C to stop this server and shut down all\\nkernels (twice to skip confirmation).\\nA Jupyter server is now running in your terminal, listening to port 8888. You can visit\\nthis server by opening your web browser to http://localhost:8888/ (this usually hap‐\\npens automatically when the server starts). You should see your empty workspace\\ndirectory (containing only the env directory if you followed the preceding virtualenv\\ninstructions).\\nNow create a new Python notebook by clicking on the New button and selecting the\\nappropriate Python version9 (see Figure 2-3).\\nThis does three things: first, it creates a new notebook file called Untitled.ipynb in\\nyour workspace; second, it starts a Jupyter Python kernel to run this notebook; and\\nGet the Data \\n| \\n47'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 73}, page_content='third, it opens this notebook in a new tab. You should start by renaming this note‐\\nbook to “Housing” (this will automatically rename the file to Housing.ipynb) by click‐\\ning Untitled and typing the new name.\\nFigure 2-3. Your workspace in Jupyter\\nA notebook contains a list of cells. Each cell can contain executable code or formatted\\ntext. Right now the notebook contains only one empty code cell, labeled “In [1]:”. Try\\ntyping print(\"Hello world!\") in the cell, and click on the play button (see\\nFigure 2-4) or press Shift-Enter. This sends the current cell to this notebook’s Python\\nkernel, which runs it and returns the output. The result is displayed below the cell,\\nand since we reached the end of the notebook, a new cell is automatically created. Go\\nthrough the User Interface Tour from Jupyter’s Help menu to learn the basics.\\nFigure 2-4. Hello world Python notebook\\n48 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 74}, page_content='10 You might also need to check legal constraints, such as private fields that should never be copied to unsafe\\ndatastores.\\n11 In a real project you would save this code in a Python file, but for now you can just write it in your Jupyter\\nnotebook.\\nDownload the Data\\nIn typical environments your data would be available in a relational database (or\\nsome other common datastore) and spread across multiple tables/documents/files. To\\naccess it, you would first need to get your credentials and access authorizations,10 and\\nfamiliarize yourself with the data schema. In this project, however, things are much\\nsimpler: you will just download a single compressed file, housing.tgz, which contains a\\ncomma-separated value (CSV) file called housing.csv with all the data.\\nYou could use your web browser to download it, and run tar xzf housing.tgz to\\ndecompress the file and extract the CSV file, but it is preferable to create a small func‐\\ntion to do that. It is useful in particular if data changes regularly, as it allows you to\\nwrite a small script that you can run whenever you need to fetch the latest data (or\\nyou can set up a scheduled job to do that automatically at regular intervals). Auto‐\\nmating the process of fetching the data is also useful if you need to install the dataset\\non multiple machines.\\nHere is the function to fetch the data:11\\nimport os\\nimport tarfile\\nfrom six.moves import urllib\\nDOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\\n    if not os.path.isdir(housing_path):\\n        os.makedirs(housing_path)\\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\\n    urllib.request.urlretrieve(housing_url, tgz_path)\\n    housing_tgz = tarfile.open(tgz_path)\\n    housing_tgz.extractall(path=housing_path)\\n    housing_tgz.close()\\nNow when you call fetch_housing_data(), it creates a datasets/housing directory in\\nyour workspace, downloads the housing.tgz file, and extracts the housing.csv from it in\\nthis directory.\\nNow let’s load the data using Pandas. Once again you should write a small function to\\nload the data:\\nGet the Data \\n| \\n49'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 75}, page_content='import pandas as pd\\ndef load_housing_data(housing_path=HOUSING_PATH):\\n    csv_path = os.path.join(housing_path, \"housing.csv\")\\n    return pd.read_csv(csv_path)\\nThis function returns a Pandas DataFrame object containing all the data.\\nTake a Quick Look at the Data Structure\\nLet’s take a look at the top five rows using the DataFrame’s head() method (see\\nFigure 2-5).\\nFigure 2-5. Top five rows in the dataset\\nEach row represents one district. There are 10 attributes (you can see the first 6 in the\\nscreenshot): longitude, latitude, housing_median_age, total_rooms, total_bed\\nrooms, \\npopulation, \\nhouseholds, \\nmedian_income, \\nmedian_house_value, and\\nocean_proximity.\\nThe info() method is useful to get a quick description of the data, in particular the\\ntotal number of rows, and each attribute’s type and number of non-null values (see\\nFigure 2-6).\\n50 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 76}, page_content='Figure 2-6. Housing info\\nThere are 20,640 instances in the dataset, which means that it is fairly small by\\nMachine Learning standards, but it’s perfect to get started. Notice that the total_bed\\nrooms attribute has only 20,433 non-null values, meaning that 207 districts are miss‐\\ning this feature. We will need to take care of this later.\\nAll attributes are numerical, except the ocean_proximity field. Its type is object, so it\\ncould hold any kind of Python object, but since you loaded this data from a CSV file\\nyou know that it must be a text attribute. When you looked at the top five rows, you\\nprobably noticed that the values in the ocean_proximity column were repetitive,\\nwhich means that it is probably a categorical attribute. You can find out what cate‐\\ngories exist and how many districts belong to each category by using the\\nvalue_counts() method:\\n>>> housing[\"ocean_proximity\"].value_counts()\\n<1H OCEAN     9136\\nINLAND        6551\\nNEAR OCEAN    2658\\nNEAR BAY      2290\\nISLAND           5\\nName: ocean_proximity, dtype: int64\\nLet’s look at the other fields. The describe() method shows a summary of the\\nnumerical attributes (Figure 2-7).\\nGet the Data \\n| \\n51'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 77}, page_content='12 The standard deviation is generally denoted σ (the Greek letter sigma), and it is the square root of the var‐\\niance, which is the average of the squared deviation from the mean. When a feature has a bell-shaped normal\\ndistribution (also called a Gaussian distribution), which is very common, the “68-95-99.7” rule applies: about\\n68% of the values fall within 1σ of the mean, 95% within 2σ, and 99.7% within 3σ.\\nFigure 2-7. Summary of each numerical attribute\\nThe count, mean, min, and max rows are self-explanatory. Note that the null values are\\nignored (so, for example, count of total_bedrooms is 20,433, not 20,640). The std\\nrow shows the standard deviation, which measures how dispersed the values are.12\\nThe 25%, 50%, and 75% rows show the corresponding percentiles: a percentile indi‐\\ncates the value below which a given percentage of observations in a group of observa‐\\ntions falls. For example, 25% of the districts have a housing_median_age lower than\\n18, while 50% are lower than 29 and 75% are lower than 37. These are often called the\\n25th percentile (or 1st quartile), the median, and the 75th percentile (or 3rd quartile).\\nAnother quick way to get a feel of the type of data you are dealing with is to plot a \\nhistogram for each numerical attribute. A histogram shows the number of instances\\n(on the vertical axis) that have a given value range (on the horizontal axis). You can\\neither plot this one attribute at a time, or you can call the hist() method on the\\nwhole dataset, and it will plot a histogram for each numerical attribute (see\\nFigure 2-8). For example, you can see that slightly over 800 districts have a\\nmedian_house_value equal to about $100,000.\\n%matplotlib inline   # only in a Jupyter notebook\\nimport matplotlib.pyplot as plt\\nhousing.hist(bins=50, figsize=(20,15))\\nplt.show()\\n52 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 78}, page_content='The hist() method relies on Matplotlib, which in turn relies on a\\nuser-specified graphical backend to draw on your screen. So before\\nyou can plot anything, you need to specify which backend Matplot‐\\nlib should use. The simplest option is to use Jupyter’s magic com‐\\nmand %matplotlib inline. This tells Jupyter to set up Matplotlib\\nso it uses Jupyter’s own backend. Plots are then rendered within the\\nnotebook itself. Note that calling show() is optional in a Jupyter\\nnotebook, as Jupyter will automatically display plots when a cell is\\nexecuted.\\nFigure 2-8. A histogram for each numerical attribute\\nNotice a few things in these histograms:\\n1. First, the median income attribute does not look like it is expressed in US dollars\\n(USD). After checking with the team that collected the data, you are told that the\\ndata has been scaled and capped at 15 (actually 15.0001) for higher median\\nincomes, and at 0.5 (actually 0.4999) for lower median incomes. The numbers\\nrepresent roughly tens of thousands of dollars (e.g., 3 actually means about\\n$30,000). Working with preprocessed attributes is common in Machine Learning,\\nGet the Data \\n| \\n53'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 79}, page_content='and it is not necessarily a problem, but you should try to understand how the\\ndata was computed.\\n2. The housing median age and the median house value were also capped. The lat‐\\nter may be a serious problem since it is your target attribute (your labels). Your\\nMachine Learning algorithms may learn that prices never go beyond that limit.\\nYou need to check with your client team (the team that will use your system’s out‐\\nput) to see if this is a problem or not. If they tell you that they need precise pre‐\\ndictions even beyond $500,000, then you have mainly two options:\\na. Collect proper labels for the districts whose labels were capped.\\nb. Remove those districts from the training set (and also from the test set, since\\nyour system should not be evaluated poorly if it predicts values beyond\\n$500,000).\\n3. These attributes have very different scales. We will discuss this later in this chap‐\\nter when we explore feature scaling.\\n4. Finally, many histograms are tail heavy: they extend much farther to the right of\\nthe median than to the left. This may make it a bit harder for some Machine\\nLearning algorithms to detect patterns. We will try transforming these attributes\\nlater on to have more bell-shaped distributions.\\nHopefully you now have a better understanding of the kind of data you are dealing\\nwith.\\nWait! Before you look at the data any further, you need to create a\\ntest set, put it aside, and never look at it.\\nCreate a Test Set\\nIt may sound strange to voluntarily set aside part of the data at this stage. After all,\\nyou have only taken a quick glance at the data, and surely you should learn a whole\\nlot more about it before you decide what algorithms to use, right? This is true, but\\nyour brain is an amazing pattern detection system, which means that it is highly\\nprone to overfitting: if you look at the test set, you may stumble upon some seemingly\\ninteresting pattern in the test data that leads you to select a particular kind of\\nMachine Learning model. When you estimate the generalization error using the test\\nset, your estimate will be too optimistic and you will launch a system that will not\\nperform as well as expected. This is called data snooping bias.\\nCreating a test set is theoretically quite simple: just pick some instances randomly,\\ntypically 20% of the dataset (or less if your dataset is very large), and set them aside:\\n54 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 80}, page_content='13 In this book, when a code example contains a mix of code and outputs, as is the case here, it is formatted like\\nin the Python interpreter, for better readability: the code lines are prefixed with >>> (or ... for indented\\nblocks), and the outputs have no prefix.\\n14 You will often see people set the random seed to 42. This number has no special property, other than to be\\nThe Answer to the Ultimate Question of Life, the Universe, and Everything.\\nimport numpy as np\\ndef split_train_test(data, test_ratio):\\n    shuffled_indices = np.random.permutation(len(data))\\n    test_set_size = int(len(data) * test_ratio)\\n    test_indices = shuffled_indices[:test_set_size]\\n    train_indices = shuffled_indices[test_set_size:]\\n    return data.iloc[train_indices], data.iloc[test_indices]\\nYou can then use this function like this:13\\n>>> train_set, test_set = split_train_test(housing, 0.2)\\n>>> len(train_set)\\n16512\\n>>> len(test_set)\\n4128\\nWell, this works, but it is not perfect: if you run the program again, it will generate a\\ndifferent test set! Over time, you (or your Machine Learning algorithms) will get to\\nsee the whole dataset, which is what you want to avoid.\\nOne solution is to save the test set on the first run and then load it in subsequent\\nruns. Another option is to set the random number generator’s seed (e.g., np.ran\\ndom.seed(42))14 before calling np.random.permutation(), so that it always generates\\nthe same shuffled indices.\\nBut both these solutions will break next time you fetch an updated dataset. A com‐\\nmon solution is to use each instance’s identifier to decide whether or not it should go\\nin the test set (assuming instances have a unique and immutable identifier). For\\nexample, you could compute a hash of each instance’s identifier and put that instance\\nin the test set if the hash is lower or equal to 20% of the maximum hash value. This\\nensures that the test set will remain consistent across multiple runs, even if you\\nrefresh the dataset. The new test set will contain 20% of the new instances, but it will\\nnot contain any instance that was previously in the training set. Here is a possible\\nimplementation:\\nfrom zlib import crc32\\ndef test_set_check(identifier, test_ratio):\\n    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\\ndef split_train_test_by_id(data, test_ratio, id_column):\\n    ids = data[id_column]\\nGet the Data \\n| \\n55'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 81}, page_content='15 The location information is actually quite coarse, and as a result many districts will have the exact same ID, so\\nthey will end up in the same set (test or train). This introduces some unfortunate sampling bias.\\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\\n    return data.loc[~in_test_set], data.loc[in_test_set]\\nUnfortunately, the housing dataset does not have an identifier column. The simplest\\nsolution is to use the row index as the ID:\\nhousing_with_id = housing.reset_index()   # adds an `index` column\\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\\nIf you use the row index as a unique identifier, you need to make sure that new data\\ngets appended to the end of the dataset, and no row ever gets deleted. If this is not\\npossible, then you can try to use the most stable features to build a unique identifier.\\nFor example, a district’s latitude and longitude are guaranteed to be stable for a few\\nmillion years, so you could combine them into an ID like so:15\\nhousing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")\\nScikit-Learn provides a few functions to split datasets into multiple subsets in various\\nways. The simplest function is train_test_split, which does pretty much the same\\nthing as the function split_train_test defined earlier, with a couple of additional\\nfeatures. First there is a random_state parameter that allows you to set the random\\ngenerator seed as explained previously, and second you can pass it multiple datasets\\nwith an identical number of rows, and it will split them on the same indices (this is\\nvery useful, for example, if you have a separate DataFrame for labels):\\nfrom sklearn.model_selection import train_test_split\\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\\nSo far we have considered purely random sampling methods. This is generally fine if\\nyour dataset is large enough (especially relative to the number of attributes), but if it\\nis not, you run the risk of introducing a significant sampling bias. When a survey\\ncompany decides to call 1,000 people to ask them a few questions, they don’t just pick\\n1,000 people randomly in a phone book. They try to ensure that these 1,000 people\\nare representative of the whole population. For example, the US population is com‐\\nposed of 51.3% female and 48.7% male, so a well-conducted survey in the US would\\ntry to maintain this ratio in the sample: 513 female and 487 male. This is called strati‐\\nfied sampling: the population is divided into homogeneous subgroups called strata,\\nand the right number of instances is sampled from each stratum to guarantee that the\\ntest set is representative of the overall population. If they used purely random sam‐\\npling, there would be about 12% chance of sampling a skewed test set with either less\\nthan 49% female or more than 54% female. Either way, the survey results would be\\nsignificantly biased.\\n56 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 82}, page_content='Suppose you chatted with experts who told you that the median income is a very\\nimportant attribute to predict median housing prices. You may want to ensure that\\nthe test set is representative of the various categories of incomes in the whole dataset.\\nSince the median income is a continuous numerical attribute, you first need to create\\nan income category attribute. Let’s look at the median income histogram more closely\\n(back in Figure 2-8): most median income values are clustered around 1.5 to 6 (i.e.,\\n$15,000–$60,000), but some median incomes go far beyond 6. It is important to have\\na sufficient number of instances in your dataset for each stratum, or else the estimate\\nof the stratum’s importance may be biased. This means that you should not have too\\nmany strata, and each stratum should be large enough. The following code uses the\\npd.cut() function to create an income category attribute with 5 categories (labeled\\nfrom 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from\\n1.5 to 3, and so on:\\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\\n                               labels=[1, 2, 3, 4, 5])\\nThese income categories are represented in Figure 2-9:\\nhousing[\"income_cat\"].hist()\\nFigure 2-9. Histogram of income categories\\nNow you are ready to do stratified sampling based on the income category. For this\\nyou can use Scikit-Learn’s StratifiedShuffleSplit class:\\nfrom sklearn.model_selection import StratifiedShuffleSplit\\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\\n    strat_train_set = housing.loc[train_index]\\n    strat_test_set = housing.loc[test_index]\\nGet the Data \\n| \\n57'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 83}, page_content='Let’s see if this worked as expected. You can start by looking at the income category\\nproportions in the test set:\\n>>> strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\\n3    0.350533\\n2    0.318798\\n4    0.176357\\n5    0.114583\\n1    0.039729\\nName: income_cat, dtype: float64\\nWith similar code you can measure the income category proportions in the full data‐\\nset. Figure 2-10 compares the income category proportions in the overall dataset, in\\nthe test set generated with stratified sampling, and in a test set generated using purely\\nrandom sampling. As you can see, the test set generated using stratified sampling has\\nincome category proportions almost identical to those in the full dataset, whereas the\\ntest set generated using purely random sampling is quite skewed.\\nFigure 2-10. Sampling bias comparison of stratified versus purely random sampling\\nNow you should remove the income_cat attribute so the data is back to its original\\nstate:\\nfor set_ in (strat_train_set, strat_test_set):\\n    set_.drop(\"income_cat\", axis=1, inplace=True)\\nWe spent quite a bit of time on test set generation for a good reason: this is an often\\nneglected but critical part of a Machine Learning project. Moreover, many of these\\nideas will be useful later when we discuss cross-validation. Now it’s time to move on\\nto the next stage: exploring the data.\\nDiscover and Visualize the Data to Gain Insights\\nSo far you have only taken a quick glance at the data to get a general understanding of\\nthe kind of data you are manipulating. Now the goal is to go a little bit more in depth.\\nFirst, make sure you have put the test set aside and you are only exploring the train‐\\ning set. Also, if the training set is very large, you may want to sample an exploration\\n58 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 84}, page_content='set, to make manipulations easy and fast. In our case, the set is quite small so you can\\njust work directly on the full set. Let’s create a copy so you can play with it without\\nharming the training set:\\nhousing = strat_train_set.copy()\\nVisualizing Geographical Data\\nSince there is geographical information (latitude and longitude), it is a good idea to\\ncreate a scatterplot of all districts to visualize the data (Figure 2-11):\\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\\nFigure 2-11. A geographical scatterplot of the data\\nThis looks like California all right, but other than that it is hard to see any particular\\npattern. Setting the alpha option to 0.1 makes it much easier to visualize the places\\nwhere there is a high density of data points (Figure 2-12):\\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\\nDiscover and Visualize the Data to Gain Insights \\n| \\n59'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 85}, page_content='16 If you are reading this in grayscale, grab a red pen and scribble over most of the coastline from the Bay Area\\ndown to San Diego (as you might expect). You can add a patch of yellow around Sacramento as well.\\nFigure 2-12. A better visualization highlighting high-density areas\\nNow that’s much better: you can clearly see the high-density areas, namely the Bay\\nArea and around Los Angeles and San Diego, plus a long line of fairly high density in\\nthe Central Valley, in particular around Sacramento and Fresno.\\nMore generally, our brains are very good at spotting patterns on pictures, but you\\nmay need to play around with visualization parameters to make the patterns stand\\nout.\\nNow let’s look at the housing prices (Figure 2-13). The radius of each circle represents\\nthe district’s population (option s), and the color represents the price (option c). We\\nwill use a predefined color map (option cmap) called jet, which ranges from blue\\n(low values) to red (high prices):16\\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\\n    s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\\n    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\\n)\\nplt.legend()\\n60 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 86}, page_content='Figure 2-13. California housing prices\\nDiscover and Visualize the Data to Gain Insights \\n| \\n61'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 87}, page_content='This image tells you that the housing prices are very much related to the location\\n(e.g., close to the ocean) and to the population density, as you probably knew already.\\nIt will probably be useful to use a clustering algorithm to detect the main clusters, and\\nadd new features that measure the proximity to the cluster centers. The ocean prox‐\\nimity attribute may be useful as well, although in Northern California the housing\\nprices in coastal districts are not too high, so it is not a simple rule.\\nLooking for Correlations\\nSince the dataset is not too large, you can easily compute the standard correlation\\ncoefficient (also called Pearson’s r) between every pair of attributes using the corr()\\nmethod:\\ncorr_matrix = housing.corr()\\nNow let’s look at how much each attribute correlates with the median house value:\\n>>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)\\nmedian_house_value    1.000000\\nmedian_income         0.687170\\ntotal_rooms           0.135231\\nhousing_median_age    0.114220\\nhouseholds            0.064702\\ntotal_bedrooms        0.047865\\npopulation           -0.026699\\nlongitude            -0.047279\\nlatitude             -0.142826\\nName: median_house_value, dtype: float64\\nThe correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\\nthere is a strong positive correlation; for example, the median house value tends to go\\nup when the median income goes up. When the coefficient is close to –1, it means\\nthat there is a strong negative correlation; you can see a small negative correlation\\nbetween the latitude and the median house value (i.e., prices have a slight tendency to\\ngo down when you go north). Finally, coefficients close to zero mean that there is no\\nlinear correlation. Figure 2-14 shows various plots along with the correlation coeffi‐\\ncient between their horizontal and vertical axes.\\n62 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 88}, page_content='Figure 2-14. Standard correlation coefficient of various datasets (source: Wikipedia;\\npublic domain image)\\nThe correlation coefficient only measures linear correlations (“if x\\ngoes up, then y generally goes up/down”). It may completely miss\\nout on nonlinear relationships (e.g., “if x is close to zero then y gen‐\\nerally goes up”). Note how all the plots of the bottom row have a\\ncorrelation coefficient equal to zero despite the fact that their axes\\nare clearly not independent: these are examples of nonlinear rela‐\\ntionships. Also, the second row shows examples where the correla‐\\ntion coefficient is equal to 1 or –1; notice that this has nothing to\\ndo with the slope. For example, your height in inches has a correla‐\\ntion coefficient of 1 with your height in feet or in nanometers.\\nAnother way to check for correlation between attributes is to use Pandas’\\nscatter_matrix function, which plots every numerical attribute against every other\\nnumerical attribute. Since there are now 11 numerical attributes, you would get 112 =\\n121 plots, which would not fit on a page, so let’s just focus on a few promising\\nattributes that seem most correlated with the median housing value (Figure 2-15):\\nfrom pandas.plotting import scatter_matrix\\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\\n              \"housing_median_age\"]\\nscatter_matrix(housing[attributes], figsize=(12, 8))\\nDiscover and Visualize the Data to Gain Insights \\n| \\n63'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 89}, page_content='Figure 2-15. Scatter matrix\\nThe main diagonal (top left to bottom right) would be full of straight lines if Pandas\\nplotted each variable against itself, which would not be very useful. So instead Pandas\\ndisplays a histogram of each attribute (other options are available; see Pandas’ docu‐\\nmentation for more details).\\nThe most promising attribute to predict the median house value is the median\\nincome, so let’s zoom in on their correlation scatterplot (Figure 2-16):\\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\\n             alpha=0.1)\\nThis plot reveals a few things. First, the correlation is indeed very strong; you can\\nclearly see the upward trend and the points are not too dispersed. Second, the price\\ncap that we noticed earlier is clearly visible as a horizontal line at $500,000. But this\\nplot reveals other less obvious straight lines: a horizontal line around $450,000,\\nanother around $350,000, perhaps one around $280,000, and a few more below that.\\nYou may want to try removing the corresponding districts to prevent your algorithms\\nfrom learning to reproduce these data quirks.\\n64 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 90}, page_content='Figure 2-16. Median income versus median house value\\nExperimenting with Attribute Combinations\\nHopefully the previous sections gave you an idea of a few ways you can explore the\\ndata and gain insights. You identified a few data quirks that you may want to clean up\\nbefore feeding the data to a Machine Learning algorithm, and you found interesting\\ncorrelations between attributes, in particular with the target attribute. You also\\nnoticed that some attributes have a tail-heavy distribution, so you may want to trans‐\\nform them (e.g., by computing their logarithm). Of course, your mileage will vary\\nconsiderably with each project, but the general ideas are similar.\\nOne last thing you may want to do before actually preparing the data for Machine\\nLearning algorithms is to try out various attribute combinations. For example, the\\ntotal number of rooms in a district is not very useful if you don’t know how many\\nhouseholds there are. What you really want is the number of rooms per household.\\nSimilarly, the total number of bedrooms by itself is not very useful: you probably\\nwant to compare it to the number of rooms. And the population per household also\\nseems like an interesting attribute combination to look at. Let’s create these new\\nattributes:\\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\\nhousing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\\nAnd now let’s look at the correlation matrix again:\\n>>> corr_matrix = housing.corr()\\n>>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)\\nmedian_house_value          1.000000\\nDiscover and Visualize the Data to Gain Insights \\n| \\n65'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 91}, page_content='median_income               0.687160\\nrooms_per_household         0.146285\\ntotal_rooms                 0.135097\\nhousing_median_age          0.114110\\nhouseholds                  0.064506\\ntotal_bedrooms              0.047689\\npopulation_per_household   -0.021985\\npopulation                 -0.026920\\nlongitude                  -0.047432\\nlatitude                   -0.142724\\nbedrooms_per_room          -0.259984\\nName: median_house_value, dtype: float64\\nHey, not bad! The new bedrooms_per_room attribute is much more correlated with\\nthe median house value than the total number of rooms or bedrooms. Apparently\\nhouses with a lower bedroom/room ratio tend to be more expensive. The number of\\nrooms per household is also more informative than the total number of rooms in a\\ndistrict—obviously the larger the houses, the more expensive they are.\\nThis round of exploration does not have to be absolutely thorough; the point is to\\nstart off on the right foot and quickly gain insights that will help you get a first rea‐\\nsonably good prototype. But this is an iterative process: once you get a prototype up\\nand running, you can analyze its output to gain more insights and come back to this\\nexploration step.\\nPrepare the Data for Machine Learning Algorithms\\nIt’s time to prepare the data for your Machine Learning algorithms. Instead of just\\ndoing this manually, you should write functions to do that, for several good reasons:\\n• This will allow you to reproduce these transformations easily on any dataset (e.g.,\\nthe next time you get a fresh dataset).\\n• You will gradually build a library of transformation functions that you can reuse\\nin future projects.\\n• You can use these functions in your live system to transform the new data before\\nfeeding it to your algorithms.\\n• This will make it possible for you to easily try various transformations and see\\nwhich combination of transformations works best.\\nBut first let’s revert to a clean training set (by copying strat_train_set once again),\\nand let’s separate the predictors and the labels since we don’t necessarily want to apply\\nthe same transformations to the predictors and the target values (note that drop() \\ncreates a copy of the data and does not affect strat_train_set):\\nhousing = strat_train_set.drop(\"median_house_value\", axis=1)\\nhousing_labels = strat_train_set[\"median_house_value\"].copy()\\n66 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 92}, page_content='Data Cleaning\\nMost Machine Learning algorithms cannot work with missing features, so let’s create\\na few functions to take care of them. You noticed earlier that the total_bedrooms\\nattribute has some missing values, so let’s fix this. You have three options:\\n• Get rid of the corresponding districts.\\n• Get rid of the whole attribute.\\n• Set the values to some value (zero, the mean, the median, etc.).\\nYou can accomplish these easily using DataFrame’s dropna(), drop(), and fillna()\\nmethods:\\nhousing.dropna(subset=[\"total_bedrooms\"])    # option 1\\nhousing.drop(\"total_bedrooms\", axis=1)       # option 2\\nmedian = housing[\"total_bedrooms\"].median()  # option 3\\nhousing[\"total_bedrooms\"].fillna(median, inplace=True)\\nIf you choose option 3, you should compute the median value on the training set, and\\nuse it to fill the missing values in the training set, but also don’t forget to save the\\nmedian value that you have computed. You will need it later to replace missing values\\nin the test set when you want to evaluate your system, and also once the system goes\\nlive to replace missing values in new data.\\nScikit-Learn provides a handy class to take care of missing values: SimpleImputer.\\nHere is how to use it. First, you need to create a SimpleImputer instance, specifying\\nthat you want to replace each attribute’s missing values with the median of that\\nattribute:\\nfrom sklearn.impute import SimpleImputer\\nimputer = SimpleImputer(strategy=\"median\")\\nSince the median can only be computed on numerical attributes, we need to create a\\ncopy of the data without the text attribute ocean_proximity:\\nhousing_num = housing.drop(\"ocean_proximity\", axis=1)\\nNow you can fit the imputer instance to the training data using the fit() method:\\nimputer.fit(housing_num)\\nThe imputer has simply computed the median of each attribute and stored the result\\nin its statistics_ instance variable. Only the total_bedrooms attribute had missing\\nvalues, but we cannot be sure that there won’t be any missing values in new data after\\nthe system goes live, so it is safer to apply the imputer to all the numerical attributes:\\n>>> imputer.statistics_\\narray([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\\nPrepare the Data for Machine Learning Algorithms \\n| \\n67'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 93}, page_content='17 For more details on the design principles, see “API design for machine learning software: experiences from\\nthe scikit-learn project,” L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. Müller, et al. (2013).\\n>>> housing_num.median().values\\narray([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\\nNow you can use this “trained” imputer to transform the training set by replacing\\nmissing values by the learned medians:\\nX = imputer.transform(housing_num)\\nThe result is a plain NumPy array containing the transformed features. If you want to\\nput it back into a Pandas DataFrame, it’s simple:\\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns)\\nScikit-Learn Design\\nScikit-Learn’s API is remarkably well designed. The main design principles are:17\\n• Consistency. All objects share a consistent and simple interface:\\n— Estimators. Any object that can estimate some parameters based on a dataset\\nis called an estimator (e.g., an imputer is an estimator). The estimation itself is\\nperformed by the fit() method, and it takes only a dataset as a parameter (or\\ntwo for supervised learning algorithms; the second dataset contains the\\nlabels). Any other parameter needed to guide the estimation process is con‐\\nsidered a hyperparameter (such as an imputer’s strategy), and it must be set\\nas an instance variable (generally via a constructor parameter).\\n— Transformers. Some estimators (such as an imputer) can also transform a\\ndataset; these are called transformers. Once again, the API is quite simple: the\\ntransformation is performed by the transform() method with the dataset to\\ntransform as a parameter. It returns the transformed dataset. This transforma‐\\ntion generally relies on the learned parameters, as is the case for an imputer.\\nAll transformers also have a convenience method called fit_transform() \\nthat is equivalent to calling fit() and then transform() (but sometimes\\nfit_transform() is optimized and runs much faster).\\n— Predictors. Finally, some estimators are capable of making predictions given a\\ndataset; they are called predictors. For example, the LinearRegression model \\nin the previous chapter was a predictor: it predicted life satisfaction given a\\ncountry’s GDP per capita. A predictor has a predict() method that takes a\\ndataset of new instances and returns a dataset of corresponding predictions. It\\nalso has a score() method that measures the quality of the predictions given\\n68 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 94}, page_content='18 Some predictors also provide methods to measure the confidence of their predictions.\\n19 This class is available since Scikit-Learn 0.20. If you use an earlier version, please consider upgrading, or use\\nPandas’ Series.factorize() method.\\na test set (and the corresponding labels in the case of supervised learning\\nalgorithms).18\\n• Inspection. All the estimator’s hyperparameters are accessible directly via public\\ninstance variables (e.g., imputer.strategy), and all the estimator’s learned\\nparameters are also accessible via public instance variables with an underscore\\nsuffix (e.g., imputer.statistics_).\\n• Nonproliferation of classes. Datasets are represented as NumPy arrays or SciPy\\nsparse matrices, instead of homemade classes. Hyperparameters are just regular\\nPython strings or numbers.\\n• Composition. Existing building blocks are reused as much as possible. For\\nexample, it is easy to create a Pipeline estimator from an arbitrary sequence of\\ntransformers followed by a final estimator, as we will see.\\n• Sensible defaults. Scikit-Learn provides reasonable default values for most\\nparameters, making it easy to create a baseline working system quickly.\\nHandling Text and Categorical Attributes\\nEarlier we left out the categorical attribute ocean_proximity because it is a text\\nattribute so we cannot compute its median:\\n>>> housing_cat = housing[[\"ocean_proximity\"]]\\n>>> housing_cat.head(10)\\n      ocean_proximity\\n17606       <1H OCEAN\\n18632       <1H OCEAN\\n14650      NEAR OCEAN\\n3230           INLAND\\n3555        <1H OCEAN\\n19480          INLAND\\n8879        <1H OCEAN\\n13685          INLAND\\n4937        <1H OCEAN\\n4861        <1H OCEAN\\nMost Machine Learning algorithms prefer to work with numbers anyway, so let’s con‐\\nvert these categories from text to numbers. For this, we can use Scikit-Learn’s Ordina\\nlEncoder class19:\\n>>> from sklearn.preprocessing import OrdinalEncoder\\n>>> ordinal_encoder = OrdinalEncoder()\\nPrepare the Data for Machine Learning Algorithms \\n| \\n69'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 95}, page_content=\"20 Before Scikit-Learn 0.20, it could only encode integer categorical values, but since 0.20 it can also handle\\nother types of inputs, including text categorical inputs.\\n>>> housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\\n>>> housing_cat_encoded[:10]\\narray([[0.],\\n       [0.],\\n       [4.],\\n       [1.],\\n       [0.],\\n       [1.],\\n       [0.],\\n       [1.],\\n       [0.],\\n       [0.]])\\nYou can get the list of categories using the categories_ instance variable. It is a list\\ncontaining a 1D array of categories for each categorical attribute (in this case, a list\\ncontaining a single array since there is just one categorical attribute):\\n>>> ordinal_encoder.categories_\\n[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\\n       dtype=object)]\\nOne issue with this representation is that ML algorithms will assume that two nearby\\nvalues are more similar than two distant values. This may be fine in some cases (e.g.,\\nfor ordered categories such as “bad”, “average”, “good”, “excellent”), but it is obviously\\nnot the case for the ocean_proximity column (for example, categories 0 and 4 are\\nclearly more similar than categories 0 and 1). To fix this issue, a common solution is\\nto create one binary attribute per category: one attribute equal to 1 when the category\\nis “<1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is\\n“INLAND” (and 0 otherwise), and so on. This is called one-hot encoding, because\\nonly one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new\\nattributes are sometimes called dummy attributes. Scikit-Learn provides a OneHotEn\\ncoder class to convert categorical values into one-hot vectors20:\\n>>> from sklearn.preprocessing import OneHotEncoder\\n>>> cat_encoder = OneHotEncoder()\\n>>> housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\\n>>> housing_cat_1hot\\n<16512x5 sparse matrix of type '<class 'numpy.float64'>'\\n  with 16512 stored elements in Compressed Sparse Row format>\\nNotice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very\\nuseful when you have categorical attributes with thousands of categories. After one-\\nhot encoding we get a matrix with thousands of columns, and the matrix is full of\\nzeros except for a single 1 per row. Using up tons of memory mostly to store zeros\\nwould be very wasteful, so instead a sparse matrix only stores the location of the non‐\\n70 \\n| \\nChapter 2: End-to-End Machine Learning Project\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 96}, page_content=\"21 See SciPy’s documentation for more details.\\nzero elements. You can use it mostly like a normal 2D array,21 but if you really want to\\nconvert it to a (dense) NumPy array, just call the toarray() method:\\n>>> housing_cat_1hot.toarray()\\narray([[1., 0., 0., 0., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 0., 1.],\\n       ...,\\n       [0., 1., 0., 0., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 1., 0.]])\\nOnce again, you can get the list of categories using the encoder’s categories_\\ninstance variable:\\n>>> cat_encoder.categories_\\n[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\\n       dtype=object)]\\nIf a categorical attribute has a large number of possible categories\\n(e.g., country code, profession, species, etc.), then one-hot encod‐\\ning will result in a large number of input features. This may slow\\ndown training and degrade performance. If this happens, you may\\nwant to replace the categorical input with useful numerical features\\nrelated to the categories: for example, you could replace the\\nocean_proximity feature with the distance to the ocean (similarly,\\na country code could be replaced with the country’s population and\\nGDP per capita). Alternatively, you could replace each category\\nwith a learnable low dimensional vector called an embedding. Each\\ncategory’s representation would be learned during training: this is\\nan example of representation learning (see Chapter 13 and ??? for\\nmore details).\\nCustom Transformers\\nAlthough Scikit-Learn provides many useful transformers, you will need to write\\nyour own for tasks such as custom cleanup operations or combining specific\\nattributes. You will want your transformer to work seamlessly with Scikit-Learn func‐\\ntionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher‐\\nitance), all you need is to create a class and implement three methods: fit()\\n(returning self), transform(), and fit_transform(). You can get the last one for\\nfree by simply adding TransformerMixin as a base class. Also, if you add BaseEstima\\ntor as a base class (and avoid *args and **kargs in your constructor) you will get\\ntwo extra methods (get_params() and set_params()) that will be useful for auto‐\\nPrepare the Data for Machine Learning Algorithms \\n| \\n71\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 97}, page_content='matic hyperparameter tuning. For example, here is a small transformer class that adds\\nthe combined attributes we discussed earlier:\\nfrom sklearn.base import BaseEstimator, TransformerMixin\\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\\n        self.add_bedrooms_per_room = add_bedrooms_per_room\\n    def fit(self, X, y=None):\\n        return self  # nothing else to do\\n    def transform(self, X, y=None):\\n        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\\n        population_per_household = X[:, population_ix] / X[:, households_ix]\\n        if self.add_bedrooms_per_room:\\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\\n            return np.c_[X, rooms_per_household, population_per_household,\\n                         bedrooms_per_room]\\n        else:\\n            return np.c_[X, rooms_per_household, population_per_household]\\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\\nhousing_extra_attribs = attr_adder.transform(housing.values)\\nIn this example the transformer has one hyperparameter, add_bedrooms_per_room,\\nset to True by default (it is often helpful to provide sensible defaults). This hyperpara‐\\nmeter will allow you to easily find out whether adding this attribute helps the\\nMachine Learning algorithms or not. More generally, you can add a hyperparameter\\nto gate any data preparation step that you are not 100% sure about. The more you\\nautomate these data preparation steps, the more combinations you can automatically\\ntry out, making it much more likely that you will find a great combination (and sav‐\\ning you a lot of time).\\nFeature Scaling\\nOne of the most important transformations you need to apply to your data is feature\\nscaling. With few exceptions, Machine Learning algorithms don’t perform well when\\nthe input numerical attributes have very different scales. This is the case for the hous‐\\ning data: the total number of rooms ranges from about 6 to 39,320, while the median\\nincomes only range from 0 to 15. Note that scaling the target values is generally not\\nrequired.\\nThere are two common ways to get all attributes to have the same scale: min-max\\nscaling and standardization.\\nMin-max scaling (many people call this normalization) is quite simple: values are\\nshifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract‐\\ning the min value and dividing by the max minus the min. Scikit-Learn provides a\\n72 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 98}, page_content='transformer called MinMaxScaler for this. It has a feature_range hyperparameter\\nthat lets you change the range if you don’t want 0–1 for some reason.\\nStandardization is quite different: first it subtracts the mean value (so standardized\\nvalues always have a zero mean), and then it divides by the standard deviation so that\\nthe resulting distribution has unit variance. Unlike min-max scaling, standardization\\ndoes not bound values to a specific range, which may be a problem for some algo‐\\nrithms (e.g., neural networks often expect an input value ranging from 0 to 1). How‐\\never, standardization is much less affected by outliers. For example, suppose a district\\nhad a median income equal to 100 (by mistake). Min-max scaling would then crush\\nall the other values from 0–15 down to 0–0.15, whereas standardization would not be\\nmuch affected. Scikit-Learn provides a transformer called StandardScaler for stand‐\\nardization.\\nAs with all the transformations, it is important to fit the scalers to\\nthe training data only, not to the full dataset (including the test set).\\nOnly then can you use them to transform the training set and the\\ntest set (and new data).\\nTransformation Pipelines\\nAs you can see, there are many data transformation steps that need to be executed in\\nthe right order. Fortunately, Scikit-Learn provides the Pipeline class to help with\\nsuch sequences of transformations. Here is a small pipeline for the numerical\\nattributes:\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nnum_pipeline = Pipeline([\\n        (\\'imputer\\', SimpleImputer(strategy=\"median\")),\\n        (\\'attribs_adder\\', CombinedAttributesAdder()),\\n        (\\'std_scaler\\', StandardScaler()),\\n    ])\\nhousing_num_tr = num_pipeline.fit_transform(housing_num)\\nThe Pipeline constructor takes a list of name/estimator pairs defining a sequence of\\nsteps. All but the last estimator must be transformers (i.e., they must have a\\nfit_transform() method). The names can be anything you like (as long as they are\\nunique and don’t contain double underscores “__”): they will come in handy later for\\nhyperparameter tuning.\\nWhen you call the pipeline’s fit() method, it calls fit_transform() sequentially on\\nall transformers, passing the output of each call as the parameter to the next call, until\\nit reaches the final estimator, for which it just calls the fit() method.\\nPrepare the Data for Machine Learning Algorithms \\n| \\n73'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 99}, page_content='22 Just like for pipelines, the name can be anything as long as it does not contain double underscores.\\nThe pipeline exposes the same methods as the final estimator. In this example, the last\\nestimator is a StandardScaler, which is a transformer, so the pipeline has a trans\\nform() method that applies all the transforms to the data in sequence (and of course\\nalso a fit_transform() method, which is the one we used).\\nSo far, we have handled the categorical columns and the numerical columns sepa‐\\nrately. It would be more convenient to have a single transformer able to handle all col‐\\numns, applying the appropriate transformations to each column. In version 0.20,\\nScikit-Learn introduced the ColumnTransformer for this purpose, and the good news\\nis that it works great with Pandas DataFrames. Let’s use it to apply all the transforma‐\\ntions to the housing data:\\nfrom sklearn.compose import ColumnTransformer\\nnum_attribs = list(housing_num)\\ncat_attribs = [\"ocean_proximity\"]\\nfull_pipeline = ColumnTransformer([\\n        (\"num\", num_pipeline, num_attribs),\\n        (\"cat\", OneHotEncoder(), cat_attribs),\\n    ])\\nhousing_prepared = full_pipeline.fit_transform(housing)\\nHere is how this works: first we import the ColumnTransformer class, next we get the\\nlist of numerical column names and the list of categorical column names, and we\\nconstruct a ColumnTransformer. The constructor requires a list of tuples, where each\\ntuple contains a name22, a transformer and a list of names (or indices) of columns\\nthat the transformer should be applied to. In this example, we specify that the numer‐\\nical columns should be transformed using the num_pipeline that we defined earlier,\\nand the categorical columns should be transformed using a OneHotEncoder. Finally,\\nwe apply this ColumnTransformer to the housing data: it applies each transformer to\\nthe appropriate columns and concatenates the outputs along the second axis (the\\ntransformers must return the same number of rows).\\nNote that the OneHotEncoder returns a sparse matrix, while the num_pipeline returns\\na dense matrix. When there is such a mix of sparse and dense matrices, the Colum\\nnTransformer estimates the density of the final matrix (i.e., the ratio of non-zero\\ncells), and it returns a sparse matrix if the density is lower than a given threshold (by\\ndefault, sparse_threshold=0.3). In this example, it returns a dense matrix. And\\nthat’s it! We have a preprocessing pipeline that takes the full housing data and applies\\nthe appropriate transformations to each column.\\n74 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 100}, page_content='Instead of a transformer, you can specify the string \"drop\" if you\\nwant the columns to be dropped. Or you can specify \"pass\\nthrough\" if you want the columns to be left untouched. By default,\\nthe remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the remainder hyperparameter to any\\ntransformer (or to \"passthrough\") if you want these columns to be\\nhandled differently.\\nIf you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\\nsklearn-pandas, or roll out your own custom transformer to get the same function‐\\nality as the ColumnTransformer. Alternatively, you can use the FeatureUnion class\\nwhich can also apply different transformers and concatenate their outputs, but you\\ncannot specify different columns for each transformer, they all apply to the whole\\ndata. It is possible to work around this limitation using a custom transformer for col‐\\numn selection (see the Jupyter notebook for an example).\\nSelect and Train a Model\\nAt last! You framed the problem, you got the data and explored it, you sampled a\\ntraining set and a test set, and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically. You are now ready\\nto select and train a Machine Learning model.\\nTraining and Evaluating on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going to be\\nmuch simpler than you might think. Let’s first train a Linear Regression model, like\\nwe did in the previous chapter:\\nfrom sklearn.linear_model import LinearRegression\\nlin_reg = LinearRegression()\\nlin_reg.fit(housing_prepared, housing_labels)\\nDone! You now have a working Linear Regression model. Let’s try it out on a few\\ninstances from the training set:\\n>>> some_data = housing.iloc[:5]\\n>>> some_labels = housing_labels.iloc[:5]\\n>>> some_data_prepared = full_pipeline.transform(some_data)\\n>>> print(\"Predictions:\", lin_reg.predict(some_data_prepared))\\nPredictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\\n>>> print(\"Labels:\", list(some_labels))\\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\\nSelect and Train a Model \\n| \\n75'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 101}, page_content='It works, although the predictions are not exactly accurate (e.g., the first prediction is\\noff by close to 40%!). Let’s measure this regression model’s RMSE on the whole train‐\\ning set using Scikit-Learn’s mean_squared_error function:\\n>>> from sklearn.metrics import mean_squared_error\\n>>> housing_predictions = lin_reg.predict(housing_prepared)\\n>>> lin_mse = mean_squared_error(housing_labels, housing_predictions)\\n>>> lin_rmse = np.sqrt(lin_mse)\\n>>> lin_rmse\\n68628.19819848922\\nOkay, this is better than nothing but clearly not a great score: most districts’\\nmedian_housing_values range between $120,000 and $265,000, so a typical predic‐\\ntion error of $68,628 is not very satisfying. This is an example of a model underfitting\\nthe training data. When this happens it can mean that the features do not provide\\nenough information to make good predictions, or that the model is not powerful\\nenough. As we saw in the previous chapter, the main ways to fix underfitting are to\\nselect a more powerful model, to feed the training algorithm with better features, or\\nto reduce the constraints on the model. This model is not regularized, so this rules\\nout the last option. You could try to add more features (e.g., the log of the popula‐\\ntion), but first let’s try a more complex model to see how it does.\\nLet’s train a DecisionTreeRegressor. This is a powerful model, capable of finding\\ncomplex nonlinear relationships in the data (Decision Trees are presented in more\\ndetail in Chapter 6). The code should look familiar by now:\\nfrom sklearn.tree import DecisionTreeRegressor\\ntree_reg = DecisionTreeRegressor()\\ntree_reg.fit(housing_prepared, housing_labels)\\nNow that the model is trained, let’s evaluate it on the training set:\\n>>> housing_predictions = tree_reg.predict(housing_prepared)\\n>>> tree_mse = mean_squared_error(housing_labels, housing_predictions)\\n>>> tree_rmse = np.sqrt(tree_mse)\\n>>> tree_rmse\\n0.0\\nWait, what!? No error at all? Could this model really be absolutely perfect? Of course,\\nit is much more likely that the model has badly overfit the data. How can you be sure?\\nAs we saw earlier, you don’t want to touch the test set until you are ready to launch a\\nmodel you are confident about, so you need to use part of the training set for train‐\\ning, and part for model validation.\\nBetter Evaluation Using Cross-Validation\\nOne way to evaluate the Decision Tree model would be to use the train_test_split\\nfunction to split the training set into a smaller training set and a validation set, then\\n76 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 102}, page_content='train your models against the smaller training set and evaluate them against the vali‐\\ndation set. It’s a bit of work, but nothing too difficult and it would work fairly well.\\nA great alternative is to use Scikit-Learn’s K-fold cross-validation feature. The follow‐\\ning code randomly splits the training set into 10 distinct subsets called folds, then it\\ntrains and evaluates the Decision Tree model 10 times, picking a different fold for\\nevaluation every time and training on the other 9 folds. The result is an array con‐\\ntaining the 10 evaluation scores:\\nfrom sklearn.model_selection import cross_val_score\\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\\n                         scoring=\"neg_mean_squared_error\", cv=10)\\ntree_rmse_scores = np.sqrt(-scores)\\nScikit-Learn’s cross-validation features expect a utility function\\n(greater is better) rather than a cost function (lower is better), so\\nthe scoring function is actually the opposite of the MSE (i.e., a neg‐\\native value), which is why the preceding code computes -scores\\nbefore calculating the square root.\\nLet’s look at the results:\\n>>> def display_scores(scores):\\n...     print(\"Scores:\", scores)\\n...     print(\"Mean:\", scores.mean())\\n...     print(\"Standard deviation:\", scores.std())\\n...\\n>>> display_scores(tree_rmse_scores)\\nScores: [70194.33680785 66855.16363941 72432.58244769 70758.73896782\\n 71115.88230639 75585.14172901 70262.86139133 70273.6325285\\n 75366.87952553 71231.65726027]\\nMean: 71407.68766037929\\nStandard deviation: 2439.4345041191004\\nNow the Decision Tree doesn’t look as good as it did earlier. In fact, it seems to per‐\\nform worse than the Linear Regression model! Notice that cross-validation allows\\nyou to get not only an estimate of the performance of your model, but also a measure\\nof how precise this estimate is (i.e., its standard deviation). The Decision Tree has a\\nscore of approximately 71,407, generally ±2,439. You would not have this information\\nif you just used one validation set. But cross-validation comes at the cost of training\\nthe model several times, so it is not always possible.\\nLet’s compute the same scores for the Linear Regression model just to be sure:\\n>>> lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\\n...                              scoring=\"neg_mean_squared_error\", cv=10)\\n...\\n>>> lin_rmse_scores = np.sqrt(-lin_scores)\\n>>> display_scores(lin_rmse_scores)\\nSelect and Train a Model \\n| \\n77'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 103}, page_content='Scores: [66782.73843989 66960.118071   70347.95244419 74739.57052552\\n 68031.13388938 71193.84183426 64969.63056405 68281.61137997\\n 71552.91566558 67665.10082067]\\nMean: 69052.46136345083\\nStandard deviation: 2731.674001798348\\nThat’s right: the Decision Tree model is overfitting so badly that it performs worse\\nthan the Linear Regression model.\\nLet’s try one last model now: the RandomForestRegressor. As we will see in Chap‐\\nter 7, Random Forests work by training many Decision Trees on random subsets of\\nthe features, then averaging out their predictions. Building a model on top of many\\nother models is called Ensemble Learning, and it is often a great way to push ML algo‐\\nrithms even further. We will skip most of the code since it is essentially the same as\\nfor the other models:\\n>>> from sklearn.ensemble import RandomForestRegressor\\n>>> forest_reg = RandomForestRegressor()\\n>>> forest_reg.fit(housing_prepared, housing_labels)\\n>>> [...]\\n>>> forest_rmse\\n18603.515021376355\\n>>> display_scores(forest_rmse_scores)\\nScores: [49519.80364233 47461.9115823  50029.02762854 52325.28068953\\n 49308.39426421 53446.37892622 48634.8036574  47585.73832311\\n 53490.10699751 50021.5852922 ]\\nMean: 50182.303100336096\\nStandard deviation: 2097.0810550985693\\nWow, this is much better: Random Forests look very promising. However, note that\\nthe score on the training set is still much lower than on the validation sets, meaning\\nthat the model is still overfitting the training set. Possible solutions for overfitting are\\nto simplify the model, constrain it (i.e., regularize it), or get a lot more training data.\\nHowever, before you dive much deeper in Random Forests, you should try out many\\nother models from various categories of Machine Learning algorithms (several Sup‐\\nport Vector Machines with different kernels, possibly a neural network, etc.), without\\nspending too much time tweaking the hyperparameters. The goal is to shortlist a few\\n(two to five) promising models.\\n78 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 104}, page_content='You should save every model you experiment with, so you can\\ncome back easily to any model you want. Make sure you save both\\nthe hyperparameters and the trained parameters, as well as the\\ncross-validation scores and perhaps the actual predictions as well.\\nThis will allow you to easily compare scores across model types,\\nand compare the types of errors they make. You can easily save\\nScikit-Learn models by using Python’s pickle module, or using\\nsklearn.externals.joblib, which is more efficient at serializing \\nlarge NumPy arrays:\\nfrom sklearn.externals import joblib\\njoblib.dump(my_model, \"my_model.pkl\")\\n# and later...\\nmy_model_loaded = joblib.load(\"my_model.pkl\")\\nFine-Tune Your Model\\nLet’s assume that you now have a shortlist of promising models. You now need to\\nfine-tune them. Let’s look at a few ways you can do that.\\nGrid Search\\nOne way to do that would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. This would be very tedious work,\\nand you may not have time to explore many combinations.\\nInstead you should get Scikit-Learn’s GridSearchCV to search for you. All you need to\\ndo is tell it which hyperparameters you want it to experiment with, and what values to\\ntry out, and it will evaluate all the possible combinations of hyperparameter values,\\nusing cross-validation. For example, the following code searches for the best combi‐\\nnation of hyperparameter values for the RandomForestRegressor:\\nfrom sklearn.model_selection import GridSearchCV\\nparam_grid = [\\n    {\\'n_estimators\\': [3, 10, 30], \\'max_features\\': [2, 4, 6, 8]},\\n    {\\'bootstrap\\': [False], \\'n_estimators\\': [3, 10], \\'max_features\\': [2, 3, 4]},\\n  ]\\nforest_reg = RandomForestRegressor()\\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\\n                           scoring=\\'neg_mean_squared_error\\',\\n                           return_train_score=True)\\ngrid_search.fit(housing_prepared, housing_labels)\\nFine-Tune Your Model \\n| \\n79'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 105}, page_content='When you have no idea what value a hyperparameter should have,\\na simple approach is to try out consecutive powers of 10 (or a\\nsmaller number if you want a more fine-grained search, as shown\\nin this example with the n_estimators hyperparameter).\\nThis param_grid tells Scikit-Learn to first evaluate all 3 × 4 = 12 combinations of\\nn_estimators and max_features hyperparameter values specified in the first dict\\n(don’t worry about what these hyperparameters mean for now; they will be explained\\nin Chapter 7), then try all 2 × 3 = 6 combinations of hyperparameter values in the\\nsecond dict, but this time with the bootstrap hyperparameter set to False instead of\\nTrue (which is the default value for this hyperparameter).\\nAll in all, the grid search will explore 12 + 6 = 18 combinations of RandomForestRe\\ngressor hyperparameter values, and it will train each model five times (since we are\\nusing five-fold cross validation). In other words, all in all, there will be 18 × 5 = 90\\nrounds of training! It may take quite a long time, but when it is done you can get the\\nbest combination of parameters like this:\\n>>> grid_search.best_params_\\n{\\'max_features\\': 8, \\'n_estimators\\': 30}\\nSince 8 and 30 are the maximum values that were evaluated, you\\nshould probably try searching again with higher values, since the\\nscore may continue to improve.\\nYou can also get the best estimator directly:\\n>>> grid_search.best_estimator_\\nRandomForestRegressor(bootstrap=True, criterion=\\'mse\\', max_depth=None,\\n           max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0,\\n           min_impurity_split=None, min_samples_leaf=1,\\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\\n           n_estimators=30, n_jobs=None, oob_score=False, random_state=None,\\n           verbose=0, warm_start=False)\\nIf GridSearchCV is initialized with refit=True (which is the\\ndefault), then once it finds the best estimator using cross-\\nvalidation, it retrains it on the whole training set. This is usually a\\ngood idea since feeding it more data will likely improve its perfor‐\\nmance.\\nAnd of course the evaluation scores are also available:\\n>>> cvres = grid_search.cv_results_\\n>>> for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\\n80 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 106}, page_content=\"...     print(np.sqrt(-mean_score), params)\\n...\\n63669.05791727153 {'max_features': 2, 'n_estimators': 3}\\n55627.16171305252 {'max_features': 2, 'n_estimators': 10}\\n53384.57867637289 {'max_features': 2, 'n_estimators': 30}\\n60965.99185930139 {'max_features': 4, 'n_estimators': 3}\\n52740.98248528835 {'max_features': 4, 'n_estimators': 10}\\n50377.344409590376 {'max_features': 4, 'n_estimators': 30}\\n58663.84733372485 {'max_features': 6, 'n_estimators': 3}\\n52006.15355973719 {'max_features': 6, 'n_estimators': 10}\\n50146.465964159885 {'max_features': 6, 'n_estimators': 30}\\n57869.25504027614 {'max_features': 8, 'n_estimators': 3}\\n51711.09443660957 {'max_features': 8, 'n_estimators': 10}\\n49682.25345942335 {'max_features': 8, 'n_estimators': 30}\\n62895.088889905004 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\\n54658.14484390074 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\\n59470.399594730654 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\\n52725.01091081235 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\\n57490.612956065226 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\\n51009.51445842374 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\\nIn this example, we obtain the best solution by setting the max_features hyperpara‐\\nmeter to 8, and the n_estimators hyperparameter to 30. The RMSE score for this\\ncombination is 49,682, which is slightly better than the score you got earlier using the\\ndefault hyperparameter values (which was 50,182). Congratulations, you have suc‐\\ncessfully fine-tuned your best model!\\nDon’t forget that you can treat some of the data preparation steps as\\nhyperparameters. For example, the grid search will automatically\\nfind out whether or not to add a feature you were not sure about\\n(e.g., using the add_bedrooms_per_room hyperparameter of your\\nCombinedAttributesAdder transformer). It may similarly be used\\nto automatically find the best way to handle outliers, missing fea‐\\ntures, feature selection, and more.\\nRandomized Search\\nThe grid search approach is fine when you are exploring relatively few combinations,\\nlike in the previous example, but when the hyperparameter search space is large, it is\\noften preferable to use RandomizedSearchCV instead. This class can be used in much\\nthe same way as the GridSearchCV class, but instead of trying out all possible combi‐\\nnations, it evaluates a given number of random combinations by selecting a random\\nvalue for each hyperparameter at every iteration. This approach has two main bene‐\\nfits:\\nFine-Tune Your Model \\n| \\n81\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 107}, page_content='• If you let the randomized search run for, say, 1,000 iterations, this approach will\\nexplore 1,000 different values for each hyperparameter (instead of just a few val‐\\nues per hyperparameter with the grid search approach).\\n• You have more control over the computing budget you want to allocate to hyper‐\\nparameter search, simply by setting the number of iterations.\\nEnsemble Methods\\nAnother way to fine-tune your system is to try to combine the models that perform\\nbest. The group (or “ensemble”) will often perform better than the best individual\\nmodel (just like Random Forests perform better than the individual Decision Trees\\nthey rely on), especially if the individual models make very different types of errors.\\nWe will cover this topic in more detail in Chapter 7.\\nAnalyze the Best Models and Their Errors\\nYou will often gain good insights on the problem by inspecting the best models. For\\nexample, the RandomForestRegressor can indicate the relative importance of each\\nattribute for making accurate predictions:\\n>>> feature_importances = grid_search.best_estimator_.feature_importances_\\n>>> feature_importances\\narray([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\\n       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\\nLet’s display these importance scores next to their corresponding attribute names:\\n>>> extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\\n>>> cat_encoder = full_pipeline.named_transformers_[\"cat\"]\\n>>> cat_one_hot_attribs = list(cat_encoder.categories_[0])\\n>>> attributes = num_attribs + extra_attribs + cat_one_hot_attribs\\n>>> sorted(zip(feature_importances, attributes), reverse=True)\\n[(0.3661589806181342, \\'median_income\\'),\\n (0.1647809935615905, \\'INLAND\\'),\\n (0.10879295677551573, \\'pop_per_hhold\\'),\\n (0.07334423551601242, \\'longitude\\'),\\n (0.0629090704826203, \\'latitude\\'),\\n (0.05641917918195401, \\'rooms_per_hhold\\'),\\n (0.05335107734767581, \\'bedrooms_per_room\\'),\\n (0.041143798478729635, \\'housing_median_age\\'),\\n (0.014874280890402767, \\'population\\'),\\n (0.014672685420543237, \\'total_rooms\\'),\\n (0.014257599323407807, \\'households\\'),\\n (0.014106483453584102, \\'total_bedrooms\\'),\\n (0.010311488326303787, \\'<1H OCEAN\\'),\\n (0.002856474637320158, \\'NEAR OCEAN\\'),\\n82 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 108}, page_content='(0.00196041559947807, \\'NEAR BAY\\'),\\n (6.028038672736599e-05, \\'ISLAND\\')]\\nWith this information, you may want to try dropping some of the less useful features\\n(e.g., apparently only one ocean_proximity category is really useful, so you could try\\ndropping the others).\\nYou should also look at the specific errors that your system makes, then try to under‐\\nstand why it makes them and what could fix the problem (adding extra features or, on\\nthe contrary, getting rid of uninformative ones, cleaning up outliers, etc.).\\nEvaluate Your System on the Test Set\\nAfter tweaking your models for a while, you eventually have a system that performs\\nsufficiently well. Now is the time to evaluate the final model on the test set. There is\\nnothing special about this process; just get the predictors and the labels from your\\ntest set, run your full_pipeline to transform the data (call transform(), not\\nfit_transform(), you do not want to fit the test set!), and evaluate the final model\\non the test set:\\nfinal_model = grid_search.best_estimator_\\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\\ny_test = strat_test_set[\"median_house_value\"].copy()\\nX_test_prepared = full_pipeline.transform(X_test)\\nfinal_predictions = final_model.predict(X_test_prepared)\\nfinal_mse = mean_squared_error(y_test, final_predictions)\\nfinal_rmse = np.sqrt(final_mse)   # => evaluates to 47,730.2\\nIn some cases, such a point estimate of the generalization error will not be quite\\nenough to convince you to launch: what if it is just 0.1% better than the model cur‐\\nrently in production? You might want to have an idea of how precise this estimate is.\\nFor this, you can compute a 95% confidence interval for the generalization error using\\nscipy.stats.t.interval():\\n>>> from scipy import stats\\n>>> confidence = 0.95\\n>>> squared_errors = (final_predictions - y_test) ** 2\\n>>> np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\\n...                          loc=squared_errors.mean(),\\n...                          scale=stats.sem(squared_errors)))\\n...\\narray([45685.10470776, 49691.25001878])\\nThe performance will usually be slightly worse than what you measured using cross-\\nvalidation if you did a lot of hyperparameter tuning (because your system ends up\\nfine-tuned to perform well on the validation data, and will likely not perform as well\\nFine-Tune Your Model \\n| \\n83'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 109}, page_content='on unknown datasets). It is not the case in this example, but when this happens you\\nmust resist the temptation to tweak the hyperparameters to make the numbers look\\ngood on the test set; the improvements would be unlikely to generalize to new data.\\nNow comes the project prelaunch phase: you need to present your solution (high‐\\nlighting what you have learned, what worked and what did not, what assumptions\\nwere made, and what your system’s limitations are), document everything, and create\\nnice presentations with clear visualizations and easy-to-remember statements (e.g.,\\n“the median income is the number one predictor of housing prices”). In this Califor‐\\nnia housing example, the final performance of the system is not better than the\\nexperts’, but it may still be a good idea to launch it, especially if this frees up some\\ntime for the experts so they can work on more interesting and productive tasks.\\nLaunch, Monitor, and Maintain Your System\\nPerfect, you got approval to launch! You need to get your solution ready for produc‐\\ntion, in particular by plugging the production input data sources into your system\\nand writing tests.\\nYou also need to write monitoring code to check your system’s live performance at\\nregular intervals and trigger alerts when it drops. This is important to catch not only\\nsudden breakage, but also performance degradation. This is quite common because\\nmodels tend to “rot” as data evolves over time, unless the models are regularly trained\\non fresh data.\\nEvaluating your system’s performance will require sampling the system’s predictions\\nand evaluating them. This will generally require a human analysis. These analysts\\nmay be field experts, or workers on a crowdsourcing platform (such as Amazon\\nMechanical Turk or CrowdFlower). Either way, you need to plug the human evalua‐\\ntion pipeline into your system.\\nYou should also make sure you evaluate the system’s input data quality. Sometimes\\nperformance will degrade slightly because of a poor quality signal (e.g., a malfunc‐\\ntioning sensor sending random values, or another team’s output becoming stale), but\\nit may take a while before your system’s performance degrades enough to trigger an\\nalert. If you monitor your system’s inputs, you may catch this earlier. Monitoring the\\ninputs is particularly important for online learning systems.\\nFinally, you will generally want to train your models on a regular basis using fresh\\ndata. You should automate this process as much as possible. If you don’t, you are very\\nlikely to refresh your model only every six months (at best), and your system’s perfor‐\\nmance may fluctuate severely over time. If your system is an online learning system,\\nyou should make sure you save snapshots of its state at regular intervals so you can\\neasily roll back to a previously working state.\\n84 \\n| \\nChapter 2: End-to-End Machine Learning Project'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 110}, page_content='Try It Out!\\nHopefully this chapter gave you a good idea of what a Machine Learning project\\nlooks like, and showed you some of the tools you can use to train a great system. As\\nyou can see, much of the work is in the data preparation step, building monitoring\\ntools, setting up human evaluation pipelines, and automating regular model training.\\nThe Machine Learning algorithms are also important, of course, but it is probably\\npreferable to be comfortable with the overall process and know three or four algo‐\\nrithms well rather than to spend all your time exploring advanced algorithms and not\\nenough time on the overall process.\\nSo, if you have not already done so, now is a good time to pick up a laptop, select a\\ndataset that you are interested in, and try to go through the whole process from A to\\nZ. A good place to start is on a competition website such as http://kaggle.com/: you\\nwill have a dataset to play with, a clear goal, and people to share the experience with.\\nExercises\\nUsing this chapter’s housing dataset:\\n1. Try a Support Vector Machine regressor (sklearn.svm.SVR), with various hyper‐\\nparameters such as kernel=\"linear\" (with various values for the C hyperpara‐\\nmeter) or kernel=\"rbf\" (with various values for the C and gamma\\nhyperparameters). Don’t worry about what these hyperparameters mean for now.\\nHow does the best SVR predictor perform?\\n2. Try replacing GridSearchCV with RandomizedSearchCV.\\n3. Try adding a transformer in the preparation pipeline to select only the most\\nimportant attributes.\\n4. Try creating a single pipeline that does the full data preparation plus the final\\nprediction.\\n5. Automatically explore some preparation options using GridSearchCV.\\nSolutions to these exercises are available in the online Jupyter notebooks at https://\\ngithub.com/ageron/handson-ml2.\\nTry It Out! \\n| \\n85'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 111}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 112}, page_content='1 By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data.\\nCHAPTER 3\\nClassification\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 3 in the final\\nrelease of the book.\\nIn Chapter 1 we mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). In Chapter 2 we\\nexplored a regression task, predicting housing values, using various algorithms such\\nas Linear Regression, Decision Trees, and Random Forests (which will be explained\\nin further detail in later chapters). Now we will turn our attention to classification\\nsystems.\\nMNIST\\nIn this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\\nimages of digits handwritten by high school students and employees of the US Cen‐\\nsus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\\nied so much that it is often called the “Hello World” of Machine Learning: whenever\\npeople come up with a new classification algorithm, they are curious to see how it\\nwill perform on MNIST. Whenever someone learns Machine Learning, sooner or\\nlater they tackle MNIST.\\nScikit-Learn provides many helper functions to download popular datasets. MNIST is\\none of them. The following code fetches the MNIST dataset:1\\n87'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 113}, page_content='>>> from sklearn.datasets import fetch_openml\\n>>> mnist = fetch_openml(\\'mnist_784\\', version=1)\\n>>> mnist.keys()\\ndict_keys([\\'data\\', \\'target\\', \\'feature_names\\', \\'DESCR\\', \\'details\\',\\n           \\'categories\\', \\'url\\'])\\nDatasets loaded by Scikit-Learn generally have a similar dictionary structure includ‐\\ning:\\n• A DESCR key describing the dataset\\n• A data key containing an array with one row per instance and one column per\\nfeature\\n• A target key containing an array with the labels\\nLet’s look at these arrays:\\n>>> X, y = mnist[\"data\"], mnist[\"target\"]\\n>>> X.shape\\n(70000, 784)\\n>>> y.shape\\n(70000,)\\nThere are 70,000 images, and each image has 784 features. This is because each image\\nis 28×28 pixels, and each feature simply represents one pixel’s intensity, from 0\\n(white) to 255 (black). Let’s take a peek at one digit from the dataset. All you need to\\ndo is grab an instance’s feature vector, reshape it to a 28×28 array, and display it using\\nMatplotlib’s imshow() function:\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nsome_digit = X[0]\\nsome_digit_image = some_digit.reshape(28, 28)\\nplt.imshow(some_digit_image, cmap = mpl.cm.binary, interpolation=\"nearest\")\\nplt.axis(\"off\")\\nplt.show()\\nThis looks like a 5, and indeed that’s what the label tells us:\\n88 \\n| \\nChapter 3: Classification'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 114}, page_content=\">>> y[0]\\n'5'\\nNote that the label is a string. We prefer numbers, so let’s cast y to integers:\\n>>> y = y.astype(np.uint8)\\nFigure 3-1 shows a few more images from the MNIST dataset to give you a feel for\\nthe complexity of the classification task.\\nFigure 3-1. A few digits from the MNIST dataset\\nBut wait! You should always create a test set and set it aside before inspecting the data\\nclosely. The MNIST dataset is actually already split into a training set (the first 60,000\\nimages) and a test set (the last 10,000 images):\\nX_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\\nThe training set is already shuffled for us, which is good as this guarantees that all\\ncross-validation folds will be similar (you don’t want one fold to be missing some dig‐\\nits). Moreover, some learning algorithms are sensitive to the order of the training\\nMNIST \\n| \\n89\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 115}, page_content='2 Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\\nstock market prices or weather conditions). We will explore this in the next chapters.\\ninstances, and they perform poorly if they get many similar instances in a row. Shuf‐\\nfling the dataset ensures that this won’t happen.2\\nTraining a Binary Classifier\\nLet’s simplify the problem for now and only try to identify one digit—for example,\\nthe number 5. This “5-detector” will be an example of a binary classifier, capable of\\ndistinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\\nthis classification task:\\ny_train_5 = (y_train == 5)  # True for all 5s, False for all other digits.\\ny_test_5 = (y_test == 5)\\nOkay, now let’s pick a classifier and train it. A good place to start is with a Stochastic\\nGradient Descent (SGD) classifier, using Scikit-Learn’s SGDClassifier class. This clas‐\\nsifier has the advantage of being capable of handling very large datasets efficiently.\\nThis is in part because SGD deals with training instances independently, one at a time\\n(which also makes SGD well suited for online learning), as we will see later. Let’s create\\nan SGDClassifier and train it on the whole training set:\\nfrom sklearn.linear_model import SGDClassifier\\nsgd_clf = SGDClassifier(random_state=42)\\nsgd_clf.fit(X_train, y_train_5)\\nThe SGDClassifier relies on randomness during training (hence\\nthe name “stochastic”). If you want reproducible results, you\\nshould set the random_state parameter.\\nNow you can use it to detect images of the number 5:\\n>>> sgd_clf.predict([some_digit])\\narray([ True])\\nThe classifier guesses that this image represents a 5 (True). Looks like it guessed right\\nin this particular case! Now, let’s evaluate this model’s performance.\\nPerformance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a regressor, so we\\nwill spend a large part of this chapter on this topic. There are many performance\\n90 \\n| \\nChapter 3: Classification'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 116}, page_content='measures available, so grab another coffee and get ready to learn many new concepts\\nand acronyms!\\nMeasuring Accuracy Using Cross-Validation\\nA good way to evaluate a model is to use cross-validation, just as you did in Chap‐\\nter 2.\\nImplementing Cross-Validation\\nOccasionally you will need more control over the cross-validation process than what\\nScikit-Learn provides off-the-shelf. In these cases, you can implement cross-\\nvalidation yourself; it is actually fairly straightforward. The following code does\\nroughly the same thing as Scikit-Learn’s cross_val_score() function, and prints the \\nsame result:\\nfrom sklearn.model_selection import StratifiedKFold\\nfrom sklearn.base import clone\\nskfolds = StratifiedKFold(n_splits=3, random_state=42)\\nfor train_index, test_index in skfolds.split(X_train, y_train_5):\\n    clone_clf = clone(sgd_clf)\\n    X_train_folds = X_train[train_index]\\n    y_train_folds = y_train_5[train_index]\\n    X_test_fold = X_train[test_index]\\n    y_test_fold = y_train_5[test_index]\\n    clone_clf.fit(X_train_folds, y_train_folds)\\n    y_pred = clone_clf.predict(X_test_fold)\\n    n_correct = sum(y_pred == y_test_fold)\\n    print(n_correct / len(y_pred))  # prints 0.9502, 0.96565 and 0.96495\\nThe StratifiedKFold class performs stratified sampling (as explained in Chapter 2)\\nto produce folds that contain a representative ratio of each class. At each iteration the\\ncode creates a clone of the classifier, trains that clone on the training folds, and makes\\npredictions on the test fold. Then it counts the number of correct predictions and\\noutputs the ratio of correct predictions.\\nLet’s use the cross_val_score() function to evaluate your SGDClassifier model\\nusing K-fold cross-validation, with three folds. Remember that K-fold cross-\\nvalidation means splitting the training set into K-folds (in this case, three), then mak‐\\ning predictions and evaluating them on each fold using a model trained on the\\nremaining folds (see Chapter 2):\\nPerformance Measures \\n| \\n91'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 117}, page_content='>>> from sklearn.model_selection import cross_val_score\\n>>> cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\\narray([0.96355, 0.93795, 0.95615])\\nWow! Above 93% accuracy (ratio of correct predictions) on all cross-validation folds? \\nThis looks amazing, doesn’t it? Well, before you get too excited, let’s look at a very\\ndumb classifier that just classifies every single image in the “not-5” class:\\nfrom sklearn.base import BaseEstimator\\nclass Never5Classifier(BaseEstimator):\\n    def fit(self, X, y=None):\\n        pass\\n    def predict(self, X):\\n        return np.zeros((len(X), 1), dtype=bool)\\nCan you guess this model’s accuracy? Let’s find out:\\n>>> never_5_clf = Never5Classifier()\\n>>> cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\\narray([0.91125, 0.90855, 0.90915])\\nThat’s right, it has over 90% accuracy! This is simply because only about 10% of the\\nimages are 5s, so if you always guess that an image is not a 5, you will be right about\\n90% of the time. Beats Nostradamus.\\nThis demonstrates why accuracy is generally not the preferred performance measure\\nfor classifiers, especially when you are dealing with skewed datasets (i.e., when some\\nclasses are much more frequent than others).\\nConfusion Matrix\\nA much better way to evaluate the performance of a classifier is to look at the confu‐\\nsion matrix. The general idea is to count the number of times instances of class A are\\nclassified as class B. For example, to know the number of times the classifier confused\\nimages of 5s with 3s, you would look in the 5th row and 3rd column of the confusion\\nmatrix.\\nTo compute the confusion matrix, you first need to have a set of predictions, so they\\ncan be compared to the actual targets. You could make predictions on the test set, but\\nlet’s keep it untouched for now (remember that you want to use the test set only at the\\nvery end of your project, once you have a classifier that you are ready to launch).\\nInstead, you can use the cross_val_predict() function:\\nfrom sklearn.model_selection import cross_val_predict\\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\\nJust like the cross_val_score() function, cross_val_predict() performs K-fold\\ncross-validation, but instead of returning the evaluation scores, it returns the predic‐\\n92 \\n| \\nChapter 3: Classification'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 118}, page_content='tions made on each test fold. This means that you get a clean prediction for each\\ninstance in the training set (“clean” meaning that the prediction is made by a model\\nthat never saw the data during training).\\nNow you are ready to get the confusion matrix using the confusion_matrix() func‐\\ntion. Just pass it the target classes (y_train_5) and the predicted classes\\n(y_train_pred):\\n>>> from sklearn.metrics import confusion_matrix\\n>>> confusion_matrix(y_train_5, y_train_pred)\\narray([[53057,  1522],\\n       [ 1325,  4096]])\\nEach row in a confusion matrix represents an actual class, while each column repre‐\\nsents a predicted class. The first row of this matrix considers non-5 images (the nega‐\\ntive class): 53,057 of them were correctly classified as non-5s (they are called true\\nnegatives), while the remaining 1,522 were wrongly classified as 5s (false positives).\\nThe second row considers the images of 5s (the positive class): 1,325 were wrongly\\nclassified as non-5s (false negatives), while the remaining 4,096 were correctly classi‐\\nfied as 5s (true positives). A perfect classifier would have only true positives and true\\nnegatives, so its confusion matrix would have nonzero values only on its main diago‐\\nnal (top left to bottom right):\\n>>> y_train_perfect_predictions = y_train_5  # pretend we reached perfection\\n>>> confusion_matrix(y_train_5, y_train_perfect_predictions)\\narray([[54579,     0],\\n       [    0,  5421]])\\nThe confusion matrix gives you a lot of information, but sometimes you may prefer a\\nmore concise metric. An interesting one to look at is the accuracy of the positive pre‐\\ndictions; this is called the precision of the classifier (Equation 3-1).\\nEquation 3-1. Precision\\nprecision =\\nTP\\nTP + FP\\nTP is the number of true positives, and FP is the number of false positives.\\nA trivial way to have perfect precision is to make one single positive prediction and\\nensure it is correct (precision = 1/1 = 100%). This would not be very useful since the\\nclassifier would ignore all but one positive instance. So precision is typically used\\nalong with another metric named recall, also called sensitivity or true positive rate\\nPerformance Measures \\n| \\n93'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 119}, page_content='(TPR): this is the ratio of positive instances that are correctly detected by the classifier\\n(Equation 3-2).\\nEquation 3-2. Recall\\nrecall =\\nTP\\nTP + FN\\nFN is of course the number of false negatives.\\nIf you are confused about the confusion matrix, Figure 3-2 may help.\\nFigure 3-2. An illustrated confusion matrix\\nPrecision and Recall\\nScikit-Learn provides several functions to compute classifier metrics, including preci‐\\nsion and recall:\\n>>> from sklearn.metrics import precision_score, recall_score\\n>>> precision_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1522)\\n0.7290850836596654\\n>>> recall_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1325)\\n0.7555801512636044\\nNow your 5-detector does not look as shiny as it did when you looked at its accuracy.\\nWhen it claims an image represents a 5, it is correct only 72.9% of the time. More‐\\nover, it only detects 75.6% of the 5s.\\nIt is often convenient to combine precision and recall into a single metric called the F1\\nscore, in particular if you need a simple way to compare two classifiers. The F1 score is \\nthe harmonic mean of precision and recall (Equation 3-3). Whereas the regular mean\\n94 \\n| \\nChapter 3: Classification'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 120}, page_content='treats all values equally, the harmonic mean gives much more weight to low values.\\nAs a result, the classifier will only get a high F1 score if both recall and precision are\\nhigh.\\nEquation 3-3. F1\\nF1 =\\n2\\n1\\nprecision +\\n1\\nrecall\\n= 2 × precision × recall\\nprecision + recall =\\nTP\\nTP + FN + FP\\n2\\nTo compute the F1 score, simply call the f1_score() function:\\n>>> from sklearn.metrics import f1_score\\n>>> f1_score(y_train_5, y_train_pred)\\n0.7420962043663375\\nThe F1 score favors classifiers that have similar precision and recall. This is not always\\nwhat you want: in some contexts you mostly care about precision, and in other con‐\\ntexts you really care about recall. For example, if you trained a classifier to detect vid‐\\neos that are safe for kids, you would probably prefer a classifier that rejects many\\ngood videos (low recall) but keeps only safe ones (high precision), rather than a clas‐\\nsifier that has a much higher recall but lets a few really bad videos show up in your\\nproduct (in such cases, you may even want to add a human pipeline to check the clas‐\\nsifier’s video selection). On the other hand, suppose you train a classifier to detect\\nshoplifters on surveillance images: it is probably fine if your classifier has only 30%\\nprecision as long as it has 99% recall (sure, the security guards will get a few false\\nalerts, but almost all shoplifters will get caught).\\nUnfortunately, you can’t have it both ways: increasing precision reduces recall, and\\nvice versa. This is called the precision/recall tradeoff.\\nPrecision/Recall Tradeoff\\nTo understand this tradeoff, let’s look at how the SGDClassifier makes its classifica‐\\ntion decisions. For each instance, it computes a score based on a decision function, \\nand if that score is greater than a threshold, it assigns the instance to the positive\\nclass, or else it assigns it to the negative class. Figure 3-3 shows a few digits positioned\\nfrom the lowest score on the left to the highest score on the right. Suppose the deci‐\\nsion threshold is positioned at the central arrow (between the two 5s): you will find 4\\ntrue positives (actual 5s) on the right of that threshold, and one false positive (actually\\na 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6\\nactual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). Now if you\\nraise the threshold (move it to the arrow on the right), the false positive (the 6)\\nbecomes a true negative, thereby increasing precision (up to 100% in this case), but\\none true positive becomes a false negative, decreasing recall down to 50%. Conversely,\\nlowering the threshold increases recall and reduces precision.\\nPerformance Measures \\n| \\n95'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 121}, page_content='Figure 3-3. Decision threshold and precision/recall tradeoff\\nScikit-Learn does not let you set the threshold directly, but it does give you access to\\nthe decision scores that it uses to make predictions. Instead of calling the classifier’s\\npredict() method, you can call its decision_function() method, which returns a\\nscore for each instance, and then make predictions based on those scores using any\\nthreshold you want:\\n>>> y_scores = sgd_clf.decision_function([some_digit])\\n>>> y_scores\\narray([2412.53175101])\\n>>> threshold = 0\\n>>> y_some_digit_pred = (y_scores > threshold)\\narray([ True])\\nThe SGDClassifier uses a threshold equal to 0, so the previous code returns the same\\nresult as the predict() method (i.e., True). Let’s raise the threshold:\\n>>> threshold = 8000\\n>>> y_some_digit_pred = (y_scores > threshold)\\n>>> y_some_digit_pred\\narray([False])\\nThis confirms that raising the threshold decreases recall. The image actually repre‐\\nsents a 5, and the classifier detects it when the threshold is 0, but it misses it when the\\nthreshold is increased to 8,000.\\nNow how do you decide which threshold to use? For this you will first need to get the\\nscores of all instances in the training set using the cross_val_predict() function\\nagain, but this time specifying that you want it to return decision scores instead of\\npredictions:\\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\\n                             method=\"decision_function\")\\nNow with these scores you can compute precision and recall for all possible thresh‐\\nolds using the precision_recall_curve() function:\\n96 \\n| \\nChapter 3: Classification'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 122}, page_content='from sklearn.metrics import precision_recall_curve\\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\\nFinally, you can plot precision and recall as functions of the threshold value using\\nMatplotlib (Figure 3-4):\\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\\n    [...] # highlight the threshold, add the legend, axis label and grid\\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\\nplt.show()\\nFigure 3-4. Precision and recall versus the decision threshold\\nYou may wonder why the precision curve is bumpier than the recall\\ncurve in Figure 3-4. The reason is that precision may sometimes go\\ndown when you raise the threshold (although in general it will go\\nup). To understand why, look back at Figure 3-3 and notice what\\nhappens when you start from the central threshold and move it just\\none digit to the right: precision goes from 4/5 (80%) down to 3/4\\n(75%). On the other hand, recall can only go down when the thres‐\\nhold is increased, which explains why its curve looks smooth.\\nAnother way to select a good precision/recall tradeoff is to plot precision directly\\nagainst recall, as shown in Figure 3-5 (the same threshold as earlier is highlighed).\\nPerformance Measures \\n| \\n97'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 123}, page_content='Figure 3-5. Precision versus recall\\nYou can see that precision really starts to fall sharply around 80% recall. You will\\nprobably want to select a precision/recall tradeoff just before that drop—for example,\\nat around 60% recall. But of course the choice depends on your project.\\nSo let’s suppose you decide to aim for 90% precision. You look up the first plot and\\nfind that you need to use a threshold of about 8,000. To be more precise you can\\nsearch for the lowest threshold that gives you at least 90% precision (np.argmax()\\nwill give us the first index of the maximum value, which in this case means the first\\nTrue value):\\nthreshold_90_precision = thresholds[np.argmax(precisions >= 0.90)] # ~7816\\nTo make predictions (on the training set for now), instead of calling the classifier’s\\npredict() method, you can just run this code:\\ny_train_pred_90 = (y_scores >= threshold_90_precision)\\nLet’s check these predictions’ precision and recall:\\n>>> precision_score(y_train_5, y_train_pred_90)\\n0.9000380083618396\\n>>> recall_score(y_train_5, y_train_pred_90)\\n0.4368197749492714\\nGreat, you have a 90% precision classifier ! As you can see, it is fairly easy to create a\\nclassifier with virtually any precision you want: just set a high enough threshold, and\\nyou’re done. Hmm, not so fast. A high-precision classifier is not very useful if its \\nrecall is too low!\\n98 \\n| \\nChapter 3: Classification'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 124}, page_content=\"If someone says “let’s reach 99% precision,” you should ask, “at\\nwhat recall?”\\nThe ROC Curve\\nThe receiver operating characteristic (ROC) curve is another common tool used with\\nbinary classifiers. It is very similar to the precision/recall curve, but instead of plot‐\\nting precision versus recall, the ROC curve plots the true positive rate (another name\\nfor recall) against the false positive rate. The FPR is the ratio of negative instances that\\nare incorrectly classified as positive. It is equal to one minus the true negative rate, \\nwhich is the ratio of negative instances that are correctly classified as negative. The\\nTNR is also called specificity. Hence the ROC curve plots sensitivity (recall) versus\\n1 – specificity.\\nTo plot the ROC curve, you first need to compute the TPR and FPR for various thres‐\\nhold values, using the roc_curve() function:\\nfrom sklearn.metrics import roc_curve\\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\\nThen you can plot the FPR against the TPR using Matplotlib. This code produces the\\nplot in Figure 3-6:\\ndef plot_roc_curve(fpr, tpr, label=None):\\n    plt.plot(fpr, tpr, linewidth=2, label=label)\\n    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\\n    [...] # Add axis labels and grid\\nplot_roc_curve(fpr, tpr)\\nplt.show()\\nPerformance Measures \\n| \\n99\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 125}, page_content='Figure 3-6. ROC curve\\nOnce again there is a tradeoff: the higher the recall (TPR), the more false positives\\n(FPR) the classifier produces. The dotted line represents the ROC curve of a purely\\nrandom classifier; a good classifier stays as far away from that line as possible (toward\\nthe top-left corner).\\nOne way to compare classifiers is to measure the area under the curve (AUC). A per‐\\nfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will\\nhave a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC\\nAUC:\\n>>> from sklearn.metrics import roc_auc_score\\n>>> roc_auc_score(y_train_5, y_scores)\\n0.9611778893101814\\nSince the ROC curve is so similar to the precision/recall (or PR)\\ncurve, you may wonder how to decide which one to use. As a rule\\nof thumb, you should prefer the PR curve whenever the positive\\nclass is rare or when you care more about the false positives than\\nthe false negatives, and the ROC curve otherwise. For example,\\nlooking at the previous ROC curve (and the ROC AUC score), you\\nmay think that the classifier is really good. But this is mostly\\nbecause there are few positives (5s) compared to the negatives\\n(non-5s). In contrast, the PR curve makes it clear that the classifier\\nhas room for improvement (the curve could be closer to the top-\\nright corner).\\n100 \\n| \\nChapter 3: Classification'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 126}, page_content='Let’s train a RandomForestClassifier and compare its ROC curve and ROC AUC\\nscore to the SGDClassifier. First, you need to get scores for each instance in the\\ntraining set. But due to the way it works (see Chapter 7), the RandomForestClassi\\nfier class does not have a decision_function() method. Instead it has a pre\\ndict_proba() method. Scikit-Learn classifiers generally have one or the other. The\\npredict_proba() method returns an array containing a row per instance and a col‐\\numn per class, each containing the probability that the given instance belongs to the\\ngiven class (e.g., 70% chance that the image represents a 5):\\nfrom sklearn.ensemble import RandomForestClassifier\\nforest_clf = RandomForestClassifier(random_state=42)\\ny_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\\n                                    method=\"predict_proba\")\\nBut to plot a ROC curve, you need scores, not probabilities. A simple solution is to\\nuse the positive class’s probability as the score:\\ny_scores_forest = y_probas_forest[:, 1]   # score = proba of positive class\\nfpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)\\nNow you are ready to plot the ROC curve. It is useful to plot the first ROC curve as\\nwell to see how they compare (Figure 3-7):\\nplt.plot(fpr, tpr, \"b:\", label=\"SGD\")\\nplot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\\nplt.legend(loc=\"lower right\")\\nplt.show()\\nFigure 3-7. Comparing ROC curves\\nPerformance Measures \\n| \\n101'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 127}, page_content='As you can see in Figure 3-7, the RandomForestClassifier’s ROC curve looks much\\nbetter than the SGDClassifier’s: it comes much closer to the top-left corner. As a\\nresult, its ROC AUC score is also significantly better:\\n>>> roc_auc_score(y_train_5, y_scores_forest)\\n0.9983436731328145\\nTry measuring the precision and recall scores: you should find 99.0% precision and\\n86.6% recall. Not too bad!\\nHopefully you now know how to train binary classifiers, choose the appropriate met‐\\nric for your task, evaluate your classifiers using cross-validation, select the precision/\\nrecall tradeoff that fits your needs, and compare various models using ROC curves\\nand ROC AUC scores. Now let’s try to detect more than just the 5s.\\nMulticlass Classification\\nWhereas binary classifiers distinguish between two classes, multiclass classifiers (also\\ncalled multinomial classifiers) can distinguish between more than two classes.\\nSome algorithms (such as Random Forest classifiers or naive Bayes classifiers) are\\ncapable of handling multiple classes directly. Others (such as Support Vector Machine\\nclassifiers or Linear classifiers) are strictly binary classifiers. However, there are vari‐\\nous strategies that you can use to perform multiclass classification using multiple\\nbinary classifiers.\\nFor example, one way to create a system that can classify the digit images into 10\\nclasses (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a\\n1-detector, a 2-detector, and so on). Then when you want to classify an image, you get\\nthe decision score from each classifier for that image and you select the class whose\\nclassifier outputs the highest score. This is called the one-versus-all (OvA) strategy \\n(also called one-versus-the-rest).\\nAnother strategy is to train a binary classifier for every pair of digits: one to distin‐\\nguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on.\\nThis is called the one-versus-one (OvO) strategy. If there are N classes, you need to\\ntrain N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45\\nbinary classifiers! When you want to classify an image, you have to run the image\\nthrough all 45 classifiers and see which class wins the most duels. The main advan‐\\ntage of OvO is that each classifier only needs to be trained on the part of the training\\nset for the two classes that it must distinguish.\\nSome algorithms (such as Support Vector Machine classifiers) scale poorly with the\\nsize of the training set, so for these algorithms OvO is preferred since it is faster to\\ntrain many classifiers on small training sets than training few classifiers on large\\ntraining sets. For most binary classification algorithms, however, OvA is preferred.\\n102 \\n| \\nChapter 3: Classification'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 128}, page_content='Scikit-Learn detects when you try to use a binary classification algorithm for a multi‐\\nclass classification task, and it automatically runs OvA (except for SVM classifiers for\\nwhich it uses OvO). Let’s try this with the SGDClassifier:\\n>>> sgd_clf.fit(X_train, y_train)  # y_train, not y_train_5\\n>>> sgd_clf.predict([some_digit])\\narray([5], dtype=uint8)\\nThat was easy! This code trains the SGDClassifier on the training set using the origi‐\\nnal target classes from 0 to 9 (y_train), instead of the 5-versus-all target classes\\n(y_train_5). Then it makes a prediction (a correct one in this case). Under the hood,\\nScikit-Learn actually trained 10 binary classifiers, got their decision scores for the\\nimage, and selected the class with the highest score.\\nTo see that this is indeed the case, you can call the decision_function() method.\\nInstead of returning just one score per instance, it now returns 10 scores, one per\\nclass:\\n>>> some_digit_scores = sgd_clf.decision_function([some_digit])\\n>>> some_digit_scores\\narray([[-15955.22627845, -38080.96296175, -13326.66694897,\\n           573.52692379, -17680.6846644 ,   2412.53175101,\\n        -25526.86498156, -12290.15704709,  -7946.05205023,\\n        -10631.35888549]])\\nThe highest score is indeed the one corresponding to class 5:\\n>>> np.argmax(some_digit_scores)\\n5\\n>>> sgd_clf.classes_\\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)\\n>>> sgd_clf.classes_[5]\\n5\\nWhen a classifier is trained, it stores the list of target classes in its\\nclasses_ attribute, ordered by value. In this case, the index of each\\nclass in the classes_ array conveniently matches the class itself\\n(e.g., the class at index 5 happens to be class 5), but in general you\\nwon’t be so lucky.\\nIf you want to force ScikitLearn to use one-versus-one or one-versus-all, you can use\\nthe OneVsOneClassifier or OneVsRestClassifier classes. Simply create an instance\\nand pass a binary classifier to its constructor. For example, this code creates a multi‐\\nclass classifier using the OvO strategy, based on a SGDClassifier:\\n>>> from sklearn.multiclass import OneVsOneClassifier\\n>>> ovo_clf = OneVsOneClassifier(SGDClassifier(random_state=42))\\n>>> ovo_clf.fit(X_train, y_train)\\n>>> ovo_clf.predict([some_digit])\\nMulticlass Classification \\n| \\n103'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 129}, page_content='array([5], dtype=uint8)\\n>>> len(ovo_clf.estimators_)\\n45\\nTraining a RandomForestClassifier is just as easy:\\n>>> forest_clf.fit(X_train, y_train)\\n>>> forest_clf.predict([some_digit])\\narray([5], dtype=uint8)\\nThis time Scikit-Learn did not have to run OvA or OvO because Random Forest\\nclassifiers can directly classify instances into multiple classes. You can call\\npredict_proba() to get the list of probabilities that the classifier assigned to each\\ninstance for each class:\\n>>> forest_clf.predict_proba([some_digit])\\narray([[0.  , 0.  , 0.01, 0.08, 0.  , 0.9 , 0.  , 0.  , 0.  , 0.01]])\\nYou can see that the classifier is fairly confident about its prediction: the 0.9 at the 5th\\nindex in the array means that the model estimates a 90% probability that the image\\nrepresents a 5. It also thinks that the image could instead be a 2, a 3 or a 9, respec‐\\ntively with 1%, 8% and 1% probability.\\nNow of course you want to evaluate these classifiers. As usual, you want to use cross-\\nvalidation. Let’s evaluate the SGDClassifier’s accuracy using the cross_val_score()\\nfunction:\\n>>> cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\\narray([0.8489802 , 0.87129356, 0.86988048])\\nIt gets over 84% on all test folds. If you used a random classifier, you would get 10%\\naccuracy, so this is not such a bad score, but you can still do much better. For exam‐\\nple, simply scaling the inputs (as discussed in Chapter 2) increases accuracy above\\n89%:\\n>>> from sklearn.preprocessing import StandardScaler\\n>>> scaler = StandardScaler()\\n>>> X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\\n>>> cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")\\narray([0.89707059, 0.8960948 , 0.90693604])\\nError Analysis\\nOf course, if this were a real project, you would follow the steps in your Machine\\nLearning project checklist (see ???): exploring data preparation options, trying out\\nmultiple models, shortlisting the best ones and fine-tuning their hyperparameters\\nusing GridSearchCV, and automating as much as possible, as you did in the previous\\nchapter. Here, we will assume that you have found a promising model and you want\\nto find ways to improve it. One way to do this is to analyze the types of errors it\\nmakes.\\n104 \\n| \\nChapter 3: Classification'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 130}, page_content='First, you can look at the confusion matrix. You need to make predictions using the\\ncross_val_predict() function, then call the confusion_matrix() function, just like\\nyou did earlier:\\n>>> y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\\n>>> conf_mx = confusion_matrix(y_train, y_train_pred)\\n>>> conf_mx\\narray([[5578,    0,   22,    7,    8,   45,   35,    5,  222,    1],\\n       [   0, 6410,   35,   26,    4,   44,    4,    8,  198,   13],\\n       [  28,   27, 5232,  100,   74,   27,   68,   37,  354,   11],\\n       [  23,   18,  115, 5254,    2,  209,   26,   38,  373,   73],\\n       [  11,   14,   45,   12, 5219,   11,   33,   26,  299,  172],\\n       [  26,   16,   31,  173,   54, 4484,   76,   14,  482,   65],\\n       [  31,   17,   45,    2,   42,   98, 5556,    3,  123,    1],\\n       [  20,   10,   53,   27,   50,   13,    3, 5696,  173,  220],\\n       [  17,   64,   47,   91,    3,  125,   24,   11, 5421,   48],\\n       [  24,   18,   29,   67,  116,   39,    1,  174,  329, 5152]])\\nThat’s a lot of numbers. It’s often more convenient to look at an image representation\\nof the confusion matrix, using Matplotlib’s matshow() function:\\nplt.matshow(conf_mx, cmap=plt.cm.gray)\\nplt.show()\\nThis confusion matrix looks fairly good, since most images are on the main diagonal,\\nwhich means that they were classified correctly. The 5s look slightly darker than the\\nother digits, which could mean that there are fewer images of 5s in the dataset or that\\nthe classifier does not perform as well on 5s as on other digits. In fact, you can verify\\nthat both are the case.\\nLet’s focus the plot on the errors. First, you need to divide each value in the confusion\\nmatrix by the number of images in the corresponding class, so you can compare error\\nError Analysis \\n| \\n105'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 131}, page_content='rates instead of absolute number of errors (which would make abundant classes look\\nunfairly bad):\\nrow_sums = conf_mx.sum(axis=1, keepdims=True)\\nnorm_conf_mx = conf_mx / row_sums\\nNow let’s fill the diagonal with zeros to keep only the errors, and let’s plot the result:\\nnp.fill_diagonal(norm_conf_mx, 0)\\nplt.matshow(norm_conf_mx, cmap=plt.cm.gray)\\nplt.show()\\nNow you can clearly see the kinds of errors the classifier makes. Remember that rows\\nrepresent actual classes, while columns represent predicted classes. The column for\\nclass 8 is quite bright, which tells you that many images get misclassified as 8s. How‐\\never, the row for class 8 is not that bad, telling you that actual 8s in general get prop‐\\nerly classified as 8s. As you can see, the confusion matrix is not necessarily\\nsymmetrical. You can also see that 3s and 5s often get confused (in both directions).\\nAnalyzing the confusion matrix can often give you insights on ways to improve your\\nclassifier. Looking at this plot, it seems that your efforts should be spent on reducing\\nthe false 8s. For example, you could try to gather more training data for digits that\\nlook like 8s (but are not) so the classifier can learn to distinguish them from real 8s.\\nOr you could engineer new features that would help the classifier—for example, writ‐\\ning an algorithm to count the number of closed loops (e.g., 8 has two, 6 has one, 5 has\\nnone). Or you could preprocess the images (e.g., using Scikit-Image, Pillow, or\\nOpenCV) to make some patterns stand out more, such as closed loops.\\nAnalyzing individual errors can also be a good way to gain insights on what your\\nclassifier is doing and why it is failing, but it is more difficult and time-consuming.\\n106 \\n| \\nChapter 3: Classification'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 132}, page_content='3 But remember that our brain is a fantastic pattern recognition system, and our visual system does a lot of\\ncomplex preprocessing before any information reaches our consciousness, so the fact that it feels simple does\\nnot mean that it is.\\nFor example, let’s plot examples of 3s and 5s (the plot_digits() function just uses\\nMatplotlib’s imshow() function; see this chapter’s Jupyter notebook for details):\\ncl_a, cl_b = 3, 5\\nX_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\\nX_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\\nX_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\\nX_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\\nplt.figure(figsize=(8,8))\\nplt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\\nplt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\\nplt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\\nplt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)\\nplt.show()\\nThe two 5×5 blocks on the left show digits classified as 3s, and the two 5×5 blocks on\\nthe right show images classified as 5s. Some of the digits that the classifier gets wrong\\n(i.e., in the bottom-left and top-right blocks) are so badly written that even a human\\nwould have trouble classifying them (e.g., the 5 on the 1st row and 2nd column truly\\nlooks like a badly written 3). However, most misclassified images seem like obvious\\nerrors to us, and it’s hard to understand why the classifier made the mistakes it did.3\\nThe reason is that we used a simple SGDClassifier, which is a linear model. All it\\ndoes is assign a weight per class to each pixel, and when it sees a new image it just\\nsums up the weighted pixel intensities to get a score for each class. So since 3s and 5s\\ndiffer only by a few pixels, this model will easily confuse them.\\nError Analysis \\n| \\n107'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 133}, page_content='The main difference between 3s and 5s is the position of the small line that joins the\\ntop line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left,\\nthe classifier might classify it as a 5, and vice versa. In other words, this classifier is\\nquite sensitive to image shifting and rotation. So one way to reduce the 3/5 confusion\\nwould be to preprocess the images to ensure that they are well centered and not too\\nrotated. This will probably help reduce other errors as well.\\nMultilabel Classification\\nUntil now each instance has always been assigned to just one class. In some cases you\\nmay want your classifier to output multiple classes for each instance. For example,\\nconsider a face-recognition classifier: what should it do if it recognizes several people\\non the same picture? Of course it should attach one tag per person it recognizes. Say\\nthe classifier has been trained to recognize three faces, Alice, Bob, and Charlie; then\\nwhen it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning\\n“Alice yes, Bob no, Charlie yes”). Such a classification system that outputs multiple\\nbinary tags is called a multilabel classification system.\\nWe won’t go into face recognition just yet, but let’s look at a simpler example, just for\\nillustration purposes:\\nfrom sklearn.neighbors import KNeighborsClassifier\\ny_train_large = (y_train >= 7)\\ny_train_odd = (y_train % 2 == 1)\\ny_multilabel = np.c_[y_train_large, y_train_odd]\\nknn_clf = KNeighborsClassifier()\\nknn_clf.fit(X_train, y_multilabel)\\nThis code creates a y_multilabel array containing two target labels for each digit\\nimage: the first indicates whether or not the digit is large (7, 8, or 9) and the second\\nindicates whether or not it is odd. The next lines create a KNeighborsClassifier \\ninstance (which supports multilabel classification, but not all classifiers do) and we\\ntrain it using the multiple targets array. Now you can make a prediction, and notice\\nthat it outputs two labels:\\n>>> knn_clf.predict([some_digit])\\narray([[False,  True]])\\nAnd it gets it right! The digit 5 is indeed not large (False) and odd (True).\\nThere are many ways to evaluate a multilabel classifier, and selecting the right metric\\nreally depends on your project. For example, one approach is to measure the F1 score\\nfor each individual label (or any other binary classifier metric discussed earlier), then\\nsimply compute the average score. This code computes the average F1 score across all\\nlabels:\\n108 \\n| \\nChapter 3: Classification'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 134}, page_content='4 Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for\\nmore details.\\n>>> y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\\n>>> f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\\n0.976410265560605\\nThis assumes that all labels are equally important, which may not be the case. In par‐\\nticular, if you have many more pictures of Alice than of Bob or Charlie, you may want\\nto give more weight to the classifier’s score on pictures of Alice. One simple option is\\nto give each label a weight equal to its support (i.e., the number of instances with that\\ntarget label). To do this, simply set average=\"weighted\" in the preceding code.4\\nMultioutput Classification\\nThe last type of classification task we are going to discuss here is called multioutput-\\nmulticlass classification (or simply multioutput classification). It is simply a generaliza‐\\ntion of multilabel classification where each label can be multiclass (i.e., it can have\\nmore than two possible values).\\nTo illustrate this, let’s build a system that removes noise from images. It will take as\\ninput a noisy digit image, and it will (hopefully) output a clean digit image, repre‐\\nsented as an array of pixel intensities, just like the MNIST images. Notice that the\\nclassifier’s output is multilabel (one label per pixel) and each label can have multiple\\nvalues (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput\\nclassification system.\\nThe line between classification and regression is sometimes blurry,\\nsuch as in this example. Arguably, predicting pixel intensity is more\\nakin to regression than to classification. Moreover, multioutput\\nsystems are not limited to classification tasks; you could even have\\na system that outputs multiple labels per instance, including both\\nclass labels and value labels.\\nLet’s start by creating the training and test sets by taking the MNIST images and\\nadding noise to their pixel intensities using NumPy’s randint() function. The target\\nimages will be the original images:\\nnoise = np.random.randint(0, 100, (len(X_train), 784))\\nX_train_mod = X_train + noise\\nnoise = np.random.randint(0, 100, (len(X_test), 784))\\nX_test_mod = X_test + noise\\ny_train_mod = X_train\\ny_test_mod = X_test\\nMultioutput Classification \\n| \\n109'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 135}, page_content='5 You can use the shift() function from the scipy.ndimage.interpolation module. For example,\\nshift(image, [2, 1], cval=0) shifts the image 2 pixels down and 1 pixel to the right.\\nLet’s take a peek at an image from the test set (yes, we’re snooping on the test data, so\\nyou should be frowning right now):\\nOn the left is the noisy input image, and on the right is the clean target image. Now\\nlet’s train the classifier and make it clean this image:\\nknn_clf.fit(X_train_mod, y_train_mod)\\nclean_digit = knn_clf.predict([X_test_mod[some_index]])\\nplot_digit(clean_digit)\\nLooks close enough to the target! This concludes our tour of classification. Hopefully\\nyou should now know how to select good metrics for classification tasks, pick the\\nappropriate precision/recall tradeoff, compare classifiers, and more generally build\\ngood classification systems for a variety of tasks.\\nExercises\\n1. Try to build a classifier for the MNIST dataset that achieves over 97% accuracy\\non the test set. Hint: the KNeighborsClassifier works quite well for this task;\\nyou just need to find good hyperparameter values (try a grid search on the\\nweights and n_neighbors hyperparameters).\\n2. Write a function that can shift an MNIST image in any direction (left, right, up,\\nor down) by one pixel.5 Then, for each image in the training set, create four shif‐\\n110 \\n| \\nChapter 3: Classification'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 136}, page_content='ted copies (one per direction) and add them to the training set. Finally, train your\\nbest model on this expanded training set and measure its accuracy on the test set.\\nYou should observe that your model performs even better now! This technique of\\nartificially growing the training set is called data augmentation or training set\\nexpansion.\\n3. Tackle the Titanic dataset. A great place to start is on Kaggle.\\n4. Build a spam classifier (a more challenging exercise):\\n• Download examples of spam and ham from Apache SpamAssassin’s public\\ndatasets.\\n• Unzip the datasets and familiarize yourself with the data format.\\n• Split the datasets into a training set and a test set.\\n• Write a data preparation pipeline to convert each email into a feature vector.\\nYour preparation pipeline should transform an email into a (sparse) vector\\nindicating the presence or absence of each possible word. For example, if all\\nemails only ever contain four words, “Hello,” “how,” “are,” “you,” then the email\\n“Hello you Hello Hello you” would be converted into a vector [1, 0, 0, 1]\\n(meaning [“Hello” is present, “how” is absent, “are” is absent, “you” is\\npresent]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of\\neach word.\\n• You may want to add hyperparameters to your preparation pipeline to control\\nwhether or not to strip off email headers, convert each email to lowercase,\\nremove punctuation, replace all URLs with “URL,” replace all numbers with\\n“NUMBER,” or even perform stemming (i.e., trim off word endings; there are\\nPython libraries available to do this).\\n• Then try out several classifiers and see if you can build a great spam classifier,\\nwith both high recall and high precision.\\nSolutions to these exercises are available in the online Jupyter notebooks at https://\\ngithub.com/ageron/handson-ml2.\\nExercises \\n| \\n111'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 137}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 138}, page_content='CHAPTER 4\\nTraining Models\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 4 in the final\\nrelease of the book.\\nSo far we have treated Machine Learning models and their training algorithms mostly\\nlike black boxes. If you went through some of the exercises in the previous chapters,\\nyou may have been surprised by how much you can get done without knowing any‐\\nthing about what’s under the hood: you optimized a regression system, you improved\\na digit image classifier, and you even built a spam classifier from scratch—all this\\nwithout knowing how they actually work. Indeed, in many situations you don’t really\\nneed to know the implementation details.\\nHowever, having a good understanding of how things work can help you quickly\\nhome in on the appropriate model, the right training algorithm to use, and a good set\\nof hyperparameters for your task. Understanding what’s under the hood will also help\\nyou debug issues and perform error analysis more efficiently. Lastly, most of the top‐\\nics discussed in this chapter will be essential in understanding, building, and training\\nneural networks (discussed in Part II of this book).\\nIn this chapter, we will start by looking at the Linear Regression model, one of the\\nsimplest models there is. We will discuss two very different ways to train it:\\n• Using a direct “closed-form” equation that directly computes the model parame‐\\nters that best fit the model to the training set (i.e., the model parameters that\\nminimize the cost function over the training set).\\n113'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 139}, page_content='• Using an iterative optimization approach, called Gradient Descent (GD), that\\ngradually tweaks the model parameters to minimize the cost function over the\\ntraining set, eventually converging to the same set of parameters as the first\\nmethod. We will look at a few variants of Gradient Descent that we will use again\\nand again when we study neural networks in Part II: Batch GD, Mini-batch GD,\\nand Stochastic GD.\\nNext we will look at Polynomial Regression, a more complex model that can fit non‐\\nlinear datasets. Since this model has more parameters than Linear Regression, it is\\nmore prone to overfitting the training data, so we will look at how to detect whether\\nor not this is the case, using learning curves, and then we will look at several regulari‐\\nzation techniques that can reduce the risk of overfitting the training set.\\nFinally, we will look at two more models that are commonly used for classification\\ntasks: Logistic Regression and Softmax Regression.\\nThere will be quite a few math equations in this chapter, using basic\\nnotions of linear algebra and calculus. To understand these equa‐\\ntions, you will need to know what vectors and matrices are, how to\\ntranspose them, multiply them, and inverse them, and what partial\\nderivatives are. If you are unfamiliar with these concepts, please go\\nthrough the linear algebra and calculus introductory tutorials avail‐\\nable as Jupyter notebooks in the online supplemental material. For\\nthose who are truly allergic to mathematics, you should still go\\nthrough this chapter and simply skip the equations; hopefully, the\\ntext will be sufficient to help you understand most of the concepts.\\nLinear Regression\\nIn Chapter 1, we looked at a simple regression model of life satisfaction: life_satisfac‐\\ntion = θ0 + θ1 × GDP_per_capita.\\nThis model is just a linear function of the input feature GDP_per_capita. θ0 and θ1 are\\nthe model’s parameters.\\nMore generally, a linear model makes a prediction by simply computing a weighted\\nsum of the input features, plus a constant called the bias term (also called the intercept\\nterm), as shown in Equation 4-1.\\nEquation 4-1. Linear Regression model prediction\\ny = θ0 + θ1x1 + θ2x2 + ⋯+ θnxn\\n• ŷ is the predicted value.\\n114 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 140}, page_content='• n is the number of features.\\n• xi is the ith feature value.\\n• θj is the jth model parameter (including the bias term θ0 and the feature weights\\nθ1, θ2, ⋯, θn).\\nThis can be written much more concisely using a vectorized form, as shown in Equa‐\\ntion 4-2.\\nEquation 4-2. Linear Regression model prediction (vectorized form)\\ny = hθ x = θ · x\\n• θ is the model’s parameter vector, containing the bias term θ0 and the feature\\nweights θ1 to θn.\\n• x is the instance’s feature vector, containing x0 to xn, with x0 always equal to 1.\\n• θ · x is the dot product of the vectors θ and x, which is of course equal to\\nθ0x0 + θ1x1 + θ2x2 + ⋯+ θnxn.\\n• hθ is the hypothesis function, using the model parameters θ.\\nIn Machine Learning, vectors are often represented as column vec‐\\ntors, which are 2D arrays with a single column. If θ and x are col‐\\numn vectors, then the prediction is: y = θTx, where θT is the\\ntranspose of θ (a row vector instead of a column vector) and θTx is\\nthe matrix multiplication of θT and x. It is of course the same pre‐\\ndiction, except it is now represented as a single cell matrix rather\\nthan a scalar value. In this book we will use this notation to avoid\\nswitching between dot products and matrix multiplications.\\nOkay, that’s the Linear Regression model, so now how do we train it? Well, recall that\\ntraining a model means setting its parameters so that the model best fits the training\\nset. For this purpose, we first need a measure of how well (or poorly) the model fits\\nthe training data. In Chapter 2 we saw that the most common performance measure\\nof a regression model is the Root Mean Square Error (RMSE) (Equation 2-1). There‐\\nfore, to train a Linear Regression model, you need to find the value of θ that minimi‐\\nzes the RMSE. In practice, it is simpler to minimize the Mean Square Error (MSE)\\nLinear Regression \\n| \\n115'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 141}, page_content='1 It is often the case that a learning algorithm will try to optimize a different function than the performance\\nmeasure used to evaluate the final model. This is generally because that function is easier to compute, because\\nit has useful differentiation properties that the performance measure lacks, or because we want to constrain\\nthe model during training, as we will see when we discuss regularization.\\n2 The demonstration that this returns the value of θ that minimizes the cost function is outside the scope of this\\nbook.\\nthan the RMSE, and it leads to the same result (because the value that minimizes a\\nfunction also minimizes its square root).1\\nThe MSE of a Linear Regression hypothesis hθ on a training set X is calculated using\\nEquation 4-3.\\nEquation 4-3. MSE cost function for a Linear Regression model\\nMSE X, hθ = 1\\nm ∑\\ni = 1\\nm\\nθTx i −y i 2\\nMost of these notations were presented in Chapter 2 (see “Notations” on page 43).\\nThe only difference is that we write hθ instead of just h in order to make it clear that\\nthe model is parametrized by the vector θ. To simplify notations, we will just write\\nMSE(θ) instead of MSE(X, hθ).\\nThe Normal Equation\\nTo find the value of θ that minimizes the cost function, there is a closed-form solution\\n—in other words, a mathematical equation that gives the result directly. This is called\\nthe Normal Equation (Equation 4-4).2\\nEquation 4-4. Normal Equation\\nθ = XTX\\n−1 \\xa0 XT \\xa0 y\\n• θ is the value of θ that minimizes the cost function.\\n• y is the vector of target values containing y(1) to y(m).\\nLet’s generate some linear-looking data to test this equation on (Figure 4-1):\\nimport numpy as np\\nX = 2 * np.random.rand(100, 1)\\ny = 4 + 3 * X + np.random.randn(100, 1)\\n116 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 142}, page_content='Figure 4-1. Randomly generated linear dataset\\nNow let’s compute θ using the Normal Equation. We will use the inv() function from\\nNumPy’s Linear Algebra module (np.linalg) to compute the inverse of a matrix, and\\nthe dot() method for matrix multiplication:\\nX_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\\ntheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\\nThe actual function that we used to generate the data is y = 4 + 3x1 + Gaussian noise.\\nLet’s see what the equation found:\\n>>> theta_best\\narray([[4.21509616],\\n       [2.77011339]])\\nWe would have hoped for θ0 = 4 and θ1 = 3 instead of θ0 = 4.215 and θ1 = 2.770. Close\\nenough, but the noise made it impossible to recover the exact parameters of the origi‐\\nnal function.\\nNow you can make predictions using θ:\\n>>> X_new = np.array([[0], [2]])\\n>>> X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\\n>>> y_predict = X_new_b.dot(theta_best)\\n>>> y_predict\\narray([[4.21509616],\\n       [9.75532293]])\\nLet’s plot this model’s predictions (Figure 4-2):\\nplt.plot(X_new, y_predict, \"r-\")\\nplt.plot(X, y, \"b.\")\\nLinear Regression \\n| \\n117'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 143}, page_content='3 Note that Scikit-Learn separates the bias term (intercept_) from the feature weights (coef_).\\nplt.axis([0, 2, 0, 15])\\nplt.show()\\nFigure 4-2. Linear Regression model predictions\\nPerforming linear regression using Scikit-Learn is quite simple:3\\n>>> from sklearn.linear_model import LinearRegression\\n>>> lin_reg = LinearRegression()\\n>>> lin_reg.fit(X, y)\\n>>> lin_reg.intercept_, lin_reg.coef_\\n(array([4.21509616]), array([[2.77011339]]))\\n>>> lin_reg.predict(X_new)\\narray([[4.21509616],\\n       [9.75532293]])\\nThe LinearRegression class is based on the scipy.linalg.lstsq() function (the\\nname stands for “least squares”), which you could call directly:\\n>>> theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\\n>>> theta_best_svd\\narray([[4.21509616],\\n       [2.77011339]])\\nThis function computes θ = X+y, where �+ is the pseudoinverse of X (specifically the\\nMoore-Penrose inverse). You can use np.linalg.pinv() to compute the pseudoin‐\\nverse directly:\\n>>> np.linalg.pinv(X_b).dot(y)\\narray([[4.21509616],\\n       [2.77011339]])\\n118 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 144}, page_content='The pseudoinverse itself is computed using a standard matrix factorization technique \\ncalled Singular Value Decomposition (SVD) that can decompose the training set\\nmatrix X into the matrix multiplication of three matrices U Σ VT (see\\nnumpy.linalg.svd()). The pseudoinverse is computed as X+ = VΣ+UT. To compute\\nthe matrix Σ+, the algorithm takes Σ and sets to zero all values smaller than a tiny\\nthreshold value, then it replaces all the non-zero values with their inverse, and finally\\nit transposes the resulting matrix. This approach is more efficient than computing the\\nNormal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may\\nnot work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some\\nfeatures are redundant, but the pseudoinverse is always defined.\\nComputational Complexity\\nThe Normal Equation computes the inverse of XT X, which is an (n + 1) × (n + 1)\\nmatrix (where n is the number of features). The computational complexity of inverting\\nsuch a matrix is typically about O(n2.4) to O(n3) (depending on the implementation).\\nIn other words, if you double the number of features, you multiply the computation\\ntime by roughly 22.4 = 5.3 to 23 = 8.\\nThe SVD approach used by Scikit-Learn’s LinearRegression class is about O(n2). If\\nyou double the number of features, you multiply the computation time by roughly 4.\\nBoth the Normal Equation and the SVD approach get very slow\\nwhen the number of features grows large (e.g., 100,000). On the\\npositive side, both are linear with regards to the number of instan‐\\nces in the training set (they are O(m)), so they handle large training\\nsets efficiently, provided they can fit in memory.\\nAlso, once you have trained your Linear Regression model (using the Normal Equa‐\\ntion or any other algorithm), predictions are very fast: the computational complexity\\nis linear with regards to both the number of instances you want to make predictions\\non and the number of features. In other words, making predictions on twice as many\\ninstances (or twice as many features) will just take roughly twice as much time.\\nNow we will look at very different ways to train a Linear Regression model, better\\nsuited for cases where there are a large number of features, or too many training\\ninstances to fit in memory.\\nGradient Descent\\nGradient Descent is a very generic optimization algorithm capable of finding optimal\\nsolutions to a wide range of problems. The general idea of Gradient Descent is to\\ntweak parameters iteratively in order to minimize a cost function.\\nGradient Descent \\n| \\n119'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 145}, page_content='Suppose you are lost in the mountains in a dense fog; you can only feel the slope of\\nthe ground below your feet. A good strategy to get to the bottom of the valley quickly\\nis to go downhill in the direction of the steepest slope. This is exactly what Gradient\\nDescent does: it measures the local gradient of the error function with regards to the \\nparameter vector θ, and it goes in the direction of descending gradient. Once the gra‐\\ndient is zero, you have reached a minimum!\\nConcretely, you start by filling θ with random values (this is called random initializa‐\\ntion), and then you improve it gradually, taking one baby step at a time, each step\\nattempting to decrease the cost function (e.g., the MSE), until the algorithm converges\\nto a minimum (see Figure 4-3).\\nFigure 4-3. Gradient Descent\\nAn important parameter in Gradient Descent is the size of the steps, determined by \\nthe learning rate hyperparameter. If the learning rate is too small, then the algorithm\\nwill have to go through many iterations to converge, which will take a long time (see\\nFigure 4-4).\\n120 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 146}, page_content='Figure 4-4. Learning rate too small\\nOn the other hand, if the learning rate is too high, you might jump across the valley\\nand end up on the other side, possibly even higher up than you were before. This\\nmight make the algorithm diverge, with larger and larger values, failing to find a good\\nsolution (see Figure 4-5).\\nFigure 4-5. Learning rate too large\\nFinally, not all cost functions look like nice regular bowls. There may be holes, ridges,\\nplateaus, and all sorts of irregular terrains, making convergence to the minimum very\\ndifficult. Figure 4-6 shows the two main challenges with Gradient Descent: if the ran‐\\ndom initialization starts the algorithm on the left, then it will converge to a local mini‐\\nmum, which is not as good as the global minimum. If it starts on the right, then it will\\ntake a very long time to cross the plateau, and if you stop too early you will never\\nreach the global minimum.\\nGradient Descent \\n| \\n121'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 147}, page_content='4 Technically speaking, its derivative is Lipschitz continuous.\\n5 Since feature 1 is smaller, it takes a larger change in θ1 to affect the cost function, which is why the bowl is\\nelongated along the θ1 axis.\\nFigure 4-6. Gradient Descent pitfalls\\nFortunately, the MSE cost function for a Linear Regression model happens to be a\\nconvex function, which means that if you pick any two points on the curve, the line\\nsegment joining them never crosses the curve. This implies that there are no local\\nminima, just one global minimum. It is also a continuous function with a slope that\\nnever changes abruptly.4 These two facts have a great consequence: Gradient Descent\\nis guaranteed to approach arbitrarily close the global minimum (if you wait long\\nenough and if the learning rate is not too high).\\nIn fact, the cost function has the shape of a bowl, but it can be an elongated bowl if\\nthe features have very different scales. Figure 4-7 shows Gradient Descent on a train‐\\ning set where features 1 and 2 have the same scale (on the left), and on a training set\\nwhere feature 1 has much smaller values than feature 2 (on the right).5\\nFigure 4-7. Gradient Descent with and without feature scaling\\n122 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 148}, page_content='As you can see, on the left the Gradient Descent algorithm goes straight toward the\\nminimum, thereby reaching it quickly, whereas on the right it first goes in a direction\\nalmost orthogonal to the direction of the global minimum, and it ends with a long\\nmarch down an almost flat valley. It will eventually reach the minimum, but it will\\ntake a long time.\\nWhen using Gradient Descent, you should ensure that all features\\nhave a similar scale (e.g., using Scikit-Learn’s StandardScaler\\nclass), or else it will take much longer to converge.\\nThis diagram also illustrates the fact that training a model means searching for a\\ncombination of model parameters that minimizes a cost function (over the training\\nset). It is a search in the model’s parameter space: the more parameters a model has,\\nthe more dimensions this space has, and the harder the search is: searching for a nee‐\\ndle in a 300-dimensional haystack is much trickier than in three dimensions. Fortu‐\\nnately, since the cost function is convex in the case of Linear Regression, the needle is\\nsimply at the bottom of the bowl.\\nBatch Gradient Descent\\nTo implement Gradient Descent, you need to compute the gradient of the cost func‐\\ntion with regards to each model parameter θj. In other words, you need to calculate\\nhow much the cost function will change if you change θj just a little bit. This is called \\na partial derivative. It is like asking “what is the slope of the mountain under my feet\\nif I face east?” and then asking the same question facing north (and so on for all other\\ndimensions, if you can imagine a universe with more than three dimensions). Equa‐\\ntion 4-5 computes the partial derivative of the cost function with regards to parame‐\\nter θj, noted ∂\\n∂θj MSE(θ).\\nEquation 4-5. Partial derivatives of the cost function\\n∂\\n∂θj\\nMSE θ = 2\\nm ∑\\ni = 1\\nm\\nθTx i −y i xj\\ni\\nInstead of computing these partial derivatives individually, you can use Equation 4-6\\nto compute them all in one go. The gradient vector, noted ∇θMSE(θ), contains all the\\npartial derivatives of the cost function (one for each model parameter).\\nGradient Descent \\n| \\n123'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 149}, page_content='6 Eta (η) is the 7th letter of the Greek alphabet.\\nEquation 4-6. Gradient vector of the cost function\\n∇θ MSE θ =\\n∂\\n∂θ0\\nMSE θ\\n∂\\n∂θ1\\nMSE θ\\n⋮\\n∂\\n∂θn\\nMSE θ\\n= 2\\nmXT Xθ −y\\nNotice that this formula involves calculations over the full training\\nset X, at each Gradient Descent step! This is why the algorithm is\\ncalled Batch Gradient Descent: it uses the whole batch of training\\ndata at every step (actually, Full Gradient Descent would probably\\nbe a better name). As a result it is terribly slow on very large train‐\\ning sets (but we will see much faster Gradient Descent algorithms\\nshortly). However, Gradient Descent scales well with the number of\\nfeatures; training a Linear Regression model when there are hun‐\\ndreds of thousands of features is much faster using Gradient\\nDescent than using the Normal Equation or SVD decomposition.\\nOnce you have the gradient vector, which points uphill, just go in the opposite direc‐\\ntion to go downhill. This means subtracting ∇θMSE(θ) from θ. This is where the \\nlearning rate η comes into play:6 multiply the gradient vector by η to determine the\\nsize of the downhill step (Equation 4-7).\\nEquation 4-7. Gradient Descent step\\nθ next step = θ −η∇θ MSE θ\\nLet’s look at a quick implementation of this algorithm:\\neta = 0.1  # learning rate\\nn_iterations = 1000\\nm = 100\\ntheta = np.random.randn(2,1)  # random initialization\\nfor iteration in range(n_iterations):\\n    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\\n    theta = theta - eta * gradients\\n124 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 150}, page_content='That wasn’t too hard! Let’s look at the resulting theta:\\n>>> theta\\narray([[4.21509616],\\n       [2.77011339]])\\nHey, that’s exactly what the Normal Equation found! Gradient Descent worked per‐\\nfectly. But what if you had used a different learning rate eta? Figure 4-8 shows the\\nfirst 10 steps of Gradient Descent using three different learning rates (the dashed line\\nrepresents the starting point).\\nFigure 4-8. Gradient Descent with various learning rates\\nOn the left, the learning rate is too low: the algorithm will eventually reach the solu‐\\ntion, but it will take a long time. In the middle, the learning rate looks pretty good: in\\njust a few iterations, it has already converged to the solution. On the right, the learn‐\\ning rate is too high: the algorithm diverges, jumping all over the place and actually\\ngetting further and further away from the solution at every step.\\nTo find a good learning rate, you can use grid search (see Chapter 2). However, you\\nmay want to limit the number of iterations so that grid search can eliminate models\\nthat take too long to converge.\\nYou may wonder how to set the number of iterations. If it is too low, you will still be\\nfar away from the optimal solution when the algorithm stops, but if it is too high, you\\nwill waste time while the model parameters do not change anymore. A simple solu‐\\ntion is to set a very large number of iterations but to interrupt the algorithm when the\\ngradient vector becomes tiny—that is, when its norm becomes smaller than a tiny\\nnumber ϵ (called the tolerance)—because this happens when Gradient Descent has\\n(almost) reached the minimum.\\nGradient Descent \\n| \\n125'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 151}, page_content='7 Out-of-core algorithms are discussed in Chapter 1.\\nConvergence Rate\\nWhen the cost function is convex and its slope does not change abruptly (as is the\\ncase for the MSE cost function), Batch Gradient Descent with a fixed learning rate\\nwill eventually converge to the optimal solution, but you may have to wait a while: it\\ncan take O(1/ϵ) iterations to reach the optimum within a range of ϵ depending on the\\nshape of the cost function. If you divide the tolerance by 10 to have a more precise\\nsolution, then the algorithm may have to run about 10 times longer.\\nStochastic Gradient Descent\\nThe main problem with Batch Gradient Descent is the fact that it uses the whole\\ntraining set to compute the gradients at every step, which makes it very slow when\\nthe training set is large. At the opposite extreme, Stochastic Gradient Descent just\\npicks a random instance in the training set at every step and computes the gradients\\nbased only on that single instance. Obviously this makes the algorithm much faster\\nsince it has very little data to manipulate at every iteration. It also makes it possible to\\ntrain on huge training sets, since only one instance needs to be in memory at each\\niteration (SGD can be implemented as an out-of-core algorithm.7)\\nOn the other hand, due to its stochastic (i.e., random) nature, this algorithm is much\\nless regular than Batch Gradient Descent: instead of gently decreasing until it reaches\\nthe minimum, the cost function will bounce up and down, decreasing only on aver‐\\nage. Over time it will end up very close to the minimum, but once it gets there it will\\ncontinue to bounce around, never settling down (see Figure 4-9). So once the algo‐\\nrithm stops, the final parameter values are good, but not optimal.\\nFigure 4-9. Stochastic Gradient Descent\\n126 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 152}, page_content='When the cost function is very irregular (as in Figure 4-6), this can actually help the\\nalgorithm jump out of local minima, so Stochastic Gradient Descent has a better\\nchance of finding the global minimum than Batch Gradient Descent does.\\nTherefore randomness is good to escape from local optima, but bad because it means\\nthat the algorithm can never settle at the minimum. One solution to this dilemma is\\nto gradually reduce the learning rate. The steps start out large (which helps make\\nquick progress and escape local minima), then get smaller and smaller, allowing the\\nalgorithm to settle at the global minimum. This process is akin to simulated anneal‐\\ning, an algorithm inspired from the process of annealing in metallurgy where molten\\nmetal is slowly cooled down. The function that determines the learning rate at each\\niteration is called the learning schedule. If the learning rate is reduced too quickly, you\\nmay get stuck in a local minimum, or even end up frozen halfway to the minimum. If\\nthe learning rate is reduced too slowly, you may jump around the minimum for a\\nlong time and end up with a suboptimal solution if you halt training too early.\\nThis code implements Stochastic Gradient Descent using a simple learning schedule:\\nn_epochs = 50\\nt0, t1 = 5, 50  # learning schedule hyperparameters\\ndef learning_schedule(t):\\n    return t0 / (t + t1)\\ntheta = np.random.randn(2,1)  # random initialization\\nfor epoch in range(n_epochs):\\n    for i in range(m):\\n        random_index = np.random.randint(m)\\n        xi = X_b[random_index:random_index+1]\\n        yi = y[random_index:random_index+1]\\n        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\\n        eta = learning_schedule(epoch * m + i)\\n        theta = theta - eta * gradients\\nBy convention we iterate by rounds of m iterations; each round is called an epoch. \\nWhile the Batch Gradient Descent code iterated 1,000 times through the whole train‐\\ning set, this code goes through the training set only 50 times and reaches a fairly good\\nsolution:\\n>>> theta\\narray([[4.21076011],\\n       [2.74856079]])\\nFigure 4-10 shows the first 20 steps of training (notice how irregular the steps are).\\nGradient Descent \\n| \\n127'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 153}, page_content='Figure 4-10. Stochastic Gradient Descent first 20 steps\\nNote that since instances are picked randomly, some instances may be picked several\\ntimes per epoch while others may not be picked at all. If you want to be sure that the\\nalgorithm goes through every instance at each epoch, another approach is to shuffle\\nthe training set (making sure to shuffle the input features and the labels jointly), then\\ngo through it instance by instance, then shuffle it again, and so on. However, this gen‐\\nerally converges more slowly.\\nWhen using Stochastic Gradient Descent, the training instances\\nmust be independent and identically distributed (IID), to ensure\\nthat the parameters get pulled towards the global optimum, on\\naverage. A simple way to ensure this is to shuffle the instances dur‐\\ning training (e.g., pick each instance randomly, or shuffle the train‐\\ning set at the beginning of each epoch). If you do not do this, for\\nexample if the instances are sorted by label, then SGD will start by\\noptimizing for one label, then the next, and so on, and it will not\\nsettle close to the global minimum.\\nTo perform Linear Regression using SGD with Scikit-Learn, you can use the SGDRe\\ngressor class, which defaults to optimizing the squared error cost function. The fol‐\\nlowing code runs for maximum 1000 epochs (max_iter=1000) or until the loss drops\\nby less than 1e-3 during one epoch (tol=1e-3), starting with a learning rate of 0.1\\n(eta0=0.1), using the default learning schedule (different from the preceding one),\\nand it does not use any regularization (penalty=None; more details on this shortly):\\nfrom sklearn.linear_model import SGDRegressor\\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\\nsgd_reg.fit(X, y.ravel())\\n128 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 154}, page_content='Once again, you find a solution quite close to the one returned by the Normal Equa‐\\ntion:\\n>>> sgd_reg.intercept_, sgd_reg.coef_\\n(array([4.24365286]), array([2.8250878]))\\nMini-batch Gradient Descent\\nThe last Gradient Descent algorithm we will look at is called Mini-batch Gradient\\nDescent. It is quite simple to understand once you know Batch and Stochastic Gradi‐\\nent Descent: at each step, instead of computing the gradients based on the full train‐\\ning set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-\\nbatch GD computes the gradients on small random sets of instances called mini-\\nbatches. The main advantage of Mini-batch GD over Stochastic GD is that you can\\nget a performance boost from hardware optimization of matrix operations, especially\\nwhen using GPUs.\\nThe algorithm’s progress in parameter space is less erratic than with SGD, especially\\nwith fairly large mini-batches. As a result, Mini-batch GD will end up walking\\naround a bit closer to the minimum than SGD. But, on the other hand, it may be\\nharder for it to escape from local minima (in the case of problems that suffer from\\nlocal minima, unlike Linear Regression as we saw earlier). Figure 4-11 shows the\\npaths taken by the three Gradient Descent algorithms in parameter space during\\ntraining. They all end up near the minimum, but Batch GD’s path actually stops at the\\nminimum, while both Stochastic GD and Mini-batch GD continue to walk around.\\nHowever, don’t forget that Batch GD takes a lot of time to take each step, and Stochas‐\\ntic GD and Mini-batch GD would also reach the minimum if you used a good learn‐\\ning schedule.\\nFigure 4-11. Gradient Descent paths in parameter space\\nGradient Descent \\n| \\n129'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 155}, page_content='8 While the Normal Equation can only perform Linear Regression, the Gradient Descent algorithms can be\\nused to train many other models, as we will see.\\n9 A quadratic equation is of the form y = ax2 + bx + c.\\nLet’s compare the algorithms we’ve discussed so far for Linear Regression8 (recall that\\nm is the number of training instances and n is the number of features); see Table 4-1.\\nTable 4-1. Comparison of algorithms for Linear Regression\\nAlgorithm\\nLarge m\\nOut-of-core support\\nLarge n\\nHyperparams\\nScaling required\\nScikit-Learn\\nNormal Equation\\nFast\\nNo\\nSlow\\n0\\nNo\\nn/a\\nSVD\\nFast\\nNo\\nSlow\\n0\\nNo\\nLinearRegression\\nBatch GD\\nSlow\\nNo\\nFast\\n2\\nYes\\nSGDRegressor\\nStochastic GD\\nFast\\nYes\\nFast\\n≥2\\nYes\\nSGDRegressor\\nMini-batch GD\\nFast\\nYes\\nFast\\n≥2\\nYes\\nSGDRegressor\\nThere is almost no difference after training: all these algorithms\\nend up with very similar models and make predictions in exactly \\nthe same way.\\nPolynomial Regression\\nWhat if your data is actually more complex than a simple straight line? Surprisingly,\\nyou can actually use a linear model to fit nonlinear data. A simple way to do this is to\\nadd powers of each feature as new features, then train a linear model on this extended\\nset of features. This technique is called Polynomial Regression.\\nLet’s look at an example. First, let’s generate some nonlinear data, based on a simple\\nquadratic equation9 (plus some noise; see Figure 4-12):\\nm = 100\\nX = 6 * np.random.rand(m, 1) - 3\\ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\\n130 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 156}, page_content='Figure 4-12. Generated nonlinear and noisy dataset\\nClearly, a straight line will never fit this data properly. So let’s use Scikit-Learn’s Poly\\nnomialFeatures class to transform our training data, adding the square (2nd-degree\\npolynomial) of each feature in the training set as new features (in this case there is\\njust one feature):\\n>>> from sklearn.preprocessing import PolynomialFeatures\\n>>> poly_features = PolynomialFeatures(degree=2, include_bias=False)\\n>>> X_poly = poly_features.fit_transform(X)\\n>>> X[0]\\narray([-0.75275929])\\n>>> X_poly[0]\\narray([-0.75275929, 0.56664654])\\nX_poly now contains the original feature of X plus the square of this feature. Now you\\ncan fit a LinearRegression model to this extended training data (Figure 4-13):\\n>>> lin_reg = LinearRegression()\\n>>> lin_reg.fit(X_poly, y)\\n>>> lin_reg.intercept_, lin_reg.coef_\\n(array([1.78134581]), array([[0.93366893, 0.56456263]]))\\nPolynomial Regression \\n| \\n131'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 157}, page_content='Figure 4-13. Polynomial Regression model predictions\\nNot bad: the model estimates y = 0 . 56x1\\n2 + 0 . 93x1 + 1 . 78 when in fact the original\\nfunction was y = 0 . 5x1\\n2 + 1 . 0x1 + 2 . 0 + Gaussian noise.\\nNote that when there are multiple features, Polynomial Regression is capable of find‐\\ning relationships between features (which is something a plain Linear Regression\\nmodel cannot do). This is made possible by the fact that PolynomialFeatures also\\nadds all combinations of features up to the given degree. For example, if there were\\ntwo features a and b, PolynomialFeatures with degree=3 would not only add the\\nfeatures a2, a3, b2, and b3, but also the combinations ab, a2b, and ab2.\\nPolynomialFeatures(degree=d) transforms an array containing n\\nfeatures into an array containing n + d !\\nd! n!  features, where n! is the\\nfactorial of n, equal to 1 × 2 × 3 × ⋯ × n. Beware of the combinato‐\\nrial explosion of the number of features!\\nLearning Curves\\nIf you perform high-degree Polynomial Regression, you will likely fit the training\\ndata much better than with plain Linear Regression. For example, Figure 4-14 applies\\na 300-degree polynomial model to the preceding training data, and compares the\\nresult with a pure linear model and a quadratic model (2nd-degree polynomial).\\nNotice how the 300-degree polynomial model wiggles around to get as close as possi‐\\nble to the training instances.\\n132 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 158}, page_content='Figure 4-14. High-degree Polynomial Regression\\nOf course, this high-degree Polynomial Regression model is severely overfitting the\\ntraining data, while the linear model is underfitting it. The model that will generalize\\nbest in this case is the quadratic model. It makes sense since the data was generated\\nusing a quadratic model, but in general you won’t know what function generated the\\ndata, so how can you decide how complex your model should be? How can you tell\\nthat your model is overfitting or underfitting the data?\\nIn Chapter 2 you used cross-validation to get an estimate of a model’s generalization\\nperformance. If a model performs well on the training data but generalizes poorly\\naccording to the cross-validation metrics, then your model is overfitting. If it per‐\\nforms poorly on both, then it is underfitting. This is one way to tell when a model is\\ntoo simple or too complex.\\nAnother way is to look at the learning curves: these are plots of the model’s perfor‐\\nmance on the training set and the validation set as a function of the training set size\\n(or the training iteration). To generate the plots, simply train the model several times\\non different sized subsets of the training set. The following code defines a function\\nthat plots the learning curves of a model given some training data:\\nfrom sklearn.metrics import mean_squared_error\\nfrom sklearn.model_selection import train_test_split\\ndef plot_learning_curves(model, X, y):\\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\\n    train_errors, val_errors = [], []\\n    for m in range(1, len(X_train)):\\n        model.fit(X_train[:m], y_train[:m])\\n        y_train_predict = model.predict(X_train[:m])\\nLearning Curves \\n| \\n133'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 159}, page_content='y_val_predict = model.predict(X_val)\\n        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\\n        val_errors.append(mean_squared_error(y_val, y_val_predict))\\n    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\\n    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\\nLet’s look at the learning curves of the plain Linear Regression model (a straight line;\\nFigure 4-15):\\nlin_reg = LinearRegression()\\nplot_learning_curves(lin_reg, X, y)\\nFigure 4-15. Learning curves\\nThis deserves a bit of explanation. First, let’s look at the performance on the training\\ndata: when there are just one or two instances in the training set, the model can fit\\nthem perfectly, which is why the curve starts at zero. But as new instances are added\\nto the training set, it becomes impossible for the model to fit the training data per‐\\nfectly, both because the data is noisy and because it is not linear at all. So the error on\\nthe training data goes up until it reaches a plateau, at which point adding new instan‐\\nces to the training set doesn’t make the average error much better or worse. Now let’s\\nlook at the performance of the model on the validation data. When the model is\\ntrained on very few training instances, it is incapable of generalizing properly, which\\nis why the validation error is initially quite big. Then as the model is shown more\\ntraining examples, it learns and thus the validation error slowly goes down. However,\\nonce again a straight line cannot do a good job modeling the data, so the error ends\\nup at a plateau, very close to the other curve.\\nThese learning curves are typical of an underfitting model. Both curves have reached\\na plateau; they are close and fairly high.\\n134 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 160}, page_content='If your model is underfitting the training data, adding more train‐\\ning examples will not help. You need to use a more complex model\\nor come up with better features.\\nNow let’s look at the learning curves of a 10th-degree polynomial model on the same\\ndata (Figure 4-16):\\nfrom sklearn.pipeline import Pipeline\\npolynomial_regression = Pipeline([\\n        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\\n        (\"lin_reg\", LinearRegression()),\\n    ])\\nplot_learning_curves(polynomial_regression, X, y)\\nThese learning curves look a bit like the previous ones, but there are two very impor‐\\ntant differences:\\n• The error on the training data is much lower than with the Linear Regression\\nmodel.\\n• There is a gap between the curves. This means that the model performs signifi‐\\ncantly better on the training data than on the validation data, which is the hall‐\\nmark of an overfitting model. However, if you used a much larger training set,\\nthe two curves would continue to get closer.\\nFigure 4-16. Learning curves for the polynomial model\\nLearning Curves \\n| \\n135'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 161}, page_content='10 This notion of bias is not to be confused with the bias term of linear models.\\nOne way to improve an overfitting model is to feed it more training\\ndata until the validation error reaches the training error.\\nThe Bias/Variance Tradeoff\\nAn important theoretical result of statistics and Machine Learning is the fact that a\\nmodel’s generalization error can be expressed as the sum of three very different\\nerrors:\\nBias\\nThis part of the generalization error is due to wrong assumptions, such as assum‐\\ning that the data is linear when it is actually quadratic. A high-bias model is most\\nlikely to underfit the training data.10\\nVariance\\nThis part is due to the model’s excessive sensitivity to small variations in the\\ntraining data. A model with many degrees of freedom (such as a high-degree pol‐\\nynomial model) is likely to have high variance, and thus to overfit the training\\ndata.\\nIrreducible error\\nThis part is due to the noisiness of the data itself. The only way to reduce this\\npart of the error is to clean up the data (e.g., fix the data sources, such as broken\\nsensors, or detect and remove outliers).\\nIncreasing a model’s complexity will typically increase its variance and reduce its bias.\\nConversely, reducing a model’s complexity increases its bias and reduces its variance. \\nThis is why it is called a tradeoff.\\nRegularized Linear Models\\nAs we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the\\nmodel (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be\\nfor it to overfit the data. For example, a simple way to regularize a polynomial model\\nis to reduce the number of polynomial degrees.\\nFor a linear model, regularization is typically achieved by constraining the weights of\\nthe model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,\\nwhich implement three different ways to constrain the weights.\\n136 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 162}, page_content='11 It is common to use the notation J(θ) for cost functions that don’t have a short name; we will often use this\\nnotation throughout the rest of this book. The context will make it clear which cost function is being dis‐\\ncussed.\\n12 Norms are discussed in Chapter 2.\\nRidge Regression\\nRidge Regression (also called Tikhonov regularization) is a regularized version of Lin‐\\near Regression: a regularization term equal to α∑i = 1\\nn\\nθi\\n2 is added to the cost function. \\nThis forces the learning algorithm to not only fit the data but also keep the model\\nweights as small as possible. Note that the regularization term should only be added\\nto the cost function during training. Once the model is trained, you want to evaluate\\nthe model’s performance using the unregularized performance measure.\\nIt is quite common for the cost function used during training to be\\ndifferent from the performance measure used for testing. Apart\\nfrom regularization, another reason why they might be different is\\nthat a good training cost function should have optimization-\\nfriendly derivatives, while the performance measure used for test‐\\ning should be as close as possible to the final objective. A good\\nexample of this is a classifier trained using a cost function such as\\nthe log loss (discussed in a moment) but evaluated using precision/\\nrecall.\\nThe hyperparameter α controls how much you want to regularize the model. If α = 0\\nthen Ridge Regression is just Linear Regression. If α is very large, then all weights end\\nup very close to zero and the result is a flat line going through the data’s mean. Equa‐\\ntion 4-8 presents the Ridge Regression cost function.11\\nEquation 4-8. Ridge Regression cost function\\nJ θ = MSE θ + α1\\n2 ∑i = 1\\nn\\nθi\\n2\\nNote that the bias term θ0 is not regularized (the sum starts at i = 1, not 0). If we\\ndefine w as the vector of feature weights (θ1 to θn), then the regularization term is\\nsimply equal to ½(∥ w ∥2)2, where ∥ w ∥2 represents the ℓ2 norm of the weight vector.12\\nFor Gradient Descent, just add αw to the MSE gradient vector (Equation 4-6).\\nIt is important to scale the data (e.g., using a StandardScaler) \\nbefore performing Ridge Regression, as it is sensitive to the scale of\\nthe input features. This is true of most regularized models.\\nRegularized Linear Models \\n| \\n137'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 163}, page_content='13 A square matrix full of 0s except for 1s on the main diagonal (top-left to bottom-right).\\nFigure 4-17 shows several Ridge models trained on some linear data using different α\\nvalue. On the left, plain Ridge models are used, leading to linear predictions. On the\\nright, the data is first expanded using PolynomialFeatures(degree=10), then it is\\nscaled using a StandardScaler, and finally the Ridge models are applied to the result‐\\ning features: this is Polynomial Regression with Ridge regularization. Note how\\nincreasing α leads to flatter (i.e., less extreme, more reasonable) predictions; this\\nreduces the model’s variance but increases its bias.\\nAs with Linear Regression, we can perform Ridge Regression either by computing a \\nclosed-form equation or by performing Gradient Descent. The pros and cons are the\\nsame. Equation 4-9 shows the closed-form solution (where A is the (n + 1) × (n + 1)\\nidentity matrix13 except with a 0 in the top-left cell, corresponding to the bias term).\\nFigure 4-17. Ridge Regression\\nEquation 4-9. Ridge Regression closed-form solution\\nθ = XTX + αA\\n−1 \\xa0 XT \\xa0 y\\nHere is how to perform Ridge Regression with Scikit-Learn using a closed-form solu‐\\ntion (a variant of Equation 4-9 using a matrix factorization technique by André-Louis\\nCholesky):\\n>>> from sklearn.linear_model import Ridge\\n>>> ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\\n>>> ridge_reg.fit(X, y)\\n138 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 164}, page_content='14 Alternatively you can use the Ridge class with the \"sag\" solver. Stochastic Average GD is a variant of SGD.\\nFor more details, see the presentation “Minimizing Finite Sums with the Stochastic Average Gradient Algo‐\\nrithm” by Mark Schmidt et al. from the University of British Columbia.\\n>>> ridge_reg.predict([[1.5]])\\narray([[1.55071465]])\\nAnd using Stochastic Gradient Descent:14\\n>>> sgd_reg = SGDRegressor(penalty=\"l2\")\\n>>> sgd_reg.fit(X, y.ravel())\\n>>> sgd_reg.predict([[1.5]])\\narray([1.47012588])\\nThe penalty hyperparameter sets the type of regularization term to use. Specifying\\n\"l2\" indicates that you want SGD to add a regularization term to the cost function \\nequal to half the square of the ℓ2 norm of the weight vector: this is simply Ridge\\nRegression.\\nLasso Regression\\nLeast Absolute Shrinkage and Selection Operator Regression (simply called Lasso\\nRegression) is another regularized version of Linear Regression: just like Ridge\\nRegression, it adds a regularization term to the cost function, but it uses the ℓ1 norm\\nof the weight vector instead of half the square of the ℓ2 norm (see Equation 4-10).\\nEquation 4-10. Lasso Regression cost function\\nJ θ = MSE θ + α∑i = 1\\nn\\nθi\\nFigure 4-18 shows the same thing as Figure 4-17 but replaces Ridge models with\\nLasso models and uses smaller α values.\\nRegularized Linear Models \\n| \\n139'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 165}, page_content='Figure 4-18. Lasso Regression\\nAn important characteristic of Lasso Regression is that it tends to completely elimi‐\\nnate the weights of the least important features (i.e., set them to zero). For example,\\nthe dashed line in the right plot on Figure 4-18 (with α = 10-7) looks quadratic, almost\\nlinear: all the weights for the high-degree polynomial features are equal to zero. In\\nother words, Lasso Regression automatically performs feature selection and outputs a\\nsparse model (i.e., with few nonzero feature weights).\\nYou can get a sense of why this is the case by looking at Figure 4-19: on the top-left\\nplot, the background contours (ellipses) represent an unregularized MSE cost func‐\\ntion (α = 0), and the white circles show the Batch Gradient Descent path with that\\ncost function. The foreground contours (diamonds) represent the ℓ1 penalty, and the\\ntriangles show the BGD path for this penalty only (α → ∞). Notice how the path first\\nreaches θ1 = 0, then rolls down a gutter until it reaches θ2 = 0. On the top-right plot,\\nthe contours represent the same cost function plus an ℓ1 penalty with α = 0.5. The\\nglobal minimum is on the θ2 = 0 axis. BGD first reaches θ2 = 0, then rolls down the\\ngutter until it reaches the global minimum. The two bottom plots show the same\\nthing but uses an ℓ2 penalty instead. The regularized minimum is closer to θ = 0 than\\nthe unregularized minimum, but the weights do not get fully eliminated.\\n140 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 166}, page_content='15 You can think of a subgradient vector at a nondifferentiable point as an intermediate vector between the gra‐\\ndient vectors around that point.\\nFigure 4-19. Lasso versus Ridge regularization\\nOn the Lasso cost function, the BGD path tends to bounce across\\nthe gutter toward the end. This is because the slope changes\\nabruptly at θ2 = 0. You need to gradually reduce the learning rate in\\norder to actually converge to the global minimum.\\nThe Lasso cost function is not differentiable at θi = 0 (for i = 1, 2, ⋯, n), but Gradient\\nDescent still works fine if you use a subgradient vector g15 instead when any θi = 0.\\nEquation 4-11 shows a subgradient vector equation you can use for Gradient Descent\\nwith the Lasso cost function.\\nEquation 4-11. Lasso Regression subgradient vector\\ng θ, J = ∇θ MSE θ + α\\nsign θ1\\nsign θ2\\n⋮\\nsign θn\\n\\xa0\\xa0 where\\xa0 sign θi =\\n−1 if\\xa0θi < 0\\n0 if\\xa0θi = 0\\n+1 if\\xa0θi > 0\\nRegularized Linear Models \\n| \\n141'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 167}, page_content='Here is a small Scikit-Learn example using the Lasso class. Note that you could\\ninstead use an SGDRegressor(penalty=\"l1\").\\n>>> from sklearn.linear_model import Lasso\\n>>> lasso_reg = Lasso(alpha=0.1)\\n>>> lasso_reg.fit(X, y)\\n>>> lasso_reg.predict([[1.5]])\\narray([1.53788174])\\nElastic Net\\nElastic Net is a middle ground between Ridge Regression and Lasso Regression. The\\nregularization term is a simple mix of both Ridge and Lasso’s regularization terms,\\nand you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\\nRegression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12).\\nEquation 4-12. Elastic Net cost function\\nJ θ = MSE θ + rα∑i = 1\\nn\\nθi + 1 −r\\n2 α∑i = 1\\nn\\nθi\\n2\\nSo when should you use plain Linear Regression (i.e., without any regularization),\\nRidge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\\nregularization, so generally you should avoid plain Linear Regression. Ridge is a good\\ndefault, but if you suspect that only a few features are actually useful, you should pre‐\\nfer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to\\nzero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\\nmay behave erratically when the number of features is greater than the number of\\ntraining instances or when several features are strongly correlated.\\nHere is a short example using Scikit-Learn’s ElasticNet (l1_ratio corresponds to\\nthe mix ratio r):\\n>>> from sklearn.linear_model import ElasticNet\\n>>> elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\\n>>> elastic_net.fit(X, y)\\n>>> elastic_net.predict([[1.5]])\\narray([1.54333232])\\nEarly Stopping\\nA very different way to regularize iterative learning algorithms such as Gradient\\nDescent is to stop training as soon as the validation error reaches a minimum. This is\\ncalled early stopping. Figure 4-20 shows a complex model (in this case a high-degree\\nPolynomial Regression model) being trained using Batch Gradient Descent. As the\\nepochs go by, the algorithm learns and its prediction error (RMSE) on the training set\\nnaturally goes down, and so does its prediction error on the validation set. However,\\n142 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 168}, page_content='after a while the validation error stops decreasing and actually starts to go back up.\\nThis indicates that the model has started to overfit the training data. With early stop‐\\nping you just stop training as soon as the validation error reaches the minimum. It is\\nsuch a simple and efficient regularization technique that Geoffrey Hinton called it a\\n“beautiful free lunch.”\\nFigure 4-20. Early stopping regularization\\nWith Stochastic and Mini-batch Gradient Descent, the curves are\\nnot so smooth, and it may be hard to know whether you have\\nreached the minimum or not. One solution is to stop only after the\\nvalidation error has been above the minimum for some time (when\\nyou are confident that the model will not do any better), then roll\\nback the model parameters to the point where the validation error\\nwas at a minimum.\\nHere is a basic implementation of early stopping:\\nfrom sklearn.base import clone\\n# prepare the data\\npoly_scaler = Pipeline([\\n        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\\n        (\"std_scaler\", StandardScaler())\\n    ])\\nX_train_poly_scaled = poly_scaler.fit_transform(X_train)\\nX_val_poly_scaled = poly_scaler.transform(X_val)\\nsgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\\n                       penalty=None, learning_rate=\"constant\", eta0=0.0005)\\nRegularized Linear Models \\n| \\n143'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 169}, page_content='minimum_val_error = float(\"inf\")\\nbest_epoch = None\\nbest_model = None\\nfor epoch in range(1000):\\n    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\\n    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\\n    val_error = mean_squared_error(y_val, y_val_predict)\\n    if val_error < minimum_val_error:\\n        minimum_val_error = val_error\\n        best_epoch = epoch\\n        best_model = clone(sgd_reg)\\nNote that with warm_start=True, when the fit() method is called, it just continues\\ntraining where it left off instead of restarting from scratch.\\nLogistic Regression\\nAs we discussed in Chapter 1, some regression algorithms can be used for classifica‐\\ntion as well (and vice versa). Logistic Regression (also called Logit Regression) is com‐\\nmonly used to estimate the probability that an instance belongs to a particular class\\n(e.g., what is the probability that this email is spam?). If the estimated probability is\\ngreater than 50%, then the model predicts that the instance belongs to that class\\n(called the positive class, labeled “1”), or else it predicts that it does not (i.e., it\\nbelongs to the negative class, labeled “0”). This makes it a binary classifier.\\nEstimating Probabilities\\nSo how does it work? Just like a Linear Regression model, a Logistic Regression\\nmodel computes a weighted sum of the input features (plus a bias term), but instead\\nof outputting the result directly like the Linear Regression model does, it outputs the\\nlogistic of this result (see Equation 4-13).\\nEquation 4-13. Logistic Regression model estimated probability (vectorized form)\\np = hθ x = σ xTθ\\nThe logistic—noted σ(·)—is a sigmoid function (i.e., S-shaped) that outputs a number\\nbetween 0 and 1. It is defined as shown in Equation 4-14 and Figure 4-21.\\nEquation 4-14. Logistic function\\nσ t =\\n1\\n1 + exp\\n−t\\n144 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 170}, page_content='Figure 4-21. Logistic function\\nOnce the Logistic Regression model has estimated the probability p = hθ(x) that an\\ninstance x belongs to the positive class, it can make its prediction ŷ easily (see Equa‐\\ntion 4-15).\\nEquation 4-15. Logistic Regression model prediction\\ny = 0 if p < 0 . 5\\n1 if p ≥0 . 5\\nNotice that σ(t) < 0.5 when t < 0, and σ(t) ≥ 0.5 when t ≥ 0, so a Logistic Regression\\nmodel predicts 1 if xT θ is positive, and 0 if it is negative.\\nThe score t is often called the logit: this name comes from the fact\\nthat the logit function, defined as logit(p) = log(p / (1 - p)), is the\\ninverse of the logistic function. Indeed, if you compute the logit of\\nthe estimated probability p, you will find that the result is t. The\\nlogit is also called the log-odds, since it is the log of the ratio\\nbetween the estimated probability for the positive class and the\\nestimated probability for the negative class.\\nTraining and Cost Function\\nGood, now you know how a Logistic Regression model estimates probabilities and\\nmakes predictions. But how is it trained? The objective of training is to set the param‐\\neter vector θ so that the model estimates high probabilities for positive instances (y =\\n1) and low probabilities for negative instances (y = 0). This idea is captured by the\\ncost function shown in Equation 4-16 for a single training instance x.\\nEquation 4-16. Cost function of a single training instance\\nc θ =\\n−log p\\nif\\xa0y = 1\\n−log 1 −p if\\xa0y = 0\\nLogistic Regression \\n| \\n145'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 171}, page_content='This cost function makes sense because – log(t) grows very large when t approaches\\n0, so the cost will be large if the model estimates a probability close to 0 for a positive\\ninstance, and it will also be very large if the model estimates a probability close to 1\\nfor a negative instance. On the other hand, – log(t) is close to 0 when t is close to 1, so\\nthe cost will be close to 0 if the estimated probability is close to 0 for a negative\\ninstance or close to 1 for a positive instance, which is precisely what we want.\\nThe cost function over the whole training set is simply the average cost over all train‐\\ning instances. It can be written in a single expression (as you can verify easily), called \\nthe log loss, shown in Equation 4-17.\\nEquation 4-17. Logistic Regression cost function (log loss)\\nJ θ = −1\\nm ∑i = 1\\nm\\ny i log p i\\n+ 1 −y i log 1 −p i\\nThe bad news is that there is no known closed-form equation to compute the value of\\nθ that minimizes this cost function (there is no equivalent of the Normal Equation).\\nBut the good news is that this cost function is convex, so Gradient Descent (or any\\nother optimization algorithm) is guaranteed to find the global minimum (if the learn‐\\ning rate is not too large and you wait long enough). The partial derivatives of the cost\\nfunction with regards to the jth model parameter θj is given by Equation 4-18.\\nEquation 4-18. Logistic cost function partial derivatives\\n∂\\n∂θj\\nJ θ = 1\\nm ∑\\ni = 1\\nm\\nσ θTx i\\n−y i xj\\ni\\nThis equation looks very much like Equation 4-5: for each instance it computes the\\nprediction error and multiplies it by the jth feature value, and then it computes the\\naverage over all training instances. Once you have the gradient vector containing all\\nthe partial derivatives you can use it in the Batch Gradient Descent algorithm. That’s\\nit: you now know how to train a Logistic Regression model. For Stochastic GD you\\nwould of course just take one instance at a time, and for Mini-batch GD you would\\nuse a mini-batch at a time.\\nDecision Boundaries\\nLet’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that\\ncontains the sepal and petal length and width of 150 iris flowers of three different\\nspecies: Iris-Setosa, Iris-Versicolor, and Iris-Virginica (see Figure 4-22).\\n146 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 172}, page_content='16 Photos reproduced from the corresponding Wikipedia pages. Iris-Virginica photo by Frank Mayfield (Crea‐\\ntive Commons BY-SA 2.0), Iris-Versicolor photo by D. Gordon E. Robertson (Creative Commons BY-SA 3.0),\\nand Iris-Setosa photo is public domain.\\n17 NumPy’s reshape() function allows one dimension to be –1, which means “unspecified”: the value is inferred\\nfrom the length of the array and the remaining dimensions.\\nFigure 4-22. Flowers of three iris plant species16\\nLet’s try to build a classifier to detect the Iris-Virginica type based only on the petal\\nwidth feature. First let’s load the data:\\n>>> from sklearn import datasets\\n>>> iris = datasets.load_iris()\\n>>> list(iris.keys())\\n[\\'data\\', \\'target\\', \\'target_names\\', \\'DESCR\\', \\'feature_names\\', \\'filename\\']\\n>>> X = iris[\"data\"][:, 3:]  # petal width\\n>>> y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0\\nNow let’s train a Logistic Regression model:\\nfrom sklearn.linear_model import LogisticRegression\\nlog_reg = LogisticRegression()\\nlog_reg.fit(X, y)\\nLet’s look at the model’s estimated probabilities for flowers with petal widths varying\\nfrom 0 to 3 cm (Figure 4-23)17:\\nX_new = np.linspace(0, 3, 1000).reshape(-1, 1)\\ny_proba = log_reg.predict_proba(X_new)\\nplt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\")\\nLogistic Regression \\n| \\n147'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 173}, page_content='18 It is the the set of points x such that θ0 + θ1x1 + θ2x2 = 0, which defines a straight line.\\nplt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris-Virginica\")\\n# + more Matplotlib code to make the image look pretty\\nFigure 4-23. Estimated probabilities and decision boundary\\nThe petal width of Iris-Virginica flowers (represented by triangles) ranges from 1.4\\ncm to 2.5 cm, while the other iris flowers (represented by squares) generally have a\\nsmaller petal width, ranging from 0.1 cm to 1.8 cm. Notice that there is a bit of over‐\\nlap. Above about 2 cm the classifier is highly confident that the flower is an Iris-\\nVirginica (it outputs a high probability to that class), while below 1 cm it is highly\\nconfident that it is not an Iris-Virginica (high probability for the “Not Iris-Virginica”\\nclass). In between these extremes, the classifier is unsure. However, if you ask it to\\npredict the class (using the predict() method rather than the predict_proba()\\nmethod), it will return whichever class is the most likely. Therefore, there is a decision\\nboundary at around 1.6 cm where both probabilities are equal to 50%: if the petal\\nwidth is higher than 1.6 cm, the classifier will predict that the flower is an Iris-\\nVirginica, or else it will predict that it is not (even if it is not very confident):\\n>>> log_reg.predict([[1.7], [1.5]])\\narray([1, 0])\\nFigure 4-24 shows the same dataset but this time displaying two features: petal width\\nand length. Once trained, the Logistic Regression classifier can estimate the probabil‐\\nity that a new flower is an Iris-Virginica based on these two features. The dashed line\\nrepresents the points where the model estimates a 50% probability: this is the model’s\\ndecision boundary. Note that it is a linear boundary.18 Each parallel line represents the\\npoints where the model outputs a specific probability, from 15% (bottom left) to 90%\\n(top right). All the flowers beyond the top-right line have an over 90% chance of\\nbeing Iris-Virginica according to the model.\\n148 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 174}, page_content='Figure 4-24. Linear decision boundary\\nJust like the other linear models, Logistic Regression models can be regularized using \\nℓ1 or ℓ2 penalties. Scitkit-Learn actually adds an ℓ2 penalty by default.\\nThe hyperparameter controlling the regularization strength of a\\nScikit-Learn LogisticRegression model is not alpha (as in other\\nlinear models), but its inverse: C. The higher the value of C, the less\\nthe model is regularized.\\nSoftmax Regression\\nThe Logistic Regression model can be generalized to support multiple classes directly,\\nwithout having to train and combine multiple binary classifiers (as discussed in\\nChapter 3). This is called Softmax Regression, or Multinomial Logistic Regression.\\nThe idea is quite simple: when given an instance x, the Softmax Regression model\\nfirst computes a score sk(x) for each class k, then estimates the probability of each\\nclass by applying the softmax function (also called the normalized exponential) to the\\nscores. The equation to compute sk(x) should look familiar, as it is just like the equa‐\\ntion for Linear Regression prediction (see Equation 4-19).\\nEquation 4-19. Softmax score for class k\\nsk x = xTθ k\\nNote that each class has its own dedicated parameter vector θ(k). All these vectors are\\ntypically stored as rows in a parameter matrix Θ.\\nOnce you have computed the score of every class for the instance x, you can estimate\\nthe probability pk that the instance belongs to class k by running the scores through\\nthe softmax function (Equation 4-20): it computes the exponential of every score,\\nLogistic Regression \\n| \\n149'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 175}, page_content='then normalizes them (dividing by the sum of all the exponentials). The scores are\\ngenerally called logits or log-odds (although they are actually unnormalized log-\\nodds).\\nEquation 4-20. Softmax function\\npk = σ s x\\nk =\\nexp sk x\\n∑j = 1\\nK\\nexp sj x\\n• K is the number of classes.\\n• s(x) is a vector containing the scores of each class for the instance x.\\n• σ(s(x))k is the estimated probability that the instance x belongs to class k given\\nthe scores of each class for that instance.\\nJust like the Logistic Regression classifier, the Softmax Regression classifier predicts\\nthe class with the highest estimated probability (which is simply the class with the\\nhighest score), as shown in Equation 4-21.\\nEquation 4-21. Softmax Regression classifier prediction\\ny = argmax\\nk\\nσ s x\\nk = argmax\\nk\\nsk x = argmax\\nk\\nθ k Tx\\n• The argmax operator returns the value of a variable that maximizes a function. In\\nthis equation, it returns the value of k that maximizes the estimated probability\\nσ(s(x))k.\\nThe Softmax Regression classifier predicts only one class at a time\\n(i.e., it is multiclass, not multioutput) so it should be used only with\\nmutually exclusive classes such as different types of plants. You\\ncannot use it to recognize multiple people in one picture.\\nNow that you know how the model estimates probabilities and makes predictions,\\nlet’s take a look at training. The objective is to have a model that estimates a high\\nprobability for the target class (and consequently a low probability for the other\\nclasses). Minimizing the cost function shown in Equation 4-22, called the cross\\nentropy, should lead to this objective because it penalizes the model when it estimates\\na low probability for a target class. Cross entropy is frequently used to measure how\\n150 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 176}, page_content='well a set of estimated class probabilities match the target classes (we will use it again\\nseveral times in the following chapters).\\nEquation 4-22. Cross entropy cost function\\nJ Θ = −1\\nm ∑i = 1\\nm\\n∑k = 1\\nK\\nyk\\ni log pk\\ni\\n• yk\\ni  is the target probability that the ith instance belongs to class k. In general, it is\\neither equal to 1 or 0, depending on whether the instance belongs to the class or\\nnot.\\nNotice that when there are just two classes (K = 2), this cost function is equivalent to\\nthe Logistic Regression’s cost function (log loss; see Equation 4-17).\\nCross Entropy\\nCross entropy originated from information theory. Suppose you want to efficiently\\ntransmit information about the weather every day. If there are eight options (sunny,\\nrainy, etc.), you could encode each option using 3 bits since 23 = 8. However, if you\\nthink it will be sunny almost every day, it would be much more efficient to code\\n“sunny” on just one bit (0) and the other seven options on 4 bits (starting with a 1).\\nCross entropy measures the average number of bits you actually send per option. If\\nyour assumption about the weather is perfect, cross entropy will just be equal to the\\nentropy of the weather itself (i.e., its intrinsic unpredictability). But if your assump‐\\ntions are wrong (e.g., if it rains often), cross entropy will be greater by an amount \\ncalled the Kullback–Leibler divergence.\\nThe cross entropy between two probability distributions p and q is defined as\\nH p, q = −∑x p x log q x  (at least when the distributions are discrete). For more\\ndetails, check out this video.\\nThe gradient vector of this cost function with regards to θ(k) is given by Equation\\n4-23:\\nEquation 4-23. Cross entropy gradient vector for class k\\n∇\\nθ k J Θ = 1\\nm ∑\\ni = 1\\nm\\npk\\ni −yk\\ni x i\\nNow you can compute the gradient vector for every class, then use Gradient Descent\\n(or any other optimization algorithm) to find the parameter matrix Θ that minimizes\\nthe cost function.\\nLogistic Regression \\n| \\n151'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 177}, page_content='Let’s use Softmax Regression to classify the iris flowers into all three classes. Scikit-\\nLearn’s LogisticRegression uses one-versus-all by default when you train it on more\\nthan two classes, but you can set the multi_class hyperparameter to \"multinomial\"\\nto switch it to Softmax Regression instead. You must also specify a solver that sup‐\\nports Softmax Regression, such as the \"lbfgs\" solver (see Scikit-Learn’s documenta‐\\ntion for more details). It also applies ℓ2 regularization by default, which you can\\ncontrol using the hyperparameter C.\\nX = iris[\"data\"][:, (2, 3)]  # petal length, petal width\\ny = iris[\"target\"]\\nsoftmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)\\nsoftmax_reg.fit(X, y)\\nSo the next time you find an iris with 5 cm long and 2 cm wide petals, you can ask\\nyour model to tell you what type of iris it is, and it will answer Iris-Virginica (class 2)\\nwith 94.2% probability (or Iris-Versicolor with 5.8% probability):\\n>>> softmax_reg.predict([[5, 2]])\\narray([2])\\n>>> softmax_reg.predict_proba([[5, 2]])\\narray([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])\\nFigure 4-25 shows the resulting decision boundaries, represented by the background\\ncolors. Notice that the decision boundaries between any two classes are linear. The\\nfigure also shows the probabilities for the Iris-Versicolor class, represented by the\\ncurved lines (e.g., the line labeled with 0.450 represents the 45% probability bound‐\\nary). Notice that the model can predict a class that has an estimated probability below\\n50%. For example, at the point where all decision boundaries meet, all classes have an\\nequal estimated probability of 33%.\\nFigure 4-25. Softmax Regression decision boundaries\\n152 \\n| \\nChapter 4: Training Models'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 178}, page_content='Exercises\\n1. What Linear Regression training algorithm can you use if you have a training set\\nwith millions of features?\\n2. Suppose the features in your training set have very different scales. What algo‐\\nrithms might suffer from this, and how? What can you do about it?\\n3. Can Gradient Descent get stuck in a local minimum when training a Logistic\\nRegression model?\\n4. Do all Gradient Descent algorithms lead to the same model provided you let\\nthem run long enough?\\n5. Suppose you use Batch Gradient Descent and you plot the validation error at\\nevery epoch. If you notice that the validation error consistently goes up, what is\\nlikely going on? How can you fix this?\\n6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali‐\\ndation error goes up?\\n7. Which Gradient Descent algorithm (among those we discussed) will reach the\\nvicinity of the optimal solution the fastest? Which will actually converge? How\\ncan you make the others converge as well?\\n8. Suppose you are using Polynomial Regression. You plot the learning curves and\\nyou notice that there is a large gap between the training error and the validation\\nerror. What is happening? What are three ways to solve this?\\n9. Suppose you are using Ridge Regression and you notice that the training error\\nand the validation error are almost equal and fairly high. Would you say that the\\nmodel suffers from high bias or high variance? Should you increase the regulari‐\\nzation hyperparameter α or reduce it?\\n10. Why would you want to use:\\n• Ridge Regression instead of plain Linear Regression (i.e., without any regulari‐\\nzation)?\\n• Lasso instead of Ridge Regression?\\n• Elastic Net instead of Lasso?\\n11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.\\nShould you implement two Logistic Regression classifiers or one Softmax Regres‐\\nsion classifier?\\n12. Implement Batch Gradient Descent with early stopping for Softmax Regression \\n(without using Scikit-Learn).\\nSolutions to these exercises are available in ???.\\nExercises \\n| \\n153'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 179}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 180}, page_content='CHAPTER 5\\nSupport Vector Machines\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 5 in the final\\nrelease of the book.\\nA Support Vector Machine (SVM) is a very powerful and versatile Machine Learning\\nmodel, capable of performing linear or nonlinear classification, regression, and even\\noutlier detection. It is one of the most popular models in Machine Learning, and any‐\\none interested in Machine Learning should have it in their toolbox. SVMs are partic‐\\nularly well suited for classification of complex but small- or medium-sized datasets.\\nThis chapter will explain the core concepts of SVMs, how to use them, and how they\\nwork.\\nLinear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\\nshows part of the iris dataset that was introduced at the end of Chapter 4. The two\\nclasses can clearly be separated easily with a straight line (they are linearly separable).\\nThe left plot shows the decision boundaries of three possible linear classifiers. The\\nmodel whose decision boundary is represented by the dashed line is so bad that it\\ndoes not even separate the classes properly. The other two models work perfectly on\\nthis training set, but their decision boundaries come so close to the instances that\\nthese models will probably not perform as well on new instances. In contrast, the\\nsolid line in the plot on the right represents the decision boundary of an SVM classi‐\\nfier; this line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. You can think of an SVM classifier as fitting the\\n155'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 181}, page_content='widest possible street (represented by the parallel dashed lines) between the classes.\\nThis is called large margin classification.\\nFigure 5-1. Large margin classification\\nNotice that adding more training instances “off the street” will not affect the decision\\nboundary at all: it is fully determined (or “supported”) by the instances located on the\\nedge of the street. These instances are called the support vectors (they are circled in\\nFigure 5-1).\\nSVMs are sensitive to the feature scales, as you can see in\\nFigure 5-2: on the left plot, the vertical scale is much larger than the\\nhorizontal scale, so the widest possible street is close to horizontal.\\nAfter feature scaling (e.g., using Scikit-Learn’s StandardScaler), \\nthe decision boundary looks much better (on the right plot).\\nFigure 5-2. Sensitivity to feature scales\\nSoft Margin Classification\\nIf we strictly impose that all instances be off the street and on the right side, this is\\ncalled hard margin classification. There are two main issues with hard margin classifi‐\\ncation. First, it only works if the data is linearly separable, and second it is quite sensi‐\\ntive to outliers. Figure 5-3 shows the iris dataset with just one additional outlier: on\\nthe left, it is impossible to find a hard margin, and on the right the decision boundary\\nends up very different from the one we saw in Figure 5-1 without the outlier, and it\\nwill probably not generalize as well.\\n156 \\n| \\nChapter 5: Support Vector Machines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 182}, page_content='Figure 5-3. Hard margin sensitivity to outliers\\nTo avoid these issues it is preferable to use a more flexible model. The objective is to\\nfind a good balance between keeping the street as large as possible and limiting the\\nmargin violations (i.e., instances that end up in the middle of the street or even on the\\nwrong side). This is called soft margin classification.\\nIn Scikit-Learn’s SVM classes, you can control this balance using the C hyperparame‐\\nter: a smaller C value leads to a wider street but more margin violations. Figure 5-4\\nshows the decision boundaries and margins of two soft margin SVM classifiers on a\\nnonlinearly separable dataset. On the left, using a low C value the margin is quite\\nlarge, but many instances end up on the street. On the right, using a high C value the\\nclassifier makes fewer margin violations but ends up with a smaller margin. However,\\nit seems likely that the first classifier will generalize better: in fact even on this train‐\\ning set it makes fewer prediction errors, since most of the margin violations are\\nactually on the correct side of the decision boundary.\\nFigure 5-4. Large margin (left) versus fewer margin violations (right)\\nIf your SVM model is overfitting, you can try regularizing it by\\nreducing C.\\nThe following Scikit-Learn code loads the iris dataset, scales the features, and then\\ntrains a linear SVM model (using the LinearSVC class with C = 1 and the hinge loss\\nfunction, described shortly) to detect Iris-Virginica flowers. The resulting model is\\nrepresented on the left of Figure 5-4.\\nLinear SVM Classification \\n| \\n157'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 183}, page_content='import numpy as np\\nfrom sklearn import datasets\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import LinearSVC\\niris = datasets.load_iris()\\nX = iris[\"data\"][:, (2, 3)]  # petal length, petal width\\ny = (iris[\"target\"] == 2).astype(np.float64)  # Iris-Virginica\\nsvm_clf = Pipeline([\\n        (\"scaler\", StandardScaler()),\\n        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\\n    ])\\nsvm_clf.fit(X, y)\\nThen, as usual, you can use the model to make predictions:\\n>>> svm_clf.predict([[5.5, 1.7]])\\narray([1.])\\nUnlike Logistic Regression classifiers, SVM classifiers do not out‐\\nput probabilities for each class.\\nAlternatively, you could use the SVC class, using SVC(kernel=\"linear\", C=1), but it\\nis much slower, especially with large training sets, so it is not recommended. Another\\noption is to use the SGDClassifier class, with SGDClassifier(loss=\"hinge\",\\nalpha=1/(m*C)). This applies regular Stochastic Gradient Descent (see Chapter 4) to\\ntrain a linear SVM classifier. It does not converge as fast as the LinearSVC class, but it\\ncan be useful to handle huge datasets that do not fit in memory (out-of-core train‐\\ning), or to handle online classification tasks.\\nThe LinearSVC class regularizes the bias term, so you should center\\nthe training set first by subtracting its mean. This is automatic if\\nyou scale the data using the StandardScaler. Moreover, make sure\\nyou set the loss hyperparameter to \"hinge\", as it is not the default\\nvalue. Finally, for better performance you should set the dual\\nhyperparameter to False, unless there are more features than\\ntraining instances (we will discuss duality later in the chapter).\\n158 \\n| \\nChapter 5: Support Vector Machines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 184}, page_content='Nonlinear SVM Classification\\nAlthough linear SVM classifiers are efficient and work surprisingly well in many\\ncases, many datasets are not even close to being linearly separable. One approach to\\nhandling nonlinear datasets is to add more features, such as polynomial features (as\\nyou did in Chapter 4); in some cases this can result in a linearly separable dataset.\\nConsider the left plot in Figure 5-5: it represents a simple dataset with just one feature\\nx1. This dataset is not linearly separable, as you can see. But if you add a second fea‐\\nture x2 = (x1)2, the resulting 2D dataset is perfectly linearly separable.\\nFigure 5-5. Adding features to make a dataset linearly separable\\nTo implement this idea using Scikit-Learn, you can create a Pipeline containing a\\nPolynomialFeatures transformer (discussed in “Polynomial Regression” on page\\n130), followed by a StandardScaler and a LinearSVC. Let’s test this on the moons\\ndataset: this is a toy dataset for binary classification in which the data points are sha‐\\nped as two interleaving half circles (see Figure 5-6). You can generate this dataset\\nusing the make_moons() function:\\nfrom sklearn.datasets import make_moons\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import PolynomialFeatures\\npolynomial_svm_clf = Pipeline([\\n        (\"poly_features\", PolynomialFeatures(degree=3)),\\n        (\"scaler\", StandardScaler()),\\n        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\\n    ])\\npolynomial_svm_clf.fit(X, y)\\nNonlinear SVM Classification \\n| \\n159'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 185}, page_content='Figure 5-6. Linear SVM classifier using polynomial features\\nPolynomial Kernel\\nAdding polynomial features is simple to implement and can work great with all sorts\\nof Machine Learning algorithms (not just SVMs), but at a low polynomial degree it\\ncannot deal with very complex datasets, and with a high polynomial degree it creates\\na huge number of features, making the model too slow.\\nFortunately, when using SVMs you can apply an almost miraculous mathematical\\ntechnique called the kernel trick (it is explained in a moment). It makes it possible to\\nget the same result as if you added many polynomial features, even with very high-\\ndegree polynomials, without actually having to add them. So there is no combinato‐\\nrial explosion of the number of features since you don’t actually add any features. This\\ntrick is implemented by the SVC class. Let’s test it on the moons dataset:\\nfrom sklearn.svm import SVC\\npoly_kernel_svm_clf = Pipeline([\\n        (\"scaler\", StandardScaler()),\\n        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\\n    ])\\npoly_kernel_svm_clf.fit(X, y)\\nThis code trains an SVM classifier using a 3rd-degree polynomial kernel. It is repre‐\\nsented on the left of Figure 5-7. On the right is another SVM classifier using a 10th-\\ndegree polynomial kernel. Obviously, if your model is overfitting, you might want to\\n160 \\n| \\nChapter 5: Support Vector Machines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 186}, page_content='reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing\\nit. The hyperparameter coef0 controls how much the model is influenced by high-\\ndegree polynomials versus low-degree polynomials.\\nFigure 5-7. SVM classifiers with a polynomial kernel\\nA common approach to find the right hyperparameter values is to\\nuse grid search (see Chapter 2). It is often faster to first do a very\\ncoarse grid search, then a finer grid search around the best values\\nfound. Having a good sense of what each hyperparameter actually\\ndoes can also help you search in the right part of the hyperparame‐\\nter space.\\nAdding Similarity Features\\nAnother technique to tackle nonlinear problems is to add features computed using a\\nsimilarity function that measures how much each instance resembles a particular\\nlandmark. For example, let’s take the one-dimensional dataset discussed earlier and\\nadd two landmarks to it at x1 = –2 and x1 = 1 (see the left plot in Figure 5-8). Next,\\nlet’s define the similarity function to be the Gaussian Radial Basis Function (RBF)\\nwith γ = 0.3 (see Equation 5-1).\\nEquation 5-1. Gaussian RBF\\nϕγ x, ℓ= exp −γ∥x −ℓ∥2\\nIt is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at\\nthe landmark). Now we are ready to compute the new features. For example, let’s look\\nat the instance x1 = –1: it is located at a distance of 1 from the first landmark, and 2\\nfrom the second landmark. Therefore its new features are x2 = exp (–0.3 × 12) ≈ 0.74\\nand x3 = exp (–0.3 × 22) ≈ 0.30. The plot on the right of Figure 5-8 shows the trans‐\\nformed dataset (dropping the original features). As you can see, it is now linearly\\nseparable.\\nNonlinear SVM Classification \\n| \\n161'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 187}, page_content='Figure 5-8. Similarity features using the Gaussian RBF\\nYou may wonder how to select the landmarks. The simplest approach is to create a\\nlandmark at the location of each and every instance in the dataset. This creates many\\ndimensions and thus increases the chances that the transformed training set will be\\nlinearly separable. The downside is that a training set with m instances and n features\\ngets transformed into a training set with m instances and m features (assuming you\\ndrop the original features). If your training set is very large, you end up with an\\nequally large number of features.\\nGaussian RBF Kernel\\nJust like the polynomial features method, the similarity features method can be useful\\nwith any Machine Learning algorithm, but it may be computationally expensive to\\ncompute all the additional features, especially on large training sets. However, once\\nagain the kernel trick does its SVM magic: it makes it possible to obtain a similar\\nresult as if you had added many similarity features, without actually having to add\\nthem. Let’s try the Gaussian RBF kernel using the SVC class:\\nrbf_kernel_svm_clf = Pipeline([\\n        (\"scaler\", StandardScaler()),\\n        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\\n    ])\\nrbf_kernel_svm_clf.fit(X, y)\\nThis model is represented on the bottom left of Figure 5-9. The other plots show\\nmodels trained with different values of hyperparameters gamma (γ) and C. Increasing\\ngamma makes the bell-shape curve narrower (see the left plot of Figure 5-8), and as a\\nresult each instance’s range of influence is smaller: the decision boundary ends up\\nbeing more irregular, wiggling around individual instances. Conversely, a small gamma \\nvalue makes the bell-shaped curve wider, so instances have a larger range of influ‐\\nence, and the decision boundary ends up smoother. So γ acts like a regularization\\nhyperparameter: if your model is overfitting, you should reduce it, and if it is under‐\\nfitting, you should increase it (similar to the C hyperparameter).\\n162 \\n| \\nChapter 5: Support Vector Machines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 188}, page_content='1 “A Dual Coordinate Descent Method for Large-scale Linear SVM,” Lin et al. (2008).\\nFigure 5-9. SVM classifiers using an RBF kernel\\nOther kernels exist but are used much more rarely. For example, some kernels are\\nspecialized for specific data structures. String kernels are sometimes used when classi‐\\nfying text documents or DNA sequences (e.g., using the string subsequence kernel or\\nkernels based on the Levenshtein distance).\\nWith so many kernels to choose from, how can you decide which\\none to use? As a rule of thumb, you should always try the linear\\nkernel first (remember that LinearSVC is much faster than SVC(ker\\nnel=\"linear\")), especially if the training set is very large or if it\\nhas plenty of features. If the training set is not too large, you should\\ntry the Gaussian RBF kernel as well; it works well in most cases.\\nThen if you have spare time and computing power, you can also\\nexperiment with a few other kernels using cross-validation and grid\\nsearch, especially if there are kernels specialized for your training\\nset’s data structure.\\nComputational Complexity\\nThe LinearSVC class is based on the liblinear library, which implements an optimized\\nalgorithm for linear SVMs.1 It does not support the kernel trick, but it scales almost\\nNonlinear SVM Classification \\n| \\n163'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 189}, page_content='2 “Sequential Minimal Optimization (SMO),” J. Platt (1998).\\nlinearly with the number of training instances and the number of features: its training\\ntime complexity is roughly O(m × n).\\nThe algorithm takes longer if you require a very high precision. This is controlled by\\nthe tolerance hyperparameter ϵ (called tol in Scikit-Learn). In most classification\\ntasks, the default tolerance is fine.\\nThe SVC class is based on the libsvm library, which implements an algorithm that sup‐\\nports the kernel trick.2 The training time complexity is usually between O(m2 × n)\\nand O(m3 × n). Unfortunately, this means that it gets dreadfully slow when the num‐\\nber of training instances gets large (e.g., hundreds of thousands of instances). This\\nalgorithm is perfect for complex but small or medium training sets. However, it scales\\nwell with the number of features, especially with sparse features (i.e., when each\\ninstance has few nonzero features). In this case, the algorithm scales roughly with the\\naverage number of nonzero features per instance. Table 5-1 compares Scikit-Learn’s\\nSVM classification classes.\\nTable 5-1. Comparison of Scikit-Learn classes for SVM classification\\nClass\\nTime complexity\\nOut-of-core support\\nScaling required\\nKernel trick\\nLinearSVC\\nO(m × n)\\nNo\\nYes\\nNo\\nSGDClassifier O(m × n)\\nYes\\nYes\\nNo\\nSVC\\nO(m² × n) to O(m³ × n) No\\nYes\\nYes\\nSVM Regression\\nAs we mentioned earlier, the SVM algorithm is quite versatile: not only does it sup‐\\nport linear and nonlinear classification, but it also supports linear and nonlinear\\nregression. The trick is to reverse the objective: instead of trying to fit the largest pos‐\\nsible street between two classes while limiting margin violations, SVM Regression\\ntries to fit as many instances as possible on the street while limiting margin violations\\n(i.e., instances off the street). The width of the street is controlled by a hyperparame‐\\nter ϵ. Figure 5-10 shows two linear SVM Regression models trained on some random\\nlinear data, one with a large margin (ϵ = 1.5) and the other with a small margin (ϵ =\\n0.5).\\n164 \\n| \\nChapter 5: Support Vector Machines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 190}, page_content='Figure 5-10. SVM Regression\\nAdding more training instances within the margin does not affect the model’s predic‐\\ntions; thus, the model is said to be ϵ-insensitive.\\nYou can use Scikit-Learn’s LinearSVR class to perform linear SVM Regression. The\\nfollowing code produces the model represented on the left of Figure 5-10 (the train‐\\ning data should be scaled and centered first):\\nfrom sklearn.svm import LinearSVR\\nsvm_reg = LinearSVR(epsilon=1.5)\\nsvm_reg.fit(X, y)\\nTo tackle nonlinear regression tasks, you can use a kernelized SVM model. For exam‐\\nple, Figure 5-11 shows SVM Regression on a random quadratic training set, using a\\n2nd-degree polynomial kernel. There is little regularization on the left plot (i.e., a large\\nC value), and much more regularization on the right plot (i.e., a small C value).\\nFigure 5-11. SVM regression using a 2nd-degree polynomial kernel\\nSVM Regression \\n| \\n165'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 191}, page_content='The following code produces the model represented on the left of Figure 5-11 using\\nScikit-Learn’s SVR class (which supports the kernel trick). The SVR class is the regres‐\\nsion equivalent of the SVC class, and the LinearSVR class is the regression equivalent\\nof the LinearSVC class. The LinearSVR class scales linearly with the size of the train‐\\ning set (just like the LinearSVC class), while the SVR class gets much too slow when\\nthe training set grows large (just like the SVC class).\\nfrom sklearn.svm import SVR\\nsvm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\\nsvm_poly_reg.fit(X, y)\\nSVMs can also be used for outlier detection; see Scikit-Learn’s doc‐\\numentation for more details.\\nUnder the Hood\\nThis section explains how SVMs make predictions and how their training algorithms\\nwork, starting with linear SVM classifiers. You can safely skip it and go straight to the\\nexercises at the end of this chapter if you are just getting started with Machine Learn‐\\ning, and come back later when you want to get a deeper understanding of SVMs.\\nFirst, a word about notations: in Chapter 4 we used the convention of putting all the \\nmodel parameters in one vector θ, including the bias term θ0 and the input feature\\nweights θ1 to θn, and adding a bias input x0 = 1 to all instances. In this chapter, we will\\nuse a different convention, which is more convenient (and more common) when you\\nare dealing with SVMs: the bias term will be called b and the feature weights vector\\nwill be called w. No bias feature will be added to the input feature vectors.\\nDecision Function and Predictions\\nThe linear SVM classifier model predicts the class of a new instance x by simply com‐\\nputing the decision function wT x + b = w1 x1 + ⋯ + wn xn + b: if the result is positive,\\nthe predicted class ŷ is the positive class (1), or else it is the negative class (0); see\\nEquation 5-2.\\nEquation 5-2. Linear SVM classifier prediction\\ny = 0 if wTx + b < 0,\\n1 if wTx + b ≥0\\n166 \\n| \\nChapter 5: Support Vector Machines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 192}, page_content='3 More generally, when there are n features, the decision function is an n-dimensional hyperplane, and the deci‐\\nsion boundary is an (n – 1)-dimensional hyperplane.\\nFigure 5-12 shows the decision function that corresponds to the model on the left of\\nFigure 5-4: it is a two-dimensional plane since this dataset has two features (petal\\nwidth and petal length). The decision boundary is the set of points where the decision\\nfunction is equal to 0: it is the intersection of two planes, which is a straight line (rep‐\\nresented by the thick solid line).3\\nFigure 5-12. Decision function for the iris dataset\\nThe dashed lines represent the points where the decision function is equal to 1 or –1:\\nthey are parallel and at equal distance to the decision boundary, forming a margin\\naround it. Training a linear SVM classifier means finding the value of w and b that\\nmake this margin as wide as possible while avoiding margin violations (hard margin)\\nor limiting them (soft margin).\\nTraining Objective\\nConsider the slope of the decision function: it is equal to the norm of the weight vec‐\\ntor, ∥ w ∥. If we divide this slope by 2, the points where the decision function is equal\\nto ±1 are going to be twice as far away from the decision boundary. In other words,\\ndividing the slope by 2 will multiply the margin by 2. Perhaps this is easier to visual‐\\nize in 2D in Figure 5-13. The smaller the weight vector w, the larger the margin.\\nUnder the Hood \\n| \\n167'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 193}, page_content='4 Zeta (ζ) is the 6th letter of the Greek alphabet.\\nFigure 5-13. A smaller weight vector results in a larger margin\\nSo we want to minimize ∥ w ∥ to get a large margin. However, if we also want to avoid\\nany margin violation (hard margin), then we need the decision function to be greater\\nthan 1 for all positive training instances, and lower than –1 for negative training\\ninstances. If we define t(i) = –1 for negative instances (if y(i) = 0) and t(i) = 1 for positive\\ninstances (if y(i) = 1), then we can express this constraint as t(i)(wT x(i) + b) ≥ 1 for all\\ninstances.\\nWe can therefore express the hard margin linear SVM classifier objective as the con‐\\nstrained optimization problem in Equation 5-3.\\nEquation 5-3. Hard margin linear SVM classifier objective\\nminimize\\nw, b\\n1\\n2wTw\\nsubject to\\nt i wTx i + b ≥1\\nfor i = 1, 2, ⋯, m\\nWe are minimizing 1\\n2wT w, which is equal to 1\\n2∥ w ∥2, rather than\\nminimizing ∥ w ∥. Indeed, 1\\n2∥ w ∥2 has a nice and simple derivative\\n(it is just w) while ∥ w ∥ is not differentiable at w = 0. Optimization\\nalgorithms work much better on differentiable functions.\\nTo get the soft margin objective, we need to introduce a slack variable ζ(i) ≥ 0 for each\\ninstance:4 ζ(i) measures how much the ith instance is allowed to violate the margin. We\\nnow have two conflicting objectives: making the slack variables as small as possible to\\nreduce the margin violations, and making 1\\n2wT w as small as possible to increase the\\nmargin. This is where the C hyperparameter comes in: it allows us to define the trade‐\\n168 \\n| \\nChapter 5: Support Vector Machines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 194}, page_content='5 To learn more about Quadratic Programming, you can start by reading Stephen Boyd and Lieven Vanden‐\\nberghe, Convex Optimization (Cambridge, UK: Cambridge University Press, 2004) or watch Richard Brown’s\\nseries of video lectures.\\noff between these two objectives. This gives us the constrained optimization problem\\nin Equation 5-4.\\nEquation 5-4. Soft margin linear SVM classifier objective\\nminimize\\nw, b, ζ\\n1\\n2wTw + C ∑\\ni = 1\\nm\\nζ i\\nsubject to\\nt i wTx i + b ≥1 −ζ i\\nand\\nζ i ≥0\\nfor i = 1, 2, ⋯, m\\nQuadratic Programming\\nThe hard margin and soft margin problems are both convex quadratic optimization\\nproblems with linear constraints. Such problems are known as Quadratic Program‐\\nming (QP) problems. Many off-the-shelf solvers are available to solve QP problems\\nusing a variety of techniques that are outside the scope of this book.5 The general\\nproblem formulation is given by Equation 5-5.\\nEquation 5-5. Quadratic Programming problem\\nMinimize\\np\\n1\\n2pTHp\\n+\\nfTp\\nsubject to\\nAp ≤b\\nwhere\\np\\nis an np‐dimensional vector (np = number of parameters),\\nH\\nis an np × np matrix,\\nf\\nis an np‐dimensional vector,\\nA\\nis an nc × np matrix (nc = number of constraints),\\nb\\nis an nc‐dimensional vector.\\nNote that the expression A p ≤ b actually defines nc constraints: pT a(i) ≤ b(i) for i = 1,\\n2, ⋯, nc, where a(i) is the vector containing the elements of the ith row of A and b(i) is\\nthe ith element of b.\\nYou can easily verify that if you set the QP parameters in the following way, you get\\nthe hard margin linear SVM classifier objective:\\n• np = n + 1, where n is the number of features (the +1 is for the bias term).\\nUnder the Hood \\n| \\n169'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 195}, page_content='6 The objective function is convex, and the inequality constraints are continuously differentiable and convex\\nfunctions.\\n• nc = m, where m is the number of training instances.\\n• H is the np × np identity matrix, except with a zero in the top-left cell (to ignore\\nthe bias term).\\n• f = 0, an np-dimensional vector full of 0s.\\n• b = –1, an nc-dimensional vector full of –1s.\\n• a(i) = –t(i) x˙ (i), where x˙ (i) is equal to x(i) with an extra bias feature x˙ 0 = 1.\\nSo one way to train a hard margin linear SVM classifier is just to use an off-the-shelf\\nQP solver by passing it the preceding parameters. The resulting vector p will contain\\nthe bias term b = p0 and the feature weights wi = pi for i = 1, 2, ⋯, n. Similarly, you\\ncan use a QP solver to solve the soft margin problem (see the exercises at the end of\\nthe chapter).\\nHowever, to use the kernel trick we are going to look at a different constrained opti‐\\nmization problem.\\nThe Dual Problem\\nGiven a constrained optimization problem, known as the primal problem, it is possi‐\\nble to express a different but closely related problem, called its dual problem. The sol‐\\nution to the dual problem typically gives a lower bound to the solution of the primal\\nproblem, but under some conditions it can even have the same solutions as the pri‐\\nmal problem. Luckily, the SVM problem happens to meet these conditions,6 so you\\ncan choose to solve the primal problem or the dual problem; both will have the same\\nsolution. Equation 5-6 shows the dual form of the linear SVM objective (if you are\\ninterested in knowing how to derive the dual problem from the primal problem,\\nsee ???).\\nEquation 5-6. Dual form of the linear SVM objective\\nminimize\\nα\\n1\\n2 ∑\\ni = 1\\nm\\n∑\\nj = 1\\nm\\nα i α j t i t j x i Tx j\\n−\\n∑\\ni = 1\\nm\\nα i\\nsubject to\\nα i ≥0\\nfor i = 1, 2, ⋯, m\\n170 \\n| \\nChapter 5: Support Vector Machines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 196}, page_content='7 As explained in Chapter 4, the dot product of two vectors a and b is normally noted a · b. However, in\\nMachine Learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the\\ndot product is achieved by computing aTb. To remain consistent with the rest of the book, we will use this\\nnotation here, ignoring the fact that this technically results in a single-cell matrix rather than a scalar value.\\nOnce you find the vector α that minimizes this equation (using a QP solver), you can\\ncompute w and b that minimize the primal problem by using Equation 5-7.\\nEquation 5-7. From the dual solution to the primal solution\\nw = ∑\\ni = 1\\nm\\nα i t i x i\\nb = 1\\nns ∑\\ni = 1\\nα i > 0\\nm\\nt i −wTx i\\nThe dual problem is faster to solve than the primal when the number of training\\ninstances is smaller than the number of features. More importantly, it makes the ker‐\\nnel trick possible, while the primal does not. So what is this kernel trick anyway?\\nKernelized SVM\\nSuppose you want to apply a 2nd-degree polynomial transformation to a two-\\ndimensional training set (such as the moons training set), then train a linear SVM\\nclassifier on the transformed training set. Equation 5-8 shows the 2nd-degree polyno‐\\nmial mapping function ϕ that you want to apply.\\nEquation 5-8. Second-degree polynomial mapping\\nϕ x = ϕ\\nx1\\nx2\\n=\\nx1\\n2\\n2 x1x2\\nx2\\n2\\nNotice that the transformed vector is three-dimensional instead of two-dimensional.\\nNow let’s look at what happens to a couple of two-dimensional vectors, a and b, if we\\napply this 2nd-degree polynomial mapping and then compute the dot product7 of the\\ntransformed vectors (See Equation 5-9).\\nUnder the Hood \\n| \\n171'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 197}, page_content='Equation 5-9. Kernel trick for a 2nd-degree polynomial mapping\\nϕ a Tϕ b\\n=\\na1\\n2\\n2 a1a2\\na2\\n2\\nT\\nb1\\n2\\n2 b1b2\\nb2\\n2\\n= a1\\n2b1\\n2 + 2a1b1a2b2 + a2\\n2b2\\n2\\n= a1b1 + a2b2\\n2 =\\na1\\na2\\nT b1\\nb2\\n2\\n= aTb\\n2\\nHow about that? The dot product of the transformed vectors is equal to the square of\\nthe dot product of the original vectors: ϕ(a)T ϕ(b) = (aT b)2.\\nNow here is the key insight: if you apply the transformation ϕ to all training instan‐\\nces, then the dual problem (see Equation 5-6) will contain the dot product ϕ(x(i))T\\nϕ(x(j)). But if ϕ is the 2nd-degree polynomial transformation defined in Equation 5-8,\\nthen you can replace this dot product of transformed vectors simply by x i Tx j\\n2\\n. So\\nyou don’t actually need to transform the training instances at all: just replace the dot\\nproduct by its square in Equation 5-6. The result will be strictly the same as if you\\nwent through the trouble of actually transforming the training set then fitting a linear\\nSVM algorithm, but this trick makes the whole process much more computationally\\nefficient. This is the essence of the kernel trick.\\nThe function K(a, b) = (aT b)2 is called a 2nd-degree polynomial kernel. In Machine\\nLearning, a kernel is a function capable of computing the dot product ϕ(a)T ϕ(b)\\nbased only on the original vectors a and b, without having to compute (or even to\\nknow about) the transformation ϕ. Equation 5-10 lists some of the most commonly\\nused kernels.\\nEquation 5-10. Common kernels\\nLinear:\\nK a, b = aTb\\nPolynomial:\\nK a, b = γaTb + r\\nd\\nGaussian RBF:\\nK a, b = exp −γ∥a −b ∥2\\nSigmoid:\\nK a, b = tanh γaTb + r\\n172 \\n| \\nChapter 5: Support Vector Machines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 198}, page_content='Mercer’s Theorem\\nAccording to Mercer’s theorem, if a function K(a, b) respects a few mathematical con‐\\nditions called Mercer’s conditions (K must be continuous, symmetric in its arguments\\nso K(a, b) = K(b, a), etc.), then there exists a function ϕ that maps a and b into\\nanother space (possibly with much higher dimensions) such that K(a, b) = ϕ(a)T ϕ(b).\\nSo you can use K as a kernel since you know ϕ exists, even if you don’t know what ϕ\\nis. In the case of the Gaussian RBF kernel, it can be shown that ϕ actually maps each\\ntraining instance to an infinite-dimensional space, so it’s a good thing you don’t need\\nto actually perform the mapping!\\nNote that some frequently used kernels (such as the Sigmoid kernel) don’t respect all\\nof Mercer’s conditions, yet they generally work well in practice.\\nThere is still one loose end we must tie. Equation 5-7 shows how to go from the dual\\nsolution to the primal solution in the case of a linear SVM classifier, but if you apply\\nthe kernel trick you end up with equations that include ϕ(x(i)). In fact, w must have\\nthe same number of dimensions as ϕ(x(i)), which may be huge or even infinite, so you\\ncan’t compute it. But how can you make predictions without knowing w? Well, the\\ngood news is that you can plug in the formula for w from Equation 5-7 into the deci‐\\nsion function for a new instance x(n), and you get an equation with only dot products\\nbetween input vectors. This makes it possible to use the kernel trick, once again\\n(Equation 5-11).\\nEquation 5-11. Making predictions with a kernelized SVM\\nhw, b ϕ x n\\n= wTϕ x n\\n+ b = ∑\\ni = 1\\nm\\nα i t i ϕ x i\\nT\\nϕ x n\\n+ b\\n= ∑\\ni = 1\\nm\\nα i t i ϕ x i Tϕ x n\\n+ b\\n=\\n∑\\ni = 1\\nα i > 0\\nm\\nα i t i K x i , x n\\n+ b\\nNote that since α(i) ≠ 0 only for support vectors, making predictions involves comput‐\\ning the dot product of the new input vector x(n) with only the support vectors, not all\\nthe training instances. Of course, you also need to compute the bias term b, using the\\nsame trick (Equation 5-12).\\nUnder the Hood \\n| \\n173'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 199}, page_content='Equation 5-12. Computing the bias term using the kernel trick\\nb = 1\\nns ∑\\ni = 1\\nα i > 0\\nm\\nt i −wTϕ x i\\n= 1\\nns ∑\\ni = 1\\nα i > 0\\nm\\nt i −∑\\nj = 1\\nm\\nα j t j ϕ x j\\nT\\nϕ x i\\n= 1\\nns ∑\\ni = 1\\nα i > 0\\nm\\nt i −\\n∑\\nj = 1\\nα j > 0\\nm\\nα j t j K x i , x j\\nIf you are starting to get a headache, it’s perfectly normal: it’s an unfortunate side\\neffect of the kernel trick.\\nOnline SVMs\\nBefore concluding this chapter, let’s take a quick look at online SVM classifiers (recall\\nthat online learning means learning incrementally, typically as new instances arrive).\\nFor linear SVM classifiers, one method is to use Gradient Descent (e.g., using\\nSGDClassifier) to minimize the cost function in Equation 5-13, which is derived\\nfrom the primal problem. Unfortunately it converges much more slowly than the\\nmethods based on QP.\\nEquation 5-13. Linear SVM classifier cost function\\nJ w, b = 1\\n2wTw\\n+\\nC ∑\\ni = 1\\nm\\nmax 0, 1 −t i wTx i + b\\nThe first sum in the cost function will push the model to have a small weight vector\\nw, leading to a larger margin. The second sum computes the total of all margin viola‐\\ntions. An instance’s margin violation is equal to 0 if it is located off the street and on\\nthe correct side, or else it is proportional to the distance to the correct side of the\\nstreet. Minimizing this term ensures that the model makes the margin violations as\\nsmall and as few as possible\\nHinge Loss\\nThe function max(0, 1 – t) is called the hinge loss function (represented below). It is\\nequal to 0 when t ≥ 1. Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1. It is not\\ndifferentiable at t = 1, but just like for Lasso Regression (see “Lasso Regression” on\\npage 139) you can still use Gradient Descent using any subderivative at t = 1 (i.e., any\\nvalue between –1 and 0).\\n174 \\n| \\nChapter 5: Support Vector Machines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 200}, page_content='8 “Incremental and Decremental Support Vector Machine Learning,” G. Cauwenberghs, T. Poggio (2001).\\n9 “Fast Kernel Classifiers with Online and Active Learning,“ A. Bordes, S. Ertekin, J. Weston, L. Bottou (2005).\\nIt is also possible to implement online kernelized SVMs—for example, using “Incre‐\\nmental and Decremental SVM Learning”8 or “Fast Kernel Classifiers with Online and\\nActive Learning.”9 However, these are implemented in Matlab and C++. For large-\\nscale nonlinear problems, you may want to consider using neural networks instead \\n(see Part II).\\nExercises\\n1. What is the fundamental idea behind Support Vector Machines?\\n2. What is a support vector?\\n3. Why is it important to scale the inputs when using SVMs?\\n4. Can an SVM classifier output a confidence score when it classifies an instance?\\nWhat about a probability?\\n5. Should you use the primal or the dual form of the SVM problem to train a model\\non a training set with millions of instances and hundreds of features?\\n6. Say you trained an SVM classifier with an RBF kernel. It seems to underfit the\\ntraining set: should you increase or decrease γ (gamma)? What about C?\\n7. How should you set the QP parameters (H, f, A, and b) to solve the soft margin\\nlinear SVM classifier problem using an off-the-shelf QP solver?\\n8. Train a LinearSVC on a linearly separable dataset. Then train an SVC and a\\nSGDClassifier on the same dataset. See if you can get them to produce roughly\\nthe same model.\\n9. Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary\\nclassifiers, you will need to use one-versus-all to classify all 10 digits. You may\\nExercises \\n| \\n175'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 201}, page_content='want to tune the hyperparameters using small validation sets to speed up the pro‐\\ncess. What accuracy can you reach?\\n10. Train an SVM regressor on the California housing dataset.\\nSolutions to these exercises are available in ???.\\n176 \\n| \\nChapter 5: Support Vector Machines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 202}, page_content='CHAPTER 6\\nDecision Trees\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 6 in the final\\nrelease of the book.\\nLike SVMs, Decision Trees are versatile Machine Learning algorithms that can per‐\\nform both classification and regression tasks, and even multioutput tasks. They are\\nvery powerful algorithms, capable of fitting complex datasets. For example, in Chap‐\\nter 2 you trained a DecisionTreeRegressor model on the California housing dataset,\\nfitting it perfectly (actually overfitting it).\\nDecision Trees are also the fundamental components of Random Forests (see Chap‐\\nter 7), which are among the most powerful Machine Learning algorithms available\\ntoday.\\nIn this chapter we will start by discussing how to train, visualize, and make predic‐\\ntions with Decision Trees. Then we will go through the CART training algorithm\\nused by Scikit-Learn, and we will discuss how to regularize trees and use them for\\nregression tasks. Finally, we will discuss some of the limitations of Decision Trees.\\nTraining and Visualizing a Decision Tree\\nTo understand Decision Trees, let’s just build one and take a look at how it makes pre‐\\ndictions. The following code trains a DecisionTreeClassifier on the iris dataset\\n(see Chapter 4):\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.tree import DecisionTreeClassifier\\n177'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 203}, page_content='1 Graphviz is an open source graph visualization software package, available at http://www.graphviz.org/.\\niris = load_iris()\\nX = iris.data[:, 2:] # petal length and width\\ny = iris.target\\ntree_clf = DecisionTreeClassifier(max_depth=2)\\ntree_clf.fit(X, y)\\nYou can visualize the trained Decision Tree by first using the export_graphviz() \\nmethod to output a graph definition file called iris_tree.dot:\\nfrom sklearn.tree import export_graphviz\\nexport_graphviz(\\n        tree_clf,\\n        out_file=image_path(\"iris_tree.dot\"),\\n        feature_names=iris.feature_names[2:],\\n        class_names=iris.target_names,\\n        rounded=True,\\n        filled=True\\n    )\\nThen you can convert this .dot file to a variety of formats such as PDF or PNG using\\nthe dot command-line tool from the graphviz package.1 This command line converts\\nthe .dot file to a .png image file:\\n$ dot -Tpng iris_tree.dot -o iris_tree.png\\nYour first decision tree looks like Figure 6-1.\\n178 \\n| \\nChapter 6: Decision Trees'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 204}, page_content='Figure 6-1. Iris Decision Tree\\nMaking Predictions\\nLet’s see how the tree represented in Figure 6-1 makes predictions. Suppose you find\\nan iris flower and you want to classify it. You start at the root node (depth 0, at the\\ntop): this node asks whether the flower’s petal length is smaller than 2.45 cm. If it is,\\nthen you move down to the root’s left child node (depth 1, left). In this case, it is a leaf\\nnode (i.e., it does not have any children nodes), so it does not ask any questions: you\\ncan simply look at the predicted class for that node and the Decision Tree predicts\\nthat your flower is an Iris-Setosa (class=setosa).\\nNow suppose you find another flower, but this time the petal length is greater than\\n2.45 cm. You must move down to the root’s right child node (depth 1, right), which is\\nnot a leaf node, so it asks another question: is the petal width smaller than 1.75 cm? If\\nit is, then your flower is most likely an Iris-Versicolor (depth 2, left). If not, it is likely\\nan Iris-Virginica (depth 2, right). It’s really that simple.\\nOne of the many qualities of Decision Trees is that they require\\nvery little data preparation. In particular, they don’t require feature\\nscaling or centering at all.\\nMaking Predictions \\n| \\n179'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 205}, page_content='A node’s samples attribute counts how many training instances it applies to. For\\nexample, 100 training instances have a petal length greater than 2.45 cm (depth 1,\\nright), among which 54 have a petal width smaller than 1.75 cm (depth 2, left). A\\nnode’s value attribute tells you how many training instances of each class this node\\napplies to: for example, the bottom-right node applies to 0 Iris-Setosa, 1 Iris-\\nVersicolor, and 45 Iris-Virginica. Finally, a node’s gini attribute measures its impur‐\\nity: a node is “pure” (gini=0) if all training instances it applies to belong to the same\\nclass. For example, since the depth-1 left node applies only to Iris-Setosa training\\ninstances, it is pure and its gini score is 0. Equation 6-1 shows how the training algo‐\\nrithm computes the gini score Gi of the ith node. For example, the depth-2 left node\\nhas a gini score equal to 1 – (0/54)2 – (49/54)2 – (5/54)2 ≈ 0.168. Another impurity\\nmeasure is discussed shortly.\\nEquation 6-1. Gini impurity\\nGi = 1 −∑\\nk = 1\\nn\\npi, k\\n2\\n• pi,k is the ratio of class k instances among the training instances in the ith node.\\nScikit-Learn uses the CART algorithm, which produces only binary\\ntrees: nonleaf nodes always have two children (i.e., questions only\\nhave yes/no answers). However, other algorithms such as ID3 can\\nproduce Decision Trees with nodes that have more than two chil‐\\ndren.\\nFigure 6-2 shows this Decision Tree’s decision boundaries. The thick vertical line rep‐\\nresents the decision boundary of the root node (depth 0): petal length = 2.45 cm.\\nSince the left area is pure (only Iris-Setosa), it cannot be split any further. However,\\nthe right area is impure, so the depth-1 right node splits it at petal width = 1.75 cm\\n(represented by the dashed line). Since max_depth was set to 2, the Decision Tree\\nstops right there. However, if you set max_depth to 3, then the two depth-2 nodes\\nwould each add another decision boundary (represented by the dotted lines).\\n180 \\n| \\nChapter 6: Decision Trees'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 206}, page_content='Figure 6-2. Decision Tree decision boundaries\\nModel Interpretation: White Box Versus Black Box\\nAs you can see Decision Trees are fairly intuitive and their decisions are easy to inter‐\\npret. Such models are often called white box models. In contrast, as we will see, Ran‐\\ndom Forests or neural networks are generally considered black box models. They\\nmake great predictions, and you can easily check the calculations that they performed\\nto make these predictions; nevertheless, it is usually hard to explain in simple terms\\nwhy the predictions were made. For example, if a neural network says that a particu‐\\nlar person appears on a picture, it is hard to know what actually contributed to this\\nprediction: did the model recognize that person’s eyes? Her mouth? Her nose? Her\\nshoes? Or even the couch that she was sitting on? Conversely, Decision Trees provide\\nnice and simple classification rules that can even be applied manually if need be (e.g.,\\nfor flower classification).\\nEstimating Class Probabilities\\nA Decision Tree can also estimate the probability that an instance belongs to a partic‐\\nular class k: first it traverses the tree to find the leaf node for this instance, and then it\\nreturns the ratio of training instances of class k in this node. For example, suppose\\nyou have found a flower whose petals are 5 cm long and 1.5 cm wide. The corre‐\\nsponding leaf node is the depth-2 left node, so the Decision Tree should output the\\nfollowing probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54),\\nand 9.3% for Iris-Virginica (5/54). And of course if you ask it to predict the class, it\\nshould output Iris-Versicolor (class 1) since it has the highest probability. Let’s check\\nthis:\\n>>> tree_clf.predict_proba([[5, 1.5]])\\narray([[0.        , 0.90740741, 0.09259259]])\\nEstimating Class Probabilities \\n| \\n181'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 207}, page_content='>>> tree_clf.predict([[5, 1.5]])\\narray([1])\\nPerfect! Notice that the estimated probabilities would be identical anywhere else in\\nthe bottom-right rectangle of Figure 6-2—for example, if the petals were 6 cm long\\nand 1.5 cm wide (even though it seems obvious that it would most likely be an Iris-\\nVirginica in this case).\\nThe CART Training Algorithm\\nScikit-Learn uses the Classification And Regression Tree (CART) algorithm to train\\nDecision Trees (also called “growing” trees). The idea is really quite simple: the algo‐\\nrithm first splits the training set in two subsets using a single feature k and a thres‐\\nhold tk (e.g., “petal length ≤ 2.45 cm”). How does it choose k and tk? It searches for the\\npair (k, tk) that produces the purest subsets (weighted by their size). The cost function\\nthat the algorithm tries to minimize is given by Equation 6-2.\\nEquation 6-2. CART cost function for classification\\nJ k, tk =\\nmleft\\nm Gleft +\\nmright\\nm\\nGright\\nwhere\\nGleft/right measures the impurity of the left/right subset,\\nmleft/right is the number of instances in the left/right subset.\\nOnce it has successfully split the training set in two, it splits the subsets using the\\nsame logic, then the sub-subsets and so on, recursively. It stops recursing once it rea‐\\nches the maximum depth (defined by the max_depth hyperparameter), or if it cannot\\nfind a split that will reduce impurity. A few other hyperparameters (described in a\\nmoment) control additional stopping conditions (min_samples_split, min_sam\\nples_leaf, min_weight_fraction_leaf, and max_leaf_nodes).\\n182 \\n| \\nChapter 6: Decision Trees'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 208}, page_content='2 P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\\nbe verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\\nin polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐\\ntion is whether or not P = NP. If P ≠ NP (which seems likely), then no polynomial algorithm will ever be\\nfound for any NP-Complete problem (except perhaps on a quantum computer).\\n3 log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).\\n4 A reduction of entropy is often called an information gain.\\nAs you can see, the CART algorithm is a greedy algorithm: it greed‐\\nily searches for an optimum split at the top level, then repeats the\\nprocess at each level. It does not check whether or not the split will\\nlead to the lowest possible impurity several levels down. A greedy\\nalgorithm often produces a reasonably good solution, but it is not\\nguaranteed to be the optimal solution.\\nUnfortunately, finding the optimal tree is known to be an NP-\\nComplete problem:2 it requires O(exp(m)) time, making the prob‐\\nlem intractable even for fairly small training sets. This is why we\\nmust settle for a “reasonably good” solution.\\nComputational Complexity\\nMaking predictions requires traversing the Decision Tree from the root to a leaf.\\nDecision Trees are generally approximately balanced, so traversing the Decision Tree\\nrequires going through roughly O(log2(m)) nodes.3 Since each node only requires\\nchecking the value of one feature, the overall prediction complexity is just O(log2(m)),\\nindependent of the number of features. So predictions are very fast, even when deal‐\\ning with large training sets.\\nHowever, the training algorithm compares all features (or less if max_features is set)\\non all samples at each node. This results in a training complexity of O(n × m log(m)).\\nFor small training sets (less than a few thousand instances), Scikit-Learn can speed up\\ntraining by presorting the data (set presort=True), but this slows down training con‐\\nsiderably for larger training sets.\\nGini Impurity or Entropy?\\nBy default, the Gini impurity measure is used, but you can select the entropy impurity\\nmeasure instead by setting the criterion hyperparameter to \"entropy\". The concept\\nof entropy originated in thermodynamics as a measure of molecular disorder:\\nentropy approaches zero when molecules are still and well ordered. It later spread to a\\nwide variety of domains, including Shannon’s information theory, where it measures\\nthe average information content of a message:4 entropy is zero when all messages are\\nidentical. In Machine Learning, it is frequently used as an impurity measure: a set’s\\nComputational Complexity \\n| \\n183'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 209}, page_content='5 See Sebastian Raschka’s interesting analysis for more details.\\nentropy is zero when it contains instances of only one class. Equation 6-3 shows the\\ndefinition of the entropy of the ith node. For example, the depth-2 left node in\\nFigure 6-1 has an entropy equal to −49\\n54 log2\\n49\\n54 −5\\n54 log2\\n5\\n54  ≈ 0.445.\\nEquation 6-3. Entropy\\nHi = −\\n∑\\nk = 1\\npi, k ≠0\\nn\\npi, k log2 pi, k\\nSo should you use Gini impurity or entropy? The truth is, most of the time it does not\\nmake a big difference: they lead to similar trees. Gini impurity is slightly faster to\\ncompute, so it is a good default. However, when they differ, Gini impurity tends to\\nisolate the most frequent class in its own branch of the tree, while entropy tends to\\nproduce slightly more balanced trees.5\\nRegularization Hyperparameters\\nDecision Trees make very few assumptions about the training data (as opposed to lin‐\\near models, which obviously assume that the data is linear, for example). If left\\nunconstrained, the tree structure will adapt itself to the training data, fitting it very\\nclosely, and most likely overfitting it. Such a model is often called a nonparametric\\nmodel, not because it does not have any parameters (it often has a lot) but because the\\nnumber of parameters is not determined prior to training, so the model structure is\\nfree to stick closely to the data. In contrast, a parametric model such as a linear model\\nhas a predetermined number of parameters, so its degree of freedom is limited,\\nreducing the risk of overfitting (but increasing the risk of underfitting).\\nTo avoid overfitting the training data, you need to restrict the Decision Tree’s freedom\\nduring training. As you know by now, this is called regularization. The regularization\\nhyperparameters depend on the algorithm used, but generally you can at least restrict\\nthe maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the\\nmax_depth hyperparameter (the default value is None, which means unlimited).\\nReducing max_depth will regularize the model and thus reduce the risk of overfitting.\\nThe DecisionTreeClassifier class has a few other parameters that similarly restrict\\nthe shape of the Decision Tree: min_samples_split (the minimum number of sam‐\\nples a node must have before it can be split), min_samples_leaf (the minimum num‐\\nber of samples a leaf node must have), min_weight_fraction_leaf (same as\\nmin_samples_leaf but expressed as a fraction of the total number of weighted\\n184 \\n| \\nChapter 6: Decision Trees'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 210}, page_content='instances), max_leaf_nodes (maximum number of leaf nodes), and max_features\\n(maximum number of features that are evaluated for splitting at each node). Increas‐\\ning min_* hyperparameters or reducing max_* hyperparameters will regularize the\\nmodel.\\nOther algorithms work by first training the Decision Tree without\\nrestrictions, then pruning (deleting) unnecessary nodes. A node\\nwhose children are all leaf nodes is considered unnecessary if the\\npurity improvement it provides is not statistically significant. Stan‐\\ndard statistical tests, such as the χ2 test, are used to estimate the\\nprobability that the improvement is purely the result of chance\\n(which is called the null hypothesis). If this probability, called the p-\\nvalue, is higher than a given threshold (typically 5%, controlled by\\na hyperparameter), then the node is considered unnecessary and its\\nchildren are deleted. The pruning continues until all unnecessary\\nnodes have been pruned.\\nFigure 6-3 shows two Decision Trees trained on the moons dataset (introduced in\\nChapter 5). On the left, the Decision Tree is trained with the default hyperparameters\\n(i.e., no restrictions), and on the right the Decision Tree is trained with min_sam\\nples_leaf=4. It is quite obvious that the model on the left is overfitting, and the\\nmodel on the right will probably generalize better.\\nFigure 6-3. Regularization using min_samples_leaf\\nRegression\\nDecision Trees are also capable of performing regression tasks. Let’s build a regres‐\\nsion tree using Scikit-Learn’s DecisionTreeRegressor class, training it on a noisy\\nquadratic dataset with max_depth=2:\\nfrom sklearn.tree import DecisionTreeRegressor\\nRegression \\n| \\n185'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 211}, page_content='tree_reg = DecisionTreeRegressor(max_depth=2)\\ntree_reg.fit(X, y)\\nThe resulting tree is represented on Figure 6-4.\\nFigure 6-4. A Decision Tree for regression\\nThis tree looks very similar to the classification tree you built earlier. The main differ‐\\nence is that instead of predicting a class in each node, it predicts a value. For example,\\nsuppose you want to make a prediction for a new instance with x1 = 0.6. You traverse\\nthe tree starting at the root, and you eventually reach the leaf node that predicts\\nvalue=0.1106. This prediction is simply the average target value of the 110 training\\ninstances associated to this leaf node. This prediction results in a Mean Squared Error\\n(MSE) equal to 0.0151 over these 110 instances.\\nThis model’s predictions are represented on the left of Figure 6-5. If you set\\nmax_depth=3, you get the predictions represented on the right. Notice how the pre‐\\ndicted value for each region is always the average target value of the instances in that\\nregion. The algorithm splits each region in a way that makes most training instances\\nas close as possible to that predicted value.\\n186 \\n| \\nChapter 6: Decision Trees'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 212}, page_content='Figure 6-5. Predictions of two Decision Tree regression models\\nThe CART algorithm works mostly the same way as earlier, except that instead of try‐\\ning to split the training set in a way that minimizes impurity, it now tries to split the\\ntraining set in a way that minimizes the MSE. Equation 6-4 shows the cost function\\nthat the algorithm tries to minimize.\\nEquation 6-4. CART cost function for regression\\nJ k, tk =\\nmleft\\nm MSEleft +\\nmright\\nm\\nMSEright\\nwhere\\nMSEnode =\\n∑\\ni ∈node\\nynode −y i 2\\nynode =\\n1\\nmnode\\n∑\\ni ∈node\\ny i\\nJust like for classification tasks, Decision Trees are prone to overfitting when dealing\\nwith regression tasks. Without any regularization (i.e., using the default hyperpara‐\\nmeters), you get the predictions on the left of Figure 6-6. It is obviously overfitting\\nthe training set very badly. Just setting min_samples_leaf=10 results in a much more\\nreasonable model, represented on the right of Figure 6-6.\\nFigure 6-6. Regularizing a Decision Tree regressor\\nRegression \\n| \\n187'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 213}, page_content='6 It randomly selects the set of features to evaluate at each node.\\nInstability\\nHopefully by now you are convinced that Decision Trees have a lot going for them:\\nthey are simple to understand and interpret, easy to use, versatile, and powerful.\\nHowever they do have a few limitations. First, as you may have noticed, Decision\\nTrees love orthogonal decision boundaries (all splits are perpendicular to an axis),\\nwhich makes them sensitive to training set rotation. For example, Figure 6-7 shows a\\nsimple linearly separable dataset: on the left, a Decision Tree can split it easily, while\\non the right, after the dataset is rotated by 45°, the decision boundary looks unneces‐\\nsarily convoluted. Although both Decision Trees fit the training set perfectly, it is very\\nlikely that the model on the right will not generalize well. One way to limit this prob‐\\nlem is to use PCA (see Chapter 8), which often results in a better orientation of the\\ntraining data.\\nFigure 6-7. Sensitivity to training set rotation\\nMore generally, the main issue with Decision Trees is that they are very sensitive to\\nsmall variations in the training data. For example, if you just remove the widest Iris-\\nVersicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide)\\nand train a new Decision Tree, you may get the model represented in Figure 6-8. As\\nyou can see, it looks very different from the previous Decision Tree (Figure 6-2).\\nActually, since the training algorithm used by Scikit-Learn is stochastic6 you may\\nget very different models even on the same training data (unless you set the\\nrandom_state hyperparameter).\\n188 \\n| \\nChapter 6: Decision Trees'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 214}, page_content='Figure 6-8. Sensitivity to training set details\\nRandom Forests can limit this instability by averaging predictions over many trees, as\\nwe will see in the next chapter.\\nExercises\\n1. What is the approximate depth of a Decision Tree trained (without restrictions)\\non a training set with 1 million instances?\\n2. Is a node’s Gini impurity generally lower or greater than its parent’s? Is it gener‐\\nally lower/greater, or always lower/greater?\\n3. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing\\nmax_depth?\\n4. If a Decision Tree is underfitting the training set, is it a good idea to try scaling\\nthe input features?\\n5. If it takes one hour to train a Decision Tree on a training set containing 1 million\\ninstances, roughly how much time will it take to train another Decision Tree on a\\ntraining set containing 10 million instances?\\n6. If your training set contains 100,000 instances, will setting presort=True speed\\nup training?\\n7. Train and fine-tune a Decision Tree for the moons dataset.\\na. Generate a moons dataset using make_moons(n_samples=10000, noise=0.4).\\nb. Split it into a training set and a test set using train_test_split().\\nExercises \\n| \\n189'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 215}, page_content='c. Use grid search with cross-validation (with the help of the GridSearchCV\\nclass) to find good hyperparameter values for a DecisionTreeClassifier. \\nHint: try various values for max_leaf_nodes.\\nd. Train it on the full training set using these hyperparameters, and measure\\nyour model’s performance on the test set. You should get roughly 85% to 87%\\naccuracy.\\n8. Grow a forest.\\na. Continuing the previous exercise, generate 1,000 subsets of the training set,\\neach containing 100 instances selected randomly. Hint: you can use Scikit-\\nLearn’s ShuffleSplit class for this.\\nb. Train one Decision Tree on each subset, using the best hyperparameter values\\nfound above. Evaluate these 1,000 Decision Trees on the test set. Since they\\nwere trained on smaller sets, these Decision Trees will likely perform worse\\nthan the first Decision Tree, achieving only about 80% accuracy.\\nc. Now comes the magic. For each test set instance, generate the predictions of\\nthe 1,000 Decision Trees, and keep only the most frequent prediction (you can\\nuse SciPy’s mode() function for this). This gives you majority-vote predictions\\nover the test set.\\nd. Evaluate these predictions on the test set: you should obtain a slightly higher\\naccuracy than your first model (about 0.5 to 1.5% higher). Congratulations,\\nyou have trained a Random Forest classifier!\\nSolutions to these exercises are available in ???.\\n190 \\n| \\nChapter 6: Decision Trees'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 216}, page_content='CHAPTER 7\\nEnsemble Learning and Random Forests\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 7 in the final\\nrelease of the book.\\nSuppose you ask a complex question to thousands of random people, then aggregate\\ntheir answers. In many cases you will find that this aggregated answer is better than\\nan expert’s answer. This is called the wisdom of the crowd. Similarly, if you aggregate\\nthe predictions of a group of predictors (such as classifiers or regressors), you will\\noften get better predictions than with the best individual predictor. A group of pre‐\\ndictors is called an ensemble; thus, this technique is called Ensemble Learning, and an\\nEnsemble Learning algorithm is called an Ensemble method.\\nFor example, you can train a group of Decision Tree classifiers, each on a different\\nrandom subset of the training set. To make predictions, you just obtain the predic‐\\ntions of all individual trees, then predict the class that gets the most votes (see the last\\nexercise in Chapter 6). Such an ensemble of Decision Trees is called a Random Forest, \\nand despite its simplicity, this is one of the most powerful Machine Learning algo‐\\nrithms available today.\\nMoreover, as we discussed in Chapter 2, you will often use Ensemble methods near\\nthe end of a project, once you have already built a few good predictors, to combine\\nthem into an even better predictor. In fact, the winning solutions in Machine Learn‐\\ning competitions often involve several Ensemble methods (most famously in the Net‐\\nflix Prize competition).\\nIn this chapter we will discuss the most popular Ensemble methods, including bag‐\\nging, boosting, stacking, and a few others. We will also explore Random Forests.\\n191'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 217}, page_content='Voting Classifiers\\nSuppose you have trained a few classifiers, each one achieving about 80% accuracy.\\nYou may have a Logistic Regression classifier, an SVM classifier, a Random Forest\\nclassifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1).\\nFigure 7-1. Training diverse classifiers\\nA very simple way to create an even better classifier is to aggregate the predictions of\\neach classifier and predict the class that gets the most votes. This majority-vote classi‐\\nfier is called a hard voting classifier (see Figure 7-2).\\nFigure 7-2. Hard voting classifier predictions\\n192 \\n| \\nChapter 7: Ensemble Learning and Random Forests'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 218}, page_content='Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the\\nbest classifier in the ensemble. In fact, even if each classifier is a weak learner (mean‐\\ning it does only slightly better than random guessing), the ensemble can still be a\\nstrong learner (achieving high accuracy), provided there are a sufficient number of\\nweak learners and they are sufficiently diverse.\\nHow is this possible? The following analogy can help shed some light on this mystery.\\nSuppose you have a slightly biased coin that has a 51% chance of coming up heads,\\nand 49% chance of coming up tails. If you toss it 1,000 times, you will generally get\\nmore or less 510 heads and 490 tails, and hence a majority of heads. If you do the\\nmath, you will find that the probability of obtaining a majority of heads after 1,000\\ntosses is close to 75%. The more you toss the coin, the higher the probability (e.g.,\\nwith 10,000 tosses, the probability climbs over 97%). This is due to the law of large\\nnumbers: as you keep tossing the coin, the ratio of heads gets closer and closer to the\\nprobability of heads (51%). Figure 7-3 shows 10 series of biased coin tosses. You can\\nsee that as the number of tosses increases, the ratio of heads approaches 51%. Eventu‐\\nally all 10 series end up so close to 51% that they are consistently above 50%.\\nFigure 7-3. The law of large numbers\\nSimilarly, suppose you build an ensemble containing 1,000 classifiers that are individ‐\\nually correct only 51% of the time (barely better than random guessing). If you pre‐\\ndict the majority voted class, you can hope for up to 75% accuracy! However, this is\\nonly true if all classifiers are perfectly independent, making uncorrelated errors,\\nwhich is clearly not the case since they are trained on the same data. They are likely to\\nmake the same types of errors, so there will be many majority votes for the wrong\\nclass, reducing the ensemble’s accuracy.\\nVoting Classifiers \\n| \\n193'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 219}, page_content='Ensemble methods work best when the predictors are as independ‐\\nent from one another as possible. One way to get diverse classifiers\\nis to train them using very different algorithms. This increases the\\nchance that they will make very different types of errors, improving\\nthe ensemble’s accuracy.\\nThe following code creates and trains a voting classifier in Scikit-Learn, composed of\\nthree diverse classifiers (the training set is the moons dataset, introduced in Chap‐\\nter 5):\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import VotingClassifier\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC\\nlog_clf = LogisticRegression()\\nrnd_clf = RandomForestClassifier()\\nsvm_clf = SVC()\\nvoting_clf = VotingClassifier(\\n    estimators=[(\\'lr\\', log_clf), (\\'rf\\', rnd_clf), (\\'svc\\', svm_clf)],\\n    voting=\\'hard\\')\\nvoting_clf.fit(X_train, y_train)\\nLet’s look at each classifier’s accuracy on the test set:\\n>>> from sklearn.metrics import accuracy_score\\n>>> for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\\n...     clf.fit(X_train, y_train)\\n...     y_pred = clf.predict(X_test)\\n...     print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\\n...\\nLogisticRegression 0.864\\nRandomForestClassifier 0.896\\nSVC 0.888\\nVotingClassifier 0.904\\nThere you have it! The voting classifier slightly outperforms all the individual classifi‐\\ners.\\nIf all classifiers are able to estimate class probabilities (i.e., they have a pre\\ndict_proba() method), then you can tell Scikit-Learn to predict the class with the\\nhighest class probability, averaged over all the individual classifiers. This is called soft\\nvoting. It often achieves higher performance than hard voting because it gives more\\nweight to highly confident votes. All you need to do is replace voting=\"hard\" with\\nvoting=\"soft\" and ensure that all classifiers can estimate class probabilities. This is\\nnot the case of the SVC class by default, so you need to set its probability hyperpara‐\\nmeter to True (this will make the SVC class use cross-validation to estimate class prob‐\\nabilities, slowing down training, and it will add a predict_proba() method). If you\\n194 \\n| \\nChapter 7: Ensemble Learning and Random Forests'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 220}, page_content='1 “Bagging Predictors,” L. Breiman (1996).\\n2 In statistics, resampling with replacement is called bootstrapping.\\n3 “Pasting small votes for classification in large databases and on-line,” L. Breiman (1999).\\nmodify the preceding code to use soft voting, you will find that the voting classifier\\nachieves over 91.2% accuracy!\\nBagging and Pasting\\nOne way to get a diverse set of classifiers is to use very different training algorithms,\\nas just discussed. Another approach is to use the same training algorithm for every\\npredictor, but to train them on different random subsets of the training set. When\\nsampling is performed with replacement, this method is called bagging1 (short for\\nbootstrap aggregating2). When sampling is performed without replacement, it is called\\npasting.3\\nIn other words, both bagging and pasting allow training instances to be sampled sev‐\\neral times across multiple predictors, but only bagging allows training instances to be\\nsampled several times for the same predictor. This sampling and training process is\\nrepresented in Figure 7-4.\\nFigure 7-4. Pasting/bagging training set sampling and training\\nOnce all predictors are trained, the ensemble can make a prediction for a new\\ninstance by simply aggregating the predictions of all predictors. The aggregation\\nfunction is typically the statistical mode (i.e., the most frequent prediction, just like a\\nhard voting classifier) for classification, or the average for regression. Each individual\\nBagging and Pasting \\n| \\n195'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 221}, page_content='4 Bias and variance were introduced in Chapter 4.\\n5 max_samples can alternatively be set to a float between 0.0 and 1.0, in which case the max number of instances\\nto sample is equal to the size of the training set times max_samples.\\npredictor has a higher bias than if it were trained on the original training set, but\\naggregation reduces both bias and variance.4 Generally, the net result is that the\\nensemble has a similar bias but a lower variance than a single predictor trained on the\\noriginal training set.\\nAs you can see in Figure 7-4, predictors can all be trained in parallel, via different\\nCPU cores or even different servers. Similarly, predictions can be made in parallel.\\nThis is one of the reasons why bagging and pasting are such popular methods: they\\nscale very well.\\nBagging and Pasting in Scikit-Learn\\nScikit-Learn offers a simple API for both bagging and pasting with the BaggingClas\\nsifier class (or BaggingRegressor for regression). The following code trains an\\nensemble of 500 Decision Tree classifiers,5 each trained on 100 training instances ran‐\\ndomly sampled from the training set with replacement (this is an example of bagging,\\nbut if you want to use pasting instead, just set bootstrap=False). The n_jobs param‐\\neter tells Scikit-Learn the number of CPU cores to use for training and predictions\\n(–1 tells Scikit-Learn to use all available cores):\\nfrom sklearn.ensemble import BaggingClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\nbag_clf = BaggingClassifier(\\n    DecisionTreeClassifier(), n_estimators=500,\\n    max_samples=100, bootstrap=True, n_jobs=-1)\\nbag_clf.fit(X_train, y_train)\\ny_pred = bag_clf.predict(X_test)\\nThe BaggingClassifier automatically performs soft voting\\ninstead of hard voting if the base classifier can estimate class proba‐\\nbilities (i.e., if it has a predict_proba() method), which is the case\\nwith Decision Trees classifiers.\\nFigure 7-5 compares the decision boundary of a single Decision Tree with the deci‐\\nsion boundary of a bagging ensemble of 500 trees (from the preceding code), both\\ntrained on the moons dataset. As you can see, the ensemble’s predictions will likely\\ngeneralize much better than the single Decision Tree’s predictions: the ensemble has a\\ncomparable bias but a smaller variance (it makes roughly the same number of errors\\non the training set, but the decision boundary is less irregular).\\n196 \\n| \\nChapter 7: Ensemble Learning and Random Forests'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 222}, page_content='6 As m grows, this ratio approaches 1 – exp(–1) ≈ 63.212%.\\nFigure 7-5. A single Decision Tree versus a bagging ensemble of 500 trees\\nBootstrapping introduces a bit more diversity in the subsets that each predictor is\\ntrained on, so bagging ends up with a slightly higher bias than pasting, but this also\\nmeans that predictors end up being less correlated so the ensemble’s variance is\\nreduced. Overall, bagging often results in better models, which explains why it is gen‐\\nerally preferred. However, if you have spare time and CPU power you can use cross-\\nvalidation to evaluate both bagging and pasting and select the one that works best.\\nOut-of-Bag Evaluation\\nWith bagging, some instances may be sampled several times for any given predictor,\\nwhile others may not be sampled at all. By default a BaggingClassifier samples m\\ntraining instances with replacement (bootstrap=True), where m is the size of the\\ntraining set. This means that only about 63% of the training instances are sampled on\\naverage for each predictor.6 The remaining 37% of the training instances that are not\\nsampled are called out-of-bag (oob) instances. Note that they are not the same 37%\\nfor all predictors.\\nSince a predictor never sees the oob instances during training, it can be evaluated on\\nthese instances, without the need for a separate validation set. You can evaluate the\\nensemble itself by averaging out the oob evaluations of each predictor.\\nIn Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to\\nrequest an automatic oob evaluation after training. The following code demonstrates\\nthis. The resulting evaluation score is available through the oob_score_ variable:\\n>>> bag_clf = BaggingClassifier(\\n...     DecisionTreeClassifier(), n_estimators=500,\\n...     bootstrap=True, n_jobs=-1, oob_score=True)\\n...\\n>>> bag_clf.fit(X_train, y_train)\\nBagging and Pasting \\n| \\n197'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 223}, page_content='7 “Ensembles on Random Patches,” G. Louppe and P. Geurts (2012).\\n8 “The random subspace method for constructing decision forests,” Tin Kam Ho (1998).\\n>>> bag_clf.oob_score_\\n0.90133333333333332\\nAccording to this oob evaluation, this BaggingClassifier is likely to achieve about\\n90.1% accuracy on the test set. Let’s verify this:\\n>>> from sklearn.metrics import accuracy_score\\n>>> y_pred = bag_clf.predict(X_test)\\n>>> accuracy_score(y_test, y_pred)\\n0.91200000000000003\\nWe get 91.2% accuracy on the test set—close enough!\\nThe oob decision function for each training instance is also available through the\\noob_decision_function_ variable. In this case (since the base estimator has a pre\\ndict_proba() method) the decision function returns the class probabilities for each\\ntraining instance. For example, the oob evaluation estimates that the first training\\ninstance has a 68.25% probability of belonging to the positive class (and 31.75% of\\nbelonging to the negative class):\\n>>> bag_clf.oob_decision_function_\\narray([[0.31746032, 0.68253968],\\n       [0.34117647, 0.65882353],\\n       [1.        , 0.        ],\\n       ...\\n       [1.        , 0.        ],\\n       [0.03108808, 0.96891192],\\n       [0.57291667, 0.42708333]])\\nRandom Patches and Random Subspaces\\nThe BaggingClassifier class supports sampling the features as well. This is con‐\\ntrolled by two hyperparameters: max_features and bootstrap_features. They work\\nthe same way as max_samples and bootstrap, but for feature sampling instead of\\ninstance sampling. Thus, each predictor will be trained on a random subset of the\\ninput features.\\nThis is particularly useful when you are dealing with high-dimensional inputs (such\\nas images). Sampling both training instances and features is called the Random\\nPatches method.7 Keeping all training instances (i.e., bootstrap=False and max_sam\\nples=1.0) but sampling features (i.e., bootstrap_features=True and/or max_fea\\ntures smaller than 1.0) is called the Random Subspaces method.8\\n198 \\n| \\nChapter 7: Ensemble Learning and Random Forests'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 224}, page_content='9 “Random Decision Forests,” T. Ho (1995).\\n10 The BaggingClassifier class remains useful if you want a bag of something other than Decision Trees.\\n11 There are a few notable exceptions: splitter is absent (forced to \"random\"), presort is absent (forced to\\nFalse), max_samples is absent (forced to 1.0), and base_estimator is absent (forced to DecisionTreeClassi\\nfier with the provided hyperparameters).\\nSampling features results in even more predictor diversity, trading a bit more bias for\\na lower variance.\\nRandom Forests\\nAs we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally\\ntrained via the bagging method (or sometimes pasting), typically with max_samples\\nset to the size of the training set. Instead of building a BaggingClassifier and pass‐\\ning it a DecisionTreeClassifier, you can instead use the RandomForestClassifier\\nclass, which is more convenient and optimized for Decision Trees10 (similarly, there is\\na RandomForestRegressor class for regression tasks). The following code trains a\\nRandom Forest classifier with 500 trees (each limited to maximum 16 nodes), using\\nall available CPU cores:\\nfrom sklearn.ensemble import RandomForestClassifier\\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\\nrnd_clf.fit(X_train, y_train)\\ny_pred_rf = rnd_clf.predict(X_test)\\nWith a few exceptions, a RandomForestClassifier has all the hyperparameters of a\\nDecisionTreeClassifier (to control how trees are grown), plus all the hyperpara‐\\nmeters of a BaggingClassifier to control the ensemble itself.11\\nThe Random Forest algorithm introduces extra randomness when growing trees;\\ninstead of searching for the very best feature when splitting a node (see Chapter 6), it\\nsearches for the best feature among a random subset of features. This results in a\\ngreater tree diversity, which (once again) trades a higher bias for a lower variance,\\ngenerally yielding an overall better model. The following BaggingClassifier is\\nroughly equivalent to the previous RandomForestClassifier:\\nbag_clf = BaggingClassifier(\\n    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\\n    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)\\nRandom Forests \\n| \\n199'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 225}, page_content='12 “Extremely randomized trees,” P. Geurts, D. Ernst, L. Wehenkel (2005).\\nExtra-Trees\\nWhen you are growing a tree in a Random Forest, at each node only a random subset\\nof the features is considered for splitting (as discussed earlier). It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds (like regular Decision Trees do).\\nA forest of such extremely random trees is simply called an Extremely Randomized\\nTrees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a\\nlower variance. It also makes Extra-Trees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most time-consuming tasks of growing a tree.\\nYou can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\\nclass. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\\nTreesRegressor class has the same API as the RandomForestRegressor class.\\nIt is hard to tell in advance whether a RandomForestClassifier\\nwill perform better or worse than an ExtraTreesClassifier. Gen‐\\nerally, the only way to know is to try both and compare them using\\ncross-validation (and tuning the hyperparameters using grid\\nsearch).\\nFeature Importance\\nYet another great quality of Random Forests is that they make it easy to measure the \\nrelative importance of each feature. Scikit-Learn measures a feature’s importance by\\nlooking at how much the tree nodes that use that feature reduce impurity on average\\n(across all trees in the forest). More precisely, it is a weighted average, where each\\nnode’s weight is equal to the number of training samples that are associated with it\\n(see Chapter 6).\\nScikit-Learn computes this score automatically for each feature after training, then it\\nscales the results so that the sum of all importances is equal to 1. You can access the\\nresult using the feature_importances_ variable. For example, the following code\\ntrains a RandomForestClassifier on the iris dataset (introduced in Chapter 4) and\\noutputs each feature’s importance. It seems that the most important features are the\\npetal length (44%) and width (42%), while sepal length and width are rather unim‐\\nportant in comparison (11% and 2%, respectively).\\n200 \\n| \\nChapter 7: Ensemble Learning and Random Forests'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 226}, page_content='>>> from sklearn.datasets import load_iris\\n>>> iris = load_iris()\\n>>> rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\\n>>> rnd_clf.fit(iris[\"data\"], iris[\"target\"])\\n>>> for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\\n...     print(name, score)\\n...\\nsepal length (cm) 0.112492250999\\nsepal width (cm) 0.0231192882825\\npetal length (cm) 0.441030464364\\npetal width (cm) 0.423357996355\\nSimilarly, if you train a Random Forest classifier on the MNIST dataset (introduced\\nin Chapter 3) and plot each pixel’s importance, you get the image represented in\\nFigure 7-6.\\nFigure 7-6. MNIST pixel importance (according to a Random Forest classifier)\\nRandom Forests are very handy to get a quick understanding of what features\\nactually matter, in particular if you need to perform feature selection.\\nBoosting\\nBoosting (originally called hypothesis boosting) refers to any Ensemble method that\\ncan combine several weak learners into a strong learner. The general idea of most\\nboosting methods is to train predictors sequentially, each trying to correct its prede‐\\ncessor. There are many boosting methods available, but by far the most popular are\\nBoosting \\n| \\n201'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 227}, page_content='13 “A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting,” Yoav Freund,\\nRobert E. Schapire (1997).\\n14 This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost, because they\\nare slow and tend to be unstable with AdaBoost.\\nAdaBoost13 (short for Adaptive Boosting) and Gradient Boosting. Let’s start with Ada‐\\nBoost.\\nAdaBoost\\nOne way for a new predictor to correct its predecessor is to pay a bit more attention\\nto the training instances that the predecessor underfitted. This results in new predic‐\\ntors focusing more and more on the hard cases. This is the technique used by Ada‐\\nBoost.\\nFor example, to build an AdaBoost classifier, a first base classifier (such as a Decision\\nTree) is trained and used to make predictions on the training set. The relative weight\\nof misclassified training instances is then increased. A second classifier is trained\\nusing the updated weights and again it makes predictions on the training set, weights\\nare updated, and so on (see Figure 7-7).\\nFigure 7-7. AdaBoost sequential training with instance weight updates\\nFigure 7-8 shows the decision boundaries of five consecutive predictors on the\\nmoons dataset (in this example, each predictor is a highly regularized SVM classifier\\nwith an RBF kernel14). The first classifier gets many instances wrong, so their weights\\n202 \\n| \\nChapter 7: Ensemble Learning and Random Forests'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 228}, page_content='get boosted. The second classifier therefore does a better job on these instances, and\\nso on. The plot on the right represents the same sequence of predictors except that\\nthe learning rate is halved (i.e., the misclassified instance weights are boosted half as\\nmuch at every iteration). As you can see, this sequential learning technique has some\\nsimilarities with Gradient Descent, except that instead of tweaking a single predictor’s\\nparameters to minimize a cost function, AdaBoost adds predictors to the ensemble,\\ngradually making it better.\\nFigure 7-8. Decision boundaries of consecutive predictors\\nOnce all predictors are trained, the ensemble makes predictions very much like bag‐\\nging or pasting, except that predictors have different weights depending on their\\noverall accuracy on the weighted training set.\\nThere is one important drawback to this sequential learning techni‐\\nque: it cannot be parallelized (or only partially), since each predic‐\\ntor can only be trained after the previous predictor has been\\ntrained and evaluated. As a result, it does not scale as well as bag‐\\nging or pasting.\\nLet’s take a closer look at the AdaBoost algorithm. Each instance weight w(i) is initially\\nset to 1\\nm. A first predictor is trained and its weighted error rate r1 is computed on the\\ntraining set; see Equation 7-1.\\nEquation 7-1. Weighted error rate of the jth predictor\\nrj =\\n∑\\ni = 1\\ny j\\ni ≠y i\\nm\\nw i\\n∑\\ni = 1\\nm\\nw i\\nwhere y j\\ni is the jth predictor’s prediction for the ith instance.\\nBoosting \\n| \\n203'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 229}, page_content='15 The original AdaBoost algorithm does not use a learning rate hyperparameter.\\nThe predictor’s weight αj is then computed using Equation 7-2, where η is the learn‐\\ning rate hyperparameter (defaults to 1).15 The more accurate the predictor is, the\\nhigher its weight will be. If it is just guessing randomly, then its weight will be close to\\nzero. However, if it is most often wrong (i.e., less accurate than random guessing),\\nthen its weight will be negative.\\nEquation 7-2. Predictor weight\\nαj = η log\\n1 −rj\\nrj\\nNext the instance weights are updated using Equation 7-3: the misclassified instances\\nare boosted.\\nEquation 7-3. Weight update rule\\nfor i = 1, 2, ⋯, m\\nw i\\nw i\\nif yj\\ni = y i\\nw i exp αj if yj\\ni ≠y i\\nThen all the instance weights are normalized (i.e., divided by ∑i = 1\\nm\\nw i ).\\nFinally, a new predictor is trained using the updated weights, and the whole process is\\nrepeated (the new predictor’s weight is computed, the instance weights are updated,\\nthen another predictor is trained, and so on). The algorithm stops when the desired\\nnumber of predictors is reached, or when a perfect predictor is found.\\nTo make predictions, AdaBoost simply computes the predictions of all the predictors\\nand weighs them using the predictor weights αj. The predicted class is the one that\\nreceives the majority of weighted votes (see Equation 7-4).\\nEquation 7-4. AdaBoost predictions\\ny x = argmax\\nk\\n∑\\nj = 1\\ny j x = k\\nN\\nαj\\nwhere N is the number of predictors.\\n204 \\n| \\nChapter 7: Ensemble Learning and Random Forests'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 230}, page_content='16 For more details, see “Multi-Class AdaBoost,” J. Zhu et al. (2006).\\n17 First introduced in “Arcing the Edge,” L. Breiman (1997), and further developed in the paper “Greedy Func‐\\ntion Approximation: A Gradient Boosting Machine,” Jerome H. Friedman (1999).\\nScikit-Learn actually uses a multiclass version of AdaBoost called SAMME16 (which\\nstands for Stagewise Additive Modeling using a Multiclass Exponential loss function).\\nWhen there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the\\npredictors can estimate class probabilities (i.e., if they have a predict_proba()\\nmethod), Scikit-Learn can use a variant of SAMME called SAMME.R (the R stands\\nfor “Real”), which relies on class probabilities rather than predictions and generally\\nperforms better.\\nThe following code trains an AdaBoost classifier based on 200 Decision Stumps using\\nScikit-Learn’s AdaBoostClassifier class (as you might expect, there is also an Ada\\nBoostRegressor class). A Decision Stump is a Decision Tree with max_depth=1—in\\nother words, a tree composed of a single decision node plus two leaf nodes. This is\\nthe default base estimator for the AdaBoostClassifier class:\\nfrom sklearn.ensemble import AdaBoostClassifier\\nada_clf = AdaBoostClassifier(\\n    DecisionTreeClassifier(max_depth=1), n_estimators=200,\\n    algorithm=\"SAMME.R\", learning_rate=0.5)\\nada_clf.fit(X_train, y_train)\\nIf your AdaBoost ensemble is overfitting the training set, you can\\ntry reducing the number of estimators or more strongly regulariz‐\\ning the base estimator.\\nGradient Boosting\\nAnother very popular Boosting algorithm is Gradient Boosting.17 Just like AdaBoost,\\nGradient Boosting works by sequentially adding predictors to an ensemble, each one\\ncorrecting its predecessor. However, instead of tweaking the instance weights at every\\niteration like AdaBoost does, this method tries to fit the new predictor to the residual\\nerrors made by the previous predictor.\\nLet’s go through a simple regression example using Decision Trees as the base predic‐\\ntors (of course Gradient Boosting also works great with regression tasks). This is\\ncalled Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT). First, let’s\\nfit a DecisionTreeRegressor to the training set (for example, a noisy quadratic train‐\\ning set):\\nBoosting \\n| \\n205'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 231}, page_content='from sklearn.tree import DecisionTreeRegressor\\ntree_reg1 = DecisionTreeRegressor(max_depth=2)\\ntree_reg1.fit(X, y)\\nNow train a second DecisionTreeRegressor on the residual errors made by the first\\npredictor:\\ny2 = y - tree_reg1.predict(X)\\ntree_reg2 = DecisionTreeRegressor(max_depth=2)\\ntree_reg2.fit(X, y2)\\nThen we train a third regressor on the residual errors made by the second predictor:\\ny3 = y2 - tree_reg2.predict(X)\\ntree_reg3 = DecisionTreeRegressor(max_depth=2)\\ntree_reg3.fit(X, y3)\\nNow we have an ensemble containing three trees. It can make predictions on a new\\ninstance simply by adding up the predictions of all the trees:\\ny_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\\nFigure 7-9 represents the predictions of these three trees in the left column, and the\\nensemble’s predictions in the right column. In the first row, the ensemble has just one\\ntree, so its predictions are exactly the same as the first tree’s predictions. In the second\\nrow, a new tree is trained on the residual errors of the first tree. On the right you can\\nsee that the ensemble’s predictions are equal to the sum of the predictions of the first\\ntwo trees. Similarly, in the third row another tree is trained on the residual errors of\\nthe second tree. You can see that the ensemble’s predictions gradually get better as\\ntrees are added to the ensemble.\\nA simpler way to train GBRT ensembles is to use Scikit-Learn’s GradientBoostingRe\\ngressor class. Much like the RandomForestRegressor class, it has hyperparameters to\\ncontrol the growth of Decision Trees (e.g., max_depth, min_samples_leaf, and so on),\\nas well as hyperparameters to control the ensemble training, such as the number of\\ntrees (n_estimators). The following code creates the same ensemble as the previous\\none:\\nfrom sklearn.ensemble import GradientBoostingRegressor\\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\\ngbrt.fit(X, y)\\n206 \\n| \\nChapter 7: Ensemble Learning and Random Forests'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 232}, page_content='Figure 7-9. Gradient Boosting\\nThe learning_rate hyperparameter scales the contribution of each tree. If you set it\\nto a low value, such as 0.1, you will need more trees in the ensemble to fit the train‐\\ning set, but the predictions will usually generalize better. This is a regularization tech‐\\nnique called shrinkage. Figure 7-10 shows two GBRT ensembles trained with a low\\nlearning rate: the one on the left does not have enough trees to fit the training set,\\nwhile the one on the right has too many trees and overfits the training set.\\nBoosting \\n| \\n207'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 233}, page_content='Figure 7-10. GBRT ensembles with not enough predictors (left) and too many (right)\\nIn order to find the optimal number of trees, you can use early stopping (see Chap‐\\nter 4). A simple way to implement this is to use the staged_predict() method: it\\nreturns an iterator over the predictions made by the ensemble at each stage of train‐\\ning (with one tree, two trees, etc.). The following code trains a GBRT ensemble with\\n120 trees, then measures the validation error at each stage of training to find the opti‐\\nmal number of trees, and finally trains another GBRT ensemble using the optimal\\nnumber of trees:\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import mean_squared_error\\nX_train, X_val, y_train, y_val = train_test_split(X, y)\\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\\ngbrt.fit(X_train, y_train)\\nerrors = [mean_squared_error(y_val, y_pred)\\n          for y_pred in gbrt.staged_predict(X_val)]\\nbst_n_estimators = np.argmin(errors)\\ngbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\\ngbrt_best.fit(X_train, y_train)\\nThe validation errors are represented on the left of Figure 7-11, and the best model’s\\npredictions are represented on the right.\\n208 \\n| \\nChapter 7: Ensemble Learning and Random Forests'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 234}, page_content='Figure 7-11. Tuning the number of trees using early stopping\\nIt is also possible to implement early stopping by actually stopping training early\\n(instead of training a large number of trees first and then looking back to find the\\noptimal number). You can do so by setting warm_start=True, which makes Scikit-\\nLearn keep existing trees when the fit() method is called, allowing incremental\\ntraining. The following code stops training when the validation error does not\\nimprove for five iterations in a row:\\ngbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\\nmin_val_error = float(\"inf\")\\nerror_going_up = 0\\nfor n_estimators in range(1, 120):\\n    gbrt.n_estimators = n_estimators\\n    gbrt.fit(X_train, y_train)\\n    y_pred = gbrt.predict(X_val)\\n    val_error = mean_squared_error(y_val, y_pred)\\n    if val_error < min_val_error:\\n        min_val_error = val_error\\n        error_going_up = 0\\n    else:\\n        error_going_up += 1\\n        if error_going_up == 5:\\n            break  # early stopping\\nThe GradientBoostingRegressor class also supports a subsample hyperparameter,\\nwhich specifies the fraction of training instances to be used for training each tree. For\\nexample, if subsample=0.25, then each tree is trained on 25% of the training instan‐\\nces, selected randomly. As you can probably guess by now, this trades a higher bias\\nfor a lower variance. It also speeds up training considerably. This technique is called\\nStochastic Gradient Boosting.\\nBoosting \\n| \\n209'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 235}, page_content='18 “Stacked Generalization,” D. Wolpert (1992).\\nIt is possible to use Gradient Boosting with other cost functions.\\nThis is controlled by the loss hyperparameter (see Scikit-Learn’s\\ndocumentation for more details).\\nIt is worth noting that an optimized implementation of Gradient Boosting is available\\nin the popular python library XGBoost, which stands for Extreme Gradient Boosting.\\nThis package was initially developed by Tianqi Chen as part of the Distributed (Deep)\\nMachine Learning Community (DMLC), and it aims at being extremely fast, scalable\\nand portable. In fact, XGBoost is often an important component of the winning\\nentries in ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:\\nimport xgboost\\nxgb_reg = xgboost.XGBRegressor()\\nxgb_reg.fit(X_train, y_train)\\ny_pred = xgb_reg.predict(X_val)\\nXGBoost also offers several nice features, such as automatically taking care of early\\nstopping:\\nxgb_reg.fit(X_train, y_train,\\n            eval_set=[(X_val, y_val)], early_stopping_rounds=2)\\ny_pred = xgb_reg.predict(X_val)\\nYou should definitely check it out!\\nStacking\\nThe last Ensemble method we will discuss in this chapter is called stacking (short for\\nstacked generalization).18 It is based on a simple idea: instead of using trivial functions\\n(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\\nwhy don’t we train a model to perform this aggregation? Figure 7-12 shows such an\\nensemble performing a regression task on a new instance. Each of the bottom three\\npredictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor \\n(called a blender, or a meta learner) takes these predictions as inputs and makes the\\nfinal prediction (3.0).\\n210 \\n| \\nChapter 7: Ensemble Learning and Random Forests'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 236}, page_content='19 Alternatively, it is possible to use out-of-fold predictions. In some contexts this is called stacking, while using a\\nhold-out set is called blending. However, for many people these terms are synonymous.\\nFigure 7-12. Aggregating predictions using a blending predictor\\nTo train the blender, a common approach is to use a hold-out set.19 Let’s see how it\\nworks. First, the training set is split in two subsets. The first subset is used to train the\\npredictors in the first layer (see Figure 7-13).\\nFigure 7-13. Training the first layer\\nNext, the first layer predictors are used to make predictions on the second (held-out)\\nset (see Figure 7-14). This ensures that the predictions are “clean,” since the predictors\\nnever saw these instances during training. Now for each instance in the hold-out set\\nStacking \\n| \\n211'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 237}, page_content='there are three predicted values. We can create a new training set using these predic‐\\nted values as input features (which makes this new training set three-dimensional),\\nand keeping the target values. The blender is trained on this new training set, so it\\nlearns to predict the target value given the first layer’s predictions.\\nFigure 7-14. Training the blender\\nIt is actually possible to train several different blenders this way (e.g., one using Lin‐\\near Regression, another using Random Forest Regression, and so on): we get a whole\\nlayer of blenders. The trick is to split the training set into three subsets: the first one is\\nused to train the first layer, the second one is used to create the training set used to\\ntrain the second layer (using predictions made by the predictors of the first layer),\\nand the third one is used to create the training set to train the third layer (using pre‐\\ndictions made by the predictors of the second layer). Once this is done, we can make\\na prediction for a new instance by going through each layer sequentially, as shown in\\nFigure 7-15.\\n212 \\n| \\nChapter 7: Ensemble Learning and Random Forests'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 238}, page_content='Figure 7-15. Predictions in a multilayer stacking ensemble\\nUnfortunately, Scikit-Learn does not support stacking directly, but it is not too hard\\nto roll out your own implementation (see the following exercises). Alternatively, you\\ncan use an open source implementation such as brew (available at https://github.com/\\nviisar/brew).\\nExercises\\n1. If you have trained five different models on the exact same training data, and\\nthey all achieve 95% precision, is there any chance that you can combine these\\nmodels to get better results? If so, how? If not, why?\\n2. What is the difference between hard and soft voting classifiers?\\n3. Is it possible to speed up training of a bagging ensemble by distributing it across\\nmultiple servers? What about pasting ensembles, boosting ensembles, random\\nforests, or stacking ensembles?\\n4. What is the benefit of out-of-bag evaluation?\\n5. What makes Extra-Trees more random than regular Random Forests? How can\\nthis extra randomness help? Are Extra-Trees slower or faster than regular Ran‐\\ndom Forests?\\n6. If your AdaBoost ensemble underfits the training data, what hyperparameters\\nshould you tweak and how?\\nExercises \\n| \\n213'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 239}, page_content='7. If your Gradient Boosting ensemble overfits the training set, should you increase\\nor decrease the learning rate?\\n8. Load the MNIST data (introduced in Chapter 3), and split it into a training set, a\\nvalidation set, and a test set (e.g., use 50,000 instances for training, 10,000 for val‐\\nidation, and 10,000 for testing). Then train various classifiers, such as a Random\\nForest classifier, an Extra-Trees classifier, and an SVM. Next, try to combine\\nthem into an ensemble that outperforms them all on the validation set, using a\\nsoft or hard voting classifier. Once you have found one, try it on the test set. How\\nmuch better does it perform compared to the individual classifiers?\\n9. Run the individual classifiers from the previous exercise to make predictions on\\nthe validation set, and create a new training set with the resulting predictions:\\neach training instance is a vector containing the set of predictions from all your\\nclassifiers for an image, and the target is the image’s class. Train a classifier on\\nthis new training set. Congratulations, you have just trained a blender, and\\ntogether with the classifiers they form a stacking ensemble! Now let’s evaluate the\\nensemble on the test set. For each image in the test set, make predictions with all\\nyour classifiers, then feed the predictions to the blender to get the ensemble’s pre‐\\ndictions. How does it compare to the voting classifier you trained earlier?\\nSolutions to these exercises are available in ???.\\n214 \\n| \\nChapter 7: Ensemble Learning and Random Forests'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 240}, page_content='CHAPTER 8\\nDimensionality Reduction\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 8 in the final\\nrelease of the book.\\nMany Machine Learning problems involve thousands or even millions of features for\\neach training instance. Not only does this make training extremely slow, it can also\\nmake it much harder to find a good solution, as we will see. This problem is often\\nreferred to as the curse of dimensionality.\\nFortunately, in real-world problems, it is often possible to reduce the number of fea‐\\ntures considerably, turning an intractable problem into a tractable one. For example,\\nconsider the MNIST images (introduced in Chapter 3): the pixels on the image bor‐\\nders are almost always white, so you could completely drop these pixels from the\\ntraining set without losing much information. Figure 7-6 confirms that these pixels\\nare utterly unimportant for the classification task. Moreover, two neighboring pixels\\nare often highly correlated: if you merge them into a single pixel (e.g., by taking the\\nmean of the two pixel intensities), you will not lose much information.\\n215'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 241}, page_content='1 Well, four dimensions if you count time, and a few more if you are a string theorist.\\nReducing dimensionality does lose some information (just like\\ncompressing an image to JPEG can degrade its quality), so even\\nthough it will speed up training, it may also make your system per‐\\nform slightly worse. It also makes your pipelines a bit more com‐\\nplex and thus harder to maintain. So you should first try to train\\nyour system with the original data before considering using dimen‐\\nsionality reduction if training is too slow. In some cases, however,\\nreducing the dimensionality of the training data may filter out\\nsome noise and unnecessary details and thus result in higher per‐\\nformance (but in general it won’t; it will just speed up training).\\nApart from speeding up training, dimensionality reduction is also extremely useful\\nfor data visualization (or DataViz). Reducing the number of dimensions down to two\\n(or three) makes it possible to plot a condensed view of a high-dimensional training\\nset on a graph and often gain some important insights by visually detecting patterns,\\nsuch as clusters. Moreover, DataViz is essential to communicate your conclusions to\\npeople who are not data scientists, in particular decision makers who will use your\\nresults.\\nIn this chapter we will discuss the curse of dimensionality and get a sense of what\\ngoes on in high-dimensional space. Then, we will present the two main approaches to\\ndimensionality reduction (projection and Manifold Learning), and we will go\\nthrough three of the most popular dimensionality reduction techniques: PCA, Kernel\\nPCA, and LLE.\\nThe Curse of Dimensionality\\nWe are so used to living in three dimensions1 that our intuition fails us when we try\\nto imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to\\npicture in our mind (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a\\n1,000-dimensional space.\\n216 \\n| \\nChapter 8: Dimensionality Reduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 242}, page_content='2 Watch a rotating tesseract projected into 3D space at https://homl.info/30. Image by Wikipedia user Nerd‐\\nBoy1392 (Creative Commons BY-SA 3.0). Reproduced from https://en.wikipedia.org/wiki/Tesseract.\\n3 Fun fact: anyone you know is probably an extremist in at least one dimension (e.g., how much sugar they put\\nin their coffee), if you consider enough dimensions.\\nFigure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2\\nIt turns out that many things behave very differently in high-dimensional space. For\\nexample, if you pick a random point in a unit square (a 1 × 1 square), it will have only\\nabout a 0.4% chance of being located less than 0.001 from a border (in other words, it\\nis very unlikely that a random point will be “extreme” along any dimension). But in a\\n10,000-dimensional unit hypercube (a 1 × 1 × ⋯ × 1 cube, with ten thousand 1s), this\\nprobability is greater than 99.999999%. Most points in a high-dimensional hypercube\\nare very close to the border.3\\nHere is a more troublesome difference: if you pick two points randomly in a unit\\nsquare, the distance between these two points will be, on average, roughly 0.52. If you\\npick two random points in a unit 3D cube, the average distance will be roughly 0.66.\\nBut what about two points picked randomly in a 1,000,000-dimensional hypercube?\\nWell, the average distance, believe it or not, will be about 408.25 (roughly\\n1, 000, 000/6)! This is quite counterintuitive: how can two points be so far apart\\nwhen they both lie within the same unit hypercube? This fact implies that high-\\ndimensional datasets are at risk of being very sparse: most training instances are\\nlikely to be far away from each other. Of course, this also means that a new instance\\nwill likely be far away from any training instance, making predictions much less relia‐\\nble than in lower dimensions, since they will be based on much larger extrapolations.\\nIn short, the more dimensions the training set has, the greater the risk of overfitting\\nit.\\nIn theory, one solution to the curse of dimensionality could be to increase the size of\\nthe training set to reach a sufficient density of training instances. Unfortunately, in\\npractice, the number of training instances required to reach a given density grows\\nexponentially with the number of dimensions. With just 100 features (much less than\\nThe Curse of Dimensionality \\n| \\n217'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 243}, page_content='in the MNIST problem), you would need more training instances than atoms in the\\nobservable universe in order for training instances to be within 0.1 of each other on\\naverage, assuming they were spread out uniformly across all dimensions.\\nMain Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms, let’s take a look at\\nthe two main approaches to reducing dimensionality: projection and Manifold\\nLearning.\\nProjection\\nIn most real-world problems, training instances are not spread out uniformly across\\nall dimensions. Many features are almost constant, while others are highly correlated\\n(as discussed earlier for MNIST). As a result, all training instances actually lie within\\n(or close to) a much lower-dimensional subspace of the high-dimensional space. This\\nsounds very abstract, so let’s look at an example. In Figure 8-2 you can see a 3D data‐\\nset represented by the circles.\\nFigure 8-2. A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D)\\nsubspace of the high-dimensional (3D) space. Now if we project every training\\ninstance perpendicularly onto this subspace (as represented by the short lines con‐\\nnecting the instances to the plane), we get the new 2D dataset shown in Figure 8-3.\\nTa-da! We have just reduced the dataset’s dimensionality from 3D to 2D. Note that\\nthe axes correspond to new features z1 and z2 (the coordinates of the projections on\\nthe plane).\\n218 \\n| \\nChapter 8: Dimensionality Reduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 244}, page_content='Figure 8-3. The new 2D dataset after projection\\nHowever, projection is not always the best approach to dimensionality reduction. In\\nmany cases the subspace may twist and turn, such as in the famous Swiss roll toy data‐\\nset represented in Figure 8-4.\\nFigure 8-4. Swiss roll dataset\\nMain Approaches for Dimensionality Reduction \\n| \\n219'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 245}, page_content='Simply projecting onto a plane (e.g., by dropping x3) would squash different layers of\\nthe Swiss roll together, as shown on the left of Figure 8-5. However, what you really\\nwant is to unroll the Swiss roll to obtain the 2D dataset on the right of Figure 8-5.\\nFigure 8-5. Squashing by projecting onto a plane (left) versus unrolling the Swiss roll\\n(right)\\nManifold Learning\\nThe Swiss roll is an example of a 2D manifold. Put simply, a 2D manifold is a 2D\\nshape that can be bent and twisted in a higher-dimensional space. More generally, a\\nd-dimensional manifold is a part of an n-dimensional space (where d < n) that locally\\nresembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it\\nlocally resembles a 2D plane, but it is rolled in the third dimension.\\nMany dimensionality reduction algorithms work by modeling the manifold on which\\nthe training instances lie; this is called Manifold Learning. It relies on the manifold\\nassumption, also called the manifold hypothesis, which holds that most real-world\\nhigh-dimensional datasets lie close to a much lower-dimensional manifold. This\\nassumption is very often empirically observed.\\nOnce again, think about the MNIST dataset: all handwritten digit images have some\\nsimilarities. They are made of connected lines, the borders are white, they are more\\nor less centered, and so on. If you randomly generated images, only a ridiculously\\ntiny fraction of them would look like handwritten digits. In other words, the degrees\\nof freedom available to you if you try to create a digit image are dramatically lower\\nthan the degrees of freedom you would have if you were allowed to generate any\\nimage you wanted. These constraints tend to squeeze the dataset into a lower-\\ndimensional manifold.\\nThe manifold assumption is often accompanied by another implicit assumption: that\\nthe task at hand (e.g., classification or regression) will be simpler if expressed in the\\nlower-dimensional space of the manifold. For example, in the top row of Figure 8-6\\nthe Swiss roll is split into two classes: in the 3D space (on the left), the decision\\n220 \\n| \\nChapter 8: Dimensionality Reduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 246}, page_content='boundary would be fairly complex, but in the 2D unrolled manifold space (on the\\nright), the decision boundary is a simple straight line.\\nHowever, this assumption does not always hold. For example, in the bottom row of\\nFigure 8-6, the decision boundary is located at x1 = 5. This decision boundary looks\\nvery simple in the original 3D space (a vertical plane), but it looks more complex in\\nthe unrolled manifold (a collection of four independent line segments).\\nIn short, if you reduce the dimensionality of your training set before training a\\nmodel, it will usually speed up training, but it may not always lead to a better or sim‐\\npler solution; it all depends on the dataset.\\nHopefully you now have a good sense of what the curse of dimensionality is and how\\ndimensionality reduction algorithms can fight it, especially when the manifold\\nassumption holds. The rest of this chapter will go through some of the most popular\\nalgorithms.\\nFigure 8-6. The decision boundary may not always be simpler with lower dimensions\\nMain Approaches for Dimensionality Reduction \\n| \\n221'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 247}, page_content='4 “On Lines and Planes of Closest Fit to Systems of Points in Space,” K. Pearson (1901).\\nPCA\\nPrincipal Component Analysis (PCA) is by far the most popular dimensionality reduc‐\\ntion algorithm. First it identifies the hyperplane that lies closest to the data, and then\\nit projects the data onto it, just like in Figure 8-2.\\nPreserving the Variance\\nBefore you can project the training set onto a lower-dimensional hyperplane, you\\nfirst need to choose the right hyperplane. For example, a simple 2D dataset is repre‐\\nsented on the left of Figure 8-7, along with three different axes (i.e., one-dimensional\\nhyperplanes). On the right is the result of the projection of the dataset onto each of\\nthese axes. As you can see, the projection onto the solid line preserves the maximum\\nvariance, while the projection onto the dotted line preserves very little variance, and\\nthe projection onto the dashed line preserves an intermediate amount of variance.\\nFigure 8-7. Selecting the subspace onto which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of var‐\\niance, as it will most likely lose less information than the other projections. Another\\nway to justify this choice is that it is the axis that minimizes the mean squared dis‐\\ntance between the original dataset and its projection onto that axis. This is the rather\\nsimple idea behind PCA.4\\n222 \\n| \\nChapter 8: Dimensionality Reduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 248}, page_content='Principal Components\\nPCA identifies the axis that accounts for the largest amount of variance in the train‐\\ning set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the\\nfirst one, that accounts for the largest amount of remaining variance. In this 2D\\nexample there is no choice: it is the dotted line. If it were a higher-dimensional data‐\\nset, PCA would also find a third axis, orthogonal to both previous axes, and a fourth,\\na fifth, and so on—as many axes as the number of dimensions in the dataset.\\nThe unit vector that defines the ith axis is called the ith principal component (PC). In\\nFigure 8-7, the 1st PC is c1 and the 2nd PC is c2. In Figure 8-2 the first two PCs are\\nrepresented by the orthogonal arrows in the plane, and the third PC would be\\northogonal to the plane (pointing up or down).\\nThe direction of the principal components is not stable: if you per‐\\nturb the training set slightly and run PCA again, some of the new\\nPCs may point in the opposite direction of the original PCs. How‐\\never, they will generally still lie on the same axes. In some cases, a\\npair of PCs may even rotate or swap, but the plane they define will\\ngenerally remain the same.\\nSo how can you find the principal components of a training set? Luckily, there is a\\nstandard matrix factorization technique called Singular Value Decomposition (SVD)\\nthat can decompose the training set matrix X into the matrix multiplication of three\\nmatrices U Σ VT, where V contains all the principal components that we are looking\\nfor, as shown in Equation 8-1.\\nEquation 8-1. Principal components matrix\\nV =\\n∣\\n∣\\n∣\\nc1 c2 ⋯cn\\n∣\\n∣\\n∣\\nThe following Python code uses NumPy’s svd() function to obtain all the principal\\ncomponents of the training set, then extracts the first two PCs:\\nX_centered = X - X.mean(axis=0)\\nU, s, Vt = np.linalg.svd(X_centered)\\nc1 = Vt.T[:, 0]\\nc2 = Vt.T[:, 1]\\nPCA \\n| \\n223'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 249}, page_content='PCA assumes that the dataset is centered around the origin. As we\\nwill see, Scikit-Learn’s PCA classes take care of centering the data\\nfor you. However, if you implement PCA yourself (as in the pre‐\\nceding example), or if you use other libraries, don’t forget to center\\nthe data first.\\nProjecting Down to d Dimensions\\nOnce you have identified all the principal components, you can reduce the dimen‐\\nsionality of the dataset down to d dimensions by projecting it onto the hyperplane\\ndefined by the first d principal components. Selecting this hyperplane ensures that the\\nprojection will preserve as much variance as possible. For example, in Figure 8-2 the\\n3D dataset is projected down to the 2D plane defined by the first two principal com‐\\nponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐\\ntion looks very much like the original 3D dataset.\\nTo project the training set onto the hyperplane, you can simply compute the matrix\\nmultiplication of the training set matrix X by the matrix Wd, defined as the matrix\\ncontaining the first d principal components (i.e., the matrix composed of the first d\\ncolumns of V), as shown in Equation 8-2.\\nEquation 8-2. Projecting the training set down to d dimensions\\nXd‐proj = XWd\\nThe following Python code projects the training set onto the plane defined by the first\\ntwo principal components:\\nW2 = Vt.T[:, :2]\\nX2D = X_centered.dot(W2)\\nThere you have it! You now know how to reduce the dimensionality of any dataset\\ndown to any number of dimensions, while preserving as much variance as possible.\\nUsing Scikit-Learn\\nScikit-Learn’s PCA class implements PCA using SVD decomposition just like we did\\nbefore. The following code applies PCA to reduce the dimensionality of the dataset\\ndown to two dimensions (note that it automatically takes care of centering the data):\\nfrom sklearn.decomposition import PCA\\npca = PCA(n_components = 2)\\nX2D = pca.fit_transform(X)\\nAfter fitting the PCA transformer to the dataset, you can access the principal compo‐\\nnents using the components_ variable (note that it contains the PCs as horizontal vec‐\\n224 \\n| \\nChapter 8: Dimensionality Reduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 250}, page_content='tors, so, for example, the first principal component is equal to pca.components_.T[:,\\n0]).\\nExplained Variance Ratio\\nAnother very useful piece of information is the explained variance ratio of each prin‐\\ncipal component, available via the explained_variance_ratio_ variable. It indicates\\nthe proportion of the dataset’s variance that lies along the axis of each principal com‐\\nponent. For example, let’s look at the explained variance ratios of the first two compo‐\\nnents of the 3D dataset represented in Figure 8-2:\\n>>> pca.explained_variance_ratio_\\narray([0.84248607, 0.14631839])\\nThis tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\\nlies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐\\nable to assume that it probably carries little information.\\nChoosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to, it is\\ngenerally preferable to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen‐\\nsionality for data visualization—in that case you will generally want to reduce the\\ndimensionality down to 2 or 3.\\nThe following code computes PCA without reducing dimensionality, then computes\\nthe minimum number of dimensions required to preserve 95% of the training set’s\\nvariance:\\npca = PCA()\\npca.fit(X_train)\\ncumsum = np.cumsum(pca.explained_variance_ratio_)\\nd = np.argmax(cumsum >= 0.95) + 1\\nYou could then set n_components=d and run PCA again. However, there is a much\\nbetter option: instead of specifying the number of principal components you want to\\npreserve, you can set n_components to be a float between 0.0 and 1.0, indicating the\\nratio of variance you wish to preserve:\\npca = PCA(n_components=0.95)\\nX_reduced = pca.fit_transform(X_train)\\nYet another option is to plot the explained variance as a function of the number of\\ndimensions (simply plot cumsum; see Figure 8-8). There will usually be an elbow in the\\ncurve, where the explained variance stops growing fast. You can think of this as the\\nintrinsic dimensionality of the dataset. In this case, you can see that reducing the\\nPCA \\n| \\n225'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 251}, page_content='dimensionality down to about 100 dimensions wouldn’t lose too much explained var‐\\niance.\\nFigure 8-8. Explained variance as a function of the number of dimensions\\nPCA for Compression\\nObviously after dimensionality reduction, the training set takes up much less space.\\nFor example, try applying PCA to the MNIST dataset while preserving 95% of its var‐\\niance. You should find that each instance will have just over 150 features, instead of\\nthe original 784 features. So while most of the variance is preserved, the dataset is\\nnow less than 20% of its original size! This is a reasonable compression ratio, and you\\ncan see how this can speed up a classification algorithm (such as an SVM classifier)\\ntremendously.\\nIt is also possible to decompress the reduced dataset back to 784 dimensions by\\napplying the inverse transformation of the PCA projection. Of course this won’t give\\nyou back the original data, since the projection lost a bit of information (within the\\n5% variance that was dropped), but it will likely be quite close to the original data.\\nThe mean squared distance between the original data and the reconstructed data\\n(compressed and then decompressed) is called the reconstruction error. For example,\\nthe following code compresses the MNIST dataset down to 154 dimensions, then uses\\nthe inverse_transform() method to decompress it back to 784 dimensions.\\nFigure 8-9 shows a few digits from the original training set (on the left), and the cor‐\\nresponding digits after compression and decompression. You can see that there is a\\nslight image quality loss, but the digits are still mostly intact.\\npca = PCA(n_components = 154)\\nX_reduced = pca.fit_transform(X_train)\\nX_recovered = pca.inverse_transform(X_reduced)\\n226 \\n| \\nChapter 8: Dimensionality Reduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 252}, page_content='Figure 8-9. MNIST compression preserving 95% of the variance\\nThe equation of the inverse transformation is shown in Equation 8-3.\\nEquation 8-3. PCA inverse transformation, back to the original number of\\ndimensions\\nXrecovered = Xd‐projWd\\nT\\nRandomized PCA\\nIf you set the svd_solver hyperparameter to \"randomized\", Scikit-Learn uses a sto‐\\nchastic algorithm called Randomized PCA that quickly finds an approximation of the\\nfirst d principal components. Its computational complexity is O(m × d2) + O(d3),\\ninstead of O(m × n2) + O(n3) for the full SVD approach, so it is dramatically faster\\nthan full SVD when d is much smaller than n:\\nrnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\\nX_reduced = rnd_pca.fit_transform(X_train)\\nBy default, svd_solver is actually set to \"auto\": Scikit-Learn automatically uses the\\nrandomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\\nor n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\\nSVD, you can set the svd_solver hyperparameter to \"full\".\\nIncremental PCA\\nOne problem with the preceding implementations of PCA is that they require the\\nwhole training set to fit in memory in order for the algorithm to run. Fortunately,\\nIncremental PCA (IPCA) algorithms have been developed: you can split the training\\nset into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\\nPCA \\n| \\n227'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 253}, page_content='5 Scikit-Learn uses the algorithm described in “Incremental Learning for Robust Visual Tracking,” D. Ross et al.\\n(2007).\\nuseful for large training sets, and also to apply PCA online (i.e., on the fly, as new\\ninstances arrive).\\nThe following code splits the MNIST dataset into 100 mini-batches (using NumPy’s\\narray_split() function) and feeds them to Scikit-Learn’s IncrementalPCA class5 to \\nreduce the dimensionality of the MNIST dataset down to 154 dimensions (just like\\nbefore). Note that you must call the partial_fit() method with each mini-batch\\nrather than the fit() method with the whole training set:\\nfrom sklearn.decomposition import IncrementalPCA\\nn_batches = 100\\ninc_pca = IncrementalPCA(n_components=154)\\nfor X_batch in np.array_split(X_train, n_batches):\\n    inc_pca.partial_fit(X_batch)\\nX_reduced = inc_pca.transform(X_train)\\nAlternatively, you can use NumPy’s memmap class, which allows you to manipulate a\\nlarge array stored in a binary file on disk as if it were entirely in memory; the class\\nloads only the data it needs in memory, when it needs it. Since the IncrementalPCA\\nclass uses only a small part of the array at any given time, the memory usage remains\\nunder control. This makes it possible to call the usual fit() method, as you can see\\nin the following code:\\nX_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\\nbatch_size = m // n_batches\\ninc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\\ninc_pca.fit(X_mm)\\nKernel PCA\\nIn Chapter 5 we discussed the kernel trick, a mathematical technique that implicitly\\nmaps instances into a very high-dimensional space (called the feature space), enabling\\nnonlinear classification and regression with Support Vector Machines. Recall that a\\nlinear decision boundary in the high-dimensional feature space corresponds to a\\ncomplex nonlinear decision boundary in the original space.\\nIt turns out that the same trick can be applied to PCA, making it possible to perform\\ncomplex nonlinear projections for dimensionality reduction. This is called Kernel\\n228 \\n| \\nChapter 8: Dimensionality Reduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 254}, page_content='6 “Kernel Principal Component Analysis,” B. Schölkopf, A. Smola, K. Müller (1999).\\nPCA (kPCA).6 It is often good at preserving clusters of instances after projection, or\\nsometimes even unrolling datasets that lie close to a twisted manifold.\\nFor example, the following code uses Scikit-Learn’s KernelPCA class to perform kPCA\\nwith an RBF kernel (see Chapter 5 for more details about the RBF kernel and the\\nother kernels):\\nfrom sklearn.decomposition import KernelPCA\\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\\nX_reduced = rbf_pca.fit_transform(X)\\nFigure 8-10 shows the Swiss roll, reduced to two dimensions using a linear kernel\\n(equivalent to simply using the PCA class), an RBF kernel, and a sigmoid kernel\\n(Logistic).\\nFigure 8-10. Swiss roll reduced to 2D using kPCA with various kernels\\nSelecting a Kernel and Tuning Hyperparameters\\nAs kPCA is an unsupervised learning algorithm, there is no obvious performance\\nmeasure to help you select the best kernel and hyperparameter values. However,\\ndimensionality reduction is often a preparation step for a supervised learning task\\n(e.g., classification), so you can simply use grid search to select the kernel and hyper‐\\nparameters that lead to the best performance on that task. For example, the following\\ncode creates a two-step pipeline, first reducing dimensionality to two dimensions\\nusing kPCA, then applying Logistic Regression for classification. Then it uses Grid\\nSearchCV to find the best kernel and gamma value for kPCA in order to get the best\\nclassification accuracy at the end of the pipeline:\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.pipeline import Pipeline\\nKernel PCA \\n| \\n229'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 255}, page_content='clf = Pipeline([\\n        (\"kpca\", KernelPCA(n_components=2)),\\n        (\"log_reg\", LogisticRegression())\\n    ])\\nparam_grid = [{\\n        \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\\n        \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\\n    }]\\ngrid_search = GridSearchCV(clf, param_grid, cv=3)\\ngrid_search.fit(X, y)\\nThe best kernel and hyperparameters are then available through the best_params_\\nvariable:\\n>>> print(grid_search.best_params_)\\n{\\'kpca__gamma\\': 0.043333333333333335, \\'kpca__kernel\\': \\'rbf\\'}\\nAnother approach, this time entirely unsupervised, is to select the kernel and hyper‐\\nparameters that yield the lowest reconstruction error. However, reconstruction is not\\nas easy as with linear PCA. Here’s why. Figure 8-11 shows the original Swiss roll 3D\\ndataset (top left), and the resulting 2D dataset after kPCA is applied using an RBF\\nkernel (top right). Thanks to the kernel trick, this is mathematically equivalent to\\nmapping the training set to an infinite-dimensional feature space (bottom right)\\nusing the feature map φ, then projecting the transformed training set down to 2D\\nusing linear PCA. Notice that if we could invert the linear PCA step for a given\\ninstance in the reduced space, the reconstructed point would lie in feature space, not\\nin the original space (e.g., like the one represented by an x in the diagram). Since the\\nfeature space is infinite-dimensional, we cannot compute the reconstructed point,\\nand therefore we cannot compute the true reconstruction error. Fortunately, it is pos‐\\nsible to find a point in the original space that would map close to the reconstructed\\npoint. This is called the reconstruction pre-image. Once you have this pre-image, you\\ncan measure its squared distance to the original instance. You can then select the ker‐\\nnel and hyperparameters that minimize this reconstruction pre-image error.\\n230 \\n| \\nChapter 8: Dimensionality Reduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 256}, page_content='7 Scikit-Learn uses the algorithm based on Kernel Ridge Regression described in Gokhan H. Bakır, Jason\\nWeston, and Bernhard Scholkopf, “Learning to Find Pre-images” (Tubingen, Germany: Max Planck Institute\\nfor Biological Cybernetics, 2004).\\nFigure 8-11. Kernel PCA and the reconstruction pre-image error\\nYou may be wondering how to perform this reconstruction. One solution is to train a\\nsupervised regression model, with the projected instances as the training set and the\\noriginal instances as the targets. Scikit-Learn will do this automatically if you set\\nfit_inverse_transform=True, as shown in the following code:7\\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433,\\n                    fit_inverse_transform=True)\\nX_reduced = rbf_pca.fit_transform(X)\\nX_preimage = rbf_pca.inverse_transform(X_reduced)\\nBy default, fit_inverse_transform=False and KernelPCA has no\\ninverse_transform() method. This method only gets created\\nwhen you set fit_inverse_transform=True.\\nKernel PCA \\n| \\n231'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 257}, page_content='8 “Nonlinear Dimensionality Reduction by Locally Linear Embedding,” S. Roweis, L. Saul (2000).\\nYou can then compute the reconstruction pre-image error:\\n>>> from sklearn.metrics import mean_squared_error\\n>>> mean_squared_error(X, X_preimage)\\n32.786308795766132\\nNow you can use grid search with cross-validation to find the kernel and hyperpara‐\\nmeters that minimize this pre-image reconstruction error.\\nLLE\\nLocally Linear Embedding (LLE)8 is another very powerful nonlinear dimensionality\\nreduction (NLDR) technique. It is a Manifold Learning technique that does not rely\\non projections like the previous algorithms. In a nutshell, LLE works by first measur‐\\ning how each training instance linearly relates to its closest neighbors (c.n.), and then\\nlooking for a low-dimensional representation of the training set where these local\\nrelationships are best preserved (more details shortly). This makes it particularly\\ngood at unrolling twisted manifolds, especially when there is not too much noise.\\nFor example, the following code uses Scikit-Learn’s LocallyLinearEmbedding class to\\nunroll the Swiss roll. The resulting 2D dataset is shown in Figure 8-12. As you can\\nsee, the Swiss roll is completely unrolled and the distances between instances are\\nlocally well preserved. However, distances are not preserved on a larger scale: the left\\npart of the unrolled Swiss roll is stretched, while the right part is squeezed. Neverthe‐\\nless, LLE did a pretty good job at modeling the manifold.\\nfrom sklearn.manifold import LocallyLinearEmbedding\\nlle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\\nX_reduced = lle.fit_transform(X)\\n232 \\n| \\nChapter 8: Dimensionality Reduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 258}, page_content='Figure 8-12. Unrolled Swiss roll using LLE\\nHere’s how LLE works: first, for each training instance x(i), the algorithm identifies its\\nk closest neighbors (in the preceding code k = 10), then tries to reconstruct x(i) as a\\nlinear function of these neighbors. More specifically, it finds the weights wi,j such that\\nthe squared distance between x(i) and ∑j = 1\\nm\\nwi, jx j  is as small as possible, assuming wi,j\\n= 0 if x(j) is not one of the k closest neighbors of x(i). Thus the first step of LLE is the\\nconstrained optimization problem described in Equation 8-4, where W is the weight\\nmatrix containing all the weights wi,j. The second constraint simply normalizes the\\nweights for each training instance x(i).\\nLLE \\n| \\n233'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 259}, page_content='Equation 8-4. LLE step 1: linearly modeling local relationships\\nW = argmin\\nW\\n∑\\ni = 1\\nm\\nx i −∑\\nj = 1\\nm\\nwi, jx j\\n2\\nsubject to\\nwi, j = 0\\nif x j is not one of the k c.n. of x i\\n∑\\nj = 1\\nm\\nwi, j = 1 for i = 1, 2, ⋯, m\\nAfter this step, the weight matrix W (containing the weights wi, j) encodes the local\\nlinear relationships between the training instances. Now the second step is to map the\\ntraining instances into a d-dimensional space (where d < n) while preserving these\\nlocal relationships as much as possible. If z(i) is the image of x(i) in this d-dimensional\\nspace, then we want the squared distance between z(i) and ∑j = 1\\nm\\nwi, jz j  to be as small\\nas possible. This idea leads to the unconstrained optimization problem described in\\nEquation 8-5. It looks very similar to the first step, but instead of keeping the instan‐\\nces fixed and finding the optimal weights, we are doing the reverse: keeping the\\nweights fixed and finding the optimal position of the instances’ images in the low-\\ndimensional space. Note that Z is the matrix containing all z(i).\\nEquation 8-5. LLE step 2: reducing dimensionality while preserving relationships\\nZ = argmin\\nZ\\n∑\\ni = 1\\nm\\nz i −∑\\nj = 1\\nm\\nwi, jz j\\n2\\nScikit-Learn’s LLE implementation has the following computational complexity:\\nO(m log(m)n log(k)) for finding the k nearest neighbors, O(mnk3) for optimizing the\\nweights, and O(dm2) for constructing the low-dimensional representations. Unfortu‐\\nnately, the m2 in the last term makes this algorithm scale poorly to very large datasets.\\nOther Dimensionality Reduction Techniques\\nThere are many other dimensionality reduction techniques, several of which are\\navailable in Scikit-Learn. Here are some of the most popular:\\n• Multidimensional Scaling (MDS) reduces dimensionality while trying to preserve\\nthe distances between the instances (see Figure 8-13).\\n234 \\n| \\nChapter 8: Dimensionality Reduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 260}, page_content='9 The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between\\nthese nodes.\\n• Isomap creates a graph by connecting each instance to its nearest neighbors, then\\nreduces dimensionality while trying to preserve the geodesic distances9 between\\nthe instances.\\n• t-Distributed Stochastic Neighbor Embedding (t-SNE) reduces dimensionality\\nwhile trying to keep similar instances close and dissimilar instances apart. It is\\nmostly used for visualization, in particular to visualize clusters of instances in\\nhigh-dimensional space (e.g., to visualize the MNIST images in 2D).\\n• Linear Discriminant Analysis (LDA) is actually a classification algorithm, but dur‐\\ning training it learns the most discriminative axes between the classes, and these\\naxes can then be used to define a hyperplane onto which to project the data. The\\nbenefit is that the projection will keep classes as far apart as possible, so LDA is a\\ngood technique to reduce dimensionality before running another classification\\nalgorithm such as an SVM classifier.\\nFigure 8-13. Reducing the Swiss roll to 2D using various techniques\\nExercises\\n1. What are the main motivations for reducing a dataset’s dimensionality? What are\\nthe main drawbacks?\\n2. What is the curse of dimensionality?\\n3. Once a dataset’s dimensionality has been reduced, is it possible to reverse the\\noperation? If so, how? If not, why?\\n4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\\n5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained\\nvariance ratio to 95%. How many dimensions will the resulting dataset have?\\nExercises \\n| \\n235'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 261}, page_content='6. In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA,\\nor Kernel PCA?\\n7. How can you evaluate the performance of a dimensionality reduction algorithm\\non your dataset?\\n8. Does it make any sense to chain two different dimensionality reduction algo‐\\nrithms?\\n9. Load the MNIST dataset (introduced in Chapter 3) and split it into a training set\\nand a test set (take the first 60,000 instances for training, and the remaining\\n10,000 for testing). Train a Random Forest classifier on the dataset and time how\\nlong it takes, then evaluate the resulting model on the test set. Next, use PCA to\\nreduce the dataset’s dimensionality, with an explained variance ratio of 95%.\\nTrain a new Random Forest classifier on the reduced dataset and see how long it\\ntakes. Was training much faster? Next evaluate the classifier on the test set: how\\ndoes it compare to the previous classifier?\\n10. Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the\\nresult using Matplotlib. You can use a scatterplot using 10 different colors to rep‐\\nresent each image’s target class. Alternatively, you can write colored digits at the\\nlocation of each instance, or even plot scaled-down versions of the digit images\\nthemselves (if you plot all digits, the visualization will be too cluttered, so you\\nshould either draw a random sample or plot an instance only if no other instance\\nhas already been plotted at a close distance). You should get a nice visualization\\nwith well-separated clusters of digits. Try using other dimensionality reduction\\nalgorithms such as PCA, LLE, or MDS and compare the resulting visualizations.\\nSolutions to these exercises are available in ???.\\n236 \\n| \\nChapter 8: Dimensionality Reduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 262}, page_content='CHAPTER 9\\nUnsupervised Learning Techniques\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 9 in the final\\nrelease of the book.\\nAlthough most of the applications of Machine Learning today are based on super‐\\nvised learning (and as a result, this is where most of the investments go to), the vast\\nmajority of the available data is actually unlabeled: we have the input features X, but\\nwe do not have the labels y. Yann LeCun famously said that “if intelligence was a cake,\\nunsupervised learning would be the cake, supervised learning would be the icing on\\nthe cake, and reinforcement learning would be the cherry on the cake”. In other\\nwords, there is a huge potential in unsupervised learning that we have only barely\\nstarted to sink our teeth into.\\nFor example, say you want to create a system that will take a few pictures of each item\\non a manufacturing production line and detect which items are defective. You can\\nfairly easily create a system that will take pictures automatically, and this might give\\nyou thousands of pictures every day. You can then build a reasonably large dataset in\\njust a few weeks. But wait, there are no labels! If you want to train a regular binary\\nclassifier that will predict whether an item is defective or not, you will need to label\\nevery single picture as “defective” or “normal”. This will generally require human\\nexperts to sit down and manually go through all the pictures. This is a long, costly\\nand tedious task, so it will usually only be done on a small subset of the available pic‐\\ntures. As a result, the labeled dataset will be quite small, and the classifier’s perfor‐\\nmance will be disappointing. Moreover, every time the company makes any change to\\nits products, the whole process will need to be started over from scratch. Wouldn’t it\\n237'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 263}, page_content='be great if the algorithm could just exploit the unlabeled data without needing\\nhumans to label every picture? Enter unsupervised learning.\\nIn Chapter 8, we looked at the most common unsupervised learning task: dimension‐\\nality reduction. In this chapter, we will look at a few more unsupervised learning tasks\\nand algorithms:\\n• Clustering: the goal is to group similar instances together into clusters. This is a\\ngreat tool for data analysis, customer segmentation, recommender systems,\\nsearch engines, image segmentation, semi-supervised learning, dimensionality\\nreduction, and more.\\n• Anomaly detection: the objective is to learn what “normal” data looks like, and\\nuse this to detect abnormal instances, such as defective items on a production\\nline or a new trend in a time series.\\n• Density estimation: this is the task of estimating the probability density function\\n(PDF) of the random process that generated the dataset. This is commonly used\\nfor anomaly detection: instances located in very low-density regions are likely to\\nbe anomalies. It is also useful for data analysis and visualization.\\nReady for some cake? We will start with clustering, using K-Means and DBSCAN,\\nand then we will discuss Gaussian mixture models and see how they can be used for\\ndensity estimation, clustering, and anomaly detection.\\nClustering\\nAs you enjoy a hike in the mountains, you stumble upon a plant you have never seen\\nbefore. You look around and you notice a few more. They are not perfectly identical,\\nyet they are sufficiently similar for you to know that they most likely belong to the\\nsame species (or at least the same genus). You may need a botanist to tell you what\\nspecies that is, but you certainly don’t need an expert to identify groups of similar-\\nlooking objects. This is called clustering: it is the task of identifying similar instances\\nand assigning them to clusters, i.e., groups of similar instances.\\nJust like in classification, each instance gets assigned to a group. However, this is an\\nunsupervised task. Consider Figure 9-1: on the left is the iris dataset (introduced in\\nChapter 4), where each instance’s species (i.e., its class) is represented with a different\\nmarker. It is a labeled dataset, for which classification algorithms such as Logistic\\nRegression, SVMs or Random Forest classifiers are well suited. On the right is the\\nsame dataset, but without the labels, so you cannot use a classification algorithm any‐\\nmore. This is where clustering algorithms step in: many of them can easily detect the\\ntop left cluster. It is also quite easy to see with our own eyes, but it is not so obvious\\nthat the lower right cluster is actually composed of two distinct sub-clusters. That\\nsaid, the dataset actually has two additional features (sepal length and width), not\\n238 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 264}, page_content='represented here, and clustering algorithms can make good use of all features, so in\\nfact they identify the three clusters fairly well (e.g., using a Gaussian mixture model,\\nonly 5 instances out of 150 are assigned to the wrong cluster).\\nFigure 9-1. Classification (left) versus clustering (right)\\nClustering is used in a wide variety of applications, including:\\n• For customer segmentation: you can cluster your customers based on their pur‐\\nchases, their activity on your website, and so on. This is useful to understand who\\nyour customers are and what they need, so you can adapt your products and\\nmarketing campaigns to each segment. For example, this can be useful in recom‐\\nmender systems to suggest content that other users in the same cluster enjoyed.\\n• For data analysis: when analyzing a new dataset, it is often useful to first discover\\nclusters of similar instances, as it is often easier to analyze clusters separately.\\n• As a dimensionality reduction technique: once a dataset has been clustered, it is\\nusually possible to measure each instance’s affinity with each cluster (affinity is\\nany measure of how well an instance fits into a cluster). Each instance’s feature\\nvector x can then be replaced with the vector of its cluster affinities. If there are k\\nclusters, then this vector is k dimensional. This is typically much lower dimen‐\\nsional than the original feature vector, but it can preserve enough information for\\nfurther processing.\\n• For anomaly detection (also called outlier detection): any instance that has a low\\naffinity to all the clusters is likely to be an anomaly. For example, if you have clus‐\\ntered the users of your website based on their behavior, you can detect users with\\nunusual behavior, such as an unusual number of requests per second, and so on.\\nAnomaly detection is particularly useful in detecting defects in manufacturing, or\\nfor fraud detection.\\n• For semi-supervised learning: if you only have a few labels, you could perform\\nclustering and propagate the labels to all the instances in the same cluster. This\\ncan greatly increase the amount of labels available for a subsequent supervised\\nlearning algorithm, and thus improve its performance.\\nClustering \\n| \\n239'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 265}, page_content='1 “Least square quantization in PCM,” Stuart P. Lloyd. (1982).\\n• For search engines: for example, some search engines let you search for images\\nthat are similar to a reference image. To build such a system, you would first\\napply a clustering algorithm to all the images in your database: similar images\\nwould end up in the same cluster. Then when a user provides a reference image,\\nall you need to do is to find this image’s cluster using the trained clustering\\nmodel, and you can then simply return all the images from this cluster.\\n• To segment an image: by clustering pixels according to their color, then replacing\\neach pixel’s color with the mean color of its cluster, it is possible to reduce the\\nnumber of different colors in the image considerably. This technique is used in\\nmany object detection and tracking systems, as it makes it easier to detect the\\ncontour of each object.\\nThere is no universal definition of what a cluster is: it really depends on the context,\\nand different algorithms will capture different kinds of clusters. For example, some\\nalgorithms look for instances centered around a particular point, called a centroid.\\nOthers look for continuous regions of densely packed instances: these clusters can\\ntake on any shape. Some algorithms are hierarchical, looking for clusters of clusters.\\nAnd the list goes on.\\nIn this section, we will look at two popular clustering algorithms: K-Means and\\nDBSCAN, and we will show some of their applications, such as non-linear dimen‐\\nsionality reduction, semi-supervised learning and anomaly detection.\\nK-Means\\nConsider the unlabeled dataset represented in Figure 9-2: you can clearly see 5 blobs\\nof instances. The K-Means algorithm is a simple algorithm capable of clustering this\\nkind of dataset very quickly and efficiently, often in just a few iterations. It was pro‐\\nposed by Stuart Lloyd at the Bell Labs in 1957 as a technique for pulse-code modula‐\\ntion, but it was only published outside of the company in 1982, in a paper titled\\n“Least square quantization in PCM”.1 By then, in 1965, Edward W. Forgy had pub‐\\nlished virtually the same algorithm, so K-Means is sometimes referred to as Lloyd-\\nForgy.\\n240 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 266}, page_content='Figure 9-2. An unlabeled dataset composed of five blobs of instances\\nLet’s train a K-Means clusterer on this dataset. It will try to find each blob’s center and\\nassign each instance to the closest blob:\\nfrom sklearn.cluster import KMeans\\nk = 5\\nkmeans = KMeans(n_clusters=k)\\ny_pred = kmeans.fit_predict(X)\\nNote that you have to specify the number of clusters k that the algorithm must find.\\nIn this example, it is pretty obvious from looking at the data that k should be set to 5,\\nbut in general it is not that easy. We will discuss this shortly.\\nEach instance was assigned to one of the 5 clusters. In the context of clustering, an\\ninstance’s label is the index of the cluster that this instance gets assigned to by the\\nalgorithm: this is not to be confused with the class labels in classification (remember\\nthat clustering is an unsupervised learning task). The KMeans instance preserves a\\ncopy of the labels of the instances it was trained on, available via the labels_ instance\\nvariable:\\n>>> y_pred\\narray([4, 0, 1, ..., 2, 1, 0], dtype=int32)\\n>>> y_pred is kmeans.labels_\\nTrue\\nWe can also take a look at the 5 centroids that the algorithm found:\\n>>> kmeans.cluster_centers_\\narray([[-2.80389616,  1.80117999],\\n       [ 0.20876306,  2.25551336],\\n       [-2.79290307,  2.79641063],\\n       [-1.46679593,  2.28585348],\\n       [-2.80037642,  1.30082566]])\\nOf course, you can easily assign new instances to the cluster whose centroid is closest:\\nClustering \\n| \\n241'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 267}, page_content='>>> X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\\n>>> kmeans.predict(X_new)\\narray([1, 1, 2, 2], dtype=int32)\\nIf you plot the cluster’s decision boundaries, you get a Voronoi tessellation (see\\nFigure 9-3, where each centroid is represented with an X):\\nFigure 9-3. K-Means decision boundaries (Voronoi tessellation)\\nThe vast majority of the instances were clearly assigned to the appropriate cluster, but\\na few instances were probably mislabeled (especially near the boundary between the\\ntop left cluster and the central cluster). Indeed, the K-Means algorithm does not\\nbehave very well when the blobs have very different diameters since all it cares about\\nwhen assigning an instance to a cluster is the distance to the centroid.\\nInstead of assigning each instance to a single cluster, which is called hard clustering, it\\ncan be useful to just give each instance a score per cluster: this is called soft clustering.\\nFor example, the score can be the distance between the instance and the centroid, or\\nconversely it can be a similarity score (or affinity) such as the Gaussian Radial Basis\\nFunction (introduced in Chapter 5). In the KMeans class, the transform() method\\nmeasures the distance from each instance to every centroid:\\n>>> kmeans.transform(X_new)\\narray([[2.81093633, 0.32995317, 2.9042344 , 1.49439034, 2.88633901],\\n       [5.80730058, 2.80290755, 5.84739223, 4.4759332 , 5.84236351],\\n       [1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031],\\n       [0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]])\\nIn this example, the first instance in X_new is located at a distance of 2.81 from the\\nfirst centroid, 0.33 from the second centroid, 2.90 from the third centroid, 1.49 from\\nthe fourth centroid and 2.87 from the fifth centroid. If you have a high-dimensional\\ndataset and you transform it this way, you end up with a k-dimensional dataset: this\\ncan be a very efficient non-linear dimensionality reduction technique.\\n242 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 268}, page_content='2 This can be proven by pointing out that the mean squared distance between the instances and their closest\\ncentroid can only go down at each step.\\nThe K-Means Algorithm\\nSo how does the algorithm work? Well it is really quite simple. Suppose you were\\ngiven the centroids: you could easily label all the instances in the dataset by assigning\\neach of them to the cluster whose centroid is closest. Conversely, if you were given all\\nthe instance labels, you could easily locate all the centroids by computing the mean of\\nthe instances for each cluster. But you are given neither the labels nor the centroids,\\nso how can you proceed? Well, just start by placing the centroids randomly (e.g., by\\npicking k instances at random and using their locations as centroids). Then label the\\ninstances, update the centroids, label the instances, update the centroids, and so on\\nuntil the centroids stop moving. The algorithm is guaranteed to converge in a finite\\nnumber of steps (usually quite small), it will not oscillate forever2. You can see the\\nalgorithm in action in Figure 9-4: the centroids are initialized randomly (top left),\\nthen the instances are labeled (top right), then the centroids are updated (center left),\\nthe instances are relabeled (center right), and so on. As you can see, in just 3 itera‐\\ntions the algorithm has reached a clustering that seems close to optimal.\\nFigure 9-4. The K-Means algorithm\\nClustering \\n| \\n243'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 269}, page_content='The computational complexity of the algorithm is generally linear\\nwith regards to the number of instances m, the number of clusters\\nk and the number of dimensions n. However, this is only true when\\nthe data has a clustering structure. If it does not, then in the worst\\ncase scenario the complexity can increase exponentially with the\\nnumber of instances. In practice, however, this rarely happens, and\\nK-Means is generally one of the fastest clustering algorithms.\\nUnfortunately, although the algorithm is guaranteed to converge, it may not converge\\nto the right solution (i.e., it may converge to a local optimum): this depends on the\\ncentroid initialization. For example, Figure 9-5 shows two sub-optimal solutions that\\nthe algorithm can converge to if you are not lucky with the random initialization step:\\nFigure 9-5. Sub-optimal solutions due to unlucky centroid initializations\\nLet’s look at a few ways you can mitigate this risk by improving the centroid initializa‐\\ntion.\\nCentroid Initialization Methods\\nIf you happen to know approximately where the centroids should be (e.g., if you ran\\nanother clustering algorithm earlier), then you can set the init hyperparameter to a\\nNumPy array containing the list of centroids, and set n_init to 1:\\ngood_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\\nkmeans = KMeans(n_clusters=5, init=good_init, n_init=1)\\nAnother solution is to run the algorithm multiple times with different random initial‐\\nizations and keep the best solution. This is controlled by the n_init hyperparameter:\\nby default, it is equal to 10, which means that the whole algorithm described earlier\\nactually runs 10 times when you call fit(), and Scikit-Learn keeps the best solution.\\nBut how exactly does it know which solution is the best? Well of course it uses a per‐\\nformance metric! It is called the model’s inertia: this is the mean squared distance\\nbetween each instance and its closest centroid. It is roughly equal to 223.3 for the\\nmodel on the left of Figure 9-5, 237.5 for the model on the right of Figure 9-5, and\\n211.6 for the model in Figure 9-3. The KMeans class runs the algorithm n_init times\\nand keeps the model with the lowest inertia: in this example, the model in Figure 9-3\\nwill be selected (unless we are very unlucky with n_init consecutive random initiali‐\\n244 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 270}, page_content='3 “k-means\\\\++: The advantages of careful seeding,” David Arthur and Sergei Vassilvitskii (2006).\\n4 “Using the Triangle Inequality to Accelerate k-Means,” Charles Elkan (2003).\\nzations). If you are curious, a model’s inertia is accessible via the inertia_ instance\\nvariable:\\n>>> kmeans.inertia_\\n211.59853725816856\\nThe score() method returns the negative inertia. Why negative? Well, it is because a\\npredictor’s score() method must always respect the \"great is better\" rule.\\n>>> kmeans.score(X)\\n-211.59853725816856\\nAn important improvement to the K-Means algorithm, called K-Means+\\\\+, was pro‐\\nposed in a 2006 paper by David Arthur and Sergei Vassilvitskii:3 they introduced a\\nsmarter initialization step that tends to select centroids that are distant from one\\nanother, and this makes the K-Means algorithm much less likely to converge to a sub-\\noptimal solution. They showed that the additional computation required for the\\nsmarter initialization step is well worth it since it makes it possible to drastically\\nreduce the number of times the algorithm needs to be run to find the optimal solu‐\\ntion. Here is the K-Means++ initialization algorithm:\\n• Take one centroid c(1), chosen uniformly at random from the dataset.\\n• Take a new centroid c(i), choosing an instance x(i) with probability: D �i\\n2\\n∑j = 1\\nm\\nD �j 2 where D(x(i)) is the distance between the instance x(i) and the closest\\ncentroid that was already chosen. This probability distribution ensures that\\ninstances further away from already chosen centroids are much more likely be\\nselected as centroids.\\n• Repeat the previous step until all k centroids have been chosen.\\nThe KMeans class actually uses this initialization method by default. If you want to\\nforce it to use the original method (i.e., picking k instances randomly to define the\\ninitial centroids), then you can set the init hyperparameter to \"random\". You will\\nrarely need to do this.\\nAccelerated K-Means and Mini-batch K-Means\\nAnother important improvement to the K-Means algorithm was proposed in a 2003\\npaper by Charles Elkan.4 It considerably accelerates the algorithm by avoiding many\\nunnecessary distance calculations: this is achieved by exploiting the triangle inequal‐\\nClustering \\n| \\n245'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 271}, page_content='5 The triangle inequality is AC ≤ AB + BC where A, B and C are three points, and AB, AC and BC are the\\ndistances between these points.\\n6 “Web-Scale K-Means Clustering,” David Sculley (2010).\\nity (i.e., the straight line is always the shortest5) and by keeping track of lower and\\nupper bounds for distances between instances and centroids. This is the algorithm\\nused by default by the KMeans class (but you can force it to use the original algorithm\\nby setting the algorithm hyperparameter to \"full\", although you probably will\\nnever need to).\\nYet another important variant of the K-Means algorithm was proposed in a 2010\\npaper by David Sculley.6 Instead of using the full dataset at each iteration, the algo‐\\nrithm is capable of using mini-batches, moving the centroids just slightly at each iter‐\\nation. This speeds up the algorithm typically by a factor of 3 or 4 and makes it\\npossible to cluster huge datasets that do not fit in memory. Scikit-Learn implements\\nthis algorithm in the MiniBatchKMeans class. You can just use this class like the\\nKMeans class:\\nfrom sklearn.cluster import MiniBatchKMeans\\nminibatch_kmeans = MiniBatchKMeans(n_clusters=5)\\nminibatch_kmeans.fit(X)\\nIf the dataset does not fit in memory, the simplest option is to use the memmap class, as\\nwe did for incremental PCA in Chapter 8. Alternatively, you can pass one mini-batch\\nat a time to the partial_fit() method, but this will require much more work, since\\nyou will need to perform multiple initializations and select the best one yourself (see\\nthe notebook for an example).\\nAlthough the Mini-batch K-Means algorithm is much faster than the regular K-\\nMeans algorithm, its inertia is generally slightly worse, especially as the number of\\nclusters increases. You can see this in Figure 9-6: the plot on the left compares the\\ninertias of Mini-batch K-Means and regular K-Means models trained on the previous\\ndataset using various numbers of clusters k. The difference between the two curves\\nremains fairly constant, but this difference becomes more and more significant as k\\nincreases, since the inertia becomes smaller and smaller. However, in the plot on the\\nright, you can see that Mini-batch K-Means is much faster than regular K-Means, and\\nthis difference increases with k.\\n246 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 272}, page_content='Figure 9-6. Mini-batch K-Means vs K-Means: worse inertia as k increases (left) but\\nmuch faster (right)\\nFinding the Optimal Number of Clusters\\nSo far, we have set the number of clusters k to 5 because it was obvious by looking at\\nthe data that this is the correct number of clusters. But in general, it will not be so\\neasy to know how to set k, and the result might be quite bad if you set it to the wrong\\nvalue. For example, as you can see in Figure 9-7, setting k to 3 or 8 results in fairly\\nbad models:\\nFigure 9-7. Bad choices for the number of clusters\\nYou might be thinking that we could just pick the model with the lowest inertia,\\nright? Unfortunately, it is not that simple. The inertia for k=3 is 653.2, which is much\\nhigher than for k=5 (which was 211.6), but with k=8, the inertia is just 119.1. The\\ninertia is not a good performance metric when trying to choose k since it keeps get‐\\nting lower as we increase k. Indeed, the more clusters there are, the closer each\\ninstance will be to its closest centroid, and therefore the lower the inertia will be. Let’s\\nplot the inertia as a function of k (see Figure 9-8):\\nClustering \\n| \\n247'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 273}, page_content='Figure 9-8. Selecting the number of clusters k using the “elbow rule”\\nAs you can see, the inertia drops very quickly as we increase k up to 4, but then it\\ndecreases much more slowly as we keep increasing k. This curve has roughly the\\nshape of an arm, and there is an “elbow” at k=4 so if we did not know better, it would\\nbe a good choice: any lower value would be dramatic, while any higher value would\\nnot help much, and we might just be splitting perfectly good clusters in half for no\\ngood reason.\\nThis technique for choosing the best value for the number of clusters is rather coarse.\\nA more precise approach (but also more computationally expensive) is to use the sil‐\\nhouette score, which is the mean silhouette coefficient over all the instances. An instan‐\\nce’s silhouette coefficient is equal to (b – a) / max(a, b) where a is the mean distance\\nto the other instances in the same cluster (it is the mean intra-cluster distance), and b\\nis the mean nearest-cluster distance, that is the mean distance to the instances of the\\nnext closest cluster (defined as the one that minimizes b, excluding the instance’s own\\ncluster). The silhouette coefficient can vary between -1 and +1: a coefficient close to\\n+1 means that the instance is well inside its own cluster and far from other clusters,\\nwhile a coefficient close to 0 means that it is close to a cluster boundary, and finally a\\ncoefficient close to -1 means that the instance may have been assigned to the wrong\\ncluster. To compute the silhouette score, you can use Scikit-Learn’s silhou\\nette_score() function, giving it all the instances in the dataset, and the labels they\\nwere assigned:\\n>>> from sklearn.metrics import silhouette_score\\n>>> silhouette_score(X, kmeans.labels_)\\n0.655517642572828\\nLet’s compare the silhouette scores for different numbers of clusters (see Figure 9-9):\\n248 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 274}, page_content='Figure 9-9. Selecting the number of clusters k using the silhouette score\\nAs you can see, this visualization is much richer than the previous one: in particular,\\nalthough it confirms that k=4 is a very good choice, it also underlines the fact that\\nk=5 is quite good as well, and much better than k=6 or 7. This was not visible when\\ncomparing inertias.\\nAn even more informative visualization is obtained when you plot every instance’s\\nsilhouette coefficient, sorted by the cluster they are assigned to and by the value of the\\ncoefficient. This is called a silhouette diagram (see Figure 9-10):\\nFigure 9-10. Silouhette analysis: comparing the silhouette diagrams for various values of\\nk\\nThe vertical dashed lines represent the silhouette score for each number of clusters.\\nWhen most of the instances in a cluster have a lower coefficient than this score (i.e., if\\nmany of the instances stop short of the dashed line, ending to the left of it), then the\\ncluster is rather bad since this means its instances are much too close to other clus‐\\nClustering \\n| \\n249'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 275}, page_content='ters. We can see that when k=3 and when k=6, we get bad clusters. But when k=4 or\\nk=5, the clusters look pretty good – most instances extend beyond the dashed line, to\\nthe right and closer to 1.0. When k=4, the cluster at index 1 (the third from the top),\\nis rather big, while when k=5, all clusters have similar sizes, so even though the over‐\\nall silhouette score from k=4 is slightly greater than for k=5, it seems like a good idea\\nto use k=5 to get clusters of similar sizes.\\nLimits of K-Means\\nDespite its many merits, most notably being fast and scalable, K-Means is not perfect.\\nAs we saw, it is necessary to run the algorithm several times to avoid sub-optimal sol‐\\nutions, plus you need to specify the number of clusters, which can be quite a hassle.\\nMoreover, K-Means does not behave very well when the clusters have varying sizes,\\ndifferent densities, or non-spherical shapes. For example, Figure 9-11 shows how K-\\nMeans clusters a dataset containing three ellipsoidal clusters of different dimensions,\\ndensities and orientations:\\nFigure 9-11. K-Means fails to cluster these ellipsoidal blobs properly\\nAs you can see, neither of these solutions are any good. The solution on the left is\\nbetter, but it still chops off 25% of the middle cluster and assigns it to the cluster on\\nthe right. The solution on the right is just terrible, even though its inertia is lower. So\\ndepending on the data, different clustering algorithms may perform better. For exam‐\\nple, on these types of elliptical clusters, Gaussian mixture models work great.\\nIt is important to scale the input features before you run K-Means,\\nor else the clusters may be very stretched, and K-Means will per‐\\nform poorly. Scaling the features does not guarantee that all the\\nclusters will be nice and spherical, but it generally improves things.\\nNow let’s look at a few ways we can benefit from clustering. We will use K-Means, but\\nfeel free to experiment with other clustering algorithms.\\n250 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 276}, page_content='Using clustering for image segmentation\\nImage segmentation is the task of partitioning an image into multiple segments. In\\nsemantic segmentation, all pixels that are part of the same object type get assigned to\\nthe same segment. For example, in a self-driving car’s vision system, all pixels that are\\npart of a pedestrian’s image might be assigned to the “pedestrian” segment (there\\nwould just be one segment containing all the pedestrians). In instance segmentation,\\nall pixels that are part of the same individual object are assigned to the same segment.\\nIn this case there would be a different segment for each pedestrian. The state of the\\nart in semantic or instance segmentation today is achieved using complex architec‐\\ntures based on convolutional neural networks (see Chapter 14). Here, we are going to\\ndo something much simpler: color segmentation. We will simply assign pixels to the\\nsame segment if they have a similar color. In some applications, this may be sufficient,\\nfor example if you want to analyze satellite images to measure how much total forest\\narea there is in a region, color segmentation may be just fine.\\nFirst, let’s load the image (see the upper left image in Figure 9-12) using Matplotlib’s\\nimread() function:\\n>>> from matplotlib.image import imread  # you could also use `imageio.imread()`\\n>>> image = imread(os.path.join(\"images\",\"clustering\",\"ladybug.png\"))\\n>>> image.shape\\n(533, 800, 3)\\nThe image is represented as a 3D array: the first dimension’s size is the height, the\\nsecond is the width, and the third is the number of color channels, in this case red,\\ngreen and blue (RGB). In other words, for each pixel there is a 3D vector containing\\nthe intensities of red, green and blue, each between 0.0 and 1.0 (or between 0 and 255\\nif you use imageio.imread()). Some images may have less channels, such as gray‐\\nscale images (one channel), or more channels, such as images with an additional\\nalpha channel for transparency, or satellite images which often contain channels for\\nmany light frequencies (e.g., infrared). The following code reshapes the array to get a\\nlong list of RGB colors, then it clusters these colors using K-Means. For example, it\\nmay identify a color cluster for all shades of green. Next, for each color (e.g., dark\\ngreen), it looks for the mean color of the pixel’s color cluster. For example, all shades\\nof green may be replaced with the same light green color (assuming the mean color of\\nthe green cluster is light green). Finally it reshapes this long list of colors to get the\\nsame shape as the original image. And we’re done!\\nX = image.reshape(-1, 3)\\nkmeans = KMeans(n_clusters=8).fit(X)\\nsegmented_img = kmeans.cluster_centers_[kmeans.labels_]\\nsegmented_img = segmented_img.reshape(image.shape)\\nThis outputs the image shown in the upper right of Figure 9-12. You can experiment\\nwith various numbers of clusters, as shown in the figure. When you use less than 8\\nclusters, notice that the ladybug’s flashy red color fails to get a cluster of its own: it\\nClustering \\n| \\n251'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 277}, page_content='gets merged with colors from the environment. This is due to the fact that the lady‐\\nbug is quite small, much smaller than the rest of the image, so even though its color is\\nflashy, K-Means fails to dedicate a cluster to it: as mentioned earlier, K-Means prefers\\nclusters of similar sizes.\\nFigure 9-12. Image segmentation using K-Means with various numbers of color clusters\\nThat was not too hard, was it? Now let’s look at another application of clustering: pre‐\\nprocessing.\\nUsing Clustering for Preprocessing\\nClustering can be an efficient approach to dimensionality reduction, in particular as a\\npreprocessing step before a supervised learning algorithm. For example, let’s tackle\\nthe digits dataset which is a simple MNIST-like dataset containing 1,797 grayscale 8×8\\nimages representing digits 0 to 9. First, let’s load the dataset:\\nfrom sklearn.datasets import load_digits\\nX_digits, y_digits = load_digits(return_X_y=True)\\nNow, let’s split it into a training set and a test set:\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits)\\nNext, let’s fit a Logistic Regression model:\\nfrom sklearn.linear_model import LogisticRegression\\nlog_reg = LogisticRegression(random_state=42)\\nlog_reg.fit(X_train, y_train)\\nLet’s evaluate its accuracy on the test set:\\n>>> log_reg.score(X_test, y_test)\\n0.9666666666666667\\n252 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 278}, page_content='Okay, that’s our baseline: 96.7% accuracy. Let’s see if we can do better by using K-\\nMeans as a preprocessing step. We will create a pipeline that will first cluster the\\ntraining set into 50 clusters and replace the images with their distances to these 50\\nclusters, then apply a logistic regression model.\\nAlthough it is tempting to define the number of clusters to 10,\\nsince there are 10 different digits, it is unlikely to perform well,\\nbecause there are several different ways to write each digit.\\nfrom sklearn.pipeline import Pipeline\\npipeline = Pipeline([\\n    (\"kmeans\", KMeans(n_clusters=50)),\\n    (\"log_reg\", LogisticRegression()),\\n])\\npipeline.fit(X_train, y_train)\\nNow let’s evaluate this classification pipeline:\\n>>> pipeline.score(X_test, y_test)\\n0.9822222222222222\\nHow about that? We almost divided the error rate by a factor of 2!\\nBut we chose the number of clusters k completely arbitrarily, we can surely do better.\\nSince K-Means is just a preprocessing step in a classification pipeline, finding a good\\nvalue for k is much simpler than earlier: there’s no need to perform silhouette analysis\\nor minimize the inertia, the best value of k is simply the one that results in the best\\nclassification performance during cross-validation. Let’s use GridSearchCV to find the\\noptimal number of clusters:\\nfrom sklearn.model_selection import GridSearchCV\\nparam_grid = dict(kmeans__n_clusters=range(2, 100))\\ngrid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)\\ngrid_clf.fit(X_train, y_train)\\nLet’s look at best value for k, and the performance of the resulting pipeline:\\n>>> grid_clf.best_params_\\n{\\'kmeans__n_clusters\\': 90}\\n>>> grid_clf.score(X_test, y_test)\\n0.9844444444444445\\nWith k=90 clusters, we get a small accuracy boost, reaching 98.4% accuracy on the\\ntest set. Cool!\\nClustering \\n| \\n253'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 279}, page_content='Using Clustering for Semi-Supervised Learning\\nAnother use case for clustering is in semi-supervised learning, when we have plenty\\nof unlabeled instances and very few labeled instances. Let’s train a logistic regression\\nmodel on a sample of 50 labeled instances from the digits dataset:\\nn_labeled = 50\\nlog_reg = LogisticRegression()\\nlog_reg.fit(X_train[:n_labeled], y_train[:n_labeled])\\nWhat is the performance of this model on the test set?\\n>>> log_reg.score(X_test, y_test)\\n0.8266666666666667\\nThe accuracy is just 82.7%: it should come as no surprise that this is much lower than\\nearlier, when we trained the model on the full training set. Let’s see how we can do\\nbetter. First, let’s cluster the training set into 50 clusters, then for each cluster let’s find\\nthe image closest to the centroid. We will call these images the representative images:\\nk = 50\\nkmeans = KMeans(n_clusters=k)\\nX_digits_dist = kmeans.fit_transform(X_train)\\nrepresentative_digit_idx = np.argmin(X_digits_dist, axis=0)\\nX_representative_digits = X_train[representative_digit_idx]\\nFigure 9-13 shows these 50 representative images:\\nFigure 9-13. Fifty representative digit images (one per cluster)\\nNow let’s look at each image and manually label it:\\ny_representative_digits = np.array([4, 8, 0, 6, 8, 3, ..., 7, 6, 2, 3, 1, 1])\\nNow we have a dataset with just 50 labeled instances, but instead of being completely\\nrandom instances, each of them is a representative image of its cluster. Let’s see if the\\nperformance is any better:\\n>>> log_reg = LogisticRegression()\\n>>> log_reg.fit(X_representative_digits, y_representative_digits)\\n>>> log_reg.score(X_test, y_test)\\n0.9244444444444444\\nWow! We jumped from 82.7% accuracy to 92.4%, although we are still only training\\nthe model on 50 instances. Since it is often costly and painful to label instances, espe‐\\n254 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 280}, page_content='cially when it has to be done manually by experts, it is a good idea to label representa‐\\ntive instances rather than just random instances.\\nBut perhaps we can go one step further: what if we propagated the labels to all the\\nother instances in the same cluster? This is called label propagation:\\ny_train_propagated = np.empty(len(X_train), dtype=np.int32)\\nfor i in range(k):\\n    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]\\nNow let’s train the model again and look at its performance:\\n>>> log_reg = LogisticRegression()\\n>>> log_reg.fit(X_train, y_train_propagated)\\n>>> log_reg.score(X_test, y_test)\\n0.9288888888888889\\nWe got a tiny little accuracy boost. Better than nothing, but not astounding. The\\nproblem is that we propagated each representative instance’s label to all the instances\\nin the same cluster, including the instances located close to the cluster boundaries,\\nwhich are more likely to be mislabeled. Let’s see what happens if we only propagate\\nthe labels to the 20% of the instances that are closest to the centroids:\\npercentile_closest = 20\\nX_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\\nfor i in range(k):\\n    in_cluster = (kmeans.labels_ == i)\\n    cluster_dist = X_cluster_dist[in_cluster]\\n    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\\n    above_cutoff = (X_cluster_dist > cutoff_distance)\\n    X_cluster_dist[in_cluster & above_cutoff] = -1\\npartially_propagated = (X_cluster_dist != -1)\\nX_train_partially_propagated = X_train[partially_propagated]\\ny_train_partially_propagated = y_train_propagated[partially_propagated]\\nNow let’s train the model again on this partially propagated dataset:\\n>>> log_reg = LogisticRegression()\\n>>> log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\\n>>> log_reg.score(X_test, y_test)\\n0.9422222222222222\\nNice! With just 50 labeled instances (only 5 examples per class on average!), we got\\n94.2% performance, which is pretty close to the performance of logistic regression on\\nthe fully labeled digits dataset (which was 96.7%). This is because the propagated\\nlabels are actually pretty good, their accuracy is very close to 99%:\\nClustering \\n| \\n255'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 281}, page_content='>>> np.mean(y_train_partially_propagated == y_train[partially_propagated])\\n0.9896907216494846\\nActive Learning\\nTo continue improving your model and your training set, the next step could be to do\\na few rounds of active learning: this is when a human expert interacts with the learn‐\\ning algorithm, providing labels when the algorithm needs them. There are many dif‐\\nferent strategies for active learning, but one of the most common ones is called\\nuncertainty sampling:\\n• The model is trained on the labeled instances gathered so far, and this model is\\nused to make predictions on all the unlabeled instances.\\n• The instances for which the model is most uncertain (i.e., when its estimated\\nprobability is lowest) must be labeled by the expert.\\n• Then you just iterate this process again and again, until the performance\\nimprovement stops being worth the labeling effort.\\nOther strategies include labeling the instances that would result in the largest model\\nchange, or the largest drop in the model’s validation error, or the instances that differ‐\\nent models disagree on (e.g., an SVM, a Random Forest, and so on).\\nBefore we move on to Gaussian mixture models, let’s take a look at DBSCAN,\\nanother popular clustering algorithm that illustrates a very different approach based\\non local density estimation. This approach allows the algorithm to identify clusters of\\narbitrary shapes.\\nDBSCAN\\nThis algorithm defines clusters as continuous regions of high density. It is actually\\nquite simple:\\n• For each instance, the algorithm counts how many instances are located within a\\nsmall distance ε (epsilon) from it. This region is called the instance’s ε-\\nneighborhood.\\n• If an instance has at least min_samples instances in its ε-neighborhood (includ‐\\ning itself), then it is considered a core instance. In other words, core instances are\\nthose that are located in dense regions.\\n• All instances in the neighborhood of a core instance belong to the same cluster.\\nThis may include other core instances, therefore a long sequence of neighboring\\ncore instances forms a single cluster.\\n256 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 282}, page_content='• Any instance that is not a core instance and does not have one in its neighbor‐\\nhood is considered an anomaly.\\nThis algorithm works well if all the clusters are dense enough, and they are well sepa‐\\nrated by low-density regions. The DBSCAN class in Scikit-Learn is as simple to use as\\nyou might expect. Let’s test it on the moons dataset, introduced in Chapter 5:\\nfrom sklearn.cluster import DBSCAN\\nfrom sklearn.datasets import make_moons\\nX, y = make_moons(n_samples=1000, noise=0.05)\\ndbscan = DBSCAN(eps=0.05, min_samples=5)\\ndbscan.fit(X)\\nThe labels of all the instances are now available in the labels_ instance variable:\\n>>> dbscan.labels_\\narray([ 0,  2, -1, -1,  1,  0,  0,  0, ...,  3,  2,  3,  3,  4,  2,  6,  3])\\nNotice that some instances have a cluster index equal to -1: this means that they are\\nconsidered as anomalies by the algorithm. The indices of the core instances are avail‐\\nable in the core_sample_indices_ instance variable, and the core instances them‐\\nselves are available in the components_ instance variable:\\n>>> len(dbscan.core_sample_indices_)\\n808\\n>>> dbscan.core_sample_indices_\\narray([ 0,  4,  5,  6,  7,  8, 10, 11, ..., 992, 993, 995, 997, 998, 999])\\n>>> dbscan.components_\\narray([[-0.02137124,  0.40618608],\\n       [-0.84192557,  0.53058695],\\n                  ...\\n       [-0.94355873,  0.3278936 ],\\n       [ 0.79419406,  0.60777171]])\\nThis clustering is represented in the left plot of Figure 9-14. As you can see, it identi‐\\nfied quite a lot of anomalies, plus 7 different clusters. How disappointing! Fortunately,\\nif we widen each instance’s neighborhood by increasing eps to 0.2, we get the cluster‐\\ning on the right, which looks perfect. Let’s continue with this model.\\nFigure 9-14. DBSCAN clustering using two different neighborhood radiuses\\nClustering \\n| \\n257'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 283}, page_content='Somewhat surprisingly, the DBSCAN class does not have a predict() method,\\nalthough it has a fit_predict() method. In other words, it cannot predict which\\ncluster a new instance belongs to. The rationale for this decision is that several classi‐\\nfication algorithms could make sense here, and it is easy enough to train one, for\\nexample a KNeighborsClassifier:\\nfrom sklearn.neighbors import KNeighborsClassifier\\nknn = KNeighborsClassifier(n_neighbors=50)\\nknn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])\\nNow, given a few new instances, we can predict which cluster they most likely belong\\nto, and even estimate a probability for each cluster. Note that we only trained them on\\nthe core instances, but we could also have chosen to train them on all the instances,\\nor all but the anomalies: this choice depends on the final task.\\n>>> X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\\n>>> knn.predict(X_new)\\narray([1, 0, 1, 0])\\n>>> knn.predict_proba(X_new)\\narray([[0.18, 0.82],\\n       [1.  , 0.  ],\\n       [0.12, 0.88],\\n       [1.  , 0.  ]])\\nThe decision boundary is represented on Figure 9-15 (the crosses represent the 4\\ninstances in X_new). Notice that since there is no anomaly in the KNN’s training set,\\nthe classifier always chooses a cluster, even when that cluster is far away. However, it\\nis fairly straightforward to introduce a maximum distance, in which case the two\\ninstances that are far away from both clusters are classified as anomalies. To do this,\\nwe can use the kneighbors() method of the KNeighborsClassifier: given a set of\\ninstances, it returns the distances and the indices of the k nearest neighbors in the\\ntraining set (two matrices, each with k columns):\\n>>> y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)\\n>>> y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]\\n>>> y_pred[y_dist > 0.2] = -1\\n>>> y_pred.ravel()\\narray([-1,  0,  1, -1])\\n258 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 284}, page_content='Figure 9-15. cluster_classification_diagram\\nIn short, DBSCAN is a very simple yet powerful algorithm, capable of identifying any\\nnumber of clusters, of any shape, it is robust to outliers, and it has just two hyper‐\\nparameters (eps and min_samples). However, if the density varies significantly across\\nthe clusters, it can be impossible for it to capture all the clusters properly. Moreover,\\nits computational complexity is roughly O(m log m), making it pretty close to linear\\nwith regards to the number of instances. However, Scikit-Learn’s implementation can\\nrequire up to O(m2) memory if eps is large.\\nOther Clustering Algorithms\\nScikit-Learn implements several more clustering algorithms that you should take a\\nlook at. We cannot cover them all in detail here, but here is a brief overview:\\n• Agglomerative clustering: a hierarchy of clusters is built from the bottom up.\\nThink of many tiny bubbles floating on water and gradually attaching to each\\nother until there’s just one big group of bubbles. Similarly, at each iteration\\nagglomerative clustering connects the nearest pair of clusters (starting with indi‐\\nvidual instances). If you draw a tree with a branch for every pair of clusters that\\nmerged, you get a binary tree of clusters, where the leaves are the individual\\ninstances. This approach scales very well to large numbers of instances or clus‐\\nters, it can capture clusters of various shapes, it produces a flexible and informa‐\\ntive cluster tree instead of forcing you to choose a particular cluster scale, and it\\ncan be used with any pairwise distance. It can scale nicely to large numbers of\\ninstances if you provide a connectivity matrix. This is a sparse m by m matrix\\nthat indicates which pairs of instances are neighbors (e.g., returned by\\nsklearn.neighbors.kneighbors_graph()). Without a connectivity matrix, the\\nalgorithm does not scale well to large datasets.\\n• Birch: this algorithm was designed specifically for very large datasets, and it can\\nbe faster than batch K-Means, with similar results, as long as the number of fea‐\\ntures is not too large (<20). It builds a tree structure during training containing\\nClustering \\n| \\n259'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 285}, page_content='just enough information to quickly assign each new instance to a cluster, without\\nhaving to store all the instances in the tree: this allows it to use limited memory,\\nwhile handle huge datasets.\\n• Mean-shift: this algorithm starts by placing a circle centered on each instance,\\nthen for each circle it computes the mean of all the instances located within it,\\nand it shifts the circle so that it is centered on the mean. Next, it iterates this\\nmean-shift step until all the circles stop moving (i.e., until each of them is cen‐\\ntered on the mean of the instances it contains). This algorithm shifts the circles\\nin the direction of higher density, until each of them has found a local density\\nmaximum. Finally, all the instances whose circles have settled in the same place\\n(or close enough) are assigned to the same cluster. This has some of the same fea‐\\ntures as DBSCAN, in particular it can find any number of clusters of any shape, it\\nhas just one hyperparameter (the radius of the circles, called the bandwidth) and\\nit relies on local density estimation. However, it tends to chop clusters into pieces\\nwhen they have internal density variations. Unfortunately, its computational\\ncomplexity is O(m2), so it is not suited for large datasets.\\n• Affinity propagation: this algorithm uses a voting system, where instances vote for\\nsimilar instances to be their representatives, and once the algorithm converges,\\neach representative and its voters form a cluster. This algorithm can detect any\\nnumber of clusters of different sizes. Unfortunately, this algorithm has a compu‐\\ntational complexity of O(m2), so it is not suited for large datasets.\\n• Spectral clustering: this algorithm takes a similarity matrix between the instances\\nand creates a low-dimensional embedding from it (i.e., it reduces its dimension‐\\nality), then it uses another clustering algorithm in this low-dimensional space\\n(Scikit-Learn’s implementation uses K-Means). Spectral clustering can capture\\ncomplex cluster structures, and it can also be used to cut graphs (e.g., to identify\\nclusters of friends on a social network), however it does not scale well to large\\nnumber of instances, and it does not behave well when the clusters have very dif‐\\nferent sizes.\\nNow let’s dive into Gaussian mixture models, which can be used for density estima‐\\ntion, clustering and anomaly detection.\\nGaussian Mixtures\\nA Gaussian mixture model (GMM) is a probabilistic model that assumes that the\\ninstances were generated from a mixture of several Gaussian distributions whose\\nparameters are unknown. All the instances generated from a single Gaussian distri‐\\nbution form a cluster that typically looks like an ellipsoid. Each cluster can have a dif‐\\nferent ellipsoidal shape, size, density and orientation, just like in Figure 9-11. When\\nyou observe an instance, you know it was generated from one of the Gaussian distri‐\\n260 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 286}, page_content='7 Phi (ϕ or φ) is the 21st letter of the Greek alphabet.\\n8 Most of these notations are standard, but a few additional notations were taken from the Wikipedia article on\\nplate notation.\\nbutions, but you are not told which one, and you do not know what the parameters of\\nthese distributions are.\\nThere are several GMM variants: in the simplest variant, implemented in the Gaus\\nsianMixture class, you must know in advance the number k of Gaussian distribu‐\\ntions. The dataset X is assumed to have been generated through the following\\nprobabilistic process:\\n• For each instance, a cluster is picked randomly among k clusters. The probability\\nof choosing the jth cluster is defined by the cluster’s weight ϕ(j).7 The index of the\\ncluster chosen for the ith instance is noted z(i).\\n• If z(i)=j, meaning the ith instance has been assigned to the jth cluster, the location\\nx(i) of this instance is sampled randomly from the Gaussian distribution with\\nmean μ(j) and covariance matrix Σ(j). This is noted �i ∼�μ j , Σ j .\\nThis generative process can be represented as a graphical model (see Figure 9-16).\\nThis is a graph which represents the structure of the conditional dependencies\\nbetween random variables.\\nFigure 9-16. Gaussian mixture model\\nHere is how to interpret it:8\\n• The circles represent random variables.\\n• The squares represent fixed values (i.e., parameters of the model).\\nGaussian Mixtures \\n| \\n261'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 287}, page_content='• The large rectangles are called plates: they indicate that their content is repeated\\nseveral times.\\n• The number indicated at the bottom right hand side of each plate indicates how\\nmany times its content is repeated, so there are m random variables z(i) (from z(1)\\nto z(m)) and m random variables x(i), and k means μ(j) and k covariance matrices\\nΣ(j), but just one weight vector ϕ (containing all the weights ϕ(1) to ϕ(k)).\\n• Each variable z(i) is drawn from the categorical distribution with weights ϕ. Each\\nvariable x(i) is drawn from the normal distribution with the mean and covariance\\nmatrix defined by its cluster z(i).\\n• The solid arrows represent conditional dependencies. For example, the probabil‐\\nity distribution for each random variable z(i) depends on the weight vector ϕ.\\nNote that when an arrow crosses a plate boundary, it means that it applies to all\\nthe repetitions of that plate, so for example the weight vector ϕ conditions the\\nprobability distributions of all the random variables x(1) to x(m).\\n• The squiggly arrow from z(i) to x(i) represents a switch: depending on the value of\\nz(i), the instance x(i) will be sampled from a different Gaussian distribution. For\\nexample, if z(i)=j, then �i ∼�μ j , Σ j .\\n• Shaded nodes indicate that the value is known, so in this case only the random\\nvariables x(i) have known values: they are called observed variables. The unknown\\nrandom variables z(i) are called latent variables.\\nSo what can you do with such a model? Well, given the dataset X, you typically want\\nto start by estimating the weights ϕ and all the distribution parameters μ(1) to μ(k) and\\nΣ(1) to Σ(k). Scikit-Learn’s GaussianMixture class makes this trivial:\\nfrom sklearn.mixture import GaussianMixture\\ngm = GaussianMixture(n_components=3, n_init=10)\\ngm.fit(X)\\nLet’s look at the parameters that the algorithm estimated:\\n>>> gm.weights_\\narray([0.20965228, 0.4000662 , 0.39028152])\\n>>> gm.means_\\narray([[ 3.39909717,  1.05933727],\\n       [-1.40763984,  1.42710194],\\n       [ 0.05135313,  0.07524095]])\\n>>> gm.covariances_\\narray([[[ 1.14807234, -0.03270354],\\n        [-0.03270354,  0.95496237]],\\n       [[ 0.63478101,  0.72969804],\\n        [ 0.72969804,  1.1609872 ]],\\n262 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 288}, page_content='[[ 0.68809572,  0.79608475],\\n        [ 0.79608475,  1.21234145]]])\\nGreat, it worked fine! Indeed, the weights that were used to generate the data were\\n0.2, 0.4 and 0.4, and similarly, the means and covariance matrices were very close to\\nthose found by the algorithm. But how? This class relies on the Expectation-\\nMaximization (EM) algorithm, which has many similarities with the K-Means algo‐\\nrithm: it also initializes the cluster parameters randomly, then it repeats two steps\\nuntil convergence, first assigning instances to clusters (this is called the expectation\\nstep) then updating the clusters (this is called the maximization step). Sounds famil‐\\niar? Indeed, in the context of clustering you can think of EM as a generalization of K-\\nMeans which not only finds the cluster centers (μ(1) to μ(k)), but also their size, shape\\nand orientation (Σ(1) to Σ(k)), as well as their relative weights (ϕ(1) to ϕ(k)). Unlike K-\\nMeans, EM uses soft cluster assignments rather than hard assignments: for each\\ninstance during the expectation step, the algorithm estimates the probability that it\\nbelongs to each cluster (based on the current cluster parameters). Then, during the\\nmaximization step, each cluster is updated using all the instances in the dataset, with\\neach instance weighted by the estimated probability that it belongs to that cluster.\\nThese probabilities are called the responsibilities of the clusters for the instances. Dur‐\\ning the maximization step, each cluster’s update will mostly be impacted by the\\ninstances it is most responsible for.\\nUnfortunately, just like K-Means, EM can end up converging to\\npoor solutions, so it needs to be run several times, keeping only the\\nbest solution. This is why we set n_init to 10. Be careful: by default\\nn_init is only set to 1.\\nYou can check whether or not the algorithm converged and how many iterations it\\ntook:\\n>>> gm.converged_\\nTrue\\n>>> gm.n_iter_\\n3\\nOkay, now that you have an estimate of the location, size, shape, orientation and rela‐\\ntive weight of each cluster, the model can easily assign each instance to the most likely\\ncluster (hard clustering) or estimate the probability that it belongs to a particular\\ncluster (soft clustering). For this, just use the predict() method for hard clustering,\\nor the predict_proba() method for soft clustering:\\n>>> gm.predict(X)\\narray([2, 2, 1, ..., 0, 0, 0])\\n>>> gm.predict_proba(X)\\narray([[2.32389467e-02, 6.77397850e-07, 9.76760376e-01],\\n       [1.64685609e-02, 6.75361303e-04, 9.82856078e-01],\\nGaussian Mixtures \\n| \\n263'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 289}, page_content='[2.01535333e-06, 9.99923053e-01, 7.49319577e-05],\\n       ...,\\n       [9.99999571e-01, 2.13946075e-26, 4.28788333e-07],\\n       [1.00000000e+00, 1.46454409e-41, 5.12459171e-16],\\n       [1.00000000e+00, 8.02006365e-41, 2.27626238e-15]])\\nIt is a generative model, meaning you can actually sample new instances from it (note\\nthat they are ordered by cluster index):\\n>>> X_new, y_new = gm.sample(6)\\n>>> X_new\\narray([[ 2.95400315,  2.63680992],\\n       [-1.16654575,  1.62792705],\\n       [-1.39477712, -1.48511338],\\n       [ 0.27221525,  0.690366  ],\\n       [ 0.54095936,  0.48591934],\\n       [ 0.38064009, -0.56240465]])\\n>>> y_new\\narray([0, 1, 2, 2, 2, 2])\\nIt is also possible to estimate the density of the model at any given location. This is\\nachieved using the score_samples() method: for each instance it is given, this\\nmethod estimates the log of the probability density function (PDF) at that location.\\nThe greater the score, the higher the density:\\n>>> gm.score_samples(X)\\narray([-2.60782346, -3.57106041, -3.33003479, ..., -3.51352783,\\n       -4.39802535, -3.80743859])\\nIf you compute the exponential of these scores, you get the value of the PDF at the\\nlocation of the given instances. These are not probabilities, but probability densities:\\nthey can take on any positive value, not just between 0 and 1. To estimate the proba‐\\nbility that an instance will fall within a particular region, you would have to integrate\\nthe PDF over that region (if you do so over the entire space of possible instance loca‐\\ntions, the result will be 1).\\nFigure 9-17 shows the cluster means, the decision boundaries (dashed lines), and the\\ndensity contours of this model:\\n264 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 290}, page_content='Figure 9-17. Cluster means, decision boundaries and density contours of a trained Gaus‐\\nsian mixture model\\nNice! The algorithm clearly found an excellent solution. Of course, we made its task\\neasy by actually generating the data using a set of 2D Gaussian distributions (unfortu‐\\nnately, real life data is not always so Gaussian and low-dimensional), and we also gave\\nthe algorithm the correct number of clusters. When there are many dimensions, or\\nmany clusters, or few instances, EM can struggle to converge to the optimal solution.\\nYou might need to reduce the difficulty of the task by limiting the number of parame‐\\nters that the algorithm has to learn: one way to do this is to limit the range of shapes\\nand orientations that the clusters can have. This can be achieved by imposing con‐\\nstraints on the covariance matrices. To do this, just set the covariance_type hyper‐\\nparameter to one of the following values:\\n• \"spherical\": all clusters must be spherical, but they can have different diameters\\n(i.e., different variances).\\n• \"diag\": clusters can take on any ellipsoidal shape of any size, but the ellipsoid’s\\naxes must be parallel to the coordinate axes (i.e., the covariance matrices must be\\ndiagonal).\\n• \"tied\": all clusters must have the same ellipsoidal shape, size and orientation\\n(i.e., all clusters share the same covariance matrix).\\nBy default, covariance_type is equal to \"full\", which means that each cluster can\\ntake on any shape, size and orientation (it has its own unconstrained covariance\\nmatrix). Figure 9-18 plots the solutions found by the EM algorithm when cova\\nriance_type is set to \"tied\" or \"spherical“.\\nGaussian Mixtures \\n| \\n265'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 291}, page_content='Figure 9-18. covariance_type_diagram\\nThe computational complexity of training a GaussianMixture\\nmodel depends on the number of instances m, the number of\\ndimensions n, the number of clusters k, and the constraints on the\\ncovariance matrices. If covariance_type is \"spherical or \"diag\",\\nit is O(kmn), assuming the data has a clustering structure. If cova\\nriance_type is \"tied\" or \"full\", it is O(kmn2 + kn3), so it will not\\nscale to large numbers of features.\\nGaussian mixture models can also be used for anomaly detection. Let’s see how.\\nAnomaly Detection using Gaussian Mixtures\\nAnomaly detection (also called outlier detection) is the task of detecting instances that\\ndeviate strongly from the norm. These instances are of course called anomalies or\\noutliers, while the normal instances are called inliers. Anomaly detection is very use‐\\nful in a wide variety of applications, for example in fraud detection, or for detecting\\ndefective products in manufacturing, or to remove outliers from a dataset before\\ntraining another model, which can significantly improve the performance of the\\nresulting model.\\nUsing a Gaussian mixture model for anomaly detection is quite simple: any instance\\nlocated in a low-density region can be considered an anomaly. You must define what\\ndensity threshold you want to use. For example, in a manufacturing company that\\ntries to detect defective products, the ratio of defective products is usually well-\\nknown. Say it is equal to 4%, then you can set the density threshold to be the value\\nthat results in having 4% of the instances located in areas below that threshold den‐\\nsity. If you notice that you get too many false positives (i.e., perfectly good products\\nthat are flagged as defective), you can lower the threshold. Conversely, if you have too\\nmany false negatives (i.e., defective products that the system does not flag as defec‐\\ntive), you can increase the threshold. This is the usual precision/recall tradeoff (see\\nChapter 3). Here is how you would identify the outliers using the 4th percentile low‐\\n266 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 292}, page_content='est density as the threshold (i.e., approximately 4% of the instances will be flagged as\\nanomalies):\\ndensities = gm.score_samples(X)\\ndensity_threshold = np.percentile(densities, 4)\\nanomalies = X[densities < density_threshold]\\nThese anomalies are represented as stars on Figure 9-19:\\nFigure 9-19. Anomaly detection using a Gaussian mixture model\\nA closely related task is novelty detection: it differs from anomaly detection in that the\\nalgorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers,\\nwhereas anomaly detection does not make this assumption. Indeed, outlier detection\\nis often precisely used to clean up a dataset.\\nGaussian mixture models try to fit all the data, including the outli‐\\ners, so if you have too many of them, this will bias the model’s view\\nof “normality”: some outliers may wrongly be considered as nor‐\\nmal. If this happens, you can try to fit the model once, use it to\\ndetect and remove the most extreme outliers, then fit the model\\nagain on the cleaned up dataset. Another approach is to use robust\\ncovariance estimation methods (see the EllipticEnvelope class).\\nJust like K-Means, the GaussianMixture algorithm requires you to specify the num‐\\nber of clusters. So how can you find it?\\nSelecting the Number of Clusters\\nWith K-Means, you could use the inertia or the silhouette score to select the appro‐\\npriate number of clusters, but with Gaussian mixtures, it is not possible to use these\\nmetrics because they are not reliable when the clusters are not spherical or have dif‐\\nferent sizes. Instead, you can try to find the model that minimizes a theoretical infor‐\\nGaussian Mixtures \\n| \\n267'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 293}, page_content='mation criterion such as the Bayesian information criterion (BIC) or the Akaike\\ninformation criterion (AIC), defined in Equation 9-1.\\nEquation 9-1. Bayesian information criterion (BIC) and Akaike information\\ncriterion (AIC)\\nBIC =\\nlog m p −2 log L\\nAIC =\\n2p −2 log L\\n• m is the number of instances, as always.\\n• p is the number of parameters learned by the model.\\n• L is the maximized value of the likelihood function of the model.\\nBoth the BIC and the AIC penalize models that have more parameters to learn (e.g.,\\nmore clusters), and reward models that fit the data well. They often end up selecting\\nthe same model, but when they differ, the model selected by the BIC tends to be sim‐\\npler (fewer parameters) than the one selected by the AIC, but it does not fit the data\\nquite as well (this is especially true for larger datasets).\\nLikelihood function\\nThe terms “probability” and “likelihood” are often used interchangeably in the\\nEnglish language, but they have very different meanings in statistics: given a statistical\\nmodel with some parameters θ, the word “probability” is used to describe how plausi‐\\nble a future outcome x is (knowing the parameter values θ), while the word “likeli‐\\nhood” is used to describe how plausible a particular set of parameter values θ are,\\nafter the outcome x is known.\\nConsider a one-dimensional mixture model of two Gaussian distributions centered at\\n-4 and +1. For simplicity, this toy model has a single parameter θ that controls the\\nstandard deviations of both distributions. The top left contour plot in Figure 9-20\\nshows the entire model f(x; θ) as a function of both x and θ. To estimate the probabil‐\\nity distribution of a future outcome x, you need to set the model parameter θ. For\\nexample, if you set it to θ=1.3 (the horizontal line), you get the probability density\\nfunction f(x; θ=1.3) shown in the lower left plot. Say you want to estimate the proba‐\\nbility that x will fall between -2 and +2, you must calculate the integral of the PDF on\\nthis range (i.e., the surface of the shaded region). On the other hand, if you have\\nobserved a single instance x=2.5 (the vertical line in the upper left plot), you get the\\nlikelihood function noted ℒ(θ|x=2.5)=f(x=2.5; θ) represented in the upper right plot.\\nIn short, the PDF is a function of x (with θ fixed) while the likelihood function is a\\nfunction of θ (with x fixed). It is important to understand that the likelihood function\\nis not a probability distribution: if you integrate a probability distribution over all\\n268 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 294}, page_content='possible values of x, you always get 1, but if you integrate the likelihood function over\\nall possible values of θ, the result can be any positive value.\\nFigure 9-20. A model’s parametric function (top left), and some derived functions: a PDF\\n(lower left), a likelihood function (top right) and a log likelihood function (lower right)\\nGiven a dataset X, a common task is to try to estimate the most likely values for the\\nmodel parameters. To do this, you must find the values that maximize the likelihood\\nfunction, given X. In this example, if you have observed a single instance x=2.5, the\\nmaximum likelihood estimate (MLE) of θ is θ=1.5. If a prior probability distribution g\\nover θ exists, it is possible to take it into account by maximizing ℒ(θ|x)g(θ) rather\\nthan just maximizing ℒ(θ|x). This is called maximum a-posteriori (MAP) estimation.\\nSince MAP constrains the parameter values, you can think of it as a regularized ver‐\\nsion of MLE.\\nNotice that it is equivalent to maximize the likelihood function or to maximize its\\nlogarithm (represented in the lower right hand side of Figure 9-20): indeed, the loga‐\\nrithm is a strictly increasing function, so if θ maximizes the log likelihood, it also\\nmaximizes the likelihood. It turns out that it is generally easier to maximize the log\\nlikelihood. For example, if you observed several independent instances x(1) to x(m), you\\nwould need to find the value of θ that maximizes the product of the individual likeli‐\\nhood functions. But it is equivalent, and much simpler, to maximize the sum (not the\\nproduct) of the log likelihood functions, thanks to the magic of the logarithm which\\nconverts products into sums: log(ab)=log(a)+log(b).\\nOnce you have estimated θ, the value of θ that maximizes the likelihood function,\\nthen you are ready to compute L = ℒθ, �. This is the value which is used to com‐\\npute the AIC and BIC: you can think of it as a measure of how well the model fits the\\ndata.\\nTo compute the BIC and AIC, just call the bic() or aic() methods:\\nGaussian Mixtures \\n| \\n269'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 295}, page_content='>>> gm.bic(X)\\n8189.74345832983\\n>>> gm.aic(X)\\n8102.518178214792\\nFigure 9-21 shows the BIC for different numbers of clusters k. As you can see, both\\nthe BIC and the AIC are lowest when k=3, so it is most likely the best choice. Note\\nthat we could also search for the best value for the covariance_type hyperparameter.\\nFor example, if it is \"spherical\" rather than \"full\", then the model has much fewer\\nparameters to learn, but it does not fit the data as well.\\nFigure 9-21. AIC and BIC for different numbers of clusters k\\nBayesian Gaussian Mixture Models\\nRather than manually searching for the optimal number of clusters, it is possible to\\nuse instead the BayesianGaussianMixture class which is capable of giving weights\\nequal (or close) to zero to unnecessary clusters. Just set the number of clusters n_com\\nponents to a value that you have good reason to believe is greater than the optimal\\nnumber of clusters (this assumes some minimal knowledge about the problem at\\nhand), and the algorithm will eliminate the unnecessary clusters automatically. For\\nexample, let’s set the number of clusters to 10 and see what happens:\\n>>> from sklearn.mixture import BayesianGaussianMixture\\n>>> bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)\\n>>> bgm.fit(X)\\n>>> np.round(bgm.weights_, 2)\\narray([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])\\nPerfect: the algorithm automatically detected that only 3 clusters are needed, and the\\nresulting clusters are almost identical to the ones in Figure 9-17.\\nIn this model, the cluster parameters (including the weights, means and covariance\\nmatrices) are not treated as fixed model parameters anymore, but as latent random\\nvariables, like the cluster assignments (see Figure 9-22). So z now includes both the\\ncluster parameters and the cluster assignments.\\n270 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 296}, page_content='Figure 9-22. Bayesian Gaussian mixture model\\nPrior knowledge about the latent variables z can be encoded in a probability distribu‐\\ntion p(z) called the prior. For example, we may have a prior belief that the clusters are\\nlikely to be few (low concentration), or conversely, that they are more likely to be\\nplentiful (high concentration). This can be adjusted using the weight_concentra\\ntion_prior hyperparameter. Setting it to 0.01 or 1000 gives very different clusterings\\n(see Figure 9-23). However, the more data we have, the less the priors matter. In fact,\\nto plot diagrams with such large differences, you must use very strong priors and lit‐\\ntle data.\\nFigure 9-23. Using different concentration priors\\nThe fact that you see only 3 regions in the right plot although there\\nare 4 centroids is not a bug: the weight of the top-right cluster is\\nmuch larger than the weight of the lower-right cluster, so the prob‐\\nability that any given point in this region belongs to the top-right\\ncluster is greater than the probability that it belongs to the lower-\\nright cluster, even near the lower-right cluster.\\nGaussian Mixtures \\n| \\n271'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 297}, page_content='Bayes’ theorem (Equation 9-2) tells us how to update the probability distribution over\\nthe latent variables after we observe some data X. It computes the posterior distribu‐\\ntion p(z|X), which is the conditional probability of z given X.\\nEquation 9-2. Bayes’ theorem\\np z X = Posterior = Likelihood×Prior\\nEvidence\\n= p X z p z\\np X\\nUnfortunately, in a Gaussian mixture model (and many other problems), the denomi‐\\nnator p(x) is intractable, as it requires integrating over all the possible values of z\\n(Equation 9-3). This means considering all possible combinations of cluster parame‐\\nters and cluster assignments.\\nEquation 9-3. The evidence p(X) is often intractable\\np X =∫p X z p z dz\\nThis is one of the central problems in Bayesian statistics, and there are several\\napproaches to solving it. One of them is variational inference, which picks a family of\\ndistributions q(z; λ) with its own variational parameters λ (lambda), then it optimizes\\nthese parameters to make q(z) a good approximation of p(z|X). This is achieved by\\nfinding the value of λ that minimizes the KL divergence from q(z) to p(z|X), noted\\nDKL(q‖p). The KL divergence equation is shown in (see Equation 9-4), and it can be\\nrewritten as the log of the evidence (log p(X)) minus the evidence lower bound\\n(ELBO). Since the log of the evidence does not depend on q, it is a constant term, so\\nminimizing the KL divergence just requires maximizing the ELBO.\\nEquation 9-4. KL divergence from q(z) to p(z|X)\\nDKL q ∥p = �q log\\nq z\\np z\\nX\\n= �q log q z −log p z\\nX\\n= �q log q z −log p z, X\\np X\\n= �q log q z −log p z, X + log p X\\n= �q log q z\\n−�q log p z, X\\n+ �q log p X\\n= �q log p X\\n−�q log p z, X\\n−�q log q z\\n=\\nlog p X −ELBO\\nwhere ELBO = �q log p z, X\\n−�q log q z\\n272 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 298}, page_content='In practice, there are different techniques to maximize the ELBO. In mean field varia‐\\ntional inference, it is necessary to pick the family of distributions q(z; λ) and the prior\\np(z) very carefully to ensure that the equation for the ELBO simplifies to a form that\\ncan actually be computed. Unfortunately, there is no general way to do this, it\\ndepends on the task and requires some mathematical skills. For example, the distribu‐\\ntions and lower bound equations used in Scikit-Learn’s BayesianGaussianMixture\\nclass are presented in the documentation. From these equations it is possible to derive\\nupdate equations for the cluster parameters and assignment variables: these are then\\nused very much like in the Expectation-Maximization algorithm. In fact, the compu‐\\ntational complexity of the BayesianGaussianMixture class is similar to that of the\\nGaussianMixture class (but generally significantly slower). A simpler approach to\\nmaximizing the ELBO is called black box stochastic variational inference (BBSVI): at\\neach iteration, a few samples are drawn from q and they are used to estimate the gra‐\\ndients of the ELBO with regards to the variational parameters λ, which are then used\\nin a gradient ascent step. This approach makes it possible to use Bayesian inference\\nwith any kind of model (provided it is differentiable), even deep neural networks: this\\nis called Bayesian deep learning.\\nIf you want to dive deeper into Bayesian statistics, check out the\\nBayesian Data Analysis book by Andrew Gelman, John Carlin, Hal\\nStern, David Dunson, Aki Vehtari, and Donald Rubin.\\nGaussian mixture models work great on clusters with ellipsoidal shapes, but if you try\\nto fit a dataset with different shapes, you may have bad surprises. For example, let’s\\nsee what happens if we use a Bayesian Gaussian mixture model to cluster the moons\\ndataset (see Figure 9-24):\\nFigure 9-24. moons_vs_bgm_diagram\\nOops, the algorithm desperately searched for ellipsoids, so it found 8 different clus‐\\nters instead of 2. The density estimation is not too bad, so this model could perhaps\\nbe used for anomaly detection, but it failed to identify the two moons. Let’s now look\\nat a few clustering algorithms capable of dealing with arbitrarily shaped clusters.\\nGaussian Mixtures \\n| \\n273'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 299}, page_content='Other Anomaly Detection and Novelty Detection Algorithms\\nScikit-Learn also implements a few algorithms dedicated to anomaly detection or\\nnovelty detection:\\n• Fast-MCD (minimum covariance determinant), implemented by the EllipticEn\\nvelope class: this algorithm is useful for outlier detection, in particular to\\ncleanup a dataset. It assumes that the normal instances (inliers) are generated\\nfrom a single Gaussian distribution (not a mixture), but it also assumes that the\\ndataset is contaminated with outliers that were not generated from this Gaussian\\ndistribution. When it estimates the parameters of the Gaussian distribution (i.e.,\\nthe shape of the elliptic envelope around the inliers), it is careful to ignore the\\ninstances that are most likely outliers. This gives a better estimation of the elliptic\\nenvelope, and thus makes it better at identifying the outliers.\\n• Isolation forest: this is an efficient algorithm for outlier detection, especially in\\nhigh-dimensional datasets. The algorithm builds a Random Forest in which each\\nDecision Tree is grown randomly: at each node, it picks a feature randomly, then\\nit picks a random threshold value (between the min and max value) to split the\\ndataset in two. The dataset gradually gets chopped into pieces this way, until all\\ninstances end up isolated from the other instances. An anomaly is usually far\\nfrom other instances, so on average (across all the Decision Trees) it tends to get\\nisolated in less steps than normal instances.\\n• Local outlier factor (LOF): this algorithm is also good for outlier detection. It\\ncompares the density of instances around a given instance to the density around\\nits neighbors. An anomaly is often more isolated than its k nearest neighbors.\\n• One-class SVM: this algorithm is better suited for novelty detection. Recall that a\\nkernelized SVM classifier separates two classes by first (implicitly) mapping all\\nthe instances to a high-dimensional space, then separating the two classes using a\\nlinear SVM classifier within this high-dimensional space (see Chapter 5). Since\\nwe just have one class of instances, the one-class SVM algorithm instead tries to\\nseparate the instances in high-dimensional space from the origin. In the original\\nspace, this will correspond to finding a small region that encompasses all the\\ninstances. If a new instance does not fall within this region, it is an anomaly.\\nThere are a few hyperparameters to tweak: the usual ones for a kernelized SVM,\\nplus a margin hyperparameter that corresponds to the probability of a new\\ninstance being mistakenly considered as novel, when it is in fact normal. It works\\ngreat, especially with high-dimensional datasets, but just like all SVMs, it does\\nnot scale to large datasets.\\n274 \\n| \\nChapter 9: Unsupervised Learning Techniques'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 300}, page_content='PART II\\nNeural Networks and Deep Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 301}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 302}, page_content='1 You can get the best of both worlds by being open to biological inspirations without being afraid to create\\nbiologically unrealistic models, as long as they work well.\\nCHAPTER 10\\nIntroduction to Artificial Neural Networks\\nwith Keras\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 10 in the final\\nrelease of the book.\\nBirds inspired us to fly, burdock plants inspired velcro, and countless more inven‐\\ntions were inspired by nature. It seems only logical, then, to look at the brain’s archi‐\\ntecture for inspiration on how to build an intelligent machine. This is the key idea\\nthat sparked artificial neural networks (ANNs). However, although planes were\\ninspired by birds, they don’t have to flap their wings. Similarly, ANNs have gradually\\nbecome quite different from their biological cousins. Some researchers even argue\\nthat we should drop the biological analogy altogether (e.g., by saying “units” rather\\nthan “neurons”), lest we restrict our creativity to biologically plausible systems.1\\nANNs are at the very core of Deep Learning. They are versatile, powerful, and scala‐\\nble, making them ideal to tackle large and highly complex Machine Learning tasks,\\nsuch as classifying billions of images (e.g., Google Images), powering speech recogni‐\\ntion services (e.g., Apple’s Siri), recommending the best videos to watch to hundreds\\nof millions of users every day (e.g., YouTube), or learning to beat the world champion\\nat the game of Go by playing millions of games against itself (DeepMind’s Alpha‐\\nZero).\\n277'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 303}, page_content='2 “A Logical Calculus of Ideas Immanent in Nervous Activity,” W. McCulloch and W. Pitts (1943).\\nIn the first part of this chapter, we will introduce artificial neural networks, starting\\nwith a quick tour of the very first ANN architectures, leading up to Multi-Layer Per‐\\nceptrons (MLPs) which are heavily used today (other architectures will be explored in\\nthe next chapters). In the second part, we will look at how to implement neural net‐\\nworks using the popular Keras API. This is a beautifully designed and simple high-\\nlevel API for building, training, evaluating and running neural networks. But don’t be\\nfooled by its simplicity: it is expressive and flexible enough to let you build a wide\\nvariety of neural network architectures. In fact, it will probably be sufficient for most\\nof your use cases. Moreover, should you ever need extra flexibility, you can always\\nwrite custom Keras components using its lower-level API, as we will see in Chap‐\\nter 12.\\nBut first, let’s go back in time to see how artificial neural networks came to be!\\nFrom Biological to Artificial Neurons\\nSurprisingly, ANNs have been around for quite a while: they were first introduced\\nback in 1943 by the neurophysiologist Warren McCulloch and the mathematician\\nWalter Pitts. In their landmark paper,2 “A Logical Calculus of Ideas Immanent in\\nNervous Activity,” McCulloch and Pitts presented a simplified computational model\\nof how biological neurons might work together in animal brains to perform complex\\ncomputations using propositional logic. This was the first artificial neural network\\narchitecture. Since then many other architectures have been invented, as we will see.\\nThe early successes of ANNs until the 1960s led to the widespread belief that we\\nwould soon be conversing with truly intelligent machines. When it became clear that\\nthis promise would go unfulfilled (at least for quite a while), funding flew elsewhere\\nand ANNs entered a long winter. In the early 1980s there was a revival of interest in \\nconnectionism (the study of neural networks), as new architectures were invented and\\nbetter training techniques were developed. But progress was slow, and by the 1990s\\nother powerful Machine Learning techniques were invented, such as Support Vector\\nMachines (see Chapter 5). These techniques seemed to offer better results and stron‐\\nger theoretical foundations than ANNs, so once again the study of neural networks\\nentered a long winter.\\nFinally, we are now witnessing yet another wave of interest in ANNs. Will this wave\\ndie out like the previous ones did? Well, there are a few good reasons to believe that\\nthis wave is different and that it will have a much more profound impact on our lives:\\n278 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 304}, page_content='• There is now a huge quantity of data available to train neural networks, and\\nANNs frequently outperform other ML techniques on very large and complex\\nproblems.\\n• The tremendous increase in computing power since the 1990s now makes it pos‐\\nsible to train large neural networks in a reasonable amount of time. This is in\\npart due to Moore’s Law, but also thanks to the gaming industry, which has pro‐\\nduced powerful GPU cards by the millions.\\n• The training algorithms have been improved. To be fair they are only slightly dif‐\\nferent from the ones used in the 1990s, but these relatively small tweaks have a\\nhuge positive impact.\\n• Some theoretical limitations of ANNs have turned out to be benign in practice.\\nFor example, many people thought that ANN training algorithms were doomed\\nbecause they were likely to get stuck in local optima, but it turns out that this is\\nrather rare in practice (or when it is the case, they are usually fairly close to the\\nglobal optimum).\\n• ANNs seem to have entered a virtuous circle of funding and progress. Amazing\\nproducts based on ANNs regularly make the headline news, which pulls more\\nand more attention and funding toward them, resulting in more and more pro‐\\ngress, and even more amazing products.\\nBiological Neurons\\nBefore we discuss artificial neurons, let’s take a quick look at a biological neuron (rep‐\\nresented in Figure 10-1). It is an unusual-looking cell mostly found in animal cerebral\\ncortexes (e.g., your brain), composed of a cell body containing the nucleus and most\\nof the cell’s complex components, and many branching extensions called dendrites,\\nplus one very long extension called the axon. The axon’s length may be just a few\\ntimes longer than the cell body, or up to tens of thousands of times longer. Near its\\nextremity the axon splits off into many branches called telodendria, and at the tip of\\nthese branches are minuscule structures called synaptic terminals (or simply synap‐\\nses), which are connected to the dendrites (or directly to the cell body) of other neu‐\\nrons. Biological neurons receive short electrical impulses called signals from other\\nneurons via these synapses. When a neuron receives a sufficient number of signals\\nfrom other neurons within a few milliseconds, it fires its own signals.\\nFrom Biological to Artificial Neurons \\n| \\n279'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 305}, page_content='3 Image by Bruce Blaus (Creative Commons 3.0). Reproduced from https://en.wikipedia.org/wiki/Neuron.\\n4 In the context of Machine Learning, the phrase “neural networks” generally refers to ANNs, not BNNs.\\n5 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.wikipe\\ndia.org/wiki/Cerebral_cortex.\\nFigure 10-1. Biological neuron3\\nThus, individual biological neurons seem to behave in a rather simple way, but they\\nare organized in a vast network of billions of neurons, each neuron typically connec‐\\nted to thousands of other neurons. Highly complex computations can be performed\\nby a vast network of fairly simple neurons, much like a complex anthill can emerge\\nfrom the combined efforts of simple ants. The architecture of biological neural net‐\\nworks (BNN)4 is still the subject of active research, but some parts of the brain have\\nbeen mapped, and it seems that neurons are often organized in consecutive layers, as \\nshown in Figure 10-2.\\nFigure 10-2. Multiple layers in a biological neural network (human cortex)5\\n280 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 306}, page_content='Logical Computations with Neurons\\nWarren McCulloch and Walter Pitts proposed a very simple model of the biological\\nneuron, which later became known as an artificial neuron: it has one or more binary\\n(on/off) inputs and one binary output. The artificial neuron simply activates its out‐\\nput when more than a certain number of its inputs are active. McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want. For example, let’s\\nbuild a few ANNs that perform various logical computations (see Figure 10-3),\\nassuming that a neuron is activated when at least two of its inputs are active.\\nFigure 10-3. ANNs performing simple logical computations\\n• The first network on the left is simply the identity function: if neuron A is activa‐\\nted, then neuron C gets activated as well (since it receives two input signals from\\nneuron A), but if neuron A is off, then neuron C is off as well.\\n• The second network performs a logical AND: neuron C is activated only when\\nboth neurons A and B are activated (a single input signal is not enough to acti‐\\nvate neuron C).\\n• The third network performs a logical OR: neuron C gets activated if either neu‐\\nron A or neuron B is activated (or both).\\n• Finally, if we suppose that an input connection can inhibit the neuron’s activity\\n(which is the case with biological neurons), then the fourth network computes a\\nslightly more complex logical proposition: neuron C is activated only if neuron A\\nis active and if neuron B is off. If neuron A is active all the time, then you get a\\nlogical NOT: neuron C is active when neuron B is off, and vice versa.\\nYou can easily imagine how these networks can be combined to compute complex\\nlogical expressions (see the exercises at the end of the chapter).\\nThe Perceptron\\nThe Perceptron is one of the simplest ANN architectures, invented in 1957 by Frank\\nRosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4) called \\nFrom Biological to Artificial Neurons \\n| \\n281'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 307}, page_content='6 The name Perceptron is sometimes used to mean a tiny network with a single TLU.\\na threshold logic unit (TLU), or sometimes a linear threshold unit (LTU): the inputs\\nand output are now numbers (instead of binary on/off values) and each input con‐\\nnection is associated with a weight. The TLU computes a weighted sum of its inputs\\n(z = w1 x1 + w2 x2 + ⋯ + wn xn = xT w), then applies a step function to that sum and\\noutputs the result: hw(x) = step(z), where z = xT w.\\nFigure 10-4. Threshold logic unit\\nThe most common step function used in Perceptrons is the Heaviside step function\\n(see Equation 10-1). Sometimes the sign function is used instead.\\nEquation 10-1. Common step functions used in Perceptrons\\nheaviside z = 0 if z < 0\\n1 if z ≥0\\nsgn z =\\n−1 if z < 0\\n0\\nif z = 0\\n+1 if z > 0\\nA single TLU can be used for simple linear binary classification. It computes a linear\\ncombination of the inputs and if the result exceeds a threshold, it outputs the positive\\nclass or else outputs the negative class (just like a Logistic Regression classifier or a\\nlinear SVM). For example, you could use a single TLU to classify iris flowers based on\\nthe petal length and width (also adding an extra bias feature x0 = 1, just like we did in\\nprevious chapters). Training a TLU in this case means finding the right values for w0,\\nw1, and w2 (the training algorithm is discussed shortly).\\nA Perceptron is simply composed of a single layer of TLUs,6 with each TLU connected\\nto all the inputs. When all the neurons in a layer are connected to every neuron in the\\nprevious layer (i.e., its input neurons), it is called a fully connected layer or a dense\\nlayer. To represent the fact that each input is sent to every TLU, it is common to draw\\nspecial passthrough neurons called input neurons: they just output whatever input\\nthey are fed. All the input neurons form the input layer. Moreover, an extra bias fea‐\\n282 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 308}, page_content='ture is generally added (x0 = 1): it is typically represented using a special type of neu‐\\nron called a bias neuron, which just outputs 1 all the time. A Perceptron with two\\ninputs and three outputs is represented in Figure 10-5. This Perceptron can classify\\ninstances simultaneously into three different binary classes, which makes it a multi‐\\noutput classifier.\\nFigure 10-5. Perceptron diagram\\nThanks to the magic of linear algebra, it is possible to efficiently compute the outputs\\nof a layer of artificial neurons for several instances at once, by using Equation 10-2:\\nEquation 10-2. Computing the outputs of a fully connected layer\\nhW, b X = ϕ XW + b\\n• As always, X represents the matrix of input features. It has one row per instance,\\none column per feature.\\n• The weight matrix W contains all the connection weights except for the ones\\nfrom the bias neuron. It has one row per input neuron and one column per artifi‐\\ncial neuron in the layer.\\n• The bias vector b contains all the connection weights between the bias neuron\\nand the artificial neurons. It has one bias term per artificial neuron.\\n• The function ϕ is called the activation function: when the artificial neurons are\\nTLUs, it is a step function (but we will discuss other activation functions shortly).\\nSo how is a Perceptron trained? The Perceptron training algorithm proposed by\\nFrank Rosenblatt was largely inspired by Hebb’s rule. In his book The Organization of\\nBehavior, published in 1949, Donald Hebb suggested that when a biological neuron\\noften triggers another neuron, the connection between these two neurons grows\\nstronger. This idea was later summarized by Siegrid Löwel in this catchy phrase:\\n“Cells that fire together, wire together.” This rule later became known as Hebb’s rule \\nFrom Biological to Artificial Neurons \\n| \\n283'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 309}, page_content='7 Note that this solution is generally not unique: in general when the data are linearly separable, there is an\\ninfinity of hyperplanes that can separate them.\\n(or Hebbian learning); that is, the connection weight between two neurons is\\nincreased whenever they have the same output. Perceptrons are trained using a var‐\\niant of this rule that takes into account the error made by the network; it reinforces\\nconnections that help reduce the error. More specifically, the Perceptron is fed one\\ntraining instance at a time, and for each instance it makes its predictions. For every\\noutput neuron that produced a wrong prediction, it reinforces the connection\\nweights from the inputs that would have contributed to the correct prediction. The\\nrule is shown in Equation 10-3.\\nEquation 10-3. Perceptron learning rule (weight update)\\nwi, j\\nnext step = wi, j + η yj −y j xi\\n• wi, j is the connection weight between the ith input neuron and the jth output neu‐\\nron.\\n• xi is the ith input value of the current training instance.\\n• y j is the output of the jth output neuron for the current training instance.\\n• yj is the target output of the jth output neuron for the current training instance.\\n• η is the learning rate.\\nThe decision boundary of each output neuron is linear, so Perceptrons are incapable\\nof learning complex patterns (just like Logistic Regression classifiers). However, if the\\ntraining instances are linearly separable, Rosenblatt demonstrated that this algorithm\\nwould converge to a solution.7 This is called the Perceptron convergence theorem.\\nScikit-Learn provides a Perceptron class that implements a single TLU network. It\\ncan be used pretty much as you would expect—for example, on the iris dataset (intro‐\\nduced in Chapter 4):\\nimport numpy as np\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.linear_model import Perceptron\\niris = load_iris()\\nX = iris.data[:, (2, 3)]  # petal length, petal width\\ny = (iris.target == 0).astype(np.int)  # Iris Setosa?\\nper_clf = Perceptron()\\nper_clf.fit(X, y)\\n284 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 310}, page_content='y_pred = per_clf.predict([[2, 0.5]])\\nYou may have noticed the fact that the Perceptron learning algorithm strongly resem‐\\nbles Stochastic Gradient Descent. In fact, Scikit-Learn’s Perceptron class is equivalent\\nto using an SGDClassifier with the following hyperparameters: loss=\"perceptron\",\\nlearning_rate=\"constant\", eta0=1 (the learning rate), and penalty=None (no regu‐\\nlarization).\\nNote that contrary to Logistic Regression classifiers, Perceptrons do not output a class\\nprobability; rather, they just make predictions based on a hard threshold. This is one\\nof the good reasons to prefer Logistic Regression over Perceptrons.\\nIn their 1969 monograph titled Perceptrons, Marvin Minsky and Seymour Papert\\nhighlighted a number of serious weaknesses of Perceptrons, in particular the fact that\\nthey are incapable of solving some trivial problems (e.g., the Exclusive OR (XOR)\\nclassification problem; see the left side of Figure 10-6). Of course this is true of any\\nother linear classification model as well (such as Logistic Regression classifiers), but\\nresearchers had expected much more from Perceptrons, and their disappointment\\nwas great, and many researchers dropped neural networks altogether in favor of\\nhigher-level problems such as logic, problem solving, and search.\\nHowever, it turns out that some of the limitations of Perceptrons can be eliminated by\\nstacking multiple Perceptrons. The resulting ANN is called a Multi-Layer Perceptron\\n(MLP). In particular, an MLP can solve the XOR problem, as you can verify by com‐\\nputing the output of the MLP represented on the right of Figure 10-6: with inputs (0,\\n0) or (1, 1) the network outputs 0, and with inputs (0, 1) or (1, 0) it outputs 1. All\\nconnections have a weight equal to 1, except the four connections where the weight is\\nshown. Try verifying that this network indeed solves the XOR problem!\\nFigure 10-6. XOR classification problem and an MLP that solves it\\nFrom Biological to Artificial Neurons \\n| \\n285'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 311}, page_content='8 In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see\\nANNs with dozens of layers, or even hundreds, so the definition of “deep” is quite fuzzy.\\n9 “Learning Internal Representations by Error Propagation,” D. Rumelhart, G. Hinton, R. Williams (1986).\\nMulti-Layer Perceptron and Backpropagation\\nAn MLP is composed of one (passthrough) input layer, one or more layers of TLUs,\\ncalled hidden layers, and one final layer of TLUs called the output layer (see\\nFigure 10-7). The layers close to the input layer are usually called the lower layers,\\nand the ones close to the outputs are usually called the upper layers. Every layer\\nexcept the output layer includes a bias neuron and is fully connected to the next layer.\\nFigure 10-7. Multi-Layer Perceptron\\nThe signal flows only in one direction (from the inputs to the out‐\\nputs), so this architecture is an example of a feedforward neural net‐\\nwork (FNN).\\nWhen an ANN contains a deep stack of hidden layers8, it is called a deep neural net‐\\nwork (DNN). The field of Deep Learning studies DNNs, and more generally models\\ncontaining deep stacks of computations. However, many people talk about Deep\\nLearning whenever neural networks are involved (even shallow ones).\\nFor many years researchers struggled to find a way to train MLPs, without success.\\nBut in 1986, David Rumelhart, Geoffrey Hinton and Ronald Williams published a\\ngroundbreaking paper9 introducing the backpropagation training algorithm, which is\\nstill used today. In short, it is simply Gradient Descent (introduced in Chapter 4)\\n286 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 312}, page_content='10 This technique was actually independently invented several times by various researchers in different fields,\\nstarting with P. Werbos in 1974.\\nusing an efficient technique for computing the gradients automatically10: in just two\\npasses through the network (one forward, one backward), the backpropagation algo‐\\nrithm is able to compute the gradient of the network’s error with regards to every sin‐\\ngle model parameter. In other words, it can find out how each connection weight and\\neach bias term should be tweaked in order to reduce the error. Once it has these gra‐\\ndients, it just performs a regular Gradient Descent step, and the whole process is\\nrepeated until the network converges to the solution.\\nAutomatically computing gradients is called automatic differentia‐\\ntion, or autodiff. There are various autodiff techniques, with differ‐\\nent pros and cons. The one used by backpropagation is called\\nreverse-mode autodiff. It is fast and precise, and is well suited when\\nthe function to differentiate has many variables (e.g., connection\\nweights) and few outputs (e.g., one loss). If you want to learn more\\nabout autodiff, check out ???.\\nLet’s run through this algorithm in a bit more detail:\\n• It handles one mini-batch at a time (for example containing 32 instances each),\\nand it goes through the full training set multiple times. Each pass is called an\\nepoch, as we saw in Chapter 4.\\n• Each mini-batch is passed to the network’s input layer, which just sends it to the\\nfirst hidden layer. The algorithm then computes the output of all the neurons in\\nthis layer (for every instance in the mini-batch). The result is passed on to the\\nnext layer, its output is computed and passed to the next layer, and so on until we\\nget the output of the last layer, the output layer. This is the forward pass: it is\\nexactly like making predictions, except all intermediate results are preserved\\nsince they are needed for the backward pass.\\n• Next, the algorithm measures the network’s output error (i.e., it uses a loss func‐\\ntion that compares the desired output and the actual output of the network, and\\nreturns some measure of the error).\\n• Then it computes how much each output connection contributed to the error.\\nThis is done analytically by simply applying the chain rule (perhaps the most fun‐\\ndamental rule in calculus), which makes this step fast and precise.\\n• The algorithm then measures how much of these error contributions came from\\neach connection in the layer below, again using the chain rule—and so on until\\nthe algorithm reaches the input layer. As we explained earlier, this reverse pass\\nefficiently measures the error gradient across all the connection weights in the\\nFrom Biological to Artificial Neurons \\n| \\n287'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 313}, page_content='network by propagating the error gradient backward through the network (hence\\nthe name of the algorithm).\\n• Finally, the algorithm performs a Gradient Descent step to tweak all the connec‐\\ntion weights in the network, using the error gradients it just computed.\\nThis algorithm is so important, it’s worth summarizing it again: for each training\\ninstance the backpropagation algorithm first makes a prediction (forward pass),\\nmeasures the error, then goes through each layer in reverse to measure the error con‐\\ntribution from each connection (reverse pass), and finally slightly tweaks the connec‐\\ntion weights to reduce the error (Gradient Descent step).\\nIt is important to initialize all the hidden layers’ connection weights\\nrandomly, or else training will fail. For example, if you initialize all\\nweights and biases to zero, then all neurons in a given layer will be\\nperfectly identical, and thus backpropagation will affect them in\\nexactly the same way, so they will remain identical. In other words,\\ndespite having hundreds of neurons per layer, your model will act\\nas if it had only one neuron per layer: it won’t be too smart. If\\ninstead you randomly initialize the weights, you break the symme‐\\ntry and allow backpropagation to train a diverse team of neurons.\\nIn order for this algorithm to work properly, the authors made a key change to the\\nMLP’s architecture: they replaced the step function with the logistic function, σ(z) =\\n1 / (1 + exp(–z)). This was essential because the step function contains only flat seg‐\\nments, so there is no gradient to work with (Gradient Descent cannot move on a flat\\nsurface), while the logistic function has a well-defined nonzero derivative every‐\\nwhere, allowing Gradient Descent to make some progress at every step. In fact, the\\nbackpropagation algorithm works well with many other activation functions, not just\\nthe logistic function. Two other popular activation functions are:\\nThe hyperbolic tangent function tanh(z) = 2σ(2z) – 1\\nJust like the logistic function it is S-shaped, continuous, and differentiable, but its\\noutput value ranges from –1 to 1 (instead of 0 to 1 in the case of the logistic func‐\\ntion), which tends to make each layer’s output more or less centered around 0 at\\nthe beginning of training. This often helps speed up convergence.\\nThe Rectified Linear Unit function: ReLU(z) = max(0, z)\\nIt is continuous but unfortunately not differentiable at z = 0 (the slope changes\\nabruptly, which can make Gradient Descent bounce around), and its derivative is\\n0 for z < 0. However, in practice it works very well and has the advantage of being\\n288 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 314}, page_content='11 Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck\\nto sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is\\none of the cases where the biological analogy was misleading.\\nfast to compute11. Most importantly, the fact that it does not have a maximum\\noutput value also helps reduce some issues during Gradient Descent (we will\\ncome back to this in Chapter 11).\\nThese popular activation functions and their derivatives are represented in\\nFigure 10-8. But wait! Why do we need activation functions in the first place? Well, if\\nyou chain several linear transformations, all you get is a linear transformation. For\\nexample, say f(x) = 2 x + 3 and g(x) = 5 x - 1, then chaining these two linear functions\\ngives you another linear function: f(g(x)) = 2(5 x - 1) + 3 = 10 x + 1. So if you don’t\\nhave some non-linearity between layers, then even a deep stack of layers is equivalent\\nto a single layer: you cannot solve very complex problems with that.\\nFigure 10-8. Activation functions and their derivatives\\nOkay! So now you know where neural nets came from, what their architecture is and\\nhow to compute their outputs, and you also learned about the backpropagation algo‐\\nrithm. But what exactly can you do with them?\\nRegression MLPs\\nFirst, MLPs can be used for regression tasks. If you want to predict a single value (e.g.,\\nthe price of a house given many of its features), then you just need a single output\\nneuron: its output is the predicted value. For multivariate regression (i.e., to predict\\nmultiple values at once), you need one output neuron per output dimension. For\\nexample, to locate the center of an object on an image, you need to predict 2D coordi‐\\nnates, so you need two output neurons. If you also want to place a bounding box\\naround the object, then you need two more numbers: the width and the height of the\\nobject. So you end up with 4 output neurons.\\nFrom Biological to Artificial Neurons \\n| \\n289'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 315}, page_content='In general, when building an MLP for regression, you do not want to use any activa‐\\ntion function for the output neurons, so they are free to output any range of values.\\nHowever, if you want to guarantee that the output will always be positive, then you\\ncan use the ReLU activation function, or the softplus activation function in the output\\nlayer. Finally, if you want to guarantee that the predictions will fall within a given\\nrange of values, then you can use the logistic function or the hyperbolic tangent, and\\nscale the labels to the appropriate range: 0 to 1 for the logistic function, or –1 to 1 for\\nthe hyperbolic tangent.\\nThe loss function to use during training is typically the mean squared error, but if you\\nhave a lot of outliers in the training set, you may prefer to use the mean absolute\\nerror instead. Alternatively, you can use the Huber loss, which is a combination of\\nboth.\\nThe Huber loss is quadratic when the error is smaller than a thres‐\\nhold δ (typically 1), but linear when the error is larger than δ. This\\nmakes it less sensitive to outliers than the mean squared error, and\\nit is often more precise and converges faster than the mean abso‐\\nlute error.\\nTable 10-1 summarizes the typical architecture of a regression MLP.\\nTable 10-1. Typical Regression MLP Architecture\\nHyperparameter\\nTypical Value\\n# input neurons\\nOne per input feature (e.g., 28 x 28 = 784 for MNIST)\\n# hidden layers\\nDepends on the problem. Typically 1 to 5.\\n# neurons per hidden layer\\nDepends on the problem. Typically 10 to 100.\\n# output neurons\\n1 per prediction dimension\\nHidden activation\\nReLU (or SELU, see Chapter 11)\\nOutput activation\\nNone or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs)\\nLoss function\\nMSE or MAE/Huber (if outliers)\\nClassification MLPs\\nMLPs can also be used for classification tasks. For a binary classification problem,\\nyou just need a single output neuron using the logistic activation function: the output\\nwill be a number between 0 and 1, which you can interpret as the estimated probabil‐\\nity of the positive class. Obviously, the estimated probability of the negative class is\\nequal to one minus that number.\\nMLPs can also easily handle multilabel binary classification tasks (see Chapter 3). For\\nexample, you could have an email classification system that predicts whether each\\nincoming email is ham or spam, and simultaneously predicts whether it is an urgent\\n290 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 316}, page_content='or non-urgent email. In this case, you would need two output neurons, both using\\nthe logistic activation function: the first would output the probability that the email is\\nspam and the second would output the probability that it is urgent. More generally,\\nyou would dedicate one output neuron for each positive class. Note that the output\\nprobabilities do not necessarily add up to one. This lets the model output any combi‐\\nnation of labels: you can have non-urgent ham, urgent ham, non-urgent spam, and\\nperhaps even urgent spam (although that would probably be an error).\\nIf each instance can belong only to a single class, out of 3 or more possible classes\\n(e.g., classes 0 through 9 for digit image classification), then you need to have one\\noutput neuron per class, and you should use the softmax activation function for the\\nwhole output layer (see Figure 10-9). The softmax function (introduced in Chapter 4)\\nwill ensure that all the estimated probabilities are between 0 and 1 and that they add\\nup to one (which is required if the classes are exclusive). This is called multiclass clas‐\\nsification.\\nFigure 10-9. A modern MLP (including ReLU and softmax) for classification\\nRegarding the loss function, since we are predicting probability distributions, the\\ncross-entropy (also called the log loss, see Chapter 4) is generally a good choice.\\nTable 10-2 summarizes the typical architecture of a classification MLP.\\nTable 10-2. Typical Classification MLP Architecture\\nHyperparameter\\nBinary classification\\nMultilabel binary classification Multiclass classification\\nInput and hidden layers Same as regression\\nSame as regression\\nSame as regression\\n# output neurons\\n1\\n1 per label\\n1 per class\\nOutput layer activation\\nLogistic\\nLogistic\\nSoftmax\\nFrom Biological to Artificial Neurons \\n| \\n291'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 317}, page_content='12 Project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\\nHyperparameter\\nBinary classification\\nMultilabel binary classification Multiclass classification\\nLoss function\\nCross-Entropy\\nCross-Entropy\\nCross-Entropy\\nBefore we go on, I recommend you go through exercise 1, at the\\nend of this chapter. You will play with various neural network\\narchitectures and visualize their outputs using the TensorFlow Play‐\\nground. This will be very useful to better understand MLPs, for\\nexample the effects of all the hyperparameters (number of layers\\nand neurons, activation functions, and more).\\nNow you have all the concepts you need to start implementing MLPs with Keras!\\nImplementing MLPs with Keras\\nKeras is a high-level Deep Learning API that allows you to easily build, train, evaluate\\nand execute all sorts of neural networks. Its documentation (or specification) is avail‐\\nable at https://keras.io. The reference implementation is simply called Keras as well, so\\nto avoid any confusion we will call it keras-team (since it is available at https://\\ngithub.com/keras-team/keras). It was developed by François Chollet as part of a\\nresearch project12 and released as an open source project in March 2015. It quickly\\ngained popularity owing to its ease-of-use, flexibility and beautiful design. To per‐\\nform the heavy computations required by neural networks, keras-team relies on a\\ncomputation backend. At the present, you can choose from three popular open\\nsource deep learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK) or\\nTheano.\\nMoreover, since late 2016, other implementations have been released. You can now\\nrun Keras on Apache MXNet, Apple’s Core ML, Javascript or Typescript (to run Keras\\ncode in a web browser), or PlaidML (which can run on all sorts of GPU devices, not\\njust Nvidia). Moreover, TensorFlow itself now comes bundled with its own Keras\\nimplementation called tf.keras. It only supports TensorFlow as the backend, but it has\\nthe advantage of offering some very useful extra features (see Figure 10-10): for\\nexample, it supports TensorFlow’s Data API which makes it quite easy to load and\\npreprocess data efficiently. For this reason, we will use tf.keras in this book. However,\\nin this chapter we will not use any of the TensorFlow-specific features, so the code\\nshould run fine on other Keras implementations as well (at least in Python), with only\\nminor modifications, such as changing the imports.\\n292 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 318}, page_content=\"Figure 10-10. Two Keras implementations: keras-team (left) and tf.keras (right)\\nAs tf.keras is bundled with TensorFlow, let’s install TensorFlow!\\nInstalling TensorFlow 2\\nAssuming you installed Jupyter and Scikit-Learn by following the installation instruc‐\\ntions in Chapter 2, you can simply use pip to install TensorFlow. If you created an\\nisolated environment using virtualenv, you first need to activate it:\\n$ cd $ML_PATH              # Your ML working directory (e.g., $HOME/ml)\\n$ source env/bin/activate  # on Linux or MacOSX\\n$ .\\\\env\\\\Scripts\\\\activate   # on Windows\\nNext, install TensorFlow 2 (if you are not using a virtualenv, you will need adminis‐\\ntrator rights, or to add the --user option):\\n$ python3 -m pip install --upgrade tensorflow\\nFor GPU support, you need to install tensorflow-gpu instead of\\ntensorflow, and there are other libraries to install. See https://\\ntensorflow.org/install/gpu for more details.\\nTo test your installation, open a Python shell or a Jupyter notebook, then import Ten‐\\nsorFlow and tf.keras, and print their versions:\\n>>> import tensorflow as tf\\n>>> from tensorflow import keras\\n>>> tf.__version__\\n'2.0.0'\\n>>> keras.__version__\\n'2.2.4-tf'\\nImplementing MLPs with Keras \\n| \\n293\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 319}, page_content=\"The second version is the version of the Keras API implemented by tf.keras. Note that\\nit ends with -tf, highlighting the fact that tf.keras implements the Keras API, plus\\nsome extra TensorFlow-specific features.\\nNow let’s use tf.keras! Let’s start by building a simple image classifier.\\nBuilding an Image Classifier Using the Sequential API\\nFirst, we need to load a dataset. We will tackle Fashion MNIST, which is a drop-in\\nreplacement of MNIST (introduced in Chapter 3). It has the exact same format as\\nMNIST (70,000 grayscale images of 28×28 pixels each, with 10 classes), but the\\nimages represent fashion items rather than handwritten digits, so each class is more\\ndiverse and the problem turns out to be significantly more challenging than MNIST.\\nFor example, a simple linear model reaches about 92% accuracy on MNIST, but only\\nabout 83% on Fashion MNIST.\\nUsing Keras to Load the Dataset\\nKeras provides some utility functions to fetch and load common datasets, including\\nMNIST, Fashion MNIST, the original California housing dataset, and more. Let’s load\\nFashion MNIST:\\nfashion_mnist = keras.datasets.fashion_mnist\\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\\nWhen loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one\\nimportant difference is that every image is represented as a 28×28 array rather than a\\n1D array of size 784. Moreover, the pixel intensities are represented as integers (from\\n0 to 255) rather than floats (from 0.0 to 255.0). Here is the shape and data type of the\\ntraining set:\\n>>> X_train_full.shape\\n(60000, 28, 28)\\n>>> X_train_full.dtype\\ndtype('uint8')\\nNote that the dataset is already split into a training set and a test set, but there is no\\nvalidation set, so let’s create one. Moreover, since we are going to train the neural net‐\\nwork using Gradient Descent, we must scale the input features. For simplicity, we just\\nscale the pixel intensities down to the 0-1 range by dividing them by 255.0 (this also\\nconverts them to floats):\\nX_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\\nWith MNIST, when the label is equal to 5, it means that the image represents the\\nhandwritten digit 5. Easy. However, for Fashion MNIST, we need the list of class\\nnames to know what we are dealing with:\\n294 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 320}, page_content='class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\\n               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\\nFor example, the first image in the training set represents a coat:\\n>>> class_names[y_train[0]]\\n\\'Coat\\'\\nFigure 10-11 shows a few samples from the Fashion MNIST dataset:\\nFigure 10-11. Samples from Fashion MNIST\\nCreating the Model Using the Sequential API\\nNow let’s build the neural network! Here is a classification MLP with two hidden lay‐\\ners:\\nmodel = keras.models.Sequential()\\nmodel.add(keras.layers.Flatten(input_shape=[28, 28]))\\nmodel.add(keras.layers.Dense(300, activation=\"relu\"))\\nmodel.add(keras.layers.Dense(100, activation=\"relu\"))\\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\\nLet’s go through this code line by line:\\n• The first line creates a Sequential model. This is the simplest kind of Keras\\nmodel, for neural networks that are just composed of a single stack of layers, con‐\\nnected sequentially. This is called the sequential API.\\n• Next, we build the first layer and add it to the model. It is a Flatten layer whose\\nrole is simply to convert each input image into a 1D array: if it receives input data\\nX, it computes X.reshape(-1, 1). This layer does not have any parameters, it is\\njust there to do some simple preprocessing. Since it is the first layer in the model,\\nyou should specify the input_shape: this does not include the batch size, only the\\nshape of the instances. Alternatively, you could add a keras.layers.InputLayer\\nas the first layer, setting shape=[28,28].\\n• Next we add a Dense hidden layer with 300 neurons. It will use the ReLU activa‐\\ntion function. Each Dense layer manages its own weight matrix, containing all the\\nconnection weights between the neurons and their inputs. It also manages a vec‐\\nImplementing MLPs with Keras \\n| \\n295'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 321}, page_content='tor of bias terms (one per neuron). When it receives some input data, it computes\\nEquation 10-2.\\n• Next we add a second Dense hidden layer with 100 neurons, also using the ReLU\\nactivation function.\\n• Finally, we add a Dense output layer with 10 neurons (one per class), using the\\nsoftmax activation function (because the classes are exclusive).\\nSpecifying \\nactivation=\"relu\" \\nis \\nequivalent \\nto \\nactiva\\ntion=keras.activations.relu. Other activation functions are\\navailable in the keras.activations package, we will use many of\\nthem in this book. See https://keras.io/activations/ for the full list.\\nInstead of adding the layers one by one as we just did, you can pass a list of layers\\nwhen creating the Sequential model:\\nmodel = keras.models.Sequential([\\n    keras.layers.Flatten(input_shape=[28, 28]),\\n    keras.layers.Dense(300, activation=\"relu\"),\\n    keras.layers.Dense(100, activation=\"relu\"),\\n    keras.layers.Dense(10, activation=\"softmax\")\\n])\\nUsing Code Examples From keras.io\\nCode examples documented on keras.io will work fine with tf.keras, but you need to\\nchange the imports. For example, consider this keras.io code:\\nfrom keras.layers import Dense\\noutput_layer = Dense(10)\\nYou must change the imports like this:\\nfrom tensorflow.keras.layers import Dense\\noutput_layer = Dense(10)\\nOr simply use full paths, if you prefer:\\nfrom tensorflow import keras\\noutput_layer = keras.layers.Dense(10)\\nThis is more verbose, but I use this approach in this book so you can easily see which\\npackages to use, and to avoid confusion between standard classes and custom classes.\\nIn production code, I use the previous approach, as do most people.\\n296 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 322}, page_content=\"13 You can also generate an image of your model using keras.utils.plot_model().\\nThe model’s summary() method displays all the model’s layers13, including each layer’s\\nname (which is automatically generated unless you set it when creating the layer), its\\noutput shape (None means the batch size can be anything), and its number of parame‐\\nters. The summary ends with the total number of parameters, including trainable and\\nnon-trainable parameters. Here we only have trainable parameters (we will see exam‐\\nples of non-trainable parameters in Chapter 11):\\n>>> model.summary()\\n_________________________________________________________________\\nLayer (type)                 Output Shape              Param #\\n=================================================================\\nflatten_1 (Flatten)          (None, 784)               0\\n_________________________________________________________________\\ndense_3 (Dense)              (None, 300)               235500\\n_________________________________________________________________\\ndense_4 (Dense)              (None, 100)               30100\\n_________________________________________________________________\\ndense_5 (Dense)              (None, 10)                1010\\n=================================================================\\nTotal params: 266,610\\nTrainable params: 266,610\\nNon-trainable params: 0\\nNote that Dense layers often have a lot of parameters. For example, the first hidden\\nlayer has 784 × 300 connection weights, plus 300 bias terms, which adds up to\\n235,500 parameters! This gives the model quite a lot of flexibility to fit the training\\ndata, but it also means that the model runs the risk of overfitting, especially when you\\ndo not have a lot of training data. We will come back to this later.\\nYou can easily get a model’s list of layers, to fetch a layer by its index, or you can fetch\\nit by name:\\n>>> model.layers\\n[<tensorflow.python.keras.layers.core.Flatten at 0x132414e48>,\\n <tensorflow.python.keras.layers.core.Dense at 0x1324149b0>,\\n <tensorflow.python.keras.layers.core.Dense at 0x1356ba8d0>,\\n <tensorflow.python.keras.layers.core.Dense at 0x13240d240>]\\n>>> model.layers[1].name\\n'dense_3'\\n>>> model.get_layer('dense_3').name\\n'dense_3'\\nAll the parameters of a layer can be accessed using its get_weights() and\\nset_weights() method. For a Dense layer, this includes both the connection weights\\nand the bias terms:\\nImplementing MLPs with Keras \\n| \\n297\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 323}, page_content='>>> weights, biases = hidden1.get_weights()\\n>>> weights\\narray([[ 0.03854964, -0.04054524,  0.00599282, ...,  0.02566582,\\n         0.01032123,  0.06914985],\\n       ...,\\n       [ 0.02632413, -0.05105981, -0.00332005, ...,  0.04175945,\\n         0.0443138 , -0.05558084]], dtype=float32)\\n>>> weights.shape\\n(784, 300)\\n>>> biases\\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., ...,  0., 0., 0.], dtype=float32)\\n>>> biases.shape\\n(300,)\\nNotice that the Dense layer initialized the connection weights randomly (which is\\nneeded to break symmetry, as we discussed earlier), and the biases were just initial‐\\nized to zeros, which is fine. If you ever want to use a different initialization method,\\nyou can set kernel_initializer (kernel is another name for the matrix of connec‐\\ntion weights) or bias_initializer when creating the layer. We will discuss initializ‐\\ners further in Chapter 11, but if you want the full list, see https://keras.io/initializers/.\\nThe shape of the weight matrix depends on the number of inputs.\\nThis is why it is recommended to specify the input_shape when\\ncreating the first layer in a Sequential model. However, if you do\\nnot specify the input shape, it’s okay: Keras will simply wait until it\\nknows the input shape before it actually builds the model. This will\\nhappen either when you feed it actual data (e.g., during training),\\nor when you call its build() method. Until the model is really\\nbuilt, the layers will not have any weights, and you will not be able\\nto do certain things (such as print the model summary or save the\\nmodel), so if you know the input shape when creating the model, it\\nis best to specify it.\\nCompiling the Model\\nAfter a model is created, you must call its compile() method to specify the loss func‐\\ntion and the optimizer to use. Optionally, you can also specify a list of extra metrics to\\ncompute during training and evaluation:\\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\\n              optimizer=\"sgd\",\\n              metrics=[\"accuracy\"])\\n298 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 324}, page_content='Using loss=\"sparse_categorical_crossentropy\" is equivalent to\\nloss=keras.losses.sparse_categorical_crossentropy. \\nSimi‐\\nlarly, optimizer=\"sgd\" is equivalent to optimizer=keras.optimiz\\ners.SGD() \\nand \\nmetrics=[\"accuracy\"] \\nis \\nequivalent \\nto\\nmetrics=[keras.metrics.sparse_categorical_accuracy] (when\\nusing this loss). We will use many other losses, optimizers and met‐\\nrics in this book, but for the full lists see https://keras.io/losses/,\\nhttps://keras.io/optimizers/ and https://keras.io/metrics/.\\nThis requires some explanation. First, we use the \"sparse_categorical_crossen\\ntropy\" loss because we have sparse labels (i.e., for each instance there is just a target\\nclass index, from 0 to 9 in this case), and the classes are exclusive. If instead we had\\none target probability per class for each instance (such as one-hot vectors, e.g. [0.,\\n0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), then we would need\\nto use the \"categorical_crossentropy\" loss instead. If we were doing binary classi‐\\nfication (with one or more binary labels), then we would use the \"sigmoid\" (i.e.,\\nlogistic) activation function in the output layer instead of the \"softmax\" activation\\nfunction, and we would use the \"binary_crossentropy\" loss.\\nIf you want to convert sparse labels (i.e., class indices) to one-hot\\nvector labels, you can use the keras.utils.to_categorical()\\nfunction. To go the other way round, you can just use the np.arg\\nmax() function with axis=1.\\nSecondly, regarding the optimizer, \"sgd\" simply means that we will train the model\\nusing simple Stochastic Gradient Descent. In other words, Keras will perform the\\nbackpropagation algorithm described earlier (i.e., reverse-mode autodiff + Gradient\\nDescent). We will discuss more efficient optimizers in Chapter 11 (they improve the\\nGradient Descent part, not the autodiff).\\nFinally, since this is a classifier, it’s useful to measure its \"accuracy\" during training\\nand evaluation.\\nTraining and Evaluating the Model\\nNow the model is ready to be trained. For this we simply need to call its fit()\\nmethod. We pass it the input features (X_train) and the target classes (y_train), as\\nwell as the number of epochs to train (or else it would default to just 1, which would\\ndefinitely not be enough to converge to a good solution). We also pass a validation set\\n(this is optional): Keras will measure the loss and the extra metrics on this set at the\\nend of each epoch, which is very useful to see how well the model really performs: if\\nthe performance on the training set is much better than on the validation set, your\\nImplementing MLPs with Keras \\n| \\n299'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 325}, page_content='model is probably overfitting the training set (or there is a bug, such as a data mis‐\\nmatch between the training set and the validation set):\\n>>> history = model.fit(X_train, y_train, epochs=30,\\n...                     validation_data=(X_valid, y_valid))\\n...\\nTrain on 55000 samples, validate on 5000 samples\\nEpoch 1/30\\n55000/55000 [==========] - 3s 55us/sample - loss: 1.4948     - acc: 0.5757\\n                                          - val_loss: 1.0042 - val_acc: 0.7166\\nEpoch 2/30\\n55000/55000 [==========] - 3s 55us/sample - loss: 0.8690     - acc: 0.7318\\n                                          - val_loss: 0.7549 - val_acc: 0.7616\\n[...]\\nEpoch 50/50\\n55000/55000 [==========] - 4s 72us/sample - loss: 0.3607     - acc: 0.8752\\n                                          - val_loss: 0.3706 - val_acc: 0.8728\\nAnd that’s it! The neural network is trained. At each epoch during training, Keras dis‐\\nplays the number of instances processed so far (along with a progress bar), the mean\\ntraining time per sample, the loss and accuracy (or any other extra metrics you asked\\nfor), both on the training set and the validation set. You can see that the training loss\\nwent down, which is a good sign, and the validation accuracy reached 87.28% after 50\\nepochs, not too far from the training accuracy, so there does not seem to be much\\noverfitting going on.\\nInstead of passing a validation set using the validation_data\\nargument, you could instead set validation_split to the ratio of\\nthe training set that you want Keras to use for validation (e.g., 0.1).\\nIf the training set was very skewed, with some classes being overrepresented and oth‐\\ners underrepresented, it would be useful to set the class_weight argument when\\ncalling the fit() method, giving a larger weight to underrepresented classes, and a\\nlower weight to overrepresented classes. These weights would be used by Keras when\\ncomputing the loss. If you need per-instance weights instead, you can set the sam\\nple_weight argument (it supersedes class_weight). This could be useful for exam‐\\nple if some instances were labeled by experts while others were labeled using a\\ncrowdsourcing platform: you might want to give more weight to the former. You can\\nalso provide sample weights (but not class weights) for the validation set by adding\\nthem as a third item in the validation_data tuple.\\nThe fit() method returns a History object containing the training parameters (his\\ntory.params), the list of epochs it went through (history.epoch), and most impor‐\\ntantly a dictionary (history.history) containing the loss and extra metrics it\\nmeasured at the end of each epoch on the training set and on the validation set (if\\n300 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 326}, page_content='any). If you create a Pandas DataFrame using this dictionary and call its plot()\\nmethod, you get the learning curves shown in Figure 10-12:\\nimport pandas as pd\\npd.DataFrame(history.history).plot(figsize=(8, 5))\\nplt.grid(True)\\nplt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\\nplt.show()\\nFigure 10-12. Learning Curves\\nYou can see that both the training and validation accuracy steadily increase during\\ntraining, while the training and validation loss decrease. Good! Moreover, the valida‐\\ntion curves are quite close to the training curves, which means that there is not too\\nmuch overfitting. In this particular case, the model performed better on the valida‐\\ntion set than on the training set at the beginning of training: this sometimes happens\\nby chance (especially when the validation set is fairly small). However, the training set\\nperformance ends up beating the validation performance, as is generally the case\\nwhen you train for long enough. You can tell that the model has not quite converged\\nyet, as the validation loss is still going down, so you should probably continue train‐\\ning. It’s as simple as calling the fit() method again, since Keras just continues train‐\\ning where it left off (you should be able to reach close to 89% validation accuracy).\\nIf you are not satisfied with the performance of your model, you should go back and\\ntune the model’s hyperparameters, for example the number of layers, the number of\\nneurons per layer, the types of activation functions we use for each hidden layer, the\\nImplementing MLPs with Keras \\n| \\n301'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 327}, page_content=\"number of training epochs, the batch size (it can be set in the fit() method using the\\nbatch_size argument, which defaults to 32). We will get back to hyperparameter\\ntuning at the end of this chapter. Once you are satisfied with your model’s validation\\naccuracy, you should evaluate it on the test set to estimate the generalization error\\nbefore you deploy the model to production. You can easily do this using the evalu\\nate() method (it also supports several other arguments, such as batch_size or sam\\nple_weight, please check the documentation for more details):\\n>>> model.evaluate(X_test, y_test)\\n8832/10000 [==========================] - ETA: 0s - loss: 0.4074 - acc: 0.8540\\n[0.40738476498126985, 0.854]\\nAs we saw in Chapter 2, it is common to get slightly lower performance on the test set\\nthan on the validation set, because the hyperparameters are tuned on the validation\\nset, not the test set (however, in this example, we did not do any hyperparameter tun‐\\ning, so the lower accuracy is just bad luck). Remember to resist the temptation to\\ntweak the hyperparameters on the test set, or else your estimate of the generalization\\nerror will be too optimistic.\\nUsing the Model to Make Predictions\\nNext, we can use the model’s predict() method to make predictions on new instan‐\\nces. Since we don’t have actual new instances, we will just use the first 3 instances of\\nthe test set:\\n>>> X_new = X_test[:3]\\n>>> y_proba = model.predict(X_new)\\n>>> y_proba.round(2)\\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.12, 0.  , 0.79],\\n       [0.  , 0.  , 0.94, 0.  , 0.02, 0.  , 0.04, 0.  , 0.  , 0.  ],\\n       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\\n      dtype=float32)\\nAs you can see, for each instance the model estimates one probability per class, from\\nclass 0 to class 9. For example, for the first image it estimates that the probability of\\nclass 9 (ankle boot) is 79%, the probability of class 7 (sneaker) is 12%, the probability\\nof class 5 (sandal) is 9%, and the other classes are negligible. In other words, it\\n“believes” it’s footwear, probably ankle boots, but it’s not entirely sure, it might be\\nsneakers or sandals instead. If you only care about the class with the highest estima‐\\nted probability (even if that probability is quite low) then you can use the pre\\ndict_classes() method instead:\\n>>> y_pred = model.predict_classes(X_new)\\n>>> y_pred\\narray([9, 2, 1])\\n>>> np.array(class_names)[y_pred]\\narray(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')\\nAnd the classifier actually classified all three images correctly:\\n302 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 328}, page_content='>>> y_new = y_test[:3]\\n>>> y_new\\narray([9, 2, 1])\\nNow you know how to build, train, evaluate and use a classification MLP using the\\nSequential API. But what about regression?\\nBuilding a Regression MLP Using the Sequential API\\nLet’s switch to the California housing problem and tackle it using a regression neural\\nnetwork. For simplicity, we will use Scikit-Learn’s fetch_california_housing()\\nfunction to load the data: this dataset is simpler than the one we used in Chapter 2,\\nsince it contains only numerical features (there is no ocean_proximity feature), and\\nthere is no missing value. After loading the data, we split it into a training set, a vali‐\\ndation set and a test set, and we scale all the features:\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nhousing = fetch_california_housing()\\nX_train_full, X_test, y_train_full, y_test = train_test_split(\\n    housing.data, housing.target)\\nX_train, X_valid, y_train, y_valid = train_test_split(\\n    X_train_full, y_train_full)\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_valid_scaled = scaler.transform(X_valid)\\nX_test_scaled = scaler.transform(X_test)\\nBuilding, training, evaluating and using a regression MLP using the Sequential API to\\nmake predictions is quite similar to what we did for classification. The main differ‐\\nences are the fact that the output layer has a single neuron (since we only want to\\npredict a single value) and uses no activation function, and the loss function is the\\nmean squared error. Since the dataset is quite noisy, we just use a single hidden layer\\nwith fewer neurons than before, to avoid overfitting:\\nmodel = keras.models.Sequential([\\n    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\\n    keras.layers.Dense(1)\\n])\\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\\nhistory = model.fit(X_train, y_train, epochs=20,\\n                    validation_data=(X_valid, y_valid))\\nmse_test = model.evaluate(X_test, y_test)\\nX_new = X_test[:3] # pretend these are new instances\\ny_pred = model.predict(X_new)\\nImplementing MLPs with Keras \\n| \\n303'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 329}, page_content='14 “Wide & Deep Learning for Recommender Systems,” Heng-Tze Cheng et al. (2016).\\nAs you can see, the Sequential API is quite easy to use. However, although sequential\\nmodels are extremely common, it is sometimes useful to build neural networks with\\nmore complex topologies, or with multiple inputs or outputs. For this purpose, Keras\\noffers the Functional API.\\nBuilding Complex Models Using the Functional API\\nOne example of a non-sequential neural network is a Wide & Deep neural network.\\nThis neural network architecture was introduced in a 2016 paper by Heng-Tze Cheng\\net al.14. It connects all or part of the inputs directly to the output layer, as shown in\\nFigure 10-13. This architecture makes it possible for the neural network to learn both\\ndeep patterns (using the deep path) and simple rules (through the short path). In\\ncontrast, a regular MLP forces all the data to flow through the full stack of layers, thus\\nsimple patterns in the data may end up being distorted by this sequence of transfor‐\\nmations.\\n304 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 330}, page_content='Figure 10-13. Wide and Deep Neural Network\\nLet’s build such a neural network to tackle the California housing problem:\\ninput = keras.layers.Input(shape=X_train.shape[1:])\\nhidden1 = keras.layers.Dense(30, activation=\"relu\")(input)\\nhidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\\nconcat = keras.layers.Concatenate()[input, hidden2])\\noutput = keras.layers.Dense(1)(concat)\\nmodel = keras.models.Model(inputs=[input], outputs=[output])\\nLet’s go through each line of this code:\\n• First, we need to create an Input object. This is needed because we may have\\nmultiple inputs, as we will see later.\\n• Next, we create a Dense layer with 30 neurons and using the ReLU activation\\nfunction. As soon as it is created, notice that we call it like a function, passing it\\nthe input. This is why this is called the Functional API. Note that we are just tell‐\\nImplementing MLPs with Keras \\n| \\n305'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 331}, page_content='ing Keras how it should connect the layers together, no actual data is being pro‐\\ncessed yet.\\n• We then create a second hidden layer, and again we use it as a function. Note\\nhowever that we pass it the output of the first hidden layer.\\n• Next, we create a Concatenate() layer, and once again we immediately use it like\\na function, to concatenate the input and the output of the second hidden layer\\n(you may prefer the keras.layers.concatenate() function, which creates a Con\\ncatenate layer and immediately calls it with the given inputs).\\n• Then we create the output layer, with a single neuron and no activation function,\\nand we call it like a function, passing it the result of the concatenation.\\n• Lastly, we create a Keras Model, specifying which inputs and outputs to use.\\nOnce you have built the Keras model, everything is exactly like earlier, so no need to\\nrepeat it here: you must compile the model, train it, evaluate it and use it to make\\npredictions.\\nBut what if you want to send a subset of the features through the wide path, and a\\ndifferent subset (possibly overlapping) through the deep path (see Figure 10-14)? In\\nthis case, one solution is to use multiple inputs. For example, suppose we want to\\nsend 5 features through the deep path (features 0 to 4), and 6 features through the\\nwide path (features 2 to 7):\\ninput_A = keras.layers.Input(shape=[5])\\ninput_B = keras.layers.Input(shape=[6])\\nhidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\\nhidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\\nconcat = keras.layers.concatenate([input_A, hidden2])\\noutput = keras.layers.Dense(1)(concat)\\nmodel = keras.models.Model(inputs=[input_A, input_B], outputs=[output])\\n306 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 332}, page_content='Figure 10-14. Handling Multiple Inputs\\nThe code is self-explanatory. Note that we specified inputs=[input_A, input_B]\\nwhen creating the model. Now we can compile the model as usual, but when we call\\nthe fit() method, instead of passing a single input matrix X_train, we must pass a\\npair of matrices (X_train_A, X_train_B): one per input. The same is true for\\nX_valid, and also for X_test and X_new when you call evaluate() or predict():\\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\\nX_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\\nX_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\\nX_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\\nX_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\\nhistory = model.fit((X_train_A, X_train_B), y_train, epochs=20,\\n                    validation_data=((X_valid_A, X_valid_B), y_valid))\\nmse_test = model.evaluate((X_test_A, X_test_B), y_test)\\ny_pred = model.predict((X_new_A, X_new_B))\\nImplementing MLPs with Keras \\n| \\n307'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 333}, page_content='There are also many use cases in which you may want to have multiple outputs:\\n• The task may demand it, for example you may want to locate and classify the\\nmain object in a picture. This is both a regression task (finding the coordinates of\\nthe object’s center, as well as its width and height) and a classification task.\\n• Similarly, you may have multiple independent tasks to perform based on the\\nsame data. Sure, you could train one neural network per task, but in many cases\\nyou will get better results on all tasks by training a single neural network with\\none output per task. This is because the neural network can learn features in the\\ndata that are useful across tasks.\\n• Another use case is as a regularization technique (i.e., a training constraint whose\\nobjective is to reduce overfitting and thus improve the model’s ability to general‐\\nize). For example, you may want to add some auxiliary outputs in a neural net‐\\nwork architecture (see Figure 10-15) to ensure that the underlying part of the\\nnetwork learns something useful on its own, without relying on the rest of the\\nnetwork.\\nFigure 10-15. Handling Multiple Outputs – Auxiliary Output for Regularization\\nAdding extra outputs is quite easy: just connect them to the appropriate layers and\\nadd them to your model’s list of outputs. For example, the following code builds the\\nnetwork represented in Figure 10-15:\\n308 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 334}, page_content='[...] # Same as above, up to the main output layer\\noutput = keras.layers.Dense(1)(concat)\\naux_output = keras.layers.Dense(1)(hidden2)\\nmodel = keras.models.Model(inputs=[input_A, input_B],\\n                           outputs=[output, aux_output])\\nEach output will need its own loss function, so when we compile the model we\\nshould pass a list of losses (if we pass a single loss, Keras will assume that the same\\nloss must be used for all outputs). By default, Keras will compute all these losses and\\nsimply add them up to get the final loss used for training. However, we care much\\nmore about the main output than about the auxiliary output (as it is just used for reg‐\\nularization), so we want to give the main output’s loss a much greater weight. Fortu‐\\nnately, it is possible to set all the loss weights when compiling the model:\\nmodel.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")\\nNow when we train the model, we need to provide some labels for each output. In\\nthis example, the main output and the auxiliary output should try to predict the same\\nthing, so they should use the same labels. So instead of passing y_train, we just need\\nto pass (y_train, y_train) (and the same goes for y_valid and y_test):\\nhistory = model.fit(\\n    [X_train_A, X_train_B], [y_train, y_train], epochs=20,\\n    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\\nWhen we evaluate the model, Keras will return the total loss, as well as all the individ‐\\nual losses:\\ntotal_loss, main_loss, aux_loss = model.evaluate(\\n    [X_test_A, X_test_B], [y_test, y_test])\\nSimilarly, the predict() method will return predictions for each output:\\ny_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\\nAs you can see, you can build any sort of architecture you want quite easily with the\\nFunctional API. Let’s look at one last way you can build Keras models.\\nBuilding Dynamic Models Using the Subclassing API\\nBoth the Sequential API and the Functional API are declarative: you start by declar‐\\ning which layers you want to use and how they should be connected, and only then\\ncan you start feeding the model some data for training or inference. This has many\\nadvantages: the model can easily be saved, cloned, shared, its structure can be dis‐\\nplayed and analyzed, the framework can infer shapes and check types, so errors can\\nbe caught early (i.e., before any data ever goes through the model). It’s also fairly easy\\nto debug, since the whole model is just a static graph of layers. But the flip side is just\\nthat: it’s static. Some models involve loops, varying shapes, conditional branching,\\nand other dynamic behaviors. For such cases, or simply if you prefer a more impera‐\\ntive programming style, the Subclassing API is for you.\\nImplementing MLPs with Keras \\n| \\n309'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 335}, page_content='15 Keras models have an output attribute, so we cannot use that name for the main output layer, which is why\\nwe renamed it to main_output.\\nSimply subclass the Model class, create the layers you need in the constructor, and use\\nthem to perform the computations you want in the call() method. For example, cre‐\\nating an instance of the following WideAndDeepModel class gives us an equivalent\\nmodel to the one we just built with the Functional API. You can then compile it, eval‐\\nuate it and use it to make predictions, exactly like we just did.\\nclass WideAndDeepModel(keras.models.Model):\\n    def __init__(self, units=30, activation=\"relu\", **kwargs):\\n        super().__init__(**kwargs) # handles standard args (e.g., name)\\n        self.hidden1 = keras.layers.Dense(units, activation=activation)\\n        self.hidden2 = keras.layers.Dense(units, activation=activation)\\n        self.main_output = keras.layers.Dense(1)\\n        self.aux_output = keras.layers.Dense(1)\\n    def call(self, inputs):\\n        input_A, input_B = inputs\\n        hidden1 = self.hidden1(input_B)\\n        hidden2 = self.hidden2(hidden1)\\n        concat = keras.layers.concatenate([input_A, hidden2])\\n        main_output = self.main_output(concat)\\n        aux_output = self.aux_output(hidden2)\\n        return main_output, aux_output\\nmodel = WideAndDeepModel()\\nThis example looks very much like the Functional API, except we do not need to cre‐\\nate the inputs, we just use the input argument to the call() method, and we separate\\nthe creation of the layers15 in the constructor from their usage in the call() method.\\nHowever, the big difference is that you can do pretty much anything you want in the\\ncall() method: for loops, if statements, low-level TensorFlow operations, your\\nimagination is the limit (see Chapter 12)! This makes it a great API for researchers\\nexperimenting with new ideas.\\nHowever, this extra flexibility comes at a cost: your model’s architecture is hidden\\nwithin the call() method, so Keras cannot easily inspect it, it cannot save or clone it,\\nand when you call the summary() method, you only get a list of layers, without any\\ninformation on how they are connected to each other. Moreover, Keras cannot check\\ntypes and shapes ahead of time, and it is easier to make mistakes. So unless you really\\nneed that extra flexibility, you should probably stick to the Sequential API or the\\nFunctional API.\\n310 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 336}, page_content='Keras models can be used just like regular layers, so you can easily\\ncompose them to build complex architectures.\\nNow that you know how to build and train neural nets using Keras, you will want to\\nsave them!\\nSaving and Restoring a Model\\nSaving a trained Keras model is as simple as it gets:\\nmodel.save(\"my_keras_model.h5\")\\nKeras will save both the model’s architecture (including every layer’s hyperparame‐\\nters) and the value of all the model parameters for every layer (e.g., connection\\nweights and biases), using the HDF5 format. It also saves the optimizer (including its\\nhyperparameters and any state it may have).\\nYou will typically have a script that trains a model and saves it, and one or more\\nscripts (or web services) that load the model and use it to make predictions. Loading\\nthe model is just as easy:\\nmodel = keras.models.load_model(\"my_keras_model.h5\")\\nThis will work when using the Sequential API or the Functional\\nAPI, but unfortunately not when using Model subclassing. How‐\\never, you can use save_weights() and load_weights() to at least\\nsave and restore the model parameters (but you will need to save\\nand restore everything else yourself).\\nBut what if training lasts several hours? This is quite common, especially when train‐\\ning on large datasets. In this case, you should not only save your model at the end of\\ntraining, but also save checkpoints at regular intervals during training. But how can\\nyou tell the fit() method to save checkpoints? The answer is: using callbacks.\\nUsing Callbacks\\nThe fit() method accepts a callbacks argument that lets you specify a list of objects\\nthat Keras will call during training at the start and end of training, at the start and end\\nof each epoch and even before and after processing each batch. For example, the Mod\\nelCheckpoint callback saves checkpoints of your model at regular intervals during\\ntraining, by default at the end of each epoch:\\nImplementing MLPs with Keras \\n| \\n311'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 337}, page_content='[...] # build and compile the model\\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\\nhistory = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])\\nMoreover, if you use a validation set during training, you can set\\nsave_best_only=True when creating the ModelCheckpoint. In this case, it will only\\nsave your model when its performance on the validation set is the best so far. This\\nway, you do not need to worry about training for too long and overfitting the training\\nset: simply restore the last model saved after training, and this will be the best model\\non the validation set. This is a simple way to implement early stopping (introduced in\\nChapter 4):\\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",\\n                                                save_best_only=True)\\nhistory = model.fit(X_train, y_train, epochs=10,\\n                    validation_data=(X_valid, y_valid),\\n                    callbacks=[checkpoint_cb])\\nmodel = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model\\nAnother way to implement early stopping is to simply use the EarlyStopping call‐\\nback. It will interrupt training when it measures no progress on the validation set for\\na number of epochs (defined by the patience argument), and it will optionally roll\\nback to the best model. You can combine both callbacks to both save checkpoints of\\nyour model (in case your computer crashes), and actually interrupt training early\\nwhen there is no more progress (to avoid wasting time and resources):\\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\\n                                                  restore_best_weights=True)\\nhistory = model.fit(X_train, y_train, epochs=100,\\n                    validation_data=(X_valid, y_valid),\\n                    callbacks=[checkpoint_cb, early_stopping_cb])\\nThe number of epochs can be set to a large value since training will stop automati‐\\ncally when there is no more progress. Moreover, there is no need to restore the best\\nmodel saved in this case since the EarlyStopping callback will keep track of the best\\nweights and restore them for us at the end of training.\\nThere are many other callbacks available in the keras.callbacks\\npackage. See https://keras.io/callbacks/.\\nIf you need extra control, you can easily write your own custom callbacks. For exam‐\\nple, the following custom callback will display the ratio between the validation loss\\nand the training loss during training (e.g., to detect overfitting):\\n312 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 338}, page_content='class PrintValTrainRatioCallback(keras.callbacks.Callback):\\n    def on_epoch_end(self, epoch, logs):\\n        print(\"\\\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\\nAs you might expect, you can implement on_train_begin(), on_train_end(),\\non_epoch_begin(), on_epoch_begin(), on_batch_end() and on_batch_end().\\nMoreover, callbacks can also be used during evaluation and predictions, should you\\never need them (e.g., for debugging). In this case, you should implement\\non_test_begin(), \\non_test_end(), \\non_test_batch_begin(), \\nor\\non_test_batch_end() (called by evaluate()), or on_predict_begin(), on_pre\\ndict_end(), on_predict_batch_begin(), or on_predict_batch_end() (called by\\npredict()).\\nNow let’s take a look at one more tool you should definitely have in your toolbox\\nwhen using tf.keras: TensorBoard.\\nVisualization Using TensorBoard\\nTensorBoard is a great interactive visualization tool that you can use to view the\\nlearning curves during training, compare learning curves between multiple runs, vis‐\\nualize the computation graph, analyze training statistics, view images generated by\\nyour model, visualize complex multidimensional data projected down to 3D and\\nautomatically clustered for you, and more! This tool is installed automatically when\\nyou install TensorFlow, so you already have it!\\nTo use it, you must modify your program so that it outputs the data you want to visu‐\\nalize to special binary log files called event files. Each binary data record is called a\\nsummary. The TensorBoard server will monitor the log directory, and it will automat‐\\nically pick up the changes and update the visualizations: this allows you to visualize\\nlive data (with a short delay), such as the learning curves during training. In general,\\nyou want to point the TensorBoard server to a root log directory, and configure your\\nprogram so that it writes to a different subdirectory every time it runs. This way, the\\nsame TensorBoard server instance will allow you to visualize and compare data from\\nmultiple runs of your program, without getting everything mixed up.\\nSo let’s start by defining the root log directory we will use for our TensorBoard logs,\\nplus a small function that will generate a subdirectory path based on the current date\\nand time, so that it is different at every run. You may want to include extra informa‐\\ntion in the log directory name, such as hyperparameter values that you are testing, to\\nmake it easier to know what you are looking at in TensorBoard:\\nroot_logdir = os.path.join(os.curdir, \"my_logs\")\\ndef get_run_logdir():\\n    import time\\n    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\\n    return os.path.join(root_logdir, run_id)\\nImplementing MLPs with Keras \\n| \\n313'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 339}, page_content='run_logdir = get_run_logdir() # e.g., \\'./my_logs/run_2019_01_16-11_28_43\\'\\nNext, the good news is that Keras provides a nice TensorBoard callback:\\n[...] # Build and compile your model\\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\\nhistory = model.fit(X_train, y_train, epochs=30,\\n                    validation_data=(X_valid, y_valid),\\n                    callbacks=[tensorboard_cb])\\nAnd that’s all there is to it! It could hardly be easier to use. If you run this code, the\\nTensorBoard callback will take care of creating the log directory for you (along with\\nits parent directories if needed), and during training it will create event files and write\\nsummaries to them. After running the program a second time (perhaps changing\\nsome hyperparameter value), you will end up with a directory structure similar to\\nthis one:\\nmy_logs\\n├── run_2019_01_16-16_51_02\\n│   └── events.out.tfevents.1547628669.mycomputer.local.v2\\n└── run_2019_01_16-16_56_50\\n    └── events.out.tfevents.1547629020.mycomputer.local.v2\\nNext you need to start the TensorBoard server. If you installed TensorFlow within a\\nvirtualenv, you should activate it. Next, run the following command at the root of the\\nproject (or from anywhere else as long as you point to the appropriate log directory).\\nIf your shell cannot find the tensorboard script, then you must update your PATH\\nenvironment variable so that it contains the directory in which the script was\\ninstalled (alternatively, you can just replace tensorboard with python3 -m tensor\\nboard.main).\\n$ tensorboard --logdir=./my_logs --port=6006\\nTensorBoard 2.0.0 at http://mycomputer.local:6006 (Press CTRL+C to quit)\\nFinally, open up a web browser to http://localhost:6006. You should see TensorBoard’s\\nweb interface. Click on the SCALARS tab to view the learning curves (see\\nFigure 10-16). Notice that the training loss went down nicely during both runs, but\\nthe second run went down much faster. Indeed, we used a larger learning rate by set‐\\nting optimizer=keras.optimizers.SGD(lr=0.05) instead of optimizer=\"sgd\",\\nwhich defaults to a learning rate of 0.001.\\n314 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 340}, page_content='Figure 10-16. Visualizing Learning Curves with TensorBoard\\nUnfortunately, at the time of writing, no other data is exported by the TensorBoard\\ncallback, but this issue will probably be fixed by the time you read these lines. In Ten‐\\nsorFlow 1, this callback exported the computation graph and many useful statistics:\\ntype help(keras.callbacks.TensorBoard) to see all the options.\\nLet’s summarize what you learned so far in this chapter: we saw where neural nets\\ncame from, what an MLP is and how you can use it for classification and regression,\\nhow to build MLPs using tf.keras’s Sequential API, or more complex architectures\\nusing the Functional API or Model Subclassing, you learned how to save and restore a\\nmodel, use callbacks for checkpointing, early stopping, and more, and finally how to\\nuse TensorBoard for visualization. You can already go ahead and use neural networks\\nto tackle many problems! However, you may wonder how to choose the number of\\nhidden layers, the number of neurons in the network, and all the other hyperparame‐\\nters. Let’s look at this now.\\nFine-Tuning Neural Network Hyperparameters\\nThe flexibility of neural networks is also one of their main drawbacks: there are many\\nhyperparameters to tweak. Not only can you use any imaginable network architec‐\\nture, but even in a simple MLP you can change the number of layers, the number of\\nneurons per layer, the type of activation function to use in each layer, the weight initi‐\\nFine-Tuning Neural Network Hyperparameters \\n| \\n315'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 341}, page_content='alization logic, and much more. How do you know what combination of hyperpara‐\\nmeters is the best for your task?\\nOne option is to simply try many combinations of hyperparameters and see which\\none works best on the validation set (or using K-fold cross-validation). For this, one\\napproach is simply use GridSearchCV or RandomizedSearchCV to explore the hyper‐\\nparameter space, as we did in Chapter 2. For this, we need to wrap our Keras models\\nin objects that mimic regular Scikit-Learn regressors. The first step is to create a func‐\\ntion that will build and compile a Keras model, given a set of hyperparameters:\\ndef build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\\n    model = keras.models.Sequential()\\n    options = {\"input_shape\": input_shape}\\n    for layer in range(n_hidden):\\n        model.add(keras.layers.Dense(n_neurons, activation=\"relu\", **options))\\n        options = {}\\n    model.add(keras.layers.Dense(1, **options))\\n    optimizer = keras.optimizers.SGD(learning_rate)\\n    model.compile(loss=\"mse\", optimizer=optimizer)\\n    return model\\nThis function creates a simple Sequential model for univariate regression (only one\\noutput neuron), with the given input shape and the given number of hidden layers\\nand neurons, and it compiles it using an SGD optimizer configured with the given\\nlearning rate. The options dict is used to ensure that the first layer is properly given\\nthe input shape (note that if n_hidden=0, the first layer will be the output layer). It is\\ngood practice to provide reasonable defaults to as many hyperparameters as you can,\\nas Scikit-Learn does.\\nNext, let’s create a KerasRegressor based on this build_model() function:\\nkeras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\\nThe KerasRegressor object is a thin wrapper around the Keras model built using\\nbuild_model(). Since we did not specify any hyperparameter when creating it, it will\\njust use the default hyperparameters we defined in build_model(). Now we can use\\nthis object like a regular Scikit-Learn regressor: we can train it using its fit()\\nmethod, then evaluate it using its score() method, and use it to make predictions\\nusing its predict() method. Note that any extra parameter you pass to the fit()\\nmethod will simply get passed to the underlying Keras model. Also note that the\\nscore will be the opposite of the MSE because Scikit-Learn wants scores, not losses\\n(i.e., higher should be better).\\nkeras_reg.fit(X_train, y_train, epochs=100,\\n              validation_data=(X_valid, y_valid),\\n              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\\nmse_test = keras_reg.score(X_test, y_test)\\ny_pred = keras_reg.predict(X_new)\\n316 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 342}, page_content='However, we do not actually want to train and evaluate a single model like this, we\\nwant to train hundreds of variants and see which one performs best on the validation\\nset. Since there are many hyperparameters, it is preferable to use a randomized search\\nrather than grid search (as we discussed in Chapter 2). Let’s try to explore the number\\nof hidden layers, the number of neurons and the learning rate:\\nfrom scipy.stats import reciprocal\\nfrom sklearn.model_selection import RandomizedSearchCV\\nparam_distribs = {\\n    \"n_hidden\": [0, 1, 2, 3],\\n    \"n_neurons\": np.arange(1, 100),\\n    \"learning_rate\": reciprocal(3e-4, 3e-2),\\n}\\nrnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\\nrnd_search_cv.fit(X_train, y_train, epochs=100,\\n                  validation_data=(X_valid, y_valid),\\n                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])\\nAs you can see, this is identical to what we did in Chapter 2, with the exception that\\nwe pass extra parameters to the fit() method: they simply get relayed to the under‐\\nlying Keras models. Note that RandomizedSearchCV uses K-fold cross-validation, so it\\ndoes not use X_valid and y_valid. These are just used for early stopping.\\nThe exploration may last many hours depending on the hardware, the size of the\\ndataset, the complexity of the model and the value of n_iter and cv. When it is over,\\nyou can access the best parameters found, the best score, and the trained Keras model\\nlike this:\\n>>> rnd_search_cv.best_params_\\n{\\'learning_rate\\': 0.0033625641252688094, \\'n_hidden\\': 2, \\'n_neurons\\': 42}\\n>>> rnd_search_cv.best_score_\\n-0.3189529188278931\\n>>> model = rnd_search_cv.best_estimator_.model\\nYou can now save this model, evaluate it on the test set, and if you are satisfied with\\nits performance, deploy it to production. Using randomized search is not too hard,\\nand it works well for many fairly simple problems. However, when training is slow\\n(e.g., for more complex problems with larger datasets), this approach will only\\nexplore a tiny portion of the hyperparameter space. You can partially alleviate this\\nproblem by assisting the search process manually: first run a quick random search\\nusing wide ranges of hyperparameter values, then run another search using smaller\\nranges of values centered on the best ones found during the first run, and so on. This\\nwill hopefully zoom in to a good set of hyperparameters. However, this is very time\\nconsuming, and probably not the best use of your time.\\nFortunately, there are many techniques to explore a search space much more effi‐\\nciently than randomly. Their core idea is simple: when a region of the space turns out\\nFine-Tuning Neural Network Hyperparameters \\n| \\n317'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 343}, page_content='16 “Population Based Training of Neural Networks,” Max Jaderberg et al. (2017).\\nto be good, it should be explored more. This takes care of the “zooming” process for\\nyou and leads to much better solutions in much less time. Here are a few Python\\nlibraries you can use to optimize hyperparameters:\\n• Hyperopt: a popular Python library for optimizing over all sorts of complex\\nsearch spaces (including real values such as the learning rate, or discrete values\\nsuch as the number of layers).\\n• Hyperas, kopt or Talos: optimizing hyperparameters for Keras model (the first\\ntwo are based on Hyperopt).\\n• Scikit-Optimize (skopt): a general-purpose optimization library. The Bayes\\nSearchCV class performs Bayesian optimization using an interface similar to Grid\\nSearchCV.\\n• Spearmint: a Bayesian optimization library.\\n• Sklearn-Deap: a hyperparameter optimization library based on evolutionary\\nalgorithms, also with a GridSearchCV-like interface.\\n• And many more!\\nMoreover, many companies offer services for hyperparameter optimization. For\\nexample Google Cloud ML Engine has a hyperparameter tuning service. Other com‐\\npanies provide APIs for hyperparameter optimization, such as Arimo, SigOpt, Oscar\\nand many more.\\nHyperparameter tuning is still an active area of research. Evolutionary algorithms are\\nmaking a comeback lately. For example, check out DeepMind’s excellent 2017 paper16,\\nwhere they jointly optimize a population of models and their hyperparameters. Goo‐\\ngle also used an evolutionary approach, not just to search for hyperparameters, but\\nalso to look for the best neural network architecture for the problem. They call this\\nAutoML, and it is already available as a cloud service. Perhaps the days of building\\nneural networks manually will soon be over? Check out Google’s post on this topic. In\\nfact, evolutionary algorithms have also been used successfully to train individual neu‐\\nral networks, replacing the ubiquitous Gradient Descent! See this 2017 post by Uber\\nwhere they introduce their Deep Neuroevolution technique.\\nDespite all this exciting progress, and all these tools and services, it still helps to have\\nan idea of what values are reasonable for each hyperparameter, so you can build a\\nquick prototype, and restrict the search space. Here are a few guidelines for choosing\\nthe number of hidden layers and neurons in an MLP, and selecting good values for\\nsome of the main hyperparameters.\\n318 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 344}, page_content='Number of Hidden Layers\\nFor many problems, you can just begin with a single hidden layer and you will get\\nreasonable results. It has actually been shown that an MLP with just one hidden layer\\ncan model even the most complex functions provided it has enough neurons. For a\\nlong time, these facts convinced researchers that there was no need to investigate any\\ndeeper neural networks. But they overlooked the fact that deep networks have a much\\nhigher parameter efficiency than shallow ones: they can model complex functions\\nusing exponentially fewer neurons than shallow nets, allowing them to reach much\\nbetter performance with the same amount of training data.\\nTo understand why, suppose you are asked to draw a forest using some drawing soft‐\\nware, but you are forbidden to use copy/paste. You would have to draw each tree\\nindividually, branch per branch, leaf per leaf. If you could instead draw one leaf,\\ncopy/paste it to draw a branch, then copy/paste that branch to create a tree, and\\nfinally copy/paste this tree to make a forest, you would be finished in no time. Real-\\nworld data is often structured in such a hierarchical way and Deep Neural Networks\\nautomatically take advantage of this fact: lower hidden layers model low-level struc‐\\ntures (e.g., line segments of various shapes and orientations), intermediate hidden\\nlayers combine these low-level structures to model intermediate-level structures (e.g.,\\nsquares, circles), and the highest hidden layers and the output layer combine these\\nintermediate structures to model high-level structures (e.g., faces).\\nNot only does this hierarchical architecture help DNNs converge faster to a good sol‐\\nution, it also improves their ability to generalize to new datasets. For example, if you\\nhave already trained a model to recognize faces in pictures, and you now want to\\ntrain a new neural network to recognize hairstyles, then you can kickstart training by\\nreusing the lower layers of the first network. Instead of randomly initializing the\\nweights and biases of the first few layers of the new neural network, you can initialize\\nthem to the value of the weights and biases of the lower layers of the first network.\\nThis way the network will not have to learn from scratch all the low-level structures\\nthat occur in most pictures; it will only have to learn the higher-level structures (e.g.,\\nhairstyles). This is called transfer learning.\\nIn summary, for many problems you can start with just one or two hidden layers and\\nit will work just fine (e.g., you can easily reach above 97% accuracy on the MNIST\\ndataset using just one hidden layer with a few hundred neurons, and above 98% accu‐\\nracy using two hidden layers with the same total amount of neurons, in roughly the\\nsame amount of training time). For more complex problems, you can gradually ramp\\nup the number of hidden layers, until you start overfitting the training set. Very com‐\\nplex tasks, such as large image classification or speech recognition, typically require\\nnetworks with dozens of layers (or even hundreds, but not fully connected ones, as\\nwe will see in Chapter 14), and they need a huge amount of training data. However,\\nyou will rarely have to train such networks from scratch: it is much more common to\\nFine-Tuning Neural Network Hyperparameters \\n| \\n319'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 345}, page_content='17 By Vincent Vanhoucke in his Deep Learning class on Udacity.com.\\nreuse parts of a pretrained state-of-the-art network that performs a similar task.\\nTraining will be a lot faster and require much less data (we will discuss this in Chap‐\\nter 11).\\nNumber of Neurons per Hidden Layer\\nObviously the number of neurons in the input and output layers is determined by the\\ntype of input and output your task requires. For example, the MNIST task requires 28\\nx 28 = 784 input neurons and 10 output neurons.\\nAs for the hidden layers, it used to be a common practice to size them to form a pyra‐\\nmid, with fewer and fewer neurons at each layer—the rationale being that many low-\\nlevel features can coalesce into far fewer high-level features. For example, a typical\\nneural network for MNIST may have three hidden layers, the first with 300 neurons,\\nthe second with 200, and the third with 100. However, this practice has been largely\\nabandoned now, as it seems that simply using the same number of neurons in all hid‐\\nden layers performs just as well in most cases, or even better, and there is just one\\nhyperparameter to tune instead of one per layer—for example, all hidden layers could\\nsimply have 150 neurons. However, depending on the dataset, it can sometimes help\\nto make the first hidden layer bigger than the others.\\nJust like for the number of layers, you can try increasing the number of neurons grad‐\\nually until the network starts overfitting. In general you will get more bang for the\\nbuck by increasing the number of layers than the number of neurons per layer.\\nUnfortunately, as you can see, finding the perfect amount of neurons is still somewhat\\nof a dark art.\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need, then use early stopping to prevent it from overfitting (and other regu‐\\nlarization techniques, such as dropout, as we will see in Chapter 11). This has been\\ndubbed the “stretch pants” approach:17 instead of wasting time looking for pants that\\nperfectly match your size, just use large stretch pants that will shrink down to the\\nright size.\\nLearning Rate, Batch Size and Other Hyperparameters\\nThe number of hidden layers and neurons are not the only hyperparameters you can\\ntweak in an MLP. Here are some of the most important ones, and some tips on how\\nto set them:\\n• The learning rate is arguably the most important hyperparameter. In general, the\\noptimal learning rate is about half of the maximum learning rate (i.e., the learn‐\\n320 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 346}, page_content='18 “Practical recommendations for gradient-based training of deep architectures,” Yoshua Bengio (2012).\\ning rate above which the training algorithm diverges, as we saw in Chapter 4). So\\na simple approach for tuning the learning rate is to start with a large value that\\nmakes the training algorithm diverge, then divide this value by 3 and try again,\\nand repeat until the training algorithm stops diverging. At that point, you gener‐\\nally won’t be too far from the optimal learning rate. That said, it is sometimes\\nuseful to reduce the learning rate during training: we will discuss this in Chap‐\\nter 11.\\n• Choosing a better optimizer than plain old Mini-batch Gradient Descent (and\\ntuning its hyperparameters) is also quite important. We will discuss this in Chap‐\\nter 11.\\n• The batch size can also have a significant impact on your model’s performance\\nand the training time. In general the optimal batch size will be lower than 32 (in\\nApril 2018, Yann Lecun even tweeted \"Friends don’t let friends use mini-batches\\nlarger than 32“). A small batch size ensures that each training iteration is very\\nfast, and although a large batch size will give a more precise estimate of the gradi‐\\nents, in practice this does not matter much since the optimization landscape is\\nquite complex and the direction of the true gradients do not point precisely in\\nthe direction of the optimum. However, having a batch size greater than 10 helps\\ntake advantage of hardware and software optimizations, in particular for matrix\\nmultiplications, so it will speed up training. Moreover, if you use Batch Normal‐\\nization (see Chapter 11), the batch size should not be too small (in general no less\\nthan 20).\\n• We discussed the choice of the activation function earlier in this chapter: in gen‐\\neral, the ReLU activation function will be a good default for all hidden layers. For\\nthe output layer, it really depends on your task.\\n• In most cases, the number of training iterations does not actually need to be\\ntweaked: just use early stopping instead.\\nFor more best practices, make sure to read Yoshua Bengio’s great 2012 paper18, which\\npresents many practical recommendations for deep networks.\\nThis concludes this introduction to artificial neural networks and their implementa‐\\ntion with Keras. In the next few chapters, we will discuss techniques to train very\\ndeep nets, we will see how to customize your models using TensorFlow’s lower-level\\nAPI and how to load and preprocess data efficiently using the Data API, and we will\\ndive into other popular neural network architectures: convolutional neural networks\\nfor image processing, recurrent neural networks for sequential data, autoencoders for\\nFine-Tuning Neural Network Hyperparameters \\n| \\n321'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 347}, page_content='19 A few extra ANN architectures are presented in ???.\\nrepresentation learning, and generative adversarial networks to model and generate\\ndata.19\\nExercises\\n1. Visit the TensorFlow Playground at https://playground.tensorflow.org/\\n• Layers and patterns: try training the default neural network by clicking the run\\nbutton (top left). Notice how it quickly finds a good solution for the classifica‐\\ntion task. Notice that the neurons in the first hidden layer have learned simple\\npatterns, while the neurons in the second hidden layer have learned to com‐\\nbine the simple patterns of the first hidden layer into more complex patterns.\\nIn general, the more layers, the more complex the patterns can be.\\n• Activation function: try replacing the Tanh activation function with the ReLU\\nactivation function, and train the network again. Notice that it finds a solution\\neven faster, but this time the boundaries are linear. This is due to the shape of\\nthe ReLU function.\\n• Local minima: modify the network architecture to have just one hidden layer\\nwith three neurons. Train it multiple times (to reset the network weights, click\\nthe reset button next to the play button). Notice that the training time varies a\\nlot, and sometimes it even gets stuck in a local minimum.\\n• Too small: now remove one neuron to keep just 2. Notice that the neural net‐\\nwork is now incapable of finding a good solution, even if you try multiple\\ntimes. The model has too few parameters and it systematically underfits the\\ntraining set.\\n• Large enough: next, set the number of neurons to 8 and train the network sev‐\\neral times. Notice that it is now consistently fast and never gets stuck. This\\nhighlights an important finding in neural network theory: large neural net‐\\nworks almost never get stuck in local minima, and even when they do these\\nlocal optima are almost as good as the global optimum. However, they can still\\nget stuck on long plateaus for a long time.\\n• Deep net and vanishing gradients: now change the dataset to be the spiral (bot‐\\ntom right dataset under “DATA”). Change the network architecture to have 4\\nhidden layers with 8 neurons each. Notice that training takes much longer, and\\noften gets stuck on plateaus for long periods of time. Also notice that the neu‐\\nrons in the highest layers (i.e. on the right) tend to evolve faster than the neu‐\\nrons in the lowest layers (i.e. on the left). This problem, called the “vanishing\\ngradients” problem, can be alleviated using better weight initialization and\\n322 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 348}, page_content='other techniques, better optimizers (such as AdaGrad or Adam), or using\\nBatch Normalization.\\n• More: go ahead and play with the other parameters to get a feel of what they\\ndo. In fact, you should definitely play with this UI for at least one hour, it will\\ngrow your intuitions about neural networks significantly.\\n2. Draw an ANN using the original artificial neurons (like the ones in Figure 10-3)\\nthat computes A ⊕ B (where ⊕ represents the XOR operation). Hint: A ⊕ B = (A\\n∧ ¬ B) ∨ (¬ A ∧ B).\\n3. Why is it generally preferable to use a Logistic Regression classifier rather than a\\nclassical Perceptron (i.e., a single layer of threshold logic units trained using the\\nPerceptron training algorithm)? How can you tweak a Perceptron to make it\\nequivalent to a Logistic Regression classifier?\\n4. Why was the logistic activation function a key ingredient in training the first\\nMLPs?\\n5. Name three popular activation functions. Can you draw them?\\n6. Suppose you have an MLP composed of one input layer with 10 passthrough\\nneurons, followed by one hidden layer with 50 artificial neurons, and finally one\\noutput layer with 3 artificial neurons. All artificial neurons use the ReLU activa‐\\ntion function.\\n• What is the shape of the input matrix X?\\n• What about the shape of the hidden layer’s weight vector Wh, and the shape of\\nits bias vector bh?\\n• What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\\n• What is the shape of the network’s output matrix Y?\\n• Write the equation that computes the network’s output matrix Y as a function\\nof X, Wh, bh, Wo and bo.\\n7. How many neurons do you need in the output layer if you want to classify email\\ninto spam or ham? What activation function should you use in the output layer?\\nIf instead you want to tackle MNIST, how many neurons do you need in the out‐\\nput layer, using what activation function? Answer the same questions for getting\\nyour network to predict housing prices as in Chapter 2.\\n8. What is backpropagation and how does it work? What is the difference between\\nbackpropagation and reverse-mode autodiff?\\n9. Can you list all the hyperparameters you can tweak in an MLP? If the MLP over‐\\nfits the training data, how could you tweak these hyperparameters to try to solve\\nthe problem?\\nExercises \\n| \\n323'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 349}, page_content='10. Train a deep MLP on the MNIST dataset and see if you can get over 98% preci‐\\nsion. Try adding all the bells and whistles (i.e., save checkpoints, use early stop‐\\nping, plot learning curves using TensorBoard, and so on).\\nSolutions to these exercises are available in ???.\\n324 \\n| \\nChapter 10: Introduction to Artificial Neural Networks with Keras'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 350}, page_content='CHAPTER 11\\nTraining Deep Neural Networks\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 11 in the final\\nrelease of the book.\\nIn Chapter 10 we introduced artificial neural networks and trained our first deep\\nneural networks. But they were very shallow nets, with just a few hidden layers. What\\nif you need to tackle a very complex problem, such as detecting hundreds of types of\\nobjects in high-resolution images? You may need to train a much deeper DNN, per‐\\nhaps with 10 layers or much more, each containing hundreds of neurons, connected\\nby hundreds of thousands of connections. This would not be a walk in the park:\\n• First, you would be faced with the tricky vanishing gradients problem (or the\\nrelated exploding gradients problem) that affects deep neural networks and makes\\nlower layers very hard to train.\\n• Second, you might not have enough training data for such a large network, or it\\nmight be too costly to label.\\n• Third, training may be extremely slow.\\n• Fourth, a model with millions of parameters would severely risk overfitting the\\ntraining set, especially if there are not enough training instances, or they are too\\nnoisy.\\nIn this chapter, we will go through each of these problems in turn and present techni‐\\nques to solve them. We will start by explaining the vanishing gradients problem and\\nexploring some of the most popular solutions to this problem. Next, we will look at\\ntransfer learning and unsupervised pretraining, which can help you tackle complex\\n325'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 351}, page_content='1 “Understanding the Difficulty of Training Deep Feedforward Neural Networks,” X. Glorot, Y Bengio (2010).\\ntasks even when you have little labeled data. Then we will discuss various optimizers\\nthat can speed up training large models tremendously compared to plain Gradient\\nDescent. Finally, we will go through a few popular regularization techniques for large\\nneural networks.\\nWith these tools, you will be able to train very deep nets: welcome to Deep Learning!\\nVanishing/Exploding Gradients Problems\\nAs we discussed in Chapter 10, the backpropagation algorithm works by going from\\nthe output layer to the input layer, propagating the error gradient on the way. Once\\nthe algorithm has computed the gradient of the cost function with regards to each\\nparameter in the network, it uses these gradients to update each parameter with a\\nGradient Descent step.\\nUnfortunately, gradients often get smaller and smaller as the algorithm progresses\\ndown to the lower layers. As a result, the Gradient Descent update leaves the lower\\nlayer connection weights virtually unchanged, and training never converges to a good\\nsolution. This is called the vanishing gradients problem. In some cases, the opposite\\ncan happen: the gradients can grow bigger and bigger, so many layers get insanely\\nlarge weight updates and the algorithm diverges. This is the exploding gradients prob‐\\nlem, which is mostly encountered in recurrent neural networks (see ???). More gener‐\\nally, deep neural networks suffer from unstable gradients; different layers may learn at\\nwidely different speeds.\\nAlthough this unfortunate behavior has been empirically observed for quite a while\\n(it was one of the reasons why deep neural networks were mostly abandoned for a\\nlong time), it is only around 2010 that significant progress was made in understand‐\\ning it. A paper titled “Understanding the Difficulty of Training Deep Feedforward\\nNeural Networks” by Xavier Glorot and Yoshua Bengio1 found a few suspects, includ‐\\ning the combination of the popular logistic sigmoid activation function and the\\nweight initialization technique that was most popular at the time, namely random ini‐\\ntialization using a normal distribution with a mean of 0 and a standard deviation of 1.\\nIn short, they showed that with this activation function and this initialization scheme,\\nthe variance of the outputs of each layer is much greater than the variance of its\\ninputs. Going forward in the network, the variance keeps increasing after each layer\\nuntil the activation function saturates at the top layers. This is actually made worse by\\nthe fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent\\nfunction has a mean of 0 and behaves slightly better than the logistic function in deep\\nnetworks).\\n326 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 352}, page_content='2 Here’s an analogy: if you set a microphone amplifier’s knob too close to zero, people won’t hear your voice, but\\nif you set it too close to the max, your voice will be saturated and people won’t understand what you are say‐\\ning. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come\\nout loud and clear at the end of the chain. Your voice has to come out of each amplifier at the same amplitude\\nas it came in.\\nLooking at the logistic activation function (see Figure 11-1), you can see that when\\ninputs become large (negative or positive), the function saturates at 0 or 1, with a\\nderivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\\nno gradient to propagate back through the network, and what little gradient exists\\nkeeps getting diluted as backpropagation progresses down through the top layers, so\\nthere is really nothing left for the lower layers.\\nFigure 11-1. Logistic activation function saturation\\nGlorot and He Initialization\\nIn their paper, Glorot and Bengio propose a way to significantly alleviate this prob‐\\nlem. We need the signal to flow properly in both directions: in the forward direction\\nwhen making predictions, and in the reverse direction when backpropagating gradi‐\\nents. We don’t want the signal to die out, nor do we want it to explode and saturate.\\nFor the signal to flow properly, the authors argue that we need the variance of the\\noutputs of each layer to be equal to the variance of its inputs,2 and we also need the\\ngradients to have equal variance before and after flowing through a layer in the\\nreverse direction (please check out the paper if you are interested in the mathematical\\ndetails). It is actually not possible to guarantee both unless the layer has an equal\\nnumber of inputs and neurons (these numbers are called the fan-in and fan-out of the\\nlayer), but they proposed a good compromise that has proven to work very well in\\npractice: the connection weights of each layer must be initialized randomly as\\nVanishing/Exploding Gradients Problems \\n| \\n327'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 353}, page_content='3 Such as “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,” K.\\nHe et al. (2015).\\ndescribed in Equation 11-1, where f anavg = f anin + f anout /2. This initialization\\nstrategy is called Xavier initialization (after the author’s first name) or Glorot initiali‐\\nzation (after his last name).\\nEquation 11-1. Glorot initialization (when using the logistic activation function)\\nNormal distribution with mean 0 and variance\\xa0σ2 =\\n1\\nfanavg\\nOr a uniform distribution between\\xa0−r\\xa0and\\xa0 + r, with\\xa0r =\\n3\\nfanavg\\nIf you just replace fanavg with fanin in Equation 11-1, you get an initialization strategy\\nthat was actually already proposed by Yann LeCun in the 1990s, called LeCun initiali‐\\nzation, which was even recommended in the 1998 book Neural Networks: Tricks of the\\nTrade by Genevieve Orr and Klaus-Robert Müller (Springer). It is equivalent to\\nGlorot initialization when fanin = fanout. It took over a decade for researchers to realize\\njust how important this trick really is. Using Glorot initialization can speed up train‐\\ning considerably, and it is one of the tricks that led to the current success of Deep\\nLearning.\\nSome papers3 have provided similar strategies for different activation functions.\\nThese strategies differ only by the scale of the variance and whether they use fanavg or\\nfanin, as shown in Table 11-1 (for the uniform distribution, just compute r =\\n3σ2).\\nThe initialization strategy for the ReLU activation function (and its variants, includ‐\\ning the ELU activation described shortly) is sometimes called He initialization (after\\nthe last name of its author). The SELU activation function will be explained later in\\nthis chapter. It should be used with LeCun initialization (preferably with a normal\\ndistribution, as we will see).\\nTable 11-1. Initialization parameters for each type of activation function\\nInitialization Activation functions\\nσ² (Normal)\\nGlorot\\nNone, Tanh, Logistic, Softmax\\n1 / fanavg\\nHe\\nReLU & variants\\n2 / fanin\\nLeCun\\nSELU\\n1 / fanin\\nBy default, Keras uses Glorot initialization with a uniform distribution. You can\\nchange this to He initialization by setting kernel_initializer=\"he_uniform\" or ker\\nnel_initializer=\"he_normal\" when creating a layer, like this:\\n328 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 354}, page_content='4 Unless it is part of the first hidden layer, a dead neuron may sometimes come back to life: gradient descent\\nmay indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neuron’s\\ninputs is positive again.\\n5 “Empirical Evaluation of Rectified Activations in Convolution Network,” B. Xu et al. (2015).\\nkeras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\\nIf you want He initialization with a uniform distribution, but based on fanavg rather\\nthan fanin, you can use the VarianceScaling initializer like this:\\nhe_avg_init = keras.initializers.VarianceScaling(scale=2., mode=\\'fan_avg\\',\\n                                                 distribution=\\'uniform\\')\\nkeras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\\nNonsaturating Activation Functions\\nOne of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\\nexploding gradients problems were in part due to a poor choice of activation func‐\\ntion. Until then most people had assumed that if Mother Nature had chosen to use\\nroughly sigmoid activation functions in biological neurons, they must be an excellent\\nchoice. But it turns out that other activation functions behave much better in deep\\nneural networks, in particular the ReLU activation function, mostly because it does\\nnot saturate for positive values (and also because it is quite fast to compute).\\nUnfortunately, the ReLU activation function is not perfect. It suffers from a problem\\nknown as the dying ReLUs: during training, some neurons effectively die, meaning\\nthey stop outputting anything other than 0. In some cases, you may find that half of\\nyour network’s neurons are dead, especially if you used a large learning rate. A neu‐\\nron dies when its weights get tweaked in such a way that the weighted sum of its\\ninputs are negative for all instances in the training set. When this happens, it just\\nkeeps outputting 0s, and gradient descent does not affect it anymore since the gradi‐\\nent of the ReLU function is 0 when its input is negative.4\\nTo solve this problem, you may want to use a variant of the ReLU function, such as\\nthe leaky ReLU. This function is defined as LeakyReLUα(z) = max(αz, z) (see\\nFigure 11-2). The hyperparameter α defines how much the function “leaks”: it is the\\nslope of the function for z < 0, and is typically set to 0.01. This small slope ensures\\nthat leaky ReLUs never die; they can go into a long coma, but they have a chance to\\neventually wake up. A 2015 paper5 compared several variants of the ReLU activation\\nfunction and one of its conclusions was that the leaky variants always outperformed\\nthe strict ReLU activation function. In fact, setting α = 0.2 (huge leak) seemed to\\nresult in better performance than α = 0.01 (small leak). They also evaluated the\\nrandomized leaky ReLU (RReLU), where α is picked randomly in a given range during\\ntraining, and it is fixed to an average value during testing. It also performed fairly well\\nand seemed to act as a regularizer (reducing the risk of overfitting the training set).\\nVanishing/Exploding Gradients Problems \\n| \\n329'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 355}, page_content='6 “Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),” D. Clevert, T. Unterthiner,\\nS. Hochreiter (2015).\\nFinally, they also evaluated the parametric leaky ReLU (PReLU), where α is authorized\\nto be learned during training (instead of being a hyperparameter, it becomes a\\nparameter that can be modified by backpropagation like any other parameter). This\\nwas reported to strongly outperform ReLU on large image datasets, but on smaller\\ndatasets it runs the risk of overfitting the training set.\\nFigure 11-2. Leaky ReLU\\nLast but not least, a 2015 paper by Djork-Arné Clevert et al.6 proposed a new activa‐\\ntion function called the exponential linear unit (ELU) that outperformed all the ReLU\\nvariants in their experiments: training time was reduced and the neural network per‐\\nformed better on the test set. It is represented in Figure 11-3, and Equation 11-2\\nshows its definition.\\nEquation 11-2. ELU activation function\\nELUα z = α exp z −1 if z < 0\\nz\\nif z ≥0\\n330 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 356}, page_content='7 “Self-Normalizing Neural Networks, \" G. Klambauer, T. Unterthiner and A. Mayr (2017).\\nFigure 11-3. ELU activation function\\nIt looks a lot like the ReLU function, with a few major differences:\\n• First it takes on negative values when z < 0, which allows the unit to have an\\naverage output closer to 0. This helps alleviate the vanishing gradients problem,\\nas discussed earlier. The hyperparameter α defines the value that the ELU func‐\\ntion approaches when z is a large negative number. It is usually set to 1, but you\\ncan tweak it like any other hyperparameter if you want.\\n• Second, it has a nonzero gradient for z < 0, which avoids the dead neurons prob‐\\nlem.\\n• Third, if α is equal to 1 then the function is smooth everywhere, including\\naround z = 0, which helps speed up Gradient Descent, since it does not bounce as\\nmuch left and right of z = 0.\\nThe main drawback of the ELU activation function is that it is slower to compute\\nthan the ReLU and its variants (due to the use of the exponential function), but dur‐\\ning training this is compensated by the faster convergence rate. However, at test time\\nan ELU network will be slower than a ReLU network.\\nMoreover, in a 2017 paper7 by Günter Klambauer et al., called “Self-Normalizing\\nNeural Networks”, the authors showed that if you build a neural network composed\\nexclusively of a stack of dense layers, and if all hidden layers use the SELU activation\\nfunction (which is just a scaled version of the ELU activation function, as its name\\nsuggests), then the network will self-normalize: the output of each layer will tend to\\npreserve mean 0 and standard deviation 1 during training, which solves the vanish‐\\ning/exploding gradients problem. As a result, this activation function often outper‐\\nVanishing/Exploding Gradients Problems \\n| \\n331'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 357}, page_content='forms other activation functions very significantly for such neural nets (especially\\ndeep ones). However, there are a few conditions for self-normalization to happen:\\n• The input features must be standardized (mean 0 and standard deviation 1).\\n• Every hidden layer’s weights must also be initialized using LeCun normal initiali‐\\nzation. In Keras, this means setting kernel_initializer=\"lecun_normal\".\\n• The network’s architecture must be sequential. Unfortunately, if you try to use\\nSELU in non-sequential architectures, such as recurrent networks (see ???) or\\nnetworks with skip connections (i.e., connections that skip layers, such as in wide\\n& deep nets), self-normalization will not be guaranteed, so SELU will not neces‐\\nsarily outperform other activation functions.\\n• The paper only guarantees self-normalization if all layers are dense. However, in\\npractice the SELU activation function seems to work great with convolutional\\nneural nets as well (see Chapter 14).\\nSo which activation function should you use for the hidden layers\\nof your deep neural networks? Although your mileage will vary, in\\ngeneral SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh\\n> logistic. If the network’s architecture prevents it from self-\\nnormalizing, then ELU may perform better than SELU (since SELU\\nis not smooth at z = 0). If you care a lot about runtime latency, then\\nyou may prefer leaky ReLU. If you don’t want to tweak yet another\\nhyperparameter, you may just use the default α values used by\\nKeras (e.g., 0.3 for the leaky ReLU). If you have spare time and\\ncomputing power, you can use cross-validation to evaluate other\\nactivation functions, in particular RReLU if your network is over‐\\nfitting, or PReLU if you have a huge training set.\\nTo use the leaky ReLU activation function, you must create a LeakyReLU instance like\\nthis:\\nleaky_relu = keras.layers.LeakyReLU(alpha=0.2)\\nlayer = keras.layers.Dense(10, activation=leaky_relu,\\n                           kernel_initializer=\"he_normal\")\\nFor PReLU, just replace LeakyRelu(alpha=0.2) with PReLU(). There is currently no\\nofficial implementation of RReLU in Keras, but you can fairly easily implement your\\nown (see the exercises at the end of Chapter 12).\\nFor SELU activation, just set \\nactivation=\"selu\" and \\nkernel_initial\\nizer=\"lecun_normal\" when creating a layer:\\nlayer = keras.layers.Dense(10, activation=\"selu\",\\n                           kernel_initializer=\"lecun_normal\")\\n332 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 358}, page_content='8 “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,” S. Ioffe\\nand C. Szegedy (2015).\\nBatch Normalization\\nAlthough using He initialization along with ELU (or any variant of ReLU) can signifi‐\\ncantly reduce the vanishing/exploding gradients problems at the beginning of train‐\\ning, it doesn’t guarantee that they won’t come back during training.\\nIn a 2015 paper,8 Sergey Ioffe and Christian Szegedy proposed a technique called\\nBatch Normalization (BN) to address the vanishing/exploding gradients problems.\\nThe technique consists of adding an operation in the model just before or after the\\nactivation function of each hidden layer, simply zero-centering and normalizing each\\ninput, then scaling and shifting the result using two new parameter vectors per layer:\\none for scaling, the other for shifting. In other words, this operation lets the model\\nlearn the optimal scale and mean of each of the layer’s inputs. In many cases, if you\\nadd a BN layer as the very first layer of your neural network, you do not need to\\nstandardize your training set (e.g., using a StandardScaler): the BN layer will do it\\nfor you (well, approximately, since it only looks at one batch at a time, and it can also\\nrescale and shift each input feature).\\nIn order to zero-center and normalize the inputs, the algorithm needs to estimate\\neach input’s mean and standard deviation. It does so by evaluating the mean and stan‐\\ndard deviation of each input over the current mini-batch (hence the name “Batch\\nNormalization”). The whole operation is summarized in Equation 11-3.\\nEquation 11-3. Batch Normalization algorithm\\n1 .\\nμB = 1\\nmB ∑\\ni = 1\\nmB\\nx i\\n2 .\\nσB\\n2 = 1\\nmB ∑\\ni = 1\\nmB\\nx i −μB\\n2\\n3 .\\nx i =\\nx i −μB\\nσB\\n2 + �\\n4 .\\nz i = γ ⊗x i + β\\n• μB is the vector of input means, evaluated over the whole mini-batch B (it con‐\\ntains one mean per input).\\nVanishing/Exploding Gradients Problems \\n| \\n333'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 359}, page_content='• σB is the vector of input standard deviations, also evaluated over the whole mini-\\nbatch (it contains one standard deviation per input).\\n• mB is the number of instances in the mini-batch.\\n• x(i) is the vector of zero-centered and normalized inputs for instance i.\\n• γ is the output scale parameter vector for the layer (it contains one scale parame‐\\nter per input).\\n• ⊗ represents element-wise multiplication (each input is multiplied by its corre‐\\nsponding output scale parameter).\\n• β is the output shift (offset) parameter vector for the layer (it contains one offset\\nparameter per input). Each input is offset by its corresponding shift parameter.\\n• ϵ is a tiny number to avoid division by zero (typically 10–5). This is called a\\nsmoothing term.\\n• z(i) is the output of the BN operation: it is a rescaled and shifted version of the\\ninputs.\\nSo during training, BN just standardizes its inputs then rescales and offsets them.\\nGood! What about at test time? Well it is not that simple. Indeed, we may need to\\nmake predictions for individual instances rather than for batches of instances: in this\\ncase, we will have no way to compute each input’s mean and standard deviation.\\nMoreover, even if we do have a batch of instances, it may be too small, or the instan‐\\nces may not be independent and identically distributed (IID), so computing statistics\\nover the batch instances would be unreliable (during training, the batches should not\\nbe too small, if possible more than 30 instances, and all instances should be IID, as we\\nsaw in Chapter 4). One solution could be to wait until the end of training, then run\\nthe whole training set through the neural network, and compute the mean and stan‐\\ndard deviation of each input of the BN layer. These “final” input means and standard\\ndeviations can then be used instead of the batch input means and standard deviations\\nwhen making predictions. However, it is often preferred to estimate these final statis‐\\ntics during training using a moving average of the layer’s input means and standard\\ndeviations. To sum up, four parameter vectors are learned in each batch-normalized\\nlayer: γ (the ouput scale vector) and β (the output offset vector) are learned through\\nregular backpropagation, and μ (the final input mean vector), and σ (the final input\\nstandard deviation vector) are estimated using an exponential moving average. Note\\nthat μ and σ are estimated during training, but they are not used at all during train‐\\ning, only after training (to replace the batch input means and standard deviations in\\nEquation 11-3).\\nThe authors demonstrated that this technique considerably improved all the deep\\nneural networks they experimented with, leading to a huge improvement in the\\nImageNet classification task (ImageNet is a large database of images classified into\\nmany classes and commonly used to evaluate computer vision systems). The vanish‐\\n334 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 360}, page_content='ing gradients problem was strongly reduced, to the point that they could use saturat‐\\ning activation functions such as the tanh and even the logistic activation function.\\nThe networks were also much less sensitive to the weight initialization. They were\\nable to use much larger learning rates, significantly speeding up the learning process.\\nSpecifically, they note that “Applied to a state-of-the-art image classification model,\\nBatch Normalization achieves the same accuracy with 14 times fewer training steps,\\nand beats the original model by a significant margin. […] Using an ensemble of\\nbatch-normalized networks, we improve upon the best published result on ImageNet\\nclassification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding\\nthe accuracy of human raters.” Finally, like a gift that keeps on giving, Batch Normal‐\\nization also acts like a regularizer, reducing the need for other regularization techni‐\\nques (such as dropout, described later in this chapter).\\nBatch Normalization does, however, add some complexity to the model (although it\\ncan remove the need for normalizing the input data, as we discussed earlier). More‐\\nover, there is a runtime penalty: the neural network makes slower predictions due to\\nthe extra computations required at each layer. So if you need predictions to be\\nlightning-fast, you may want to check how well plain ELU + He initialization perform\\nbefore playing with Batch Normalization.\\nYou may find that training is rather slow, because each epoch takes\\nmuch more time when you use batch normalization. However, this\\nis usually counterbalanced by the fact that convergence is much\\nfaster with BN, so it will take fewer epochs to reach the same per‐\\nformance. All in all, wall time will usually be smaller (this is the\\ntime measured by the clock on your wall).\\nImplementing Batch Normalization with Keras\\nAs with most things with Keras, implementing Batch Normalization is quite simple.\\nJust add a BatchNormalization layer before or after each hidden layer’s activation\\nfunction, and optionally add a BN layer as well as the first layer in your model. For\\nexample, this model applies BN after every hidden layer and as the first layer in the\\nmodel (after flattening the input images):\\nmodel = keras.models.Sequential([\\n    keras.layers.Flatten(input_shape=[28, 28]),\\n    keras.layers.BatchNormalization(),\\n    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\\n    keras.layers.BatchNormalization(),\\n    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\\n    keras.layers.BatchNormalization(),\\n    keras.layers.Dense(10, activation=\"softmax\")\\n])\\nVanishing/Exploding Gradients Problems \\n| \\n335'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 361}, page_content='9 However, they are estimated during training, based on the training data, so arguably they are trainable. In\\nKeras, “Non-trainable” really means “untouched by backpropagation”.\\nThat’s all! In this tiny example with just two hidden layers, it’s unlikely that Batch\\nNormalization will have a very positive impact, but for deeper networks it can make a\\ntremendous difference.\\nLet’s zoom in a bit. If you display the model summary, you can see that each BN layer\\nadds 4 parameters per input: γ, β, μ and σ (for example, the first BN layer adds 3136\\nparameters, which is 4 times 784). The last two parameters, μ and σ, are the moving\\naverages, they are not affected by backpropagation, so Keras calls them “Non-\\ntrainable”9 (if you count the total number of BN parameters, 3136 + 1200 + 400, and\\ndivide by two, you get 2,368, which is the total number of non-trainable params in\\nthis model).\\n>>> model.summary()\\nModel: \"sequential_3\"\\n_________________________________________________________________\\nLayer (type)                 Output Shape              Param #\\n=================================================================\\nflatten_3 (Flatten)          (None, 784)               0\\n_________________________________________________________________\\nbatch_normalization_v2 (Batc (None, 784)               3136\\n_________________________________________________________________\\ndense_50 (Dense)             (None, 300)               235500\\n_________________________________________________________________\\nbatch_normalization_v2_1 (Ba (None, 300)               1200\\n_________________________________________________________________\\ndense_51 (Dense)             (None, 100)               30100\\n_________________________________________________________________\\nbatch_normalization_v2_2 (Ba (None, 100)               400\\n_________________________________________________________________\\ndense_52 (Dense)             (None, 10)                1010\\n=================================================================\\nTotal params: 271,346\\nTrainable params: 268,978\\nNon-trainable params: 2,368\\nLet’s look at the parameters of the first BN layer. Two are trainable (by backprop), and\\ntwo are not:\\n>>> [(var.name, var.trainable) for var in model.layers[1].variables]\\n[(\\'batch_normalization_v2/gamma:0\\', True),\\n (\\'batch_normalization_v2/beta:0\\', True),\\n (\\'batch_normalization_v2/moving_mean:0\\', False),\\n (\\'batch_normalization_v2/moving_variance:0\\', False)]\\nNow when you create a BN layer in Keras, it also creates two operations that will be\\ncalled by Keras at each iteration during training. These operations will update the\\n336 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 362}, page_content='moving averages. Since we are using the TensorFlow backend, these operations are\\nTensorFlow operations (we will discuss TF operations in Chapter 12).\\n>>> model.layers[1].updates\\n[<tf.Operation \\'cond_2/Identity\\' type=Identity>,\\n <tf.Operation \\'cond_3/Identity\\' type=Identity>]\\nThe authors of the BN paper argued in favor of adding the BN layers before the acti‐\\nvation functions, rather than after (as we just did). There is some debate about this, as\\nit seems to depend on the task. So that’s one more thing you can experiment with to\\nsee which option works best on your dataset. To add the BN layers before the activa‐\\ntion functions, we must remove the activation function from the hidden layers, and\\nadd them as separate layers after the BN layers. Moreover, since a Batch Normaliza‐\\ntion layer includes one offset parameter per input, you can remove the bias term from\\nthe previous layer (just pass use_bias=False when creating it):\\nmodel = keras.models.Sequential([\\n    keras.layers.Flatten(input_shape=[28, 28]),\\n    keras.layers.BatchNormalization(),\\n    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\\n    keras.layers.BatchNormalization(),\\n    keras.layers.Activation(\"elu\"),\\n    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\\n    keras.layers.Activation(\"elu\"),\\n    keras.layers.BatchNormalization(),\\n    keras.layers.Dense(10, activation=\"softmax\")\\n])\\nThe BatchNormalization class has quite a few hyperparameters you can tweak. The\\ndefaults will usually be fine, but you may occasionally need to tweak the momentum.\\nThis hyperparameter is used when updating the exponential moving averages: given a\\nnew value v (i.e., a new vector of input means or standard deviations computed over\\nthe current batch), the running average � is updated using the following equation:\\nv\\nv × momentum + v × 1 −momentum\\nA good momentum value is typically close to 1—for example, 0.9, 0.99, or 0.999 (you\\nwant more 9s for larger datasets and smaller mini-batches).\\nAnother important hyperparameter is axis: it determines which axis should be nor‐\\nmalized. It defaults to –1, meaning that by default it will normalize the last axis (using\\nthe means and standard deviations computed across the other axes). For example,\\nwhen the input batch is 2D (i.e., the batch shape is [batch size, features]), this means\\nthat each input feature will be normalized based on the mean and standard deviation\\ncomputed across all the instances in the batch. For example, the first BN layer in the\\nprevious code example will independently normalize (and rescale and shift) each of\\nthe 784 input features. However, if we move the first BN layer before the Flatten\\nVanishing/Exploding Gradients Problems \\n| \\n337'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 363}, page_content='10 “Fixup Initialization: Residual Learning Without Normalization,” Hongyi Zhang, Yann N. Dauphin, Tengyu\\nMa (2019).\\n11 “On the difficulty of training recurrent neural networks,” R. Pascanu et al. (2013).\\nlayer, then the input batches will be 3D, with shape [batch size, height, width], there‐\\nfore the BN layer will compute 28 means and 28 standard deviations (one per column\\nof pixels, computed across all instances in the batch, and all rows in the column), and\\nit will normalize all pixels in a given column using the same mean and standard devi‐\\nation. There will also be just 28 scale parameters and 28 shift parameters. If instead\\nyou still want to treat each of the 784 pixels independently, then you should set\\naxis=[1, 2].\\nNotice that the BN layer does not perform the same computation during training and\\nafter training: it uses batch statistics during training, and the “final” statistics after\\ntraining (i.e., the final value of the moving averages). Let’s take a peek at the source\\ncode of this class to see how this is handled:\\nclass BatchNormalization(Layer):\\n    [...]\\n    def call(self, inputs, training=None):\\n        if training is None:\\n            training = keras.backend.learning_phase()\\n        [...]\\nThe call() method is the one that actually performs the computations, and as you\\ncan see it has an extra training argument: if it is None it falls back to keras.back\\nend.learning_phase(), which returns 1 during training (the fit() method ensures\\nthat). Otherwise, it returns 0. If you ever need to write a custom layer, and it needs to\\nbehave differently during training and testing, simply use the same pattern (we will\\ndiscuss custom layers in Chapter 12).\\nBatch Normalization has become one of the most used layers in deep neural net‐\\nworks, to the point that it is often omitted in the diagrams, as it is assumed that BN is\\nadded after every layer. However, a very recent paper10 by Hongyi Zhang et al. may\\nwell change this: the authors show that by using a novel fixed-update (fixup) weight\\ninitialization technique, they manage to train a very deep neural network (10,000 lay‐\\ners!) without BN, achieving state-of-the-art performance on complex image classifi‐\\ncation tasks.\\nGradient Clipping\\nAnother popular technique to lessen the exploding gradients problem is to simply\\nclip the gradients during backpropagation so that they never exceed some threshold.\\nThis is called Gradient Clipping.11 This technique is most often used in recurrent neu‐\\n338 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 364}, page_content='ral networks, as Batch Normalization is tricky to use in RNNs, as we will see in ???.\\nFor other types of networks, BN is usually sufficient.\\nIn Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or\\nclipnorm argument when creating an optimizer. For example:\\noptimizer = keras.optimizers.SGD(clipvalue=1.0)\\nmodel.compile(loss=\"mse\", optimizer=optimizer)\\nThis will clip every component of the gradient vector to a value between –1.0 and 1.0.\\nThis means that all the partial derivatives of the loss (with regards to each and every\\ntrainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyper‐\\nparameter you can tune. Note that it may change the orientation of the gradient vec‐\\ntor: for example, if the original gradient vector is [0.9, 100.0], it points mostly in the\\ndirection of the second axis, but once you clip it by value, you get [0.9, 1.0], which\\npoints roughly in the diagonal between the two axes. In practice however, this\\napproach works well. If you want to ensure that Gradient Clipping does not change\\nthe direction of the gradient vector, you should clip by norm by setting clipnorm\\ninstead of clipvalue. This will clip the whole gradient if its ℓ2 norm is greater than\\nthe threshold you picked. For example, if you set clipnorm=1.0, then the vector [0.9,\\n100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation, but\\nalmost eliminating the first component. If you observe that the gradients explode\\nduring training (you can track the size of the gradients using TensorBoard), you may\\nwant to try both clipping by value and clipping by norm, with different threshold,\\nand see which option performs best on the validation set.\\nReusing Pretrained Layers\\nIt is generally not a good idea to train a very large DNN from scratch: instead, you\\nshould always try to find an existing neural network that accomplishes a similar task\\nto the one you are trying to tackle (we will discuss how to find them in Chapter 14),\\nthen just reuse the lower layers of this network: this is called transfer learning. It will\\nnot only speed up training considerably, but will also require much less training data.\\nFor example, suppose that you have access to a DNN that was trained to classify pic‐\\ntures into 100 different categories, including animals, plants, vehicles, and everyday\\nobjects. You now want to train a DNN to classify specific types of vehicles. These\\ntasks are very similar, even partly overlapping, so you should try to reuse parts of the\\nfirst network (see Figure 11-4).\\nReusing Pretrained Layers \\n| \\n339'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 365}, page_content='Figure 11-4. Reusing pretrained layers\\nIf the input pictures of your new task don’t have the same size as\\nthe ones used in the original task, you will usually have to add a\\npreprocessing step to resize them to the size expected by the origi‐\\nnal model. More generally, transfer learning will work best when\\nthe inputs have similar low-level features.\\nThe output layer of the original model should usually be replaced since it is most\\nlikely not useful at all for the new task, and it may not even have the right number of\\noutputs for the new task.\\nSimilarly, the upper hidden layers of the original model are less likely to be as useful\\nas the lower layers, since the high-level features that are most useful for the new task\\nmay differ significantly from the ones that were most useful for the original task. You\\nwant to find the right number of layers to reuse.\\nThe more similar the tasks are, the more layers you want to reuse\\n(starting with the lower layers). For very similar tasks, you can try\\nkeeping all the hidden layers and just replace the output layer.\\nTry freezing all the reused layers first (i.e., make their weights non-trainable, so gradi‐\\nent descent won’t modify them), then train your model and see how it performs.\\nThen try unfreezing one or two of the top hidden layers to let backpropagation tweak\\nthem and see if performance improves. The more training data you have, the more\\n340 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 366}, page_content='layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze\\nreused layers: this will avoid wrecking their fine-tuned weights.\\nIf you still cannot get good performance, and you have little training data, try drop‐\\nping the top hidden layer(s) and freeze all remaining hidden layers again. You can\\niterate until you find the right number of layers to reuse. If you have plenty of train‐\\ning data, you may try replacing the top hidden layers instead of dropping them, and\\neven add more hidden layers.\\nTransfer Learning With Keras\\nLet’s look at an example. Suppose the fashion MNIST dataset only contained 8 classes,\\nfor example all classes except for sandals and shirts. Someone built and trained a\\nKeras model on that set and got reasonably good performance (>90% accuracy). Let’s\\ncall this model A. You now want to tackle a different task: you have images of sandals\\nand shirts, and you want to train a binary classifier (positive=shirts, negative=san‐\\ndals). However, your dataset is quite small, you only have 200 labeled images. When\\nyou train a new model for this task (let’s call it model B), with the same architecture\\nas model A, it performs reasonably well (97.2% accuracy), but since it’s a much easier\\ntask (there are just 2 classes), you were hoping for more. While drinking your morn‐\\ning coffee, you realize that your task is quite similar to task A, so perhaps transfer\\nlearning can help? Let’s find out!\\nFirst, you need to load model A, and create a new model based on the model A’s lay‐\\ners. Let’s reuse all layers except for the output layer:\\nmodel_A = keras.models.load_model(\"my_model_A.h5\")\\nmodel_B_on_A = keras.models.Sequential(model_A.layers[:-1])\\nmodel_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\\nNote that model_A and model_B_on_A now share some layers. When you train\\nmodel_B_on_A, it will also affect model_A. If you want to avoid that, you need to clone\\nmodel_A before you reuse its layers. To do this, you must clone model A’s architecture,\\nthen copy its weights (since clone_model() does not clone the weights):\\nmodel_A_clone = keras.models.clone_model(model_A)\\nmodel_A_clone.set_weights(model_A.get_weights())\\nNow we could just train model_B_on_A for task B, but since the new output layer was\\ninitialized randomly, it will make large errors, at least during the first few epochs, so\\nthere will be large error gradients that may wreck the reused weights. To avoid this,\\none approach is to freeze the reused layers during the first few epochs, giving the new\\nlayer some time to learn reasonable weights. To do this, simply set every layer’s train\\nable attribute to False and compile the model:\\nfor layer in model_B_on_A.layers[:-1]:\\n    layer.trainable = False\\nReusing Pretrained Layers \\n| \\n341'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 367}, page_content='model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\",\\n                     metrics=[\"accuracy\"])\\nYou must always compile your model after you freeze or unfreeze\\nlayers.\\nNext, we can train the model for a few epochs, then unfreeze the reused layers (which\\nrequires compiling the model again) and continue training to fine-tune the reused\\nlayers for task B. After unfreezing the reused layers, it is usually a good idea to reduce\\nthe learning rate, once again to avoid damaging the reused weights:\\nhistory = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\\n                           validation_data=(X_valid_B, y_valid_B))\\nfor layer in model_B_on_A.layers[:-1]:\\n    layer.trainable = True\\noptimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-3\\nmodel_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\\n                     metrics=[\"accuracy\"])\\nhistory = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\\n                           validation_data=(X_valid_B, y_valid_B))\\nSo, what’s the final verdict? Well this model’s test accuracy is 99.25%, which means\\nthat transfer learning reduced the error rate from 2.8% down to almost 0.7%! That’s a\\nfactor of 4!\\n>>> model_B_on_A.evaluate(X_test_B, y_test_B)\\n[0.06887910133600235, 0.9925]\\nAre you convinced? Well you shouldn’t be: I cheated! :) I tried many configurations\\nuntil I found one that demonstrated a strong improvement. If you try to change the\\nclasses or the random seed, you will see that the improvement generally drops, or\\neven vanishes or reverses. What I did is called “torturing the data until it confesses”.\\nWhen a paper just looks too positive, you should be suspicious: perhaps the flashy\\nnew technique does not help much (in fact, it may even degrade performance), but\\nthe authors tried many variants and reported only the best results (which may be due\\nto shear luck), without mentioning how many failures they encountered on the way.\\nMost of the time, this is not malicious at all, but it is part of the reason why so many\\nresults in Science can never be reproduced.\\nSo why did I cheat? Well it turns out that transfer learning does not work very well\\nwith small dense networks: it works best with deep convolutional neural networks, so\\nwe will revisit transfer learning in Chapter 14, using the same techniques (and this\\ntime there will be no cheating, I promise!).\\n342 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 368}, page_content='Unsupervised Pretraining\\nSuppose you want to tackle a complex task for which you don’t have much labeled\\ntraining data, but unfortunately you cannot find a model trained on a similar task.\\nDon’t lose all hope! First, you should of course try to gather more labeled training\\ndata, but if this is too hard or too expensive, you may still be able to perform unsuper‐\\nvised pretraining (see Figure 11-5). It is often rather cheap to gather unlabeled train‐\\ning examples, but quite expensive to label them. If you can gather plenty of unlabeled\\ntraining data, you can try to train the layers one by one, starting with the lowest layer\\nand then going up, using an unsupervised feature detector algorithm such as Restric‐\\nted Boltzmann Machines (RBMs; see ???) or autoencoders (see ???). Each layer is\\ntrained on the output of the previously trained layers (all layers except the one being\\ntrained are frozen). Once all layers have been trained this way, you can add the output\\nlayer for your task, and fine-tune the final network using supervised learning (i.e.,\\nwith the labeled training examples). At this point, you can unfreeze all the pretrained\\nlayers, or just some of the upper ones.\\nFigure 11-5. Unsupervised pretraining\\nThis is a rather long and tedious process, but it often works well; in fact, it is this\\ntechnique that Geoffrey Hinton and his team used in 2006 and which led to the\\nrevival of neural networks and the success of Deep Learning. Until 2010, unsuper‐\\nvised pretraining (typically using RBMs) was the norm for deep nets, and it was only\\nafter the vanishing gradients problem was alleviated that it became much more com‐\\nReusing Pretrained Layers \\n| \\n343'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 369}, page_content='mon to train DNNs purely using supervised learning. However, unsupervised pre‐\\ntraining (today typically using autoencoders rather than RBMs) is still a good option\\nwhen you have a complex task to solve, no similar model you can reuse, and little\\nlabeled training data but plenty of unlabeled training data.\\nPretraining on an Auxiliary Task\\nIf you do not have much labeled training data, one last option is to train a first neural\\nnetwork on an auxiliary task for which you can easily obtain or generate labeled\\ntraining data, then reuse the lower layers of that network for your actual task. The\\nfirst neural network’s lower layers will learn feature detectors that will likely be reusa‐\\nble by the second neural network.\\nFor example, if you want to build a system to recognize faces, you may only have a\\nfew pictures of each individual—clearly not enough to train a good classifier. Gather‐\\ning hundreds of pictures of each person would not be practical. However, you could\\ngather a lot of pictures of random people on the web and train a first neural network\\nto detect whether or not two different pictures feature the same person. Such a net‐\\nwork would learn good feature detectors for faces, so reusing its lower layers would\\nallow you to train a good face classifier using little training data.\\nFor natural language processing (NLP) applications, you can easily download millions\\nof text documents and automatically generate labeled data from it. For example, you\\ncould randomly mask out some words and train a model to predict what the missing\\nwords are (e.g., it should predict that the missing word in the sentence “What ___\\nyou saying?” is probably “are” or “were”). If you can train a model to reach good per‐\\nformance on this task, then it will already know quite a lot about language, and you\\ncan certainly reuse it for your actual task, and fine-tune it on your labeled data (we\\nwill discuss more pretraining tasks in ???).\\nSelf-supervised learning is when you automatically generate the\\nlabels from the data itself, then you train a model on the resulting\\n“labeled” dataset using supervised learning techniques. Since this\\napproach requires no human labeling whatsoever, it is best classi‐\\nfied as a form of unsupervised learning.\\nFaster Optimizers\\nTraining a very large deep neural network can be painfully slow. So far we have seen\\nfour ways to speed up training (and reach a better solution): applying a good initiali‐\\nzation strategy for the connection weights, using a good activation function, using\\nBatch Normalization, and reusing parts of a pretrained network (possibly built on an\\nauxiliary task or using unsupervised learning). Another huge speed boost comes from\\nusing a faster optimizer than the regular Gradient Descent optimizer. In this section\\n344 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 370}, page_content='12 “Some methods of speeding up the convergence of iteration methods,” B. Polyak (1964).\\nwe will present the most popular ones: Momentum optimization, Nesterov Acceler‐\\nated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization.\\nMomentum Optimization\\nImagine a bowling ball rolling down a gentle slope on a smooth surface: it will start\\nout slowly, but it will quickly pick up momentum until it eventually reaches terminal\\nvelocity (if there is some friction or air resistance). This is the very simple idea behind\\nMomentum optimization, proposed by Boris Polyak in 1964.12 In contrast, regular\\nGradient Descent will simply take small regular steps down the slope, so it will take\\nmuch more time to reach the bottom.\\nRecall that Gradient Descent simply updates the weights θ by directly subtracting the\\ngradient of the cost function J(θ) with regards to the weights (∇θJ(θ)) multiplied by\\nthe learning rate η. The equation is: θ ← θ – η∇θJ(θ). It does not care about what the\\nearlier gradients were. If the local gradient is tiny, it goes very slowly.\\nMomentum optimization cares a great deal about what previous gradients were: at\\neach iteration, it subtracts the local gradient from the momentum vector m (multi‐\\nplied by the learning rate η), and it updates the weights by simply adding this\\nmomentum vector (see Equation 11-4). In other words, the gradient is used for accel‐\\neration, not for speed. To simulate some sort of friction mechanism and prevent the\\nmomentum from growing too large, the algorithm introduces a new hyperparameter\\nβ, simply called the momentum, which must be set between 0 (high friction) and 1\\n(no friction). A typical momentum value is 0.9.\\nEquation 11-4. Momentum algorithm\\n1 .\\nm\\nβm −η∇θJ θ\\n2 .\\nθ\\nθ + m\\nYou can easily verify that if the gradient remains constant, the terminal velocity (i.e.,\\nthe maximum size of the weight updates) is equal to that gradient multiplied by the\\nlearning rate η multiplied by \\n1\\n1 −β (ignoring the sign). For example, if β = 0.9, then the\\nterminal velocity is equal to 10 times the gradient times the learning rate, so Momen‐\\ntum optimization ends up going 10 times faster than Gradient Descent! This allows\\nMomentum optimization to escape from plateaus much faster than Gradient Descent.\\nIn particular, we saw in Chapter 4 that when the inputs have very different scales the \\ncost function will look like an elongated bowl (see Figure 4-7). Gradient Descent goes\\ndown the steep slope quite fast, but then it takes a very long time to go down the val‐\\nFaster Optimizers \\n| \\n345'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 371}, page_content='13 “A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence O(1/k2),” Yurii\\nNesterov (1983).\\nley. In contrast, Momentum optimization will roll down the valley faster and faster\\nuntil it reaches the bottom (the optimum). In deep neural networks that don’t use\\nBatch Normalization, the upper layers will often end up having inputs with very dif‐\\nferent scales, so using Momentum optimization helps a lot. It can also help roll past\\nlocal optima.\\nDue to the momentum, the optimizer may overshoot a bit, then\\ncome back, overshoot again, and oscillate like this many times\\nbefore stabilizing at the minimum. This is one of the reasons why it\\nis good to have a bit of friction in the system: it gets rid of these\\noscillations and thus speeds up convergence.\\nImplementing Momentum optimization in Keras is a no-brainer: just use the SGD\\noptimizer and set its momentum hyperparameter, then lie back and profit!\\noptimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\\nThe one drawback of Momentum optimization is that it adds yet another hyperpara‐\\nmeter to tune. However, the momentum value of 0.9 usually works well in practice\\nand almost always goes faster than regular Gradient Descent.\\nNesterov Accelerated Gradient\\nOne small variant to Momentum optimization, proposed by Yurii Nesterov in 1983,13\\nis almost always faster than vanilla Momentum optimization. The idea of Nesterov\\nMomentum optimization, or Nesterov Accelerated Gradient (NAG), is to measure the\\ngradient of the cost function not at the local position but slightly ahead in the direc‐\\ntion of the momentum (see Equation 11-5). The only difference from vanilla\\nMomentum optimization is that the gradient is measured at θ + βm rather than at θ.\\nEquation 11-5. Nesterov Accelerated Gradient algorithm\\n1 .\\nm\\nβm −η∇θJ θ + βm\\n2 .\\nθ\\nθ + m\\nThis small tweak works because in general the momentum vector will be pointing in\\nthe right direction (i.e., toward the optimum), so it will be slightly more accurate to\\nuse the gradient measured a bit farther in that direction rather than using the gradi‐\\nent at the original position, as you can see in Figure 11-6 (where ∇1 represents the\\ngradient of the cost function measured at the starting point θ, and ∇2 represents the\\n346 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 372}, page_content='14 “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,” J. Duchi et al. (2011).\\ngradient at the point located at θ + βm). As you can see, the Nesterov update ends up\\nslightly closer to the optimum. After a while, these small improvements add up and\\nNAG ends up being significantly faster than regular Momentum optimization. More‐\\nover, note that when the momentum pushes the weights across a valley, ∇1 continues\\nto push further across the valley, while ∇2 pushes back toward the bottom of the val‐\\nley. This helps reduce oscillations and thus converges faster.\\nNAG will almost always speed up training compared to regular Momentum optimi‐\\nzation. To use it, simply set nesterov=True when creating the SGD optimizer:\\noptimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\\nFigure 11-6. Regular versus Nesterov Momentum optimization\\nAdaGrad\\nConsider the elongated bowl problem again: Gradient Descent starts by quickly going\\ndown the steepest slope, then slowly goes down the bottom of the valley. It would be\\nnice if the algorithm could detect this early on and correct its direction to point a bit\\nmore toward the global optimum.\\nThe AdaGrad algorithm14 achieves this by scaling down the gradient vector along the\\nsteepest dimensions (see Equation 11-6):\\nEquation 11-6. AdaGrad algorithm\\n1 .\\ns\\ns + ∇θJ θ ⊗∇θJ θ\\n2 .\\nθ\\nθ −η ∇θJ θ ⊘\\ns + �\\nFaster Optimizers \\n| \\n347'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 373}, page_content='The first step accumulates the square of the gradients into the vector s (recall that the\\n⊗ symbol represents the element-wise multiplication). This vectorized form is equiv‐\\nalent to computing si ← si + (∂ J(θ) / ∂ θi)2 for each element si of the vector s; in other\\nwords, each si accumulates the squares of the partial derivative of the cost function\\nwith regards to parameter θi. If the cost function is steep along the ith dimension, then\\nsi will get larger and larger at each iteration.\\nThe second step is almost identical to Gradient Descent, but with one big difference:\\nthe gradient vector is scaled down by a factor of �+ � (the ⊘ symbol represents the\\nelement-wise division, and ϵ is a smoothing term to avoid division by zero, typically\\nset \\nto \\n10–10). \\nThis \\nvectorized \\nform \\nis \\nequivalent \\nto \\ncomputing\\nθi\\nθi −η ∂J θ / ∂θi/ si + � for all parameters θi (simultaneously).\\nIn short, this algorithm decays the learning rate, but it does so faster for steep dimen‐\\nsions than for dimensions with gentler slopes. This is called an adaptive learning rate. \\nIt helps point the resulting updates more directly toward the global optimum (see\\nFigure 11-7). One additional benefit is that it requires much less tuning of the learn‐\\ning rate hyperparameter η.\\nFigure 11-7. AdaGrad versus Gradient Descent\\nAdaGrad often performs well for simple quadratic problems, but unfortunately it\\noften stops too early when training neural networks. The learning rate gets scaled\\ndown so much that the algorithm ends up stopping entirely before reaching the\\nglobal optimum. So even though Keras has an Adagrad optimizer, you should not use\\nit to train deep neural networks (it may be efficient for simpler tasks such as Linear\\nRegression, though). However, understanding Adagrad is helpful to grasp the other\\nadaptive learning rate optimizers.\\n348 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 374}, page_content='15 This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012, and presented by Geoffrey\\nHinton in his Coursera class on neural networks (slides: https://homl.info/57; video: https://homl.info/58).\\nAmusingly, since the authors did not write a paper to describe it, researchers often cite “slide 29 in lecture 6”\\nin their papers.\\n16 “Adam: A Method for Stochastic Optimization,” D. Kingma, J. Ba (2015).\\n17 These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\\nfirst moment, while the variance is often called the second moment, hence the name of the algorithm.\\nRMSProp\\nAlthough AdaGrad slows down a bit too fast and ends up never converging to the\\nglobal optimum, the RMSProp algorithm15 fixes this by accumulating only the gradi‐\\nents from the most recent iterations (as opposed to all the gradients since the begin‐\\nning of training). It does so by using exponential decay in the first step (see Equation\\n11-7).\\nEquation 11-7. RMSProp algorithm\\n1 .\\ns\\nβs + 1 −β ∇θJ θ ⊗∇θJ θ\\n2 .\\nθ\\nθ −η ∇θJ θ ⊘\\ns + �\\nThe decay rate β is typically set to 0.9. Yes, it is once again a new hyperparameter, but\\nthis default value often works well, so you may not need to tune it at all.\\nAs you might expect, Keras has an RMSProp optimizer:\\noptimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\\nExcept on very simple problems, this optimizer almost always performs much better\\nthan AdaGrad. In fact, it was the preferred optimization algorithm of many research‐\\ners until Adam optimization came around.\\nAdam and Nadam Optimization\\nAdam,16 which stands for adaptive moment estimation, combines the ideas of Momen‐\\ntum optimization and RMSProp: just like Momentum optimization it keeps track of\\nan exponentially decaying average of past gradients, and just like RMSProp it keeps\\ntrack of an exponentially decaying average of past squared gradients (see Equation\\n11-8).17\\nFaster Optimizers \\n| \\n349'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 375}, page_content='Equation 11-8. Adam algorithm\\n1 .\\nm\\nβ1m −1 −β1 ∇θJ θ\\n2 .\\ns\\nβ2s + 1 −β2 ∇θJ θ ⊗∇θJ θ\\n3 .\\nm\\nm\\n1 −β1\\nt\\n4 .\\ns\\ns\\n1 −β2\\nt\\n5 .\\nθ\\nθ + η m ⊘\\ns + �\\n• t represents the iteration number (starting at 1).\\nIf you just look at steps 1, 2, and 5, you will notice Adam’s close similarity to both\\nMomentum optimization and RMSProp. The only difference is that step 1 computes\\nan exponentially decaying average rather than an exponentially decaying sum, but\\nthese are actually equivalent except for a constant factor (the decaying average is just\\n1 – β1 times the decaying sum). Steps 3 and 4 are somewhat of a technical detail: since\\nm and s are initialized at 0, they will be biased toward 0 at the beginning of training,\\nso these two steps will help boost m and s at the beginning of training.\\nThe momentum decay hyperparameter β1 is typically initialized to 0.9, while the scal‐\\ning decay hyperparameter β2 is often initialized to 0.999. As earlier, the smoothing\\nterm ϵ is usually initialized to a tiny number such as 10–7. These are the default values\\nfor the Adam class (to be precise, epsilon defaults to None, which tells Keras to use\\nkeras.backend.epsilon(), which defaults to 10–7; you can change it using\\nkeras.backend.set_epsilon()).\\noptimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\\nSince Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it\\nrequires less tuning of the learning rate hyperparameter η. You can often use the\\ndefault value η = 0.001, making Adam even easier to use than Gradient Descent.\\nIf you are starting to feel overwhelmed by all these different techni‐\\nques, and wondering how to choose the right ones for your task,\\ndon’t worry: some practical guidelines are provided at the end of\\nthis chapter.\\nFinally, two variants of Adam are worth mentioning:\\n350 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 376}, page_content='18 “Incorporating Nesterov Momentum into Adam,” Timothy Dozat (2015).\\n19 “The Marginal Value of Adaptive Gradient Methods in Machine Learning,” A. C. Wilson et al. (2017).\\n• Adamax, introduced in the same paper as Adam: notice that in step 2 of Equation\\n11-8, Adam accumulates the squares of the gradients in s (with a greater weight\\nfor more recent weights). In step 5, if we ignore ϵ and steps 3 and 4 (which are\\ntechnical details anyway), Adam just scales down the parameter updates by the\\nsquare root of s. In short, Adam scales down the parameter updates by the ℓ2\\nnorm of the time-decayed gradients (recall that the ℓ2 norm is the square root of\\nthe sum of squares). Adamax just replaces the ℓ2 norm with the ℓ∞ norm (a fancy\\nway of saying the max). Specifically, it replaces step 2 in Equation 11-8 with\\n�\\nmax β2�, ∇θJ θ , it drops step 4, and in step 5 it scales down the gradient\\nupdates by a factor of s, which is just the max of the time-decayed gradients. In\\npractice, this can make Adamax more stable than Adam, but this really depends\\non the dataset, and in general Adam actually performs better. So it’s just one\\nmore optimizer you can try if you experience problems with Adam on some task.\\n• Nadam optimization18 is more important: it is simply Adam optimization plus\\nthe Nesterov trick, so it will often converge slightly faster than Adam. In his\\nreport, Timothy Dozat compares many different optimizers on various tasks, and\\nfinds that Nadam generally outperforms Adam, but is sometimes outperformed\\nby RMSProp.\\nAdaptive optimization methods (including RMSProp, Adam and\\nNadam optimization) are often great, converging fast to a good sol‐\\nution. However, a 2017 paper19 by Ashia C. Wilson et al. showed\\nthat they can lead to solutions that generalize poorly on some data‐\\nsets. So when you are disappointed by your model’s performance,\\ntry using plain Nesterov Accelerated Gradient instead: your dataset\\nmay just be allergic to adaptive gradients. Also check out the latest\\nresearch, it is moving fast (e.g., AdaBound).\\nAll the optimization techniques discussed so far only rely on the first-order partial\\nderivatives (Jacobians). The optimization literature contains amazing algorithms\\nbased on the second-order partial derivatives (the Hessians, which are the partial\\nderivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply\\nto deep neural networks because there are n2 Hessians per output (where n is the\\nnumber of parameters), as opposed to just n Jacobians per output. Since DNNs typi‐\\ncally have tens of thousands of parameters, the second-order optimization algorithms\\nFaster Optimizers \\n| \\n351'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 377}, page_content='20 “Primal-Dual Subgradient Methods for Convex Problems,” Yurii Nesterov (2005).\\n21 “Ad Click Prediction: a View from the Trenches,” H. McMahan et al. (2013).\\noften don’t even fit in memory, and even when they do, computing the Hessians is \\njust too slow.\\nTraining Sparse Models\\nAll the optimization algorithms just presented produce dense models, meaning that\\nmost parameters will be nonzero. If you need a blazingly fast model at runtime, or if\\nyou need it to take up less memory, you may prefer to end up with a sparse model\\ninstead.\\nOne trivial way to achieve this is to train the model as usual, then get rid of the tiny\\nweights (set them to 0). However, this will typically not lead to a very sparse model,\\nand it may degrade the model’s performance.\\nA better option is to apply strong ℓ1 regularization during training, as it pushes the\\noptimizer to zero out as many weights as it can (as discussed in Chapter 4 about Lasso\\nRegression).\\nHowever, in some cases these techniques may remain insufficient. One last option is\\nto apply Dual Averaging, often called Follow The Regularized Leader (FTRL), a techni‐\\nque proposed by Yurii Nesterov.20 When used with ℓ1 regularization, this technique\\noften leads to very sparse models. Keras implements a variant of FTRL called FTRL-\\nProximal21 in the FTRL optimizer.\\nLearning Rate Scheduling\\nFinding a good learning rate can be tricky. If you set it way too high, training may\\nactually diverge (as we discussed in Chapter 4). If you set it too low, training will\\neventually converge to the optimum, but it will take a very long time. If you set it\\nslightly too high, it will make progress very quickly at first, but it will end up dancing\\naround the optimum, never really settling down. If you have a limited computing\\nbudget, you may have to interrupt training before it has converged properly, yielding\\na suboptimal solution (see Figure 11-8).\\n352 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 378}, page_content='Figure 11-8. Learning curves for various learning rates η\\nAs we discussed in Chapter 10, one approach is to start with a large learning rate, and\\ndivide it by 3 until the training algorithm stops diverging. You will not be too far\\nfrom the optimal learning rate, which will learn quickly and converge to good solu‐\\ntion.\\nHowever, you can do better than a constant learning rate: if you start with a high\\nlearning rate and then reduce it once it stops making fast progress, you can reach a\\ngood solution faster than with the optimal constant learning rate. There are many dif‐\\nferent strategies to reduce the learning rate during training. These strategies are called\\nlearning schedules (we briefly introduced this concept in Chapter 4), the most com‐\\nmon of which are:\\nPower scheduling\\nSet the learning rate to a function of the iteration number t: η(t) = η0 / (1 + t/k)c.\\nThe initial learning rate η0, the power c (typically set to 1) and the steps s are\\nhyperparameters. The learning rate drops at each step, and after s steps it is down\\nto η0 / 2. After s more steps, it is down to η0 / 3. Then down to η0 / 4, then η0 / 5,\\nand so on. As you can see, this schedule first drops quickly, then more and more\\nslowly. Of course, this requires tuning η0, s (and possibly c).\\nExponential scheduling\\nSet the learning rate to: η(t) = η0 0.1t/s. The learning rate will gradually drop by a\\nfactor of 10 every s steps. While power scheduling reduces the learning rate more\\nand more slowly, exponential scheduling keeps slashing it by a factor of 10 every\\ns steps.\\nPiecewise constant scheduling\\nUse a constant learning rate for a number of epochs (e.g., η0 = 0.1 for 5 epochs),\\nthen a smaller learning rate for another number of epochs (e.g., η1 = 0.001 for 50\\nepochs), and so on. Although this solution can work very well, it requires fid‐\\nFaster Optimizers \\n| \\n353'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 379}, page_content='22 “An Empirical Study of Learning Rates in Deep Neural Networks for Speech Recognition,” A. Senior et al.\\n(2013).\\ndling around to figure out the right sequence of learning rates, and how long to\\nuse each of them.\\nPerformance scheduling\\nMeasure the validation error every N steps (just like for early stopping) and\\nreduce the learning rate by a factor of λ when the error stops dropping.\\nA 2013 paper22 by Andrew Senior et al. compared the performance of some of the\\nmost popular learning schedules when training deep neural networks for speech rec‐\\nognition using Momentum optimization. The authors concluded that, in this setting,\\nboth performance scheduling and exponential scheduling performed well. They\\nfavored exponential scheduling because it was easy to tune and it converged slightly\\nfaster to the optimal solution (they also mentioned that it was easier to implement\\nthan performance scheduling, but in Keras both options are easy).\\nImplementing power scheduling in Keras is the easiest option: just set the decay\\nhyperparameter when creating an optimizer. The decay is the inverse of s (the num‐\\nber of steps it takes to divide the learning rate by one more unit), and Keras assumes\\nthat c is equal to 1. For example:\\noptimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)\\nExponential scheduling and piecewise scheduling are quite simple too. You first need\\nto define a function that takes the current epoch and returns the learning rate. For\\nexample, let’s implement exponential scheduling:\\ndef exponential_decay_fn(epoch):\\n    return 0.01 * 0.1**(epoch / 20)\\nIf you do not want to hard-code η0 and s, you can create a function that returns a\\nconfigured function:\\ndef exponential_decay(lr0, s):\\n    def exponential_decay_fn(epoch):\\n        return lr0 * 0.1**(epoch / s)\\n    return exponential_decay_fn\\nexponential_decay_fn = exponential_decay(lr0=0.01, s=20)\\nNext, just create a LearningRateScheduler callback, giving it the schedule function,\\nand pass this callback to the fit() method:\\nlr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\\nhistory = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])\\n354 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 380}, page_content='The LearningRateScheduler will update the optimizer’s learning_rate attribute at\\nthe beginning of each epoch. Updating the learning rate just once per epoch is usually\\nenough, but if you want it to be updated more often, for example at every step, you\\nneed to write your own callback (see the notebook for an example). This can make\\nsense if there are many steps per epoch.\\nThe schedule function can optionally take the current learning rate as a second argu‐\\nment. For example, the following schedule function just multiplies the previous\\nlearning rate by 0.1&1/20, which results in the same exponential decay (except the decay\\nnow starts at the beginning of epoch 0 instead of 1). This implementation relies on\\nthe optimizer’s initial learning rate (contrary to the previous implementation), so\\nmake sure to set it appropriately.\\ndef exponential_decay_fn(epoch, lr):\\n    return lr * 0.1**(1 / 20)\\nWhen you save a model, the optimizer and its learning rate get saved along with it.\\nThis means that with this new schedule function, you could just load a trained model\\nand continue training where it left off, no problem. However, things are not so simple\\nif your schedule function uses the epoch argument: indeed, the epoch does not get\\nsaved, and it gets reset to 0 every time you call the fit() method. This could lead to a\\nvery large learning rate when you continue training a model where it left off, which\\nwould likely damage your model’s weights. One solution is to manually set the fit()\\nmethod’s initial_epoch argument so the epoch starts at the right value.\\nFor piecewise constant scheduling, you can use a schedule function like the following\\none (as earlier, you can define a more general function if you want, see the notebook\\nfor an example), then create a LearningRateScheduler callback with this function\\nand pass it to the fit() method, just like we did for exponential scheduling:\\ndef piecewise_constant_fn(epoch):\\n    if epoch < 5:\\n        return 0.01\\n    elif epoch < 15:\\n        return 0.005\\n    else:\\n        return 0.001\\nFor performance scheduling, simply use the ReduceLROnPlateau callback. For exam‐\\nple, if you pass the following callback to the fit() method, it will multiply the learn‐\\ning rate by 0.5 whenever the best validation loss does not improve for 5 consecutive\\nepochs (other options are available, please check the documentation for more\\ndetails):\\nlr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\\nLastly, tf.keras offers an alternative way to implement learning rate scheduling: just\\ndefine the learning rate using one of the schedules available in keras.optimiz\\nFaster Optimizers \\n| \\n355'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 381}, page_content='ers.schedules, then pass this learning rate to any optimizer. This approach updates\\nthe learning rate at each step rather than at each epoch. For example, here is how to\\nimplement the same exponential schedule as earlier:\\ns = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\\nlearning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\\noptimizer = keras.optimizers.SGD(learning_rate)\\nThis is nice and simple, plus when you save the model, the learning rate and its\\nschedule (including its state) get saved as well. However, this approach is not part of\\nthe Keras API, it is specific to tf.keras.\\nTo sum up, exponential decay or performance scheduling can considerably speed up\\nconvergence, so give them a try!\\nAvoiding Overfitting Through Regularization\\nWith four parameters I can fit an elephant and with five I can make him wiggle his\\ntrunk.\\n—John von Neumann, cited by Enrico Fermi in Nature 427\\nWith thousands of parameters you can fit the whole zoo. Deep neural networks typi‐\\ncally have tens of thousands of parameters, sometimes even millions. With so many\\nparameters, the network has an incredible amount of freedom and can fit a huge vari‐\\nety of complex datasets. But this great flexibility also means that it is prone to overfit‐\\nting the training set. We need regularization.\\nWe already implemented one of the best regularization techniques in Chapter 10:\\nearly stopping. Moreover, even though Batch Normalization was designed to solve\\nthe vanishing/exploding gradients problems, is also acts like a pretty good regularizer.\\nIn this section we will present other popular regularization techniques for neural net‐\\nworks: ℓ1 and ℓ2 regularization, dropout and max-norm regularization.\\nℓ1 and ℓ2 Regularization\\nJust like you did in Chapter 4 for simple linear models, you can use ℓ1 and ℓ2 regulari‐\\nzation to constrain a neural network’s connection weights (but typically not its bia‐\\nses). Here is how to apply ℓ2 regularization to a Keras layer’s connection weights,\\nusing a regularization factor of 0.01:\\nlayer = keras.layers.Dense(100, activation=\"elu\",\\n                           kernel_initializer=\"he_normal\",\\n                           kernel_regularizer=keras.regularizers.l2(0.01))\\nThe l2() function returns a regularizer that will be called to compute the regulariza‐\\ntion loss, at each step during training. This regularization loss is then added to the\\nfinal loss. As you might expect, you can just use keras.regularizers.l1() if you\\n356 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 382}, page_content='23 “Improving neural networks by preventing co-adaptation of feature detectors,” G. Hinton et al. (2012).\\n24 “Dropout: A Simple Way to Prevent Neural Networks from Overfitting,” N. Srivastava et al. (2014).\\nwant ℓ1 regularization, and if you want both ℓ1 and ℓ2 regularization, use keras.regu\\nlarizers.l1_l2() (specifying both regularization factors).\\nSince you will typically want to apply the same regularizer to all layers in your net‐\\nwork, as well as the same activation function and the same initialization strategy in all\\nhidden layers, you may find yourself repeating the same arguments over and over.\\nThis makes it ugly and error-prone. To avoid this, you can try refactoring your code\\nto use loops. Another option is to use Python’s functools.partial() function: it lets\\nyou create a thin wrapper for any callable, with some default argument values. For\\nexample:\\nfrom functools import partial\\nRegularizedDense = partial(keras.layers.Dense,\\n                           activation=\"elu\",\\n                           kernel_initializer=\"he_normal\",\\n                           kernel_regularizer=keras.regularizers.l2(0.01))\\nmodel = keras.models.Sequential([\\n    keras.layers.Flatten(input_shape=[28, 28]),\\n    RegularizedDense(300),\\n    RegularizedDense(100),\\n    RegularizedDense(10, activation=\"softmax\",\\n                     kernel_initializer=\"glorot_uniform\")\\n])\\nDropout\\nDropout is one of the most popular regularization techniques for deep neural net‐\\nworks. It was proposed23 by Geoffrey Hinton in 2012 and further detailed in a paper24\\nby Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-\\nthe-art neural networks got a 1–2% accuracy boost simply by adding dropout. This\\nmay not sound like a lot, but when a model already has 95% accuracy, getting a 2%\\naccuracy boost means dropping the error rate by almost 40% (going from 5% error to\\nroughly 3%).\\nIt is a fairly simple algorithm: at every training step, every neuron (including the\\ninput neurons, but always excluding the output neurons) has a probability p of being\\ntemporarily “dropped out,” meaning it will be entirely ignored during this training\\nstep, but it may be active during the next step (see Figure 11-9). The hyperparameter\\np is called the dropout rate, and it is typically set to 50%. After training, neurons don’t\\nget dropped anymore. And that’s all (except for a technical detail we will discuss\\nmomentarily).\\nAvoiding Overfitting Through Regularization \\n| \\n357'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 383}, page_content='Figure 11-9. Dropout regularization\\nIt is quite surprising at first that this rather brutal technique works at all. Would a\\ncompany perform better if its employees were told to toss a coin every morning to\\ndecide whether or not to go to work? Well, who knows; perhaps it would! The com‐\\npany would obviously be forced to adapt its organization; it could not rely on any sin‐\\ngle person to fill in the coffee machine or perform any other critical tasks, so this\\nexpertise would have to be spread across several people. Employees would have to\\nlearn to cooperate with many of their coworkers, not just a handful of them. The\\ncompany would become much more resilient. If one person quit, it wouldn’t make\\nmuch of a difference. It’s unclear whether this idea would actually work for compa‐\\nnies, but it certainly does for neural networks. Neurons trained with dropout cannot\\nco-adapt with their neighboring neurons; they have to be as useful as possible on\\ntheir own. They also cannot rely excessively on just a few input neurons; they must\\npay attention to each of their input neurons. They end up being less sensitive to slight\\nchanges in the inputs. In the end you get a more robust network that generalizes bet‐\\nter.\\nAnother way to understand the power of dropout is to realize that a unique neural\\nnetwork is generated at each training step. Since each neuron can be either present or\\nabsent, there is a total of 2N possible networks (where N is the total number of drop‐\\npable neurons). This is such a huge number that it is virtually impossible for the same\\nneural network to be sampled twice. Once you have run a 10,000 training steps, you\\nhave essentially trained 10,000 different neural networks (each with just one training\\ninstance). These neural networks are obviously not independent since they share\\nmany of their weights, but they are nevertheless all different. The resulting neural\\nnetwork can be seen as an averaging ensemble of all these smaller neural networks.\\nThere is one small but important technical detail. Suppose p = 50%, in which case\\nduring testing a neuron will be connected to twice as many input neurons as it was\\n(on average) during training. To compensate for this fact, we need to multiply each\\n358 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 384}, page_content='25 This is specific to tf.keras, so you may prefer to use keras.backend.set_learning_phase(1) before calling\\nthe fit() method (and set it back to 0 right after).\\nneuron’s input connection weights by 0.5 after training. If we don’t, each neuron will\\nget a total input signal roughly twice as large as what the network was trained on, and\\nit is unlikely to perform well. More generally, we need to multiply each input connec‐\\ntion weight by the keep probability (1 – p) after training. Alternatively, we can divide\\neach neuron’s output by the keep probability during training (these alternatives are\\nnot perfectly equivalent, but they work equally well).\\nTo implement dropout using Keras, you can use the keras.layers.Dropout layer.\\nDuring training, it randomly drops some inputs (setting them to 0) and divides the\\nremaining inputs by the keep probability. After training, it does nothing at all, it just\\npasses the inputs to the next layer. For example, the following code applies dropout\\nregularization before every Dense layer, using a dropout rate of 0.2:\\nmodel = keras.models.Sequential([\\n    keras.layers.Flatten(input_shape=[28, 28]),\\n    keras.layers.Dropout(rate=0.2),\\n    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\\n    keras.layers.Dropout(rate=0.2),\\n    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\\n    keras.layers.Dropout(rate=0.2),\\n    keras.layers.Dense(10, activation=\"softmax\")\\n])\\nSince dropout is only active during training, the training loss is\\npenalized compared to the validation loss, so comparing the two\\ncan be misleading. In particular, a model may be overfitting the\\ntraining set and yet have similar training and validation losses. So\\nmake sure to evaluate the training loss without dropout (e.g., after\\ntraining). Alternatively, you can call the fit() method inside a\\nwith keras.backend.learning_phase_scope(1) block: this will\\nforce dropout to be active during both training and validation.25\\nIf you observe that the model is overfitting, you can increase the dropout rate. Con‐\\nversely, you should try decreasing the dropout rate if the model underfits the training\\nset. It can also help to increase the dropout rate for large layers, and reduce it for\\nsmall ones. Moreover, many state-of-the-art architectures only use dropout after the\\nlast hidden layer, so you may want to try this if full dropout is too strong.\\nDropout does tend to significantly slow down convergence, but it usually results in a\\nmuch better model when tuned properly. So, it is generally well worth the extra time\\nand effort.\\nAvoiding Overfitting Through Regularization \\n| \\n359'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 385}, page_content='26 “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,” Y. Gal and Z.\\nGhahramani (2016).\\n27 Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian\\ninference in a specific type of probabilistic model called a deep Gaussian Process.\\nIf you want to regularize a self-normalizing network based on the\\nSELU activation function (as discussed earlier), you should use\\nAlphaDropout: this is a variant of dropout that preserves the mean\\nand standard deviation of its inputs (it was introduced in the same\\npaper as SELU, as regular dropout would break self-normalization).\\nMonte-Carlo (MC) Dropout\\nIn 2016, a paper26 by Yarin Gal and Zoubin Ghahramani added more good reasons to\\nuse dropout:\\n• First, the paper establishes a profound connection between dropout networks\\n(i.e., neural networks containing a dropout layer before every weight layer) and\\napproximate Bayesian inference27, giving dropout a solid mathematical justifica‐\\ntion.\\n• Second, they introduce a powerful technique called MC Dropout, which can\\nboost the performance of any trained dropout model, without having to retrain it\\nor even modify it at all!\\n• Moreover, MC Dropout also provides a much better measure of the model’s\\nuncertainty.\\n• Finally, it is also amazingly simple to implement. If this all sounds like a “one\\nweird trick” advertisement, then take a look at the following code. It is the full\\nimplementation of MC Dropout, boosting the dropout model we trained earlier,\\nwithout retraining it:\\nwith keras.backend.learning_phase_scope(1): # force training mode = dropout on\\n    y_probas = np.stack([model.predict(X_test_scaled)\\n                         for sample in range(100)])\\ny_proba = y_probas.mean(axis=0)\\nWe first force training mode on, using a learning_phase_scope(1) context. This\\nturns dropout on within the with block. Then we make 100 predictions over the test\\nset, and we stack them. Since dropout is on, all predictions will be different. Recall\\nthat predict() returns a matrix with one row per instance, and one column per class.\\nSince there are 10,000 instances in the test set, and 10 classes, this is a matrix of shape\\n[10000, 10]. We stack 100 such matrices, so y_probas is an array of shape [100, 10000,\\n10]. Once we average over the first dimension (axis=0), we get y_proba, an array of\\nshape [10000, 10], like we would get with a single prediction. That’s all! Averaging\\n360 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 386}, page_content='over multiple predictions with dropout on gives us a Monte Carlo estimate that is\\ngenerally more reliable than the result of a single prediction with dropout off. For\\nexample, let’s look at the model’s prediction for the first instance in the test set, with\\ndropout off:\\n>>> np.round(model.predict(X_test_scaled[:1]), 2)\\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\\n      dtype=float32)\\nThe model seems almost certain that this image belongs to class 9 (ankle boot).\\nShould you trust it? Is there really so little room for doubt? Compare this with the\\npredictions made when dropout is activated:\\n>>> np.round(y_probas[:, :1], 2)\\narray([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.17, 0.  , 0.68]],\\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.2 , 0.  , 0.64]],\\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.01, 0.  , 0.97]],\\n       [...]\\nThis tells a very different story: apparently, when we activate dropout, the model is\\nnot sure anymore. It still seems to prefer class 9, but sometimes it hesitates with\\nclasses 5 (sandal) and 7 (sneaker), which makes sense given they’re all footwear. Once\\nwe average over the first dimension, we get the following MC dropout predictions:\\n>>> np.round(y_proba[:1], 2)\\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.22, 0.  , 0.16, 0.  , 0.62]],\\n      dtype=float32)\\nThe model still thinks this image belongs to class 9, but only with a 62% confidence,\\nwhich seems much more reasonable than 99%. Plus it’s useful to know exactly which\\nother classes it thinks are likely. And you can also take a look at the standard devia‐\\ntion of the probability estimates:\\n>>> y_std = y_probas.std(axis=0)\\n>>> np.round(y_std[:1], 2)\\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.21, 0.02, 0.32]],\\n      dtype=float32)\\nApparently there’s quite a lot of variance in the probability estimates: if you were\\nbuilding a risk-sensitive system (e.g., a medical or financial system), you should prob‐\\nably treat such an uncertain prediction with extreme caution. You definitely would\\nnot treat it like a 99% confident prediction. Moreover, the model’s accuracy got a\\nsmall boost from 86.8 to 86.9:\\n>>> accuracy = np.sum(y_pred == y_test) / len(y_test)\\n>>> accuracy\\n0.8694\\nAvoiding Overfitting Through Regularization \\n| \\n361'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 387}, page_content='The number of Monte Carlo samples you use (100 in this example)\\nis a hyperparameter you can tweak. The higher it is, the more accu‐\\nrate the predictions and their uncertainty estimates will be. How‐\\never, it you double it, inference time will also be doubled.\\nMoreover, above a certain number of samples, you will notice little\\nimprovement. So your job is to find the right tradeoff between\\nlatency and accuracy, depending on your application.\\nIf your model contains other layers that behave in a special way during training (such\\nas Batch Normalization layers), then you should not force training mode like we just\\ndid. Instead, you should replace the Dropout layers with the following MCDropout\\nclass:\\nclass MCDropout(keras.layers.Dropout):\\n    def call(self, inputs):\\n        return super().call(inputs, training=True)\\nWe just sublass the Dropout layer and override the call() method to force its train\\ning argument to True (see Chapter 12). Similarly, you could define an MCAlphaDrop\\nout class by subclassing AlphaDropout instead. If you are creating a model from\\nscratch, it’s just a matter of using MCDropout rather than Dropout. But if you have a\\nmodel that was already trained using Dropout, you need to create a new model, iden‐\\ntical to the existing model except replacing the Dropout layers with MCDropout, then\\ncopy the existing model’s weights to your new model.\\nIn short, MC Dropout is a fantastic technique that boosts dropout models and pro‐\\nvides better uncertainty estimates. And of course, since it is just regular dropout dur‐\\ning training, it also acts like a regularizer.\\nMax-Norm Regularization\\nAnother regularization technique that is quite popular for neural networks is called\\nmax-norm regularization: for each neuron, it constrains the weights w of the incom‐\\ning connections such that ∥ *w* ∥2 ≤ _r_, where r is the max-norm hyperparameter\\nand ∥ · ∥2 is the ℓ2 norm.\\nMax-norm regularization does not add a regularization loss term to the overall loss\\nfunction. Instead, it is typically implemented by computing ∥w∥2 after each training\\nstep and clipping w if needed (w\\nw\\nr\\n∥w ∥2).\\nReducing r increases the amount of regularization and helps reduce overfitting. Max-\\nnorm regularization can also help alleviate the vanishing/exploding gradients prob‐\\nlems (if you are not using Batch Normalization).\\n362 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 388}, page_content='To implement max-norm regularization in Keras, just set every hidden layer’s ker\\nnel_constraint argument to a max_norm() constraint, with the appropriate max\\nvalue, for example:\\nkeras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\\n                   kernel_constraint=keras.constraints.max_norm(1.))\\nAfter each training iteration, the model’s fit() method will call the object returned\\nby max_norm(), passing it the layer’s weights and getting clipped weights in return,\\nwhich then replace the layer’s weights. As we will see in Chapter 12, you can define\\nyour own custom constraint function if you ever need to, and use it as the ker\\nnel_constraint. You can also constrain the bias terms by setting the bias_con\\nstraint argument.\\nThe max_norm() function has an axis argument that defaults to 0. A Dense layer usu‐\\nally has weights of shape [number of inputs, number of neurons], so using axis=0\\nmeans that the max norm constraint will apply independently to each neuron’s weight\\nvector. If you want to use max-norm with convolutional layers (see Chapter 14),\\nmake sure to set the max_norm() constraint’s axis argument appropriately (usually\\naxis=[0, 1, 2]).\\nSummary and Practical Guidelines\\nIn this chapter, we have covered a wide range of techniques and you may be wonder‐\\ning which ones you should use. The configuration in Table 11-2 will work fine in\\nmost cases, without requiring much hyperparameter tuning.\\nTable 11-2. Default DNN configuration\\nHyperparameter\\nDefault value\\nKernel initializer:\\nLeCun initialization\\nActivation function:\\nSELU\\nNormalization:\\nNone (self-normalization)\\nRegularization:\\nEarly stopping\\nOptimizer:\\nNadam\\nLearning rate schedule: Performance scheduling\\nDon’t forget to standardize the input features! Of course, you should also try to reuse\\nparts of a pretrained neural network if you can find one that solves a similar problem,\\nor use unsupervised pretraining if you have a lot of unlabeled data, or pretraining on\\nan auxiliary task if you have a lot of labeled data for a similar task.\\nThe default configuration in Table 11-2 may need to be tweaked:\\nSummary and Practical Guidelines \\n| \\n363'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 389}, page_content='• If your model self-normalizes:\\n— If it overfits the training set, then you should add alpha dropout (and always\\nuse early stopping as well). Do not use other regularization methods, or else\\nthey would break self-normalization.\\n• If your model cannot self-normalize (e.g., it is a recurrent net or it contains skip\\nconnections):\\n— You can try using ELU (or another activation function) instead of SELU, it\\nmay perform better. Make sure to change the initialization method accord‐\\ningly (e.g., He init for ELU or ReLU).\\n— If it is a deep network, you should use Batch Normalization after every hidden\\nlayer. If it overfits the training set, you can also try using max-norm or ℓ2 reg‐\\nularization.\\n• If you need a sparse model, you can use ℓ1 regularization (and optionally zero out\\nthe tiny weights after training). If you need an even sparser model, you can try\\nusing FTRL instead of Nadam optimization, along with ℓ1 regularization. In any\\ncase, this will break self-normalization, so you will need to switch to BN if your\\nmodel is deep.\\n• If you need a low-latency model (one that performs lightning-fast predictions),\\nyou may need to use less layers, avoid Batch Normalization, and possibly replace\\nthe SELU activation function with the leaky ReLU. Having a sparse model will\\nalso help. You may also want to reduce the float precision from 32-bits to 16-bit\\n(or even 8-bits) (see ???).\\n• If you are building a risk-sensitive application, or inference latency is not very\\nimportant in your application, you can use MC Dropout to boost performance\\nand get more reliable probability estimates, along with uncertainty estimates.\\nWith these guidelines, you are now ready to train very deep nets! I hope you are now\\nconvinced that you can go a very long way using just Keras. However, there may\\ncome a time when you need to have even more control, for example to write a custom\\nloss function or to tweak the training algorithm. For such cases, you will need to use\\nTensorFlow’s lower-level API, as we will see in the next chapter.\\nExercises\\n1. Is it okay to initialize all the weights to the same value as long as that value is\\nselected randomly using He initialization?\\n2. Is it okay to initialize the bias terms to 0?\\n3. Name three advantages of the SELU activation function over ReLU.\\n364 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 390}, page_content='4. In which cases would you want to use each of the following activation functions:\\nSELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\\n5. What may happen if you set the momentum hyperparameter too close to 1 (e.g.,\\n0.99999) when using an SGD optimizer?\\n6. Name three ways you can produce a sparse model.\\n7. Does dropout slow down training? Does it slow down inference (i.e., making\\npredictions on new instances)? What are about MC dropout?\\n8. Deep Learning.\\na. Build a DNN with five hidden layers of 100 neurons each, He initialization,\\nand the ELU activation function.\\nb. Using Adam optimization and early stopping, try training it on MNIST but\\nonly on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the\\nnext exercise. You will need a softmax output layer with five neurons, and as\\nalways make sure to save checkpoints at regular intervals and save the final\\nmodel so you can reuse it later.\\nc. Tune the hyperparameters using cross-validation and see what precision you\\ncan achieve.\\nd. Now try adding Batch Normalization and compare the learning curves: is it\\nconverging faster than before? Does it produce a better model?\\ne. Is the model overfitting the training set? Try adding dropout to every layer\\nand try again. Does it help?\\n9. Transfer learning.\\na. Create a new DNN that reuses all the pretrained hidden layers of the previous\\nmodel, freezes them, and replaces the softmax output layer with a new one.\\nb. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time\\nhow long it takes. Despite this small number of examples, can you achieve\\nhigh precision?\\nc. Try caching the frozen layers, and train the model again: how much faster is it\\nnow?\\nd. Try again reusing just four hidden layers instead of five. Can you achieve a\\nhigher precision?\\ne. Now unfreeze the top two hidden layers and continue training: can you get\\nthe model to perform even better?\\n10. Pretraining on an auxiliary task.\\na. In this exercise you will build a DNN that compares two MNIST digit images\\nand predicts whether they represent the same digit or not. Then you will reuse\\nthe lower layers of this network to train an MNIST classifier using very little\\nExercises \\n| \\n365'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 391}, page_content='training data. Start by building two DNNs (let’s call them DNN A and B), both\\nsimilar to the one you built earlier but without the output layer: each DNN\\nshould have five hidden layers of 100 neurons each, He initialization, and ELU\\nactivation. Next, add one more hidden layer with 10 units on top of both\\nDNNs. To do this, you should use a keras.layers.Concatenate layer to con‐\\ncatenate the outputs of both DNNs for each instance, then feed the result to\\nthe hidden layer. Finally, add an output layer with a single neuron using the\\nlogistic activation function.\\nb. Split the MNIST training set in two sets: split #1 should containing 55,000\\nimages, and split #2 should contain contain 5,000 images. Create a function\\nthat generates a training batch where each instance is a pair of MNIST images\\npicked from split #1. Half of the training instances should be pairs of images\\nthat belong to the same class, while the other half should be images from dif‐\\nferent classes. For each pair, the training label should be 0 if the images are\\nfrom the same class, or 1 if they are from different classes.\\nc. Train the DNN on this training set. For each image pair, you can simultane‐\\nously feed the first image to DNN A and the second image to DNN B. The\\nwhole network will gradually learn to tell whether two images belong to the\\nsame class or not.\\nd. Now create a new DNN by reusing and freezing the hidden layers of DNN A\\nand adding a softmax output layer on top with 10 neurons. Train this network\\non split #2 and see if you can achieve high performance despite having only\\n500 images per class.\\nSolutions to these exercises are available in ???.\\n366 \\n| \\nChapter 11: Training Deep Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 392}, page_content='CHAPTER 12\\nCustom Models and Training with\\nTensorFlow\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 12 in the final\\nrelease of the book.\\nSo far we have used only TensorFlow’s high level API, tf.keras, but it already got us\\npretty far: we built various neural network architectures, including regression and\\nclassification nets, wide & deep nets and self-normalizing nets, using all sorts of tech‐\\nniques, such as Batch Normalization, dropout, learning rate schedules, and more. In\\nfact, 95% of the use cases you will encounter will not require anything else than\\ntf.keras (and tf.data, see Chapter 13). But now it’s time to dive deeper into TensorFlow\\nand take a look at its lower-level Python API. This will be useful when you need extra\\ncontrol, to write custom loss functions, custom metrics, layers, models, initializers,\\nregularizers, weight constraints and more. You may even need to fully control the\\ntraining loop itself, for example to apply special transformations or constraints to the\\ngradients (beyond just clipping them), or to use multiple optimizers for different\\nparts of the network. We will cover all these cases in this chapter, then we will also\\nlook at how you can boost your custom models and training algorithms using Ten‐\\nsorFlow’s automatic graph generation feature. But first, let’s take a quick tour of Ten‐\\nsorFlow.\\n367'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 393}, page_content='1 TensorFlow also includes another Deep Learning API called the Estimators API, but it is now recommended\\nto use tf.keras instead.\\nTensorFlow 2.0 was released in March 2019, making TensorFlow\\nmuch easier to use. The first edition of this book used TF 1, while\\nthis edition uses TF 2.\\nA Quick Tour of TensorFlow\\nAs you know, TensorFlow is a powerful library for numerical computation, particu‐\\nlarly well suited and fine-tuned for large-scale Machine Learning (but you could use\\nit for anything else that requires heavy computations). It was developed by the Google\\nBrain team and it powers many of Google’s large-scale services, such as Google Cloud\\nSpeech, Google Photos, and Google Search. It was open sourced in November 2015,\\nand it is now the most popular deep learning library (in terms of citations in papers,\\nadoption in companies, stars on github, etc.): countless projects use TensorFlow for\\nall sorts of Machine Learning tasks, such as image classification, natural language\\nprocessing (NLP), recommender systems, time series forecasting, and much more.\\nSo what does TensorFlow actually offer? Here’s a summary:\\n• Its core is very similar to NumPy, but with GPU support.\\n• It also supports distributed computing (across multiple devices and servers).\\n• It includes a kind of just-in-time (JIT) compiler that allows it to optimize compu‐\\ntations for speed and memory usage: it works by extracting the computation\\ngraph from a Python function, then optimizing it (e.g., by pruning unused nodes)\\nand finally running it efficiently (e.g., by automatically running independent\\noperations in parallel).\\n• Computation graphs can be exported to a portable format, so you can train a\\nTensorFlow model in one environment (e.g., using Python on Linux), and run it\\nin another (e.g., using Java on an Android device).\\n• It implements autodiff (see Chapter 10 and ???), and provides some excellent\\noptimizers, such as RMSProp, Nadam and FTRL (see Chapter 11), so you can\\neasily minimize all sorts of loss functions.\\n• TensorFlow offers many more features, built on top of these core features: the\\nmost important is of course tf.keras1, but it also has data loading & preprocessing\\nops (tf.data, tf.io, etc.), image processing ops (tf.image), signal processing ops\\n(tf.signal), and more (see Figure 12-1 for an overview of TensorFlow’s Python\\nAPI).\\n368 \\n| \\nChapter 12: Custom Models and Training with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 394}, page_content='2 If you ever need to (but you probably won’t), you can write your own operations using the C++ API.\\n3 If you are a researcher, you may be eligible to use these TPUs for free, see https://tensorflow.org/tfrc/ for more\\ndetails.\\nFigure 12-1. TensorFlow’s Python API\\nWe will cover many of the packages and functions of the Tensor‐\\nFlow API, but it’s impossible to cover them all so you should really\\ntake some time to browse through the API: you will find that it is\\nquite rich and well documented.\\nAt the lowest level, each TensorFlow operation is implemented using highly efficient\\nC++ code2. Many operations (or ops for short) have multiple implementations, called\\nkernels: each kernel is dedicated to a specific device type, such as CPUs, GPUs, or\\neven TPUs (Tensor Processing Units). As you may know, GPUs can dramatically speed\\nup computations by splitting computations into many smaller chunks and running\\nthem in parallel across many GPU threads. TPUs are even faster. You can purchase\\nyour own GPU devices (for now, TensorFlow only supports Nvidia cards with CUDA\\nCompute Capability 3.5+), but TPUs are only available on Google Cloud Machine\\nLearning Engine (see ???).3\\nTensorFlow’s architecture is shown in Figure 12-2: most of the time your code will\\nuse the high level APIs (especially tf.keras and tf.data), but when you need more flexi‐\\nbility you will use the lower level Python API, handling tensors directly. Note that\\nAPIs for other languages are also available. In any case, TensorFlow’s execution\\nA Quick Tour of TensorFlow \\n| \\n369'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 395}, page_content='engine will take care of running the operations efficiently, even across multiple devi‐\\nces and machines if you tell it to.\\nFigure 12-2. TensorFlow’s architecture\\nTensorFlow runs not only on Windows, Linux, and MacOS, but also on mobile devi‐\\nces (using TensorFlow Lite), including both iOS and Android (see ???). If you do not\\nwant to use the Python API, there are also C++, Java, Go and Swift APIs. There is\\neven a Javascript implementation called TensorFlow.js that makes it possible to run\\nyour models directly in your browser.\\nThere’s more to TensorFlow than just the library. TensorFlow is at the center of an\\nextensive ecosystem of libraries. First, there’s TensorBoard for visualization (see\\nChapter 10). Next, there’s TensorFlow Extended (TFX), which is a set of libraries built\\nby Google to productionize TensorFlow projects: it includes tools for data validation,\\npreprocessing, model analysis and serving (with TF Serving, see ???). Google also\\nlaunched TensorFlow Hub, a way to easily download and reuse pretrained neural net‐\\nworks. You can also get many neural network architectures, some of them pretrained,\\nin TensorFlow’s model garden. Check out the TensorFlow Resources, or https://\\ngithub.com/jtoy/awesome-tensorflow for more TensorFlow-based projects. You will\\nfind hundreds of TensorFlow projects on GitHub, so it is often easy to find existing\\ncode for whatever you are trying to do.\\nMore and more ML papers are released along with their implemen‐\\ntation, and sometimes even with pretrained models. Check out\\nhttps://paperswithcode.com/ to easily find them.\\n370 \\n| \\nChapter 12: Custom Models and Training with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 396}, page_content='Last but not least, TensorFlow has a dedicated team of passionate and helpful devel‐\\nopers, and a large community contributing to improving it. To ask technical ques‐\\ntions, you should use http://stackoverflow.com/ and tag your question with tensorflow\\nand python. You can file bugs and feature requests through GitHub. For general dis‐\\ncussions, join the Google group.\\nOkay, it’s time to start coding!\\nUsing TensorFlow like NumPy\\nTensorFlow’s API revolves around tensors, hence the name Tensor-Flow. A tensor is\\nusually a multidimensional array (exactly like a NumPy ndarray), but it can also hold\\na scalar (a simple value, such as 42). These tensors will be important when we create\\ncustom cost functions, custom metrics, custom layers and more, so let’s see how to\\ncreate and manipulate them.\\nTensors and Operations\\nYou can easily create a tensor, using tf.constant(). For example, here is a tensor\\nrepresenting a matrix with two rows and three columns of floats:\\n>>> tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix\\n<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\n>>> tf.constant(42) # scalar\\n<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\\nJust like an ndarray, a tf.Tensor has a shape and a data type (dtype):\\n>>> t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\\n>>> t.shape\\nTensorShape([2, 3])\\n>>> t.dtype\\ntf.float32\\nIndexing works much like in NumPy:\\n>>> t[:, 1:]\\n<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\\narray([[2., 3.],\\n       [5., 6.]], dtype=float32)>\\n>>> t[..., 1, tf.newaxis]\\n<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\\narray([[2.],\\n       [5.]], dtype=float32)>\\nMost importantly, all sorts of tensor operations are available:\\n>>> t + 10\\n<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\\nUsing TensorFlow like NumPy \\n| \\n371'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 397}, page_content='array([[11., 12., 13.],\\n       [14., 15., 16.]], dtype=float32)>\\n>>> tf.square(t)\\n<tf.Tensor: id=20, shape=(2, 3), dtype=float32, numpy=\\narray([[ 1.,  4.,  9.],\\n       [16., 25., 36.]], dtype=float32)>\\n>>> t @ tf.transpose(t)\\n<tf.Tensor: id=24, shape=(2, 2), dtype=float32, numpy=\\narray([[14., 32.],\\n       [32., 77.]], dtype=float32)>\\nNote that writing t + 10 is equivalent to calling tf.add(t, 10) (indeed, Python calls\\nthe magic method t.__add__(10), which just calls tf.add(t, 10)). Other operators\\n(like -, *, etc.) are also supported. The @ operator was added in Python 3.5, for matrix\\nmultiplication: it is equivalent to calling the tf.matmul() function.\\nYou will find all the basic math operations you need (e.g., tf.add(), tf.multiply(),\\ntf.square(), tf.exp(), tf.sqrt()…), and more generally most operations that you\\ncan find in NumPy (e.g., tf.reshape(), tf.squeeze(), tf.tile()), but sometimes\\nwith a different name (e.g., tf.reduce_mean(), tf.reduce_sum(), tf.reduce_max(),\\ntf.math.log() are the equivalent of np.mean(), np.sum(), np.max() and np.log()).\\nWhen the name differs, there is often a good reason for it: for example, in Tensor‐\\nFlow you must write tf.transpose(t), you cannot just write t.T like in NumPy. The\\nreason is that it does not do exactly the same thing: in TensorFlow, a new tensor is\\ncreated with its own copy of the transposed data, while in NumPy, t.T is just a trans‐\\nposed view on the same data. Similarly, the tf.reduce_sum() operation is named this\\nway because its GPU kernel (i.e., GPU implementation) uses a reduce algorithm that\\ndoes not guarantee the order in which the elements are added: because 32-bit floats\\nhave limited precision, this means that the result may change ever so slightly every\\ntime you call this operation. The same is true of tf.reduce_mean() (but of course\\ntf.reduce_max() is deterministic).\\n372 \\n| \\nChapter 12: Custom Models and Training with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 398}, page_content='4 A notable exception is tf.math.log() which is commonly used but there is no tf.log() alias (as it might be\\nconfused with logging).\\nMany functions and classes have aliases. For example, tf.add()\\nand tf.math.add() are the same function. This allows TensorFlow\\nto have concise names for the most common operations4, while\\npreserving well organized packages.\\nKeras’ Low-Level API\\nThe Keras API actually has its own low-level API, located in keras.backend. It\\nincludes functions like square(), exp(), sqrt() and so on. In tf.keras, these func‐\\ntions generally just call the corresponding TensorFlow operations. If you want to\\nwrite code that will be portable to other Keras implementations, you should use these\\nKeras functions. However, they only cover a subset of all functions available in Ten‐\\nsorFlow, so in this book we will use the TensorFlow operations directly. Here is as\\nsimple example using keras.backend, which is commonly named K for short:\\n>>> from tensorflow import keras\\n>>> K = keras.backend\\n>>> K.square(K.transpose(t)) + 10\\n<tf.Tensor: id=39, shape=(3, 2), dtype=float32, numpy=\\narray([[11., 26.],\\n       [14., 35.],\\n       [19., 46.]], dtype=float32)>\\nTensors and NumPy\\nTensors play nice with NumPy: you can create a tensor from a NumPy array, and vice\\nversa, and you can even apply TensorFlow operations to NumPy arrays and NumPy\\noperations to tensors:\\n>>> a = np.array([2., 4., 5.])\\n>>> tf.constant(a)\\n<tf.Tensor: id=111, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>\\n>>> t.numpy() # or np.array(t)\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)\\n>>> tf.square(a)\\n<tf.Tensor: id=116, shape=(3,), dtype=float64, numpy=array([4., 16., 25.])>\\n>>> np.square(t)\\narray([[ 1.,  4.,  9.],\\n       [16., 25., 36.]], dtype=float32)\\nUsing TensorFlow like NumPy \\n| \\n373'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 399}, page_content=\"Notice that NumPy uses 64-bit precision by default, while Tensor‐\\nFlow uses 32-bit. This is because 32-bit precision is generally more\\nthan enough for neural networks, plus it runs faster and uses less\\nRAM. So when you create a tensor from a NumPy array, make sure\\nto set dtype=tf.float32.\\nType Conversions\\nType conversions can significantly hurt performance, and they can easily go unno‐\\nticed when they are done automatically. To avoid this, TensorFlow does not perform\\nany type conversions automatically: it just raises an exception if you try to execute an\\noperation on tensors with incompatible types. For example, you cannot add a float\\ntensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\\n>>> tf.constant(2.) + tf.constant(40)\\nTraceback[...]InvalidArgumentError[...]expected to be a float[...]\\n>>> tf.constant(2.) + tf.constant(40., dtype=tf.float64)\\nTraceback[...]InvalidArgumentError[...]expected to be a double[...]\\nThis may be a bit annoying at first, but remember that it’s for a good cause! And of\\ncourse you can use tf.cast() when you really need to convert types:\\n>>> t2 = tf.constant(40., dtype=tf.float64)\\n>>> tf.constant(2.0) + tf.cast(t2, tf.float32)\\n<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\\nVariables\\nSo far, we have used constant tensors: as their name suggests, you cannot modify\\nthem. However, the weights in a neural network need to be tweaked by backpropaga‐\\ntion, and other parameters may also need to change over time (e.g., a momentum\\noptimizer keeps track of past gradients). What we need is a tf.Variable:\\n>>> v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\\n>>> v\\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\nA tf.Variable acts much like a constant tensor: you can perform the same opera‐\\ntions with it, it plays nicely with NumPy as well, and it is just as picky with types. But\\nit can also be modified in place using the assign() method (or assign_add() or\\nassign_sub() which increment or decrement the variable by the given value). You\\ncan also modify individual cells (or slices), using the cell’s (or slice’s) assign()\\nmethod (direct item assignment will not work), or using the scatter_update() or\\nscatter_nd_update() methods:\\nv.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]\\nv[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]\\n374 \\n| \\nChapter 12: Custom Models and Training with TensorFlow\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 400}, page_content='v[:, 2].assign([0., 1.])  # => [[2., 42., 0.], [8., 10., 1.]]\\nv.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])\\n                          # => [[100., 42., 0.], [8., 10., 200.]]\\nIn practice you will rarely have to create variables manually, since\\nKeras provides an add_weight() method that will take care of it for\\nyou, as we will see. Moreover, model parameters will generally be\\nupdated directly by the optimizers, so you will rarely need to\\nupdate variables manually.\\nOther Data Structures\\nTensorFlow supports several other data structures, including the following (please see\\nthe notebook or ??? for more details):\\n• Sparse tensors (tf.SparseTensor) efficiently represent tensors containing mostly\\n0s. The tf.sparse package contains operations for sparse tensors.\\n• Tensor arrays (tf.TensorArray) are lists of tensors. They have a fixed size by\\ndefault, but can optionally be made dynamic. All tensors they contain must have\\nthe same shape and data type.\\n• Ragged tensors (tf.RaggedTensor) represent static lists of lists of tensors, where\\nevery tensor has the same shape and data type. The tf.ragged package contains\\noperations for ragged tensors.\\n• String tensors are regular tensors of type tf.string. These actually represent byte\\nstrings, not Unicode strings, so if you create a string tensor using a Unicode\\nstring (e.g., a regular Python 3 string like \"café\"`), then it will get encoded to\\nUTF-8 automatically (e.g., b\"caf\\\\xc3\\\\xa9\"). Alternatively, you can represent\\nUnicode strings using tensors of type tf.int32, where each item represents a\\nUnicode codepoint (e.g., [99, 97, 102, 233]). The tf.strings package (with\\nan s) contains ops for byte strings and Unicode strings (and to convert one into\\nthe other).\\n• Sets are just represented as regular tensors (or sparse tensors) containing one or\\nmore sets, and you can manipulate them using operations from the tf.sets\\npackage.\\n• Queues, including First In, First Out (FIFO) queues (FIFOQueue), queues that can\\nprioritize some items (PriorityQueue), queues that shuffle their items (Random\\nShuffleQueue), and queues that can batch items of different shapes by padding\\n(PaddingFIFOQueue). These classes are all in the tf.queue package.\\nWith tensors, operations, variables and various data structures at your disposal, you\\nare now ready to customize your models and training algorithms!\\nUsing TensorFlow like NumPy \\n| \\n375'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 401}, page_content='Customizing Models and Training Algorithms\\nLet’s start by creating a custom loss function, which is a simple and common use case.\\nCustom Loss Functions\\nSuppose you want to train a regression model, but your training set is a bit noisy. Of\\ncourse, you start by trying to clean up your dataset by removing or fixing the outliers,\\nbut it turns out to be insufficient, the dataset is still noisy. Which loss function should\\nyou use? The mean squared error might penalize large errors too much, so your\\nmodel will end up being imprecise. The mean absolute error would not penalize out‐\\nliers as much, but training might take a while to converge and the trained model\\nmight not be very precise. This is probably a good time to use the Huber loss (intro‐\\nduced in Chapter 10) instead of the good old MSE. The Huber loss is not currently\\npart of the official Keras API, but it is available in tf.keras (just use an instance of the\\nkeras.losses.Huber class). But let’s pretend it’s not there: implementing it is easy as\\npie! Just create a function that takes the labels and predictions as arguments, and use\\nTensorFlow operations to compute every instance’s loss:\\ndef huber_fn(y_true, y_pred):\\n    error = y_true - y_pred\\n    is_small_error = tf.abs(error) < 1\\n    squared_loss = tf.square(error) / 2\\n    linear_loss  = tf.abs(error) - 0.5\\n    return tf.where(is_small_error, squared_loss, linear_loss)\\nFor better performance, you should use a vectorized implementa‐\\ntion, as in this example. Moreover, if you want to benefit from Ten‐\\nsorFlow’s graph features, you should use only TensorFlow\\noperations.\\nIt is also preferable to return a tensor containing one loss per instance, rather than\\nreturning the mean loss. This way, Keras can apply class weights or sample weights\\nwhen requested (see Chapter 10).\\nNext, you can just use this loss when you compile the Keras model, then train your\\nmodel:\\nmodel.compile(loss=huber_fn, optimizer=\"nadam\")\\nmodel.fit(X_train, y_train, [...])\\nAnd that’s it! For each batch during training, Keras will call the huber_fn() function\\nto compute the loss, and use it to perform a Gradient Descent step. Moreover, it will\\nkeep track of the total loss since the beginning of the epoch, and it will display the\\nmean loss.\\n376 \\n| \\nChapter 12: Custom Models and Training with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 402}, page_content='But what happens to this custom loss when we save the model?\\nSaving and Loading Models That Contain Custom Components\\nSaving a model containing a custom loss function actually works fine, as Keras just\\nsaves the name of the function. However, whenever you load it, you need to provide a\\ndictionary that maps the function name to the actual function. More generally, when\\nyou load a model containing custom objects, you need to map the names to the\\nobjects:\\nmodel = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\\n                                custom_objects={\"huber_fn\": huber_fn})\\nWith the current implementation, any error between -1 and 1 is considered “small”.\\nBut what if we want a different threshold? One solution is to create a function that\\ncreates a configured loss function:\\ndef create_huber(threshold=1.0):\\n    def huber_fn(y_true, y_pred):\\n        error = y_true - y_pred\\n        is_small_error = tf.abs(error) < threshold\\n        squared_loss = tf.square(error) / 2\\n        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2\\n        return tf.where(is_small_error, squared_loss, linear_loss)\\n    return huber_fn\\nmodel.compile(loss=create_huber(2.0), optimizer=\"nadam\")\\nUnfortunately, when you save the model, the threshold will not be saved. This means\\nthat you will have to specify the threshold value when loading the model (note that\\nthe name to use is \"huber_fn\", which is the name of the function we gave Keras, not\\nthe name of the function that created it):\\nmodel = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\",\\n                                custom_objects={\"huber_fn\": create_huber(2.0)})\\nYou can solve this by creating a subclass of the keras.losses.Loss class, and imple‐\\nment its get_config() method:\\nclass HuberLoss(keras.losses.Loss):\\n    def __init__(self, threshold=1.0, **kwargs):\\n        self.threshold = threshold\\n        super().__init__(**kwargs)\\n    def call(self, y_true, y_pred):\\n        error = y_true - y_pred\\n        is_small_error = tf.abs(error) < self.threshold\\n        squared_loss = tf.square(error) / 2\\n        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\\n        return tf.where(is_small_error, squared_loss, linear_loss)\\n    def get_config(self):\\n        base_config = super().get_config()\\n        return {**base_config, \"threshold\": self.threshold}\\nCustomizing Models and Training Algorithms \\n| \\n377'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 403}, page_content='5 It would not be a good idea to use a weighted mean: if we did, then two instances with the same weight but in\\ndifferent batches would have a different impact on training, depending on the total weight of each batch.\\nThe Keras API only specifies how to use subclassing to define lay‐\\ners, models, callbacks, and regularizers. If you build other compo‐\\nnents (such as losses, metrics, initializers or constraints) using\\nsubclassing, they may not be portable to other Keras implementa‐\\ntions.\\nLet’s walk through this code:\\n• The constructor accepts **kwargs and passes them to the parent constructor,\\nwhich handles standard hyperparameters: the name of the loss and the reduction\\nalgorithm to use to aggregate the individual instance losses. By default, it is\\n\"sum_over_batch_size\", which means that the loss will be the sum of the\\ninstance losses, possibly weighted by the sample weights, if any, and then divide\\nthe result by the batch size (not by the sum of weights, so this is not the weighted\\nmean).5. Other possible values are \"sum\" and None.\\n• The call() method takes the labels and predictions, computes all the instance\\nlosses, and returns them.\\n• The get_config() method returns a dictionary mapping each hyperparameter\\nname to its value. It first calls the parent class’s get_config() method, then adds\\nthe new hyperparameters to this dictionary (note that the convenient {**x} syn‐\\ntax was added in Python 3.5).\\nYou can then use any instance of this class when you compile the model:\\nmodel.compile(loss=HuberLoss(2.), optimizer=\"nadam\")\\nWhen you save the model, the threshold will be saved along with it, and when you\\nload the model you just need to map the class name to the class itself:\\nmodel = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\",\\n                                custom_objects={\"HuberLoss\": HuberLoss})\\nWhen you save a model, Keras calls the loss instance’s get_config() method and\\nsaves the config as JSON in the HDF5 file. When you load the model, it calls the\\nfrom_config() class method on the HuberLoss class: this method is implemented by\\nthe base class (Loss) and just creates an instance of the class, passing **config to the\\nconstructor.\\nThat’s it for losses! It was not too hard, was it? Well it’s just as simple for custom acti‐\\nvation functions, initializers, regularizers, and constraints. Let’s look at these now.\\n378 \\n| \\nChapter 12: Custom Models and Training with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 404}, page_content='Custom Activation Functions, Initializers, Regularizers, and\\nConstraints\\nMost Keras functionalities, such as losses, regularizers, constraints, initializers, met‐\\nrics, activation functions, layers and even full models can be customized in very much\\nthe same way. Most of the time, you will just need to write a simple function, with the\\nappropriate inputs and outputs. For example, here are examples of a custom activa‐\\ntion function (equivalent to keras.activations.softplus or tf.nn.softplus), a\\ncustom Glorot initializer (equivalent to keras.initializers.glorot_normal), a cus‐\\ntom ℓ1 regularizer (equivalent to keras.regularizers.l1(0.01)) and a custom con‐\\nstraint \\nthat \\nensures \\nweights \\nare \\nall \\npositive \\n(equivalent \\nto\\nkeras.constraints.nonneg() or tf.nn.relu):\\ndef my_softplus(z): # return value is just tf.nn.softplus(z)\\n    return tf.math.log(tf.exp(z) + 1.0)\\ndef my_glorot_initializer(shape, dtype=tf.float32):\\n    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\\n    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\\ndef my_l1_regularizer(weights):\\n    return tf.reduce_sum(tf.abs(0.01 * weights))\\ndef my_positive_weights(weights): # return value is just tf.nn.relu(weights)\\n    return tf.where(weights < 0., tf.zeros_like(weights), weights)\\nAs you can see, the arguments depend on the type of custom function. These custom\\nfunctions can then be used normally, for example:\\nlayer = keras.layers.Dense(30, activation=my_softplus,\\n                           kernel_initializer=my_glorot_initializer,\\n                           kernel_regularizer=my_l1_regularizer,\\n                           kernel_constraint=my_positive_weights)\\nThe activation function will be applied to the output of this Dense layer, and its result\\nwill be passed on to the next layer. The layer’s weights will be initialized using the\\nvalue returned by the initializer. At each training step the weights will be passed to the\\nregularization function to compute the regularization loss, which will be added to the\\nmain loss to get the final loss used for training. Finally, the constraint function will be\\ncalled after each training step, and the layer’s weights will be replaced by the con‐\\nstrained weights.\\nIf a function has some hyperparameters that need to be saved along with the model,\\nthen you will want to subclass the appropriate class, such as keras.regulariz\\ners.Regularizer, keras.constraints.Constraint, keras.initializers.Initial\\nizer or keras.layers.Layer (for any layer, including activation functions). For\\nexample, much like we did for the custom loss, here is a simple class for ℓ1 regulariza‐\\nCustomizing Models and Training Algorithms \\n| \\n379'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 405}, page_content='6 However, the Huber loss is seldom used as a metric (the MAE or MSE are preferred).\\ntion, that saves its factor hyperparameter (this time we do not need to call the parent\\nconstructor or the get_config() method, as they are not defined by the parent class):\\nclass MyL1Regularizer(keras.regularizers.Regularizer):\\n    def __init__(self, factor):\\n        self.factor = factor\\n    def __call__(self, weights):\\n        return tf.reduce_sum(tf.abs(self.factor * weights))\\n    def get_config(self):\\n        return {\"factor\": self.factor}\\nNote that you must implement the call() method for losses, layers (including activa‐\\ntion functions) and models, or the __call__() method for regularizers, initializers\\nand constraints. For metrics, things are a bit different, as we will see now.\\nCustom Metrics\\nLosses and metrics are conceptually not the same thing: losses are used by Gradient\\nDescent to train a model, so they must be differentiable (at least where they are evalu‐\\nated) and their gradients should not be 0 everywhere. Plus, it’s okay if they are not\\neasily interpretable by humans (e.g. cross-entropy). In contrast, metrics are used to\\nevaluate a model, they must be more easily interpretable, and they can be non-\\ndifferentiable or have 0 gradients everywhere (e.g., accuracy).\\nThat said, in most cases, defining a custom metric function is exactly the same as\\ndefining a custom loss function. In fact, we could even use the Huber loss function we\\ncreated earlier as a metric6, it would work just fine (and persistence would also work\\nthe same way, in this case only saving the name of the function, \"huber_fn\"):\\nmodel.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\\nFor each batch during training, Keras will compute this metric and keep track of its\\nmean since the beginning of the epoch. Most of the time, this is exactly what you\\nwant. But not always! Consider a binary classifier’s precision, for example. As we saw\\nin Chapter 3, precision is the number of true positives divided by the number of posi‐\\ntive predictions (including both true positives and false positives). Suppose the model\\nmade 5 positive predictions in the first batch, 4 of which were correct: that’s 80% pre‐\\ncision. Then suppose the model made 3 positive predictions in the second batch, but\\nthey were all incorrect: that’s 0% precision for the second batch. If you just compute\\nthe mean of these two precisions, you get 40%. But wait a second, this is not the mod‐\\nel’s precision over these two batches! Indeed, there were a total of 4 true positives (4 +\\n0) out of 8 positive predictions (5 + 3), so the overall precision is 50%, not 40%. What\\nwe need is an object that can keep track of the number of true positives and the num‐\\n380 \\n| \\nChapter 12: Custom Models and Training with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 406}, page_content='ber of false positives, and compute their ratio when requested. This is precisely what\\nthe keras.metrics.Precision class does:\\n>>> precision = keras.metrics.Precision()\\n>>> precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])\\n<tf.Tensor: id=581729, shape=(), dtype=float32, numpy=0.8>\\n>>> precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])\\n<tf.Tensor: id=581780, shape=(), dtype=float32, numpy=0.5>\\nIn this example, we created a Precision object, then we used it like a function, pass‐\\ning it the labels and predictions for the first batch, then for the second batch (note\\nthat we could also have passed sample weights). We used the same number of true\\nand false positives as in the example we just discussed. After the first batch, it returns\\nthe precision of 80%, then after the second batch it returns 50% (which is the overall\\nprecision so far, not the second batch’s precision). This is called a streaming metric (or\\nstateful metric), as it is gradually updated, batch after batch.\\nAt any point, we can call the result() method to get the current value of the metric.\\nWe can also look at its variables (tracking the number of true and false positives)\\nusing the variables attribute, and reset these variables using the reset_states()\\nmethod:\\n>>> p.result()\\n<tf.Tensor: id=581794, shape=(), dtype=float32, numpy=0.5>\\n>>> p.variables\\n[<tf.Variable \\'true_positives:0\\' [...] numpy=array([4.], dtype=float32)>,\\n <tf.Variable \\'false_positives:0\\' [...] numpy=array([4.], dtype=float32)>]\\n>>> p.reset_states() # both variables get reset to 0.0\\nIf you need to create such a streaming metric, you can just create a subclass of the\\nkeras.metrics.Metric class. Here is a simple example that keeps track of the total\\nHuber loss and the number of instances seen so far. When asked for the result, it\\nreturns the ratio, which is simply the mean Huber loss:\\nclass HuberMetric(keras.metrics.Metric):\\n    def __init__(self, threshold=1.0, **kwargs):\\n        super().__init__(**kwargs) # handles base args (e.g., dtype)\\n        self.threshold = threshold\\n        self.huber_fn = create_huber(threshold)\\n        self.total = self.add_weight(\"total\", initializer=\"zeros\")\\n        self.count = self.add_weight(\"count\", initializer=\"zeros\")\\n    def update_state(self, y_true, y_pred, sample_weight=None):\\n        metric = self.huber_fn(y_true, y_pred)\\n        self.total.assign_add(tf.reduce_sum(metric))\\n        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\\n    def result(self):\\n        return self.total / self.count\\n    def get_config(self):\\n        base_config = super().get_config()\\n        return {**base_config, \"threshold\": self.threshold}\\nCustomizing Models and Training Algorithms \\n| \\n381'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 407}, page_content='7 This class is for illustration purposes only. A simpler and better implementation would just subclass the\\nkeras.metrics.Mean class, see the notebook for an example.\\nLet’s walk through this code:7:\\n• The constructor uses the add_weight() method to create the variables needed to\\nkeep track of the metric’s state over multiple batches, in this case the sum of all\\nHuber losses (total) and the number of instances seen so far (count). You could\\njust create variables manually if you preferred. Keras tracks any tf.Variable that\\nis set as an attribute (and more generally, any “trackable” object, such as layers or\\nmodels).\\n• The update_state() method is called when you use an instance of this class as a\\nfunction (as we did with the Precision object). It updates the variables given the\\nlabels and predictions for one batch (and sample weights, but in this case we just\\nignore them).\\n• The result() method computes and returns the final result, in this case just the\\nmean Huber metric over all instances. When you use the metric as a function, the\\nupdate_state() method gets called first, then the result() method is called,\\nand its output is returned.\\n• We also implement the get_config() method to ensure the threshold gets\\nsaved along with the model.\\n• The default implementation of the reset_states() method just resets all vari‐\\nables to 0.0 (but you can override it if needed).\\nKeras will take care of variable persistence seamlessly, no action is\\nrequired.\\nWhen you define a metric using a simple function, Keras automatically calls it for\\neach batch, and it keeps track of the mean during each epoch, just like we did man‐\\nually. So the only benefit of our HuberMetric class is that the threshold will be saved.\\nBut of course, some metrics, like precision, cannot simply be averaged over batches:\\nin thoses cases, there’s no other option than to implement a streaming metric.\\nNow that we have built a streaming metric, building a custom layer will seem like a\\nwalk in the park!\\n382 \\n| \\nChapter 12: Custom Models and Training with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 408}, page_content='Custom Layers\\nYou may occasionally want to build an architecture that contains an exotic layer for\\nwhich TensorFlow does not provide a default implementation. In this case, you will\\nneed to create a custom layer. Or sometimes you may simply want to build a very\\nrepetitive architecture, containing identical blocks of layers repeated many times, and\\nit would be convenient to treat each block of layers as a single layer. For example, if\\nthe model is a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to\\ndefine a custom layer D containing layers A, B, C, and your model would then simply\\nbe D, D, D. Let’s see how to build custom layers.\\nFirst, some layers have no weights, such as keras.layers.Flatten or keras.lay\\ners.ReLU. If you want to create a custom layer without any weights, the simplest\\noption is to write a function and wrap it in a keras.layers.Lambda layer. For exam‐\\nple, the following layer will apply the exponential function to its inputs:\\nexponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))\\nThis custom layer can then be used like any other layer, using the sequential API, the\\nfunctional API, or the subclassing API. You can also use it as an activation function\\n(or you could just use activation=tf.exp, or activation=keras.activations.expo\\nnential, or simply activation=\"exponential\"). The exponential layer is sometimes\\nused in the output layer of a regression model when the values to predict have very\\ndifferent scales (e.g., 0.001, 10., 1000.).\\nAs you probably guessed by now, to build a custom stateful layer (i.e., a layer with\\nweights), you need to create a subclass of the keras.layers.Layer class. For exam‐\\nple, the following class implements a simplified version of the Dense layer:\\nclass MyDense(keras.layers.Layer):\\n    def __init__(self, units, activation=None, **kwargs):\\n        super().__init__(**kwargs)\\n        self.units = units\\n        self.activation = keras.activations.get(activation)\\n    def build(self, batch_input_shape):\\n        self.kernel = self.add_weight(\\n            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\\n            initializer=\"glorot_normal\")\\n        self.bias = self.add_weight(\\n            name=\"bias\", shape=[self.units], initializer=\"zeros\")\\n        super().build(batch_input_shape) # must be at the end\\n    def call(self, X):\\n        return self.activation(X @ self.kernel + self.bias)\\n    def compute_output_shape(self, batch_input_shape):\\n        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\\nCustomizing Models and Training Algorithms \\n| \\n383'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 409}, page_content='8 This function is specific to tf.keras. You could use keras.activations.Activation instead.\\n9 The Keras API calls this argument input_shape, but since it also includes the batch dimension, I prefer to call\\nit batch_input_shape. Same for compute_output_shape().\\n    def get_config(self):\\n        base_config = super().get_config()\\n        return {**base_config, \"units\": self.units,\\n                \"activation\": keras.activations.serialize(self.activation)}\\nLet’s walk through this code:\\n• The constructor takes all the hyperparameters as arguments (in this example just\\nunits and activation), and importantly it also takes a **kwargs argument. It\\ncalls the parent constructor, passing it the kwargs: this takes care of standard\\narguments such as input_shape, trainable, name, and so on. Then it saves the\\nhyperparameters as attributes, converting the activation argument to the\\nappropriate activation function using the keras.activations.get() function (it\\naccepts functions, standard strings like \"relu\" or \"selu\", or simply None)8.\\n• The build() method’s role is to create the layer’s variables, by calling the\\nadd_weight() method for each weight. The build() method is called the first\\ntime the layer is used. At that point, Keras will know the shape of this layer’s\\ninputs, and it will pass it to the build() method9, which is often necessary to cre‐\\nate some of the weights. For example, we need to know the number of neurons in\\nthe previous layer in order to create the connection weights matrix (i.e., the \"ker\\nnel\"): this corresponds to the size of the last dimension of the inputs. At the end\\nof the build() method (and only at the end), you must call the parent’s build()\\nmethod: this tells Keras that the layer is built (it just sets self.built = True).\\n• The call() method actually performs the desired operations. In this case, we\\ncompute the matrix multiplication of the inputs X and the layer’s kernel, we add\\nthe bias vector, we apply the activation function to the result, and this gives us the\\noutput of the layer.\\n• The compute_output_shape() method simply returns the shape of this layer’s\\noutputs. In this case, it is the same shape as the inputs, except the last dimension\\nis replaced with the number of neurons in the layer. Note that in tf.keras, shapes\\nare instances of the tf.TensorShape class, which you can convert to Python lists\\nusing as_list().\\n• The get_config() method is just like earlier. Note that we save the activation\\nfunction’s full configuration by calling keras.activations.serialize().\\nYou can now use a MyDense layer just like any other layer!\\n384 \\n| \\nChapter 12: Custom Models and Training with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 410}, page_content='You can generally omit the compute_output_shape() method, as\\ntf.keras automatically infers the output shape, except when the\\nlayer is dynamic (as we will see shortly). In other Keras implemen‐\\ntations, this method is either required or by default it assumes the\\noutput shape is the same as the input shape.\\nTo create a layer with multiple inputs (e.g., Concatenate), the argument to the call()\\nmethod should be a tuple containing all the inputs, and similarly the argument to the\\ncompute_output_shape() method should be a tuple containing each input’s batch\\nshape. To create a layer with multiple outputs, the call() method should return the\\nlist of outputs, and the compute_output_shape() should return the list of batch out‐\\nput shapes (one per output). For example, the following toy layer takes two inputs\\nand returns three outputs:\\nclass MyMultiLayer(keras.layers.Layer):\\n    def call(self, X):\\n        X1, X2 = X\\n        return [X1 + X2, X1 * X2, X1 / X2]\\n    def compute_output_shape(self, batch_input_shape):\\n        b1, b2 = batch_input_shape\\n        return [b1, b1, b1] # should probably handle broadcasting rules\\nThis layer may now be used like any other layer, but of course only using the func‐\\ntional and subclassing APIs, not the sequential API (which only accepts layers with\\none input and one output).\\nIf your layer needs to have a different behavior during training and during testing\\n(e.g., if it uses Dropout or BatchNormalization layers), then you must add a train\\ning argument to the call() method and use this argument to decide what to do. For\\nexample, let’s create a layer that adds Gaussian noise during training (for regulariza‐\\ntion), but does nothing during testing (Keras actually has a layer that does the same\\nthing: keras.layers.GaussianNoise):\\nclass MyGaussianNoise(keras.layers.Layer):\\n    def __init__(self, stddev, **kwargs):\\n        super().__init__(**kwargs)\\n        self.stddev = stddev\\n    def call(self, X, training=None):\\n        if training:\\n            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\\n            return X + noise\\n        else:\\n            return X\\n    def compute_output_shape(self, batch_input_shape):\\n        return batch_input_shape\\nCustomizing Models and Training Algorithms \\n| \\n385'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 411}, page_content='10 The name “subclassing API” usually refers only to the creation of custom models by subclassing, although\\nmany other things can be created by subclassing, as we saw in this chapter.\\nWith that, you can now build any custom layer you need! Now let’s create custom\\nmodels.\\nCustom Models\\nWe already looked at custom model classes in Chapter 10 when we discussed the sub‐\\nclassing API.10 It is actually quite straightforward, just subclass the keras.mod\\nels.Model class, create layers and variables in the constructor, and implement the\\ncall() method to do whatever you want the model to do. For example, suppose you\\nwant to build the model represented in Figure 12-3:\\nFigure 12-3. Custom Model Example\\nThe inputs go through a first dense layer, then through a residual block composed of\\ntwo dense layers and an addition operation (as we will see in Chapter 14, a residual\\nblock adds its inputs to its outputs), then through this same residual block 3 more\\ntimes, then through a second residual block, and the final result goes through a dense\\noutput layer. Note that this model does not make much sense, it’s just an example to\\nillustrate the fact that you can easily build any kind of model you want, even contain‐\\n386 \\n| \\nChapter 12: Custom Models and Training with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 412}, page_content='ing loops and skip connections. To implement this model, it is best to first create a\\nResidualBlock layer, since we are going to create a couple identical blocks (and we\\nmight want to reuse it in another model):\\nclass ResidualBlock(keras.layers.Layer):\\n    def __init__(self, n_layers, n_neurons, **kwargs):\\n        super().__init__(**kwargs)\\n        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\\n                                          kernel_initializer=\"he_normal\")\\n                       for _ in range(n_layers)]\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.hidden:\\n            Z = layer(Z)\\n        return inputs + Z\\nThis layer is a bit special since it contains other layers. This is handled transparently\\nby Keras: it automatically detects that the hidden attribute contains trackable objects\\n(layers in this case), so their variables are automatically added to this layer’s list of\\nvariables. The rest of this class is self-explanatory. Next, let’s use the subclassing API\\nto define the model itself:\\nclass ResidualRegressor(keras.models.Model):\\n    def __init__(self, output_dim, **kwargs):\\n        super().__init__(**kwargs)\\n        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\\n                                          kernel_initializer=\"he_normal\")\\n        self.block1 = ResidualBlock(2, 30)\\n        self.block2 = ResidualBlock(2, 30)\\n        self.out = keras.layers.Dense(output_dim)\\n    def call(self, inputs):\\n        Z = self.hidden1(inputs)\\n        for _ in range(1 + 3):\\n            Z = self.block1(Z)\\n        Z = self.block2(Z)\\n        return self.out(Z)\\nWe create the layers in the constructor, and use them in the call() method. This\\nmodel can then be used like any other model (compile it, fit it, evaluate it and use it to\\nmake predictions). If you also want to be able to save the model using the save()\\nmethod, and load it using the keras.models.load_model() function, you must\\nimplement the get_config() method (as we did earlier) in both the ResidualBlock\\nclass and the ResidualRegressor class. Alternatively, you can just save and load the\\nweights using the save_weights() and load_weights() methods.\\nThe Model class is actually a subclass of the Layer class, so models can be defined and\\nused exactly like layers. But a model also has some extra functionalities, including of\\ncourse its compile(), fit(), evaluate() and predict() methods (and a few var‐\\nCustomizing Models and Training Algorithms \\n| \\n387'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 413}, page_content='iants, such as train_on_batch() or fit_generator()), plus the get_layers()\\nmethod (which can return any of the model’s layers by name or by index), and the\\nsave() method (and support for keras.models.load_model() and keras.mod\\nels.clone_model()). So if models provide more functionalities than layers, why not\\njust define every layer as a model? Well, technically you could, but it is probably\\ncleaner to distinguish the internal components of your model (layers or reusable\\nblocks of layers) from the model itself. The former should subclass the Layer class,\\nwhile the latter should subclass the Model class.\\nWith that, you can quite naturally and concisely build almost any model that you find\\nin a paper, either using the sequential API, the functional API, the subclassing API, or\\neven a mix of these. “Almost” any model? Yes, there are still a couple things that we\\nneed to look at: first, how to define losses or metrics based on model internals, and\\nsecond how to build a custom training loop.\\nLosses and Metrics Based on Model Internals\\nThe custom losses and metrics we defined earlier were all based on the labels and the\\npredictions (and optionally sample weights). However, you will occasionally want to\\ndefine losses based on other parts of your model, such as the weights or activations of\\nits hidden layers. This may be useful for regularization purposes, or to monitor some\\ninternal aspect of your model.\\nTo define a custom loss based on model internals, just compute it based on any part\\nof the model you want, then pass the result to the add_loss() method. For example,\\nthe following custom model represents a standard MLP regressor with 5 hidden lay‐\\ners, except it also implements a reconstruction loss (see ???): we add an extra Dense\\nlayer on top of the last hidden layer, and its role is to try to reconstruct the inputs of\\nthe model. Since the reconstruction must have the same shape as the model’s inputs,\\nwe need to create this Dense layer in the build() method to have access to the shape\\nof the inputs. In the call() method, we compute both the regular output of the MLP,\\nplus the output of the reconstruction layer. We then compute the mean squared dif‐\\nference between the reconstructions and the inputs, and we add this value (times\\n0.05) to the model’s list of losses by calling add_loss(). During training, Keras will\\nadd this loss to the main loss (which is why we scaled down the reconstruction loss,\\nto ensure the main loss dominates). As a result, the model will be forced to preserve\\nas much information as possible through the hidden layers, even information that is\\nnot directly useful for the regression task itself. In practice, this loss sometimes\\nimproves generalization; it is a regularization loss:\\nclass ReconstructingRegressor(keras.models.Model):\\n    def __init__(self, output_dim, **kwargs):\\n        super().__init__(**kwargs)\\n        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\\n                                          kernel_initializer=\"lecun_normal\")\\n388 \\n| \\nChapter 12: Custom Models and Training with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 414}, page_content='for _ in range(5)]\\n        self.out = keras.layers.Dense(output_dim)\\n    def build(self, batch_input_shape):\\n        n_inputs = batch_input_shape[-1]\\n        self.reconstruct = keras.layers.Dense(n_inputs)\\n        super().build(batch_input_shape)\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.hidden:\\n            Z = layer(Z)\\n        reconstruction = self.reconstruct(Z)\\n        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\\n        self.add_loss(0.05 * recon_loss)\\n        return self.out(Z)\\nSimilarly, you can add a custom metric based on model internals by computing it in\\nany way you want, as long at the result is the output of a metric object. For example,\\nyou can create a keras.metrics.Mean() object in the constructor, then call it in the\\ncall() method, passing it the recon_loss, and finally add it to the model by calling\\nthe model’s add_metric() method. This way, when you train the model, Keras will\\ndisplay both the mean loss over each epoch (the loss is the sum of the main loss plus\\n0.05 times the reconstruction loss) and the mean reconstruction error over each\\nepoch. Both will go down during training:\\nEpoch 1/5\\n11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360\\nEpoch 2/5\\n11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964\\n[...]\\nIn over 99% of the cases, everything we have discussed so far will be sufficient to\\nimplement whatever model you want to build, even with complex architectures, los‐\\nses, metrics, and so on. However, in some rare cases you may need to customize the\\ntraining loop itself. However, before we get there, we need to look at how to compute\\ngradients automatically in TensorFlow.\\nComputing Gradients Using Autodiff\\nTo understand how to use autodiff (see Chapter 10 and ???) to compute gradients\\nautomatically, let’s consider a simple toy function:\\ndef f(w1, w2):\\n    return 3 * w1 ** 2 + 2 * w1 * w2\\nIf you know calculus, you can analytically find that the partial derivative of this func‐\\ntion with regards to w1 is 6 * w1 + 2 * w2. You can also find that its partial derivative\\nwith regards to w2 is 2 * w1. For example, at the point (w1, w2) = (5, 3), these par‐\\nCustomizing Models and Training Algorithms \\n| \\n389'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 415}, page_content='tial derivatives are equal to 36 and 10, respectively, so the gradient vector at this point\\nis (36, 10). But if this were a neural network, the function would be much more com‐\\nplex, typically with tens of thousands of parameters, and finding the partial deriva‐\\ntives analytically by hand would be an almost impossible task. One solution could be\\nto compute an approximation of each partial derivative by measuring how much the\\nfunction’s output changes when you tweak the corresponding parameter:\\n>>> w1, w2 = 5, 3\\n>>> eps = 1e-6\\n>>> (f(w1 + eps, w2) - f(w1, w2)) / eps\\n36.000003007075065\\n>>> (f(w1, w2 + eps) - f(w1, w2)) / eps\\n10.000000003174137\\nLooks about right! This works rather well and it is trivial to implement, but it is just\\nan approximation, and importantly you need to call f() at least once per parameter\\n(not twice, since we could compute f(w1, w2) just once). This makes this approach\\nintractable for large neural networks. So instead we should use autodiff (see Chap‐\\nter 10 and ???). TensorFlow makes this pretty simple:\\nw1, w2 = tf.Variable(5.), tf.Variable(3.)\\nwith tf.GradientTape() as tape:\\n    z = f(w1, w2)\\ngradients = tape.gradient(z, [w1, w2])\\nWe first define two variables w1 and w2, then we create a tf.GradientTape context\\nthat will automatically record every operation that involves a variable, and finally we\\nask this tape to compute the gradients of the result z with regards to both variables\\n[w1, w2]. Let’s take a look at the gradients that TensorFlow computed:\\n>>> gradients\\n[<tf.Tensor: id=828234, shape=(), dtype=float32, numpy=36.0>,\\n <tf.Tensor: id=828229, shape=(), dtype=float32, numpy=10.0>]\\nPerfect! Not only is the result accurate (the precision is only limited by the floating\\npoint errors), but the gradient() method only goes through the recorded computa‐\\ntions once (in reverse order), no matter how many variables there are, so it is incredi‐\\nbly efficient. It’s like magic!\\nOnly put the strict minimum inside the tf.GradientTape() block,\\nto save memory. Alternatively, you can pause recording by creating\\na with tape.stop_recording() block inside the tf.Gradient\\nTape() block.\\nThe tape is automatically erased immediately after you call its gradient() method, so\\nyou will get an exception if you try to call gradient() twice:\\n390 \\n| \\nChapter 12: Custom Models and Training with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 416}, page_content='with tf.GradientTape() as tape:\\n    z = f(w1, w2)\\ndz_dw1 = tape.gradient(z, w1) # => tensor 36.0\\ndz_dw2 = tape.gradient(z, w2) # RuntimeError!\\nIf you need to call gradient() more than once, you must make the tape persistent,\\nand delete it when you are done with it to free resources:\\nwith tf.GradientTape(persistent=True) as tape:\\n    z = f(w1, w2)\\ndz_dw1 = tape.gradient(z, w1) # => tensor 36.0\\ndz_dw2 = tape.gradient(z, w2) # => tensor 10.0, works fine now!\\ndel tape\\nBy default, the tape will only track operations involving variables, so if you try to\\ncompute the gradient of z with regards to anything else than a variable, the result will\\nbe None:\\nc1, c2 = tf.constant(5.), tf.constant(3.)\\nwith tf.GradientTape() as tape:\\n    z = f(c1, c2)\\ngradients = tape.gradient(z, [c1, c2]) # returns [None, None]\\nHowever, you can force the tape to watch any tensors you like, to record every opera‐\\ntion that involves them. You can then compute gradients with regards to these ten‐\\nsors, as if they were variables:\\nwith tf.GradientTape() as tape:\\n    tape.watch(c1)\\n    tape.watch(c2)\\n    z = f(c1, c2)\\ngradients = tape.gradient(z, [c1, c2]) # returns [tensor 36., tensor 10.]\\nThis can be useful in some cases, for example if you want to implement a regulariza‐\\ntion loss that penalizes activations that vary a lot when the inputs vary little: the loss\\nwill be based on the gradient of the activations with regards to the inputs. Since the\\ninputs are not variables, you would need to tell the tape to watch them.\\nIf you compute the gradient of a list of tensors (e.g., [z1, z2, z3]) with regards to\\nsome variables (e.g., [w1, w2]), TensorFlow actually efficiently computes the sum of\\nthe gradients of these tensors (i.e., gradient(z1, [w1, w2]), plus gradient(z2,\\n[w1, w2]), plus gradient(z3, [w1, w2])). Due to the way reverse-mode autodiff\\nworks, it is not possible to compute the individual gradients (z1, z2 and z3) without\\nactually calling gradient() multiple times (once for z1, once for z2 and once for z3),\\nwhich requires making the tape persistent (and deleting it afterwards).\\nCustomizing Models and Training Algorithms \\n| \\n391'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 417}, page_content='Moreover, it is actually possible to compute second order partial derivatives (the Hes‐\\nsians, i.e., the partial derivatives of the partial derivatives)! To do this, we need to\\nrecord the operations that are performed when computing the first-order partial\\nderivatives (the Jacobians): this requires a second tape. Here is how it works:\\nwith tf.GradientTape(persistent=True) as hessian_tape:\\n    with tf.GradientTape() as jacobian_tape:\\n        z = f(w1, w2)\\n    jacobians = jacobian_tape.gradient(z, [w1, w2])\\nhessians = [hessian_tape.gradient(jacobian, [w1, w2])\\n            for jacobian in jacobians]\\ndel hessian_tape\\nThe inner tape is used to compute the Jacobians, as we did earlier. The outer tape is\\nused to compute the partial derivatives of each Jacobian. Since we need to call gradi\\nent() once for each Jacobian (or else we would get the sum of the partial derivatives\\nover all the Jabobians, as explained earlier), we need the outer tape to be persistent, so\\nwe delete it at the end. The Jacobians are obviously the same as earlier (36 and 5), but\\nnow we also have the Hessians:\\n>>> hessians # dz_dw1_dw1, dz_dw1_dw2, dz_dw2_dw1, dz_dw2_dw2\\n[[<tf.Tensor: id=830578, shape=(), dtype=float32, numpy=6.0>,\\n  <tf.Tensor: id=830595, shape=(), dtype=float32, numpy=2.0>],\\n [<tf.Tensor: id=830600, shape=(), dtype=float32, numpy=2.0>, None]]\\nLet’s verify these Hessians. The first two are the partial derivatives of 6 * w1 + 2 * w2\\n(which is, as we saw earlier, the partial derivative of f with regards to w1), with\\nregards to w1 and w2. The result is correct: 6 for w1 and 2 for w2. The next two are the\\npartial derivatives of 2 * w1 (the partial derivative of f with regards to w2), with\\nregards to w1 and w2, which are 2 for w1 and 0 for w2. Note that TensorFlow returns\\nNone instead of 0 since w2 does not appear at all in 2 * w1. TensorFlow also returns\\nNone when you use an operation whose gradients are not defined (e.g., tf.argmax()).\\nIn some rare cases you may want to stop gradients from backpropagating through\\nsome part of your neural network. To do this, you must use the tf.stop_gradient()\\nfunction: it just returns its inputs during the forward pass (like tf.identity()), but\\nit does not let gradients through during backpropagation (it acts like a constant). For\\nexample:\\ndef f(w1, w2):\\n    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\\nwith tf.GradientTape() as tape:\\n    z = f(w1, w2) # same result as without stop_gradient()\\ngradients = tape.gradient(z, [w1, w2]) # => returns [tensor 30., None]\\n392 \\n| \\nChapter 12: Custom Models and Training with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 418}, page_content='Finally, you may occasionally run into some numerical issues when computing gradi‐\\nents. For example, if you compute the gradients of the my_softplus() function for\\nlarge inputs, the result will be NaN:\\n>>> x = tf.Variable([100.])\\n>>> with tf.GradientTape() as tape:\\n...     z = my_softplus(x)\\n...\\n>>> tape.gradient(z, [x])\\n<tf.Tensor: [...] numpy=array([nan], dtype=float32)>\\nThis is because computing the gradients of this function using autodiff leads to some\\nnumerical difficulties: due to floating point precision errors, autodiff ends up com‐\\nputing infinity divided by infinity (which returns NaN). Fortunately, we can analyti‐\\ncally find that the derivative of the softplus function is just 1 / (1 + 1 / exp(x)), which\\nis numerically stable. Next, we can tell TensorFlow to use this stable function when\\ncomputing the gradients of the my_softplus() function, by decorating it with\\n@tf.custom_gradient, and making it return both its normal output and the function\\nthat computes the derivatives (note that it will receive as input the gradients that were\\nbackpropagated so far, down to the softplus function, and according to the chain rule\\nwe should multiply them with this function’s gradients):\\n@tf.custom_gradient\\ndef my_better_softplus(z):\\n    exp = tf.exp(z)\\n    def my_softplus_gradients(grad):\\n        return grad / (1 + 1 / exp)\\n    return tf.math.log(exp + 1), my_softplus_gradients\\nNow when we compute the gradients of the my_better_softplus() function, we get\\nthe proper result, even for large input values (however, the main output still explodes\\nbecause of the exponential: one workaround is to use tf.where() to just return the\\ninputs when they are large).\\nCongratulations! You can now compute the gradients of any function (provided it is\\ndifferentiable at the point where you compute it), you can even compute Hessians,\\nblock backpropagation when needed and even write your own gradient functions!\\nThis is probably more flexibility than you will ever need, even if you build your own\\ncustom training loops, as we will see now.\\nCustom Training Loops\\nIn some rare cases, the fit() method may not be flexible enough for what you need\\nto do. For example, the Wide and Deep paper we discussed in Chapter 10 actually\\nuses two different optimizers: one for the wide path and the other for the deep path.\\nSince the fit() method only uses one optimizer (the one that we specify when\\nCustomizing Models and Training Algorithms \\n| \\n393'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 419}, page_content='compiling the model), implementing this paper requires writing your own custom\\nloop.\\nYou may also like to write your own custom training loops simply to feel more confi‐\\ndent that it does precisely what you intent it to do (perhaps you are unsure about\\nsome details of the fit() method). It can sometimes feel safer to make everything\\nexplicit. However, remember that writing a custom training loop will make your code\\nlonger, more error prone and harder to maintain.\\nUnless you really need the extra flexibility, you should prefer using\\nthe fit() method rather than implementing your own training\\nloop, especially if you work in a team.\\nFirst, let’s build a simple model. No need to compile it, since we will handle the train‐\\ning loop manually:\\nl2_reg = keras.regularizers.l2(0.05)\\nmodel = keras.models.Sequential([\\n    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\\n                       kernel_regularizer=l2_reg),\\n    keras.layers.Dense(1, kernel_regularizer=l2_reg)\\n])\\nNext, let’s create a tiny function that will randomly sample a batch of instances from\\nthe training set (in Chapter 13 we will discuss the Data API, which offers a much bet‐\\nter alternative):\\ndef random_batch(X, y, batch_size=32):\\n    idx = np.random.randint(len(X), size=batch_size)\\n    return X[idx], y[idx]\\nLet’s also define a function that will display the training status, including the number\\nof steps, the total number of steps, the mean loss since the start of the epoch (i.e., we\\nwill use the Mean metric to compute it), and other metrics:\\ndef print_status_bar(iteration, total, loss, metrics=None):\\n    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\\n                         for m in [loss] + (metrics or [])])\\n    end = \"\" if iteration < total else \"\\\\n\"\\n    print(\"\\\\r{}/{} - \".format(iteration, total) + metrics,\\n          end=end)\\nThis code is self-explanatory, unless you are unfamiliar with Python string format‐\\nting: {:.4f} will format a float with 4 digits after the decimal point. Moreover, using\\n\\\\r (carriage return) along with end=\"\" ensures that the status bar always gets printed\\non the same line. In the notebook, the print_status_bar() function also includes a\\nprogress bar, but you could use the handy tqdm library instead.\\n394 \\n| \\nChapter 12: Custom Models and Training with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 420}, page_content='With that, let’s get down to business! First, we need to define some hyperparameters,\\nchoose the optimizer, the loss function and the metrics (just the MAE in this exam‐\\nple):\\nn_epochs = 5\\nbatch_size = 32\\nn_steps = len(X_train) // batch_size\\noptimizer = keras.optimizers.Nadam(lr=0.01)\\nloss_fn = keras.losses.mean_squared_error\\nmean_loss = keras.metrics.Mean()\\nmetrics = [keras.metrics.MeanAbsoluteError()]\\nAnd now we are ready to build the custom loop!\\nfor epoch in range(1, n_epochs + 1):\\n    print(\"Epoch {}/{}\".format(epoch, n_epochs))\\n    for step in range(1, n_steps + 1):\\n        X_batch, y_batch = random_batch(X_train_scaled, y_train)\\n        with tf.GradientTape() as tape:\\n            y_pred = model(X_batch, training=True)\\n            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\\n            loss = tf.add_n([main_loss] + model.losses)\\n        gradients = tape.gradient(loss, model.trainable_variables)\\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\\n        mean_loss(loss)\\n        for metric in metrics:\\n            metric(y_batch, y_pred)\\n        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\\n    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\\n    for metric in [mean_loss] + metrics:\\n        metric.reset_states()\\nThere’s a lot going on in this code, so let’s walk through it:\\n• We create two nested loops: one for the epochs, the other for the batches within\\nan epoch.\\n• Then we sample a random batch from the training set.\\n• Inside the tf.GradientTape() block, we make a prediction for one batch (using\\nthe model as a function), and we compute the loss: it is equal to the main loss\\nplus the other losses (in this model, there is one regularization loss per layer).\\nSince the mean_squared_error() function returns one loss per instance, we\\ncompute the mean over the batch using tf.reduce_mean() (if you wanted to\\napply different weights to each instance, this is where you would do it). The regu‐\\nlarization losses are already reduced to a single scalar each, so we just need to\\nsum them (using tf.add_n(), which sums multiple tensors of the same shape\\nand data type).\\nCustomizing Models and Training Algorithms \\n| \\n395'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 421}, page_content='11 The truth is we did not process every single instance in the training set because we sampled instances ran‐\\ndomly, so some were processed more than once while others were not processed at all. In practice that’s fine.\\nMoreover, if the training set size is not a multiple of the batch size, we will miss a few instances.\\n12 Alternatively, check out K.learning_phase(), K.set_learning_phase() and K.learning_phase_scope().\\n13 With the exception of optimizers, as very few people ever customize these: see the notebook for an example.\\n• Next, we ask the tape to compute the gradient of the loss with regards to each\\ntrainable variable (not all variables!), and we apply them to the optimizer to per‐\\nform a Gradient Descent step.\\n• Next we update the mean loss and the metrics (over the current epoch), and we\\ndisplay the status bar.\\n• At the end of each epoch, we display the status bar again to make it look com‐\\nplete11 and to print a line feed, and we reset the states of the mean loss and the\\nmetrics.\\nIf you set the optimizer’s clipnorm or clipvalue hyperparameters, it will take care of\\nthis for you. If you want to apply any other transformation to the gradients, simply do\\nso before calling the apply_gradients() method.\\nIf you add weight constraints to your model (e.g., by setting kernel_constraint or\\nbias_constraint when creating a layer), you should update the training loop to\\napply these constraints just after apply_gradients():\\nfor variable in model.variables:\\n    if variable.constraint is not None:\\n        variable.assign(variable.constraint(variable))\\nMost importantly, this training loop does not handle layers that behave differently\\nduring training and testing (e.g., BatchNormalization or Dropout). To handle these,\\nyou need to call the model with training=True and make sure it propagates this to\\nevery layer that needs it.12\\nAs you can see, there are quite a lot of things you need to get right, it is easy to make a\\nmistake. But on the bright side, you get full control, so it’s your call.\\nNow that you know how to customize any part of your models13 and training algo‐\\nrithms, let’s see how you can use TensorFlow’s automatic graph generation feature: it\\ncan speed up your custom code considerably, and it will also make it portable to any\\nplatform supported by TensorFlow (see ???).\\nTensorFlow Functions and Graphs\\nIn TensorFlow 1, graphs were unavoidable (as were the complexities that came with\\nthem): they were a central part of TensorFlow’s API. In TensorFlow 2, they are still\\n396 \\n| \\nChapter 12: Custom Models and Training with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 422}, page_content='there, but not as central, and much (much!) simpler to use. To demonstrate this, let’s\\nstart with a trivial function that just computes the cube of its input:\\ndef cube(x):\\n    return x ** 3\\nWe can obviously call this function with a Python value, such as an int or a float, or\\nwe can call it with a tensor:\\n>>> cube(2)\\n8\\n>>> cube(tf.constant(2.0))\\n<tf.Tensor: id=18634148, shape=(), dtype=float32, numpy=8.0>\\nNow, let’s use tf.function() to convert this Python function to a TensorFlow Func‐\\ntion:\\n>>> tf_cube = tf.function(cube)\\n>>> tf_cube\\n<tensorflow.python.eager.def_function.Function at 0x1546fc080>\\nThis TF Function can then be used exactly like the original Python function, and it\\nwill return the same result (but as tensors):\\n>>> tf_cube(2)\\n<tf.Tensor: id=18634201, shape=(), dtype=int32, numpy=8>\\n>>> tf_cube(tf.constant(2.0))\\n<tf.Tensor: id=18634211, shape=(), dtype=float32, numpy=8.0>\\nUnder the hood, tf.function() analyzed the computations performed by the cube()\\nfunction and generated an equivalent computation graph! As you can see, it was\\nrather painless (we will see how this works shortly). Alternatively, we could have used\\ntf.function as a decorator; this is actually more common:\\n@tf.function\\ndef tf_cube(x):\\n    return x ** 3\\nThe original Python function is still available via the TF Function’s python_function\\nattribute, in case you ever need it:\\n>>> tf_cube.python_function(2)\\n8\\nTensorFlow optimizes the computation graph, pruning unused nodes, simplifying\\nexpressions (e.g., 1 + 2 would get replaced with 3), and more. Once the optimized\\ngraph is ready, the TF Function efficiently executes the operations in the graph, in the\\nappropriate order (and in parallel when it can). As a result, a TF Function will usually\\nrun much faster than the original Python function, especially if it performs complex\\nTensorFlow Functions and Graphs \\n| \\n397'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 423}, page_content='14 However, in this trivial example, the computation graph is so small that there is nothing at all to optimize, so\\ntf_cube() actually runs much slower than cube().\\ncomputations.14 Most of the time you will not really need to know more than that:\\nwhen you want to boost a Python function, just transform it into a TF Function.\\nThat’s all!\\nMoreover, when you write a custom loss function, a custom metric, a custom layer or\\nany other custom function, and you use it in a Keras model (as we did throughout\\nthis chapter), Keras automatically converts your function into a TF Function, no need\\nto use tf.function(). So most of the time, all this magic is 100% transparent.\\nYou can tell Keras not to convert your Python functions to TF\\nFunctions by setting dynamic=True when creating a custom layer\\nor a custom model. Alternatively, you can set run_eagerly=True\\nwhen calling the model’s compile() method.\\nTF Function generates a new graph for every unique set of input shapes and data\\ntypes, and it caches it for subsequent calls. For example, if you call tf_cube(tf.con\\nstant(10)), a graph will be generated for int32 tensors of shape []. Then if you call\\ntf_cube(tf.constant(20)), the same graph will be reused. But if you then call\\ntf_cube(tf.constant([10, 20])), a new graph will be generated for int32 tensors\\nof shape [2]. This is how TF Functions handle polymorphism (i.e., varying argument\\ntypes and shapes). However, this is only true for tensor arguments: if you pass numer‐\\nical Python values to a TF Function, a new graph will be generated for every distinct\\nvalue: for example, calling tf_cube(10) and tf_cube(20) will generate two graphs.\\nIf you call a TF Function many times with different numerical\\nPython values, then many graphs will be generated, slowing down\\nyour program and using up a lot of RAM. Python values should be\\nreserved for arguments that will have few unique values, such as\\nhyperparameters like the number of neurons per layer. This allows\\nTensorFlow to better optimize each variant of your model.\\nAutograph and Tracing\\nSo how does TensorFlow generate graphs? Well, first it starts by analyzing the Python\\nfunction’s source code to capture all the control flow statements, such as for loops\\nand while loops, if statements, as well as break, continue and return statements.\\nThis first step is called autograph. The reason TensorFlow has to analyze the source\\ncode is that Python does not provide any other way to capture control flow state‐\\nments: it offers magic methods like __add__() or __mul__() to capture operators like\\n398 \\n| \\nChapter 12: Custom Models and Training with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 424}, page_content='+ and *, but there are no __while__() or __if__() magic methods. After analyzing\\nthe function’s code, autograph outputs an upgraded version of that function in which\\nall the control flow statements are replaced by the appropriate TensorFlow opera‐\\ntions, such as tf.while_loop() for loops and tf.cond() for if statements. For\\nexample, in Figure 12-4, autograph analyzes the source code of the sum_squares()\\nPython function, and it generates the tf__sum_squares() function. In this function,\\nthe for loop is replaced by the definition of the loop_body() function (containing\\nthe body of the original for loop), followed by a call to the for_stmt() function. This\\ncall will build the appropriate tf.while_loop() operation in the computation graph.\\nFigure 12-4. How TensorFlow generates graphs using autograph and tracing\\nNext, TensorFlow calls this “upgraded” function, but instead of passing the actual\\nargument, it passes a symbolic tensor, meaning a tensor without any actual value, only\\na name, a data type, and a shape. For example, if you call sum_squares(tf.con\\nstant(10)), then the tf__sum_squares() function will actually be called with a sym‐\\nbolic tensor of type int32 and shape []. The function will run in graph mode, meaning\\nthat each TensorFlow operation will just add a node in the graph to represent itself\\nand its output tensor(s) (as opposed to the regular mode, called eager execution, or\\neager mode). In graph mode, TF operations do not perform any actual computations.\\nThis should feel familiar if you know TensorFlow 1, as graph mode was the default\\nmode. In Figure 12-4, you can see the tf__sum_squares() function being called with\\na symbolic tensor as argument (in this case, an int32 tensor of shape []), and the final\\ngraph generated during tracing. The ellipses represent operations, and the arrows\\nrepresent tensors (both the generated function and the graph are simplified).\\nTensorFlow Functions and Graphs \\n| \\n399'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 425}, page_content='To view the generated function’s source code, you can call tf.auto\\ngraph.to_code(sum_squares.python_function). The code is not\\nmeant to be pretty, but it can sometimes help for debugging.\\nTF Function Rules\\nMost of the time, converting a Python function that performs TensorFlow operations\\ninto a TF Function is trivial: just decorate it with @tf.function or let Keras take care\\nof it for you. However, there are a few rules to respect:\\n• If you call any external library, including NumPy or even the standard library,\\nthis call will run only during tracing, it will not be part of the graph. Indeed, a\\nTensorFlow graph can only include TensorFlow constructs (tensors, operations,\\nvariables, datasets, and so on). So make sure you use tf.reduce_sum() instead of\\nnp.sum(), and tf.sort() instead of the built-in sorted() function, and so on\\n(unless you really want the code to run only during tracing).\\n— For example, if you define a TF function f(x) that just returns np.ran\\ndom.rand(), a random number will only be generated when the function is\\ntraced, so f(tf.constant(2.)) and f(tf.constant(3.)) will return the\\nsame random number, but f(tf.constant([2., 3.])) will return a different\\none. If you replace np.random.rand() with tf.random.uniform([]), then a\\nnew random number will be generated upon every call, since the operation\\nwill be part of the graph.\\n— If your non-TensorFlow code has side-effects (such as logging something or\\nupdating a Python counter), then you should not expect that side-effect to\\noccur every time you call the TF Function, as it will only occur when the func‐\\ntion is traced.\\n— You can wrap arbitrary Python code in a tf.py_function() operation, but\\nthis will hinder performance, as TensorFlow will not be able to do any graph\\noptimization on this code, and it will also reduce portability, as the graph will\\nonly run on platforms where Python is available (and the right libraries\\ninstalled).\\n• You can call other Python functions or TF Functions, but they should follow the\\nsame rules, as TensorFlow will also capture their operations in the computation\\ngraph. Note that these other functions do not need to be decorated with\\n@tf.function.\\n• If the function creates a TensorFlow variable (or any other stateful TensorFlow\\nobject, such as a dataset or a queue), it must do so upon the very first call, and\\nonly then, or else you will get an exception. It is usually preferable to create vari‐\\nables outside of the TF Function (e.g., in the build() method of a custom layer).\\n400 \\n| \\nChapter 12: Custom Models and Training with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 426}, page_content='• The source code of your Python function should be available to TensorFlow. If\\nthe source code is unavailable (for example, if you define your function in the\\nPython shell, which does not give access to the source code, or if you deploy only\\nthe compiled Python files *.pyc to production), then the graph generation pro‐\\ncess will fail or have limited functionality.\\n• TensorFlow will only capture for loops that iterate over a tensor or a Dataset. So\\nmake sure you use for i in tf.range(10) rather than for i in range(10), or\\nelse the loop will not be captured in the graph. Instead, it will run during tracing.\\nThis may be what you want, if the for loop is meant to build the graph, for exam‐\\nple to create each layer in a neural network.\\n• And as always, for performance reasons, you should prefer a vectorized imple‐\\nmentation whenever you can, rather than using loops.\\nIt’s time to sum up! In this chapter we started with a brief overview of TensorFlow,\\nthen we looked at TensorFlow’s low-level API, including tensors, operations, variables\\nand special data structures. We then used these tools to customize almost every com‐\\nponent in tf.keras. Finally, we looked at how TF Functions can boost performance,\\nhow graphs are generated using autograph and tracing, and what rules to follow when\\nyou write TF Functions (if you would like to open the black box a bit further, for\\nexample to explore the generated graphs, you will find further technical details\\nin ???).\\nIn the next chapter, we will look at how to efficiently load and preprocess data with\\nTensorFlow.\\nTensorFlow Functions and Graphs \\n| \\n401'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 427}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 428}, page_content='CHAPTER 13\\nLoading and Preprocessing Data with\\nTensorFlow\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 13 in the final\\nrelease of the book.\\nSo far we have used only datasets that fit in memory, but Deep Learning systems are\\noften trained on very large datasets that will not fit in RAM. Ingesting a large dataset\\nand preprocessing it efficiently can be tricky to implement with other Deep Learning\\nlibraries, but TensorFlow makes it easy thanks to the Data API: you just create a data‐\\nset object, tell it where to get the data, then transform it in any way you want, and\\nTensorFlow takes care of all the implementation details, such as multithreading,\\nqueuing, batching, prefetching, and so on.\\nOff the shelf, the Data API can read from text files (such as CSV files), binary files\\nwith fixed-size records, and binary files that use TensorFlow’s TFRecord format,\\nwhich supports records of varying sizes. TFRecord is a flexible and efficient binary\\nformat based on Protocol Buffers (an open source binary format). The Data API also\\nhas support for reading from SQL databases. Moreover, many Open Source exten‐\\nsions are available to read from all sorts of data sources, such as Google’s BigQuery\\nservice.\\nHowever, reading huge datasets efficiently is not the only difficulty: the data also\\nneeds to be preprocessed. Indeed, it is not always composed strictly of convenient\\nnumerical fields: sometimes there will be text features, categorical features, and so on.\\nTo handle this, TensorFlow provides the Features API: it lets you easily convert these\\nfeatures to numerical features that can be consumed by your neural network. For\\n403'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 429}, page_content='example, categorical features with a large number of categories (such as cities, or\\nwords) can be encoded using embeddings (as we will see, an embedding is a trainable\\ndense vector that represents a category).\\nBoth the Data API and the Features API work seamlessly with\\ntf.keras.\\nIn this chapter, we will cover the Data API, the TFRecord format and the Features\\nAPI in detail. We will also take a quick look at a few related projects from Tensor‐\\nFlow’s ecosystem:\\n• TF Transform (tf.Transform) makes it possible to write a single preprocessing\\nfunction that can be run both in batch mode on your full training set, before\\ntraining (to speed it up), and then exported to a TF Function and incorporated\\ninto your trained model, so that once it is deployed in production, it can take\\ncare of preprocessing new instances on the fly.\\n• TF Datasets (TFDS) provides a convenient function to download many common\\ndatasets of all kinds, including large ones like ImageNet, and it provides conve‐\\nnient dataset objects to manipulate them using the Data API.\\nSo let’s get started!\\nThe Data API\\nThe whole Data API revolves around the concept of a dataset: as you might suspect,\\nthis represents a sequence of data items. Usually you will use datasets that gradually\\nread data from disk, but for simplicity let’s just create a dataset entirely in RAM using\\ntf.data.Dataset.from_tensor_slices():\\n>>> X = tf.range(10)  # any data tensor\\n>>> dataset = tf.data.Dataset.from_tensor_slices(X)\\n>>> dataset\\n<TensorSliceDataset shapes: (), types: tf.int32>\\nThe from_tensor_slices() function takes a tensor and creates a tf.data.Dataset\\nwhose elements are all the slices of X (along the first dimension), so this dataset con‐\\ntains 10 items: tensors 0, 1, 2, …, 9. In this case we would have obtained the same\\ndataset if we had used tf.data.Dataset.range(10).\\nYou can simply iterate over a dataset’s items like this:\\n>>> for item in dataset:\\n...     print(item)\\n404 \\n| \\nChapter 13: Loading and Preprocessing Data with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 430}, page_content='...\\ntf.Tensor(0, shape=(), dtype=int32)\\ntf.Tensor(1, shape=(), dtype=int32)\\ntf.Tensor(2, shape=(), dtype=int32)\\n[...]\\ntf.Tensor(9, shape=(), dtype=int32)\\nChaining Transformations\\nOnce you have a dataset, you can apply all sorts of transformations to it by calling its\\ntransformation methods. Each method returns a new dataset, so you can chain trans‐\\nformations like this (this chain is illustrated in Figure 13-1):\\n>>> dataset = dataset.repeat(3).batch(7)\\n>>> for item in dataset:\\n...     print(item)\\n...\\ntf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\\ntf.Tensor([8 9], shape=(2,), dtype=int32)\\nFigure 13-1. Chaining Dataset Transformations\\nIn this example, we first call the repeat() method on the original dataset, and it\\nreturns a new dataset that will repeat the items of the original dataset 3 times. Of\\ncourse, this will not copy the whole data in memory 3 times! In fact, if you call this\\nmethod with no arguments, the new dataset will repeat the source dataset forever.\\nThen we call the batch() method on this new dataset, and again this creates a new\\ndataset. This one will group the items of the previous dataset in batches of 7 items.\\nFinally, we iterate over the items of this final dataset. As you can see, the batch()\\nmethod had to output a final batch of size 2 instead of 7, but you can call it with\\ndrop_remainder=True if you want it to drop this final batch so that all batches have\\nthe exact same size.\\nThe Data API \\n| \\n405'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 431}, page_content='The dataset methods do not modify datasets, they create new ones,\\nso make sure to keep a reference to these new datasets (e.g., data\\nset = ...), or else nothing will happen.\\nYou can also apply any transformation you want to the items by calling the map()\\nmethod. For example, this creates a new dataset with all items doubled:\\n>>> dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]\\nThis function is the one you will call to apply any preprocessing you want to your\\ndata. Sometimes, this will include computations that can be quite intensive, such as\\nreshaping or rotating an image, so you will usually want to spawn multiple threads to\\nspeed things up: it’s as simple as setting the num_parallel_calls argument.\\nWhile the map() applies a transformation to each item, the apply() method applies a\\ntransformation to the dataset as a whole. For example, the following code “unbatches”\\nthe dataset, by applying the unbatch() function to the dataset (this function is cur‐\\nrently experimental, but it will most likely move to the core API in a future release).\\nEach item in the new dataset will be a single integer tensor instead of a batch of 7\\nintegers:\\n>>> dataset = dataset.apply(tf.data.experimental.unbatch()) # Items: 0,2,4,...\\nIt is also possible to simply filter the dataset using the filter() method:\\n>>> dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...\\nYou will often want to look at just a few items from a dataset. You can use the take()\\nmethod for that:\\n>>> for item in dataset.take(3):\\n...     print(item)\\n...\\ntf.Tensor(0, shape=(), dtype=int64)\\ntf.Tensor(2, shape=(), dtype=int64)\\ntf.Tensor(4, shape=(), dtype=int64)\\nShuffling the Data\\nAs you know, Gradient Descent works best when the instances in the training set are\\nindependent and identically distributed (see Chapter 4). A simple way to ensure this\\nis to shuffle the instances. For this, you can just use the shuffle() method. It will\\ncreate a new dataset that will start by filling up a buffer with the first items of the\\nsource dataset, then whenever it is asked for an item, it will pull one out randomly\\nfrom the buffer, and replace it with a fresh one from the source dataset, until it has\\niterated entirely through the source dataset. At this point it continues to pull out\\nitems randomly from the buffer until it is empty. You must specify the buffer size, and\\n406 \\n| \\nChapter 13: Loading and Preprocessing Data with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 432}, page_content='1 Imagine a sorted deck of cards on your left: suppose you just take the top 3 cards and shuffle them, then pick\\none randomly and put it to your right, keeping the other 2 in your hands. Take another card on your left,\\nshuffle the 3 cards in your hands and pick one of them randomly, and put it on your right. When you are\\ndone going through all the cards like this, you will have a deck of cards on your right: do you think it will be\\nperfectly shuffled?\\nit is important to make it large enough or else shuffling will not be very efficient.1\\nHowever, obviously do not exceed the amount of RAM you have, and even if you\\nhave plenty of it, there’s no need to go well beyond the dataset’s size. You can provide\\na random seed if you want the same random order every time you run your program.\\n>>> dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\\n>>> dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\\n>>> for item in dataset:\\n...     print(item)\\n...\\ntf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\\ntf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\\ntf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\\ntf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\\ntf.Tensor([3 6], shape=(2,), dtype=int64)\\nIf you call repeat() on a shuffled dataset, by default it will generate\\na new order at every iteration. This is generally a good idea, but if\\nyou prefer to reuse the same order at each iteration (e.g., for tests\\nor debugging), you can set reshuffle_each_iteration=False.\\nFor a large dataset that does not fit in memory, this simple shuffling-buffer approach\\nmay not be sufficient, since the buffer will be small compared to the dataset. One sol‐\\nution is to shuffle the source data itself (for example, on Linux you can shuffle text\\nfiles using the shuf command). This will definitely improve shuffling a lot! However,\\neven if the source data is shuffled, you will usually want to shuffle it some more, or\\nelse the same order will be repeated at each epoch, and the model may end up being\\nbiased (e.g., due to some spurious patterns present by chance in the source data’s\\norder). To shuffle the instances some more, a common approach is to split the source\\ndata into multiple files, then read them in a random order during training. However,\\ninstances located in the same file will still end up close to each other. To avoid this\\nyou can pick multiple files randomly, and read them simultaneously, interleaving\\ntheir lines. Then on top of that you can add a shuffling buffer using the shuffle()\\nmethod. If all this sounds like a lot of work, don’t worry: the Data API actually makes\\nall this possible in just a few lines of code. Let’s see how to do this.\\nThe Data API \\n| \\n407'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 433}, page_content=\"Interleaving Lines From Multiple Files\\nFirst, let’s suppose that you loaded the California housing dataset, you shuffled it\\n(unless it was already shuffled), you split it into a training set, a validation set and a\\ntest set, then you split each set into many CSV files that each look like this (each row\\ncontains 8 input features plus the target median house value):\\nMedInc,HouseAge,AveRooms,AveBedrms,Popul,AveOccup,Lat,Long,MedianHouseValue\\n3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442\\n5.3275,5.0,6.4900,0.9910,3464.0,3.4433,33.69,-117.39,1.687\\n3.1,29.0,7.5423,1.5915,1328.0,2.2508,38.44,-122.98,1.621\\n[...]\\nLet’s also suppose train_filepaths contains the list of file paths (and you also have\\nvalid_filepaths and test_filepaths):\\n>>> train_filepaths\\n['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv',...]\\nNow let’s create a dataset containing only these file paths:\\nfilepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\\nBy default, the list_files() function returns a dataset that shuffles the file paths. In\\ngeneral this is a good thing, but you can set shuffle=False if you do not want that,\\nfor some reason.\\nNext, we can call the interleave() method to read from 5 files at a time and inter‐\\nleave their lines (skipping the first line of each file, which is the header row, using the\\nskip() method):\\nn_readers = 5\\ndataset = filepath_dataset.interleave(\\n    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\\n    cycle_length=n_readers)\\nThe interleave() method will create a dataset that will pull 5 file paths from the\\nfilepath_dataset, and for each one it will call the function we gave it (a lambda in\\nthis example) to create a new dataset, in this case a TextLineDataset. It will then\\ncycle through these 5 datasets, reading one line at a time from each until all datasets\\nare out of items. Then it will get the next 5 file paths from the filepath_dataset, and\\ninterleave them the same way, and so on until it runs out of file paths.\\nFor interleaving to work best, it is preferable to have files of identi‐\\ncal length, or else the end of the longest files will not be interleaved.\\n408 \\n| \\nChapter 13: Loading and Preprocessing Data with TensorFlow\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 434}, page_content=\"By default, interleave() does not use parallelism, it just reads one line at a time\\nfrom each file, sequentially. However, if you want it to actually read files in parallel,\\nyou can set the num_parallel_calls argument to the number of threads you want.\\nYou can even set it to tf.data.experimental.AUTOTUNE to make TensorFlow choose\\nthe right number of threads dynamically based on the available CPU (however, this is\\nan experimental feature for now). Let’s look at what the dataset contains now:\\n>>> for line in dataset.take(5):\\n...     print(line.numpy())\\n...\\nb'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'\\nb'4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215'\\nb'3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625'\\nb'3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526'\\nb'3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442'\\nThese are the first rows (ignoring the header row) of 5 CSV files, chosen randomly.\\nLooks good! But as you can see, these are just byte strings, we need to parse them,\\nand also scale the data.\\nPreprocessing the Data\\nLet’s implement a small function that will perform this preprocessing:\\nX_mean, X_std = [...] # mean and scale of each feature in the training set\\nn_inputs = 8\\ndef preprocess(line):\\n  defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\\n  fields = tf.io.decode_csv(line, record_defaults=defs)\\n  x = tf.stack(fields[:-1])\\n  y = tf.stack(fields[-1:])\\n  return (x - X_mean) / X_std, y\\nLet’s walk through this code:\\n• First, we assume that you have precomputed the mean and standard deviation of\\neach feature in the training set. X_mean and X_std are just 1D tensors (or NumPy\\narrays) containing 8 floats, one per input feature.\\n• The preprocess() function takes one CSV line, and starts by parsing it. For this,\\nit uses the tf.io.decode_csv() function, which takes two arguments: the first is\\nthe line to parse, and the second is an array containing the default value for each\\ncolumn in the CSV file. This tells TensorFlow not only the default value for each\\ncolumn, but also the number of columns and the type of each column. In this\\nexample, we tell it that all feature columns are floats and missing values should\\ndefault to 0, but we provide an empty array of type tf.float32 as the default\\nvalue for the last column (the target): this tells TensorFlow that this column con‐\\nThe Data API \\n| \\n409\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 435}, page_content=\"tains floats, but that there is no default value, so it will raise an exception if it\\nencounters a missing value.\\n• The decode_csv() function returns a list of scalar tensors (one per column) but\\nwe need to return 1D tensor arrays. So we call tf.stack() on all tensors except\\nfor the last one (the target): this will stack these tensors into a 1D array. We then\\ndo the same for the target value (this makes it a 1D tensor array with a single\\nvalue, rather than a scalar tensor).\\n• Finally, we scale the input features by subtracting the feature means and then\\ndividing by the feature standard deviations, and we return a tuple containing the\\nscaled features and the target.\\nLet’s test this preprocessing function:\\n>>> preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')\\n(<tf.Tensor: id=6227, shape=(8,), dtype=float32, numpy=\\n array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\\n        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\\n <tf.Tensor: [...], numpy=array([2.782], dtype=float32)>)\\nWe can now apply this preprocessing function to the dataset.\\nPutting Everything Together\\nTo make the code reusable, let’s put together everything we have discussed so far into\\na small helper function: it will create and return a dataset that will efficiently load Cal‐\\nifornia housing data from multiple CSV files, then shuffle it, preprocess it and batch it\\n(see Figure 13-2):\\ndef csv_reader_dataset(filepaths, repeat=None, n_readers=5,\\n                       n_read_threads=None, shuffle_buffer_size=10000,\\n                       n_parse_threads=5, batch_size=32):\\n    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\\n    dataset = dataset.interleave(\\n        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\\n        cycle_length=n_readers, num_parallel_calls=n_read_threads)\\n    dataset = dataset.shuffle(shuffle_buffer_size)\\n    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\\n    dataset = dataset.batch(batch_size)\\n    return dataset.prefetch(1)\\n410 \\n| \\nChapter 13: Loading and Preprocessing Data with TensorFlow\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 436}, page_content='2 In general, just prefetching one batch is fine, but in some cases you may need to prefetch a few more. Alterna‐\\ntively, you can let TensorFlow decide automatically by passing tf.data.experimental.AUTOTUNE (this is an\\nexperimental feature for now).\\nFigure 13-2. Loading and Preprocessing Data From Multiple CSV Files\\nEverything should make sense in this code, except the very last line (prefetch(1)),\\nwhich is actually quite important for performance.\\nPrefetching\\nBy calling prefetch(1) at the end, we are creating a dataset that will do its best to\\nalways be one batch ahead2. In other words, while our training algorithm is working\\non one batch, the dataset will already be working in parallel on getting the next batch\\nready. This can improve performance dramatically, as is illustrated on Figure 13-3. If\\nwe also ensure that loading and preprocessing are multithreaded (by setting num_par\\nallel_calls when calling interleave() and map()), we can exploit multiple cores\\non the CPU and hopefully make preparing one batch of data shorter than running a\\ntraining step on the GPU: this way the GPU will be almost 100% utilized (except for\\nthe data transfer time from the CPU to the GPU), and training will run much faster.\\nThe Data API \\n| \\n411'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 437}, page_content='Figure 13-3. Speedup Training Thanks to Prefetching and Multithreading\\nIf you plan to purchase a GPU card, its processing power and its\\nmemory size are of course very important (in particular, a large\\nRAM is crucial for computer vision), but its memory bandwidth is\\njust as important as the processing power to get good performance:\\nthis is the number of gigabytes of data it can get in or out of its\\nRAM per second.\\nWith that, you can now build efficient input pipelines to load and preprocess data\\nfrom multiple text files. We have discussed the most common dataset methods, but\\nthere are a few more you may want to look at: concatenate(), zip(), window(),\\nreduce(), cache(), shard(), flat_map() and padded_batch(). There are also a cou‐\\nple more class methods: from_generator() and from_tensors(), which create a new\\ndataset from a Python generator or a list of tensors respectively. Please check the API\\ndocumentation for more details. Also note that there are experimental features avail‐\\nable in tf.data.experimental, many of which will most likely make it to the core\\nAPI in future releases (e.g., check out the CsvDataset class and the SqlDataset\\nclasses).\\n412 \\n| \\nChapter 13: Loading and Preprocessing Data with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 438}, page_content='3 Support for datasets is specific to tf.keras, it will not work on other implementations of the Keras API.\\n4 The number of steps per epoch is optional if the dataset just goes through the data once, but if you do not\\nspecify it, the progress bar will not be displayed during the first epoch.\\n5 Note that for now the dataset must be created within the TF Function. This may be fixed by the time you read\\nthese lines (see TensorFlow issue #25414).\\nUsing the Dataset With tf.keras\\nNow we can use the csv_reader_dataset() function to create a dataset for the train‐\\ning set (ensuring it repeats the data forever), the validation set and the test set:\\ntrain_set = csv_reader_dataset(train_filepaths, repeat=None)\\nvalid_set = csv_reader_dataset(valid_filepaths)\\ntest_set = csv_reader_dataset(test_filepaths)\\nAnd now we can simply build and train a Keras model using these datasets.3 All we\\nneed to do is to call the fit() method with the datasets instead of X_train and\\ny_train, and specify the number of steps per epoch for each set:4\\nmodel = keras.models.Sequential([...])\\nmodel.compile([...])\\nmodel.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,\\n          validation_data=valid_set,\\n          validation_steps=len(X_valid) // batch_size)\\nSimilarly, we can pass a dataset to the evaluate() and predict() methods (and again\\nspecify the number of steps per epoch):\\nmodel.evaluate(test_set, steps=len(X_test) // batch_size)\\nmodel.predict(new_set, steps=len(X_new) // batch_size)\\nUnlike the other sets, the new_set will usually not contain labels (if it does, Keras will\\njust ignore them). Note that in all these cases, you can still use NumPy arrays instead\\nof datasets if you want (but of course they need to have been loaded and preprocessed\\nfirst).\\nIf you want to build your own custom training loop (as in Chapter 12), you can just\\niterate over the training set, very naturally:\\nfor X_batch, y_batch in train_set:\\n    [...] # perform one gradient descent step\\nIn fact, it is even possible to create a tf.function (see Chapter 12) that performs the\\nwhole training loop!5\\n@tf.function\\ndef train(model, optimizer, loss_fn, n_epochs, [...]):\\n    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, [...])\\n    for X_batch, y_batch in train_set:\\n        with tf.GradientTape() as tape:\\nThe Data API \\n| \\n413'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 439}, page_content='y_pred = model(X_batch)\\n            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\\n            loss = tf.add_n([main_loss] + model.losses)\\n        grads = tape.gradient(loss, model.trainable_variables)\\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\\nCongratulations, you now know how to build powerful input pipelines using the Data\\nAPI! However, so far we have used CSV files, which are common, simple and conve‐\\nnient, but they are not really efficient, and they do not support large or complex data\\nstructures very well, such as images or audio. So let’s use TFRecords instead.\\nIf you are happy with CSV files (or whatever other format you are\\nusing), you do not have to use TFRecords. As the saying goes, if it\\nain’t broke, don’t fix it! TFRecords are useful when the bottleneck\\nduring training is loading and parsing the data.\\nThe TFRecord Format\\nThe TFRecord format is TensorFlow’s preferred format for storing large amounts of\\ndata and reading it efficiently. It is a very simple binary format that just contains a\\nsequence of binary records of varying sizes (each record just has a length, a CRC\\nchecksum to check that the length was not corrupted, then the actual data, and finally\\na CRC checksum for the data). You can easily create a TFRecord file using the\\ntf.io.TFRecordWriter class:\\nwith tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\\n    f.write(b\"This is the first record\")\\n    f.write(b\"And this is the second record\")\\nAnd you can then use a tf.data.TFRecordDataset to read one or more TFRecord\\nfiles:\\nfilepaths = [\"my_data.tfrecord\"]\\ndataset = tf.data.TFRecordDataset(filepaths)\\nfor item in dataset:\\n    print(item)\\nThis will output:\\ntf.Tensor(b\\'This is the first record\\', shape=(), dtype=string)\\ntf.Tensor(b\\'And this is the second record\\', shape=(), dtype=string)\\nBy default, a TFRecordDataset will read files one by one, but you\\ncan make it read multiple files in parallel and interleave their\\nrecords by setting num_parallel_reads. Alternatively, you could\\nobtain the same result by using list_files() and interleave()\\nas we did earlier to read multiple CSV files.\\n414 \\n| \\nChapter 13: Loading and Preprocessing Data with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 440}, page_content='6 Since protobuf objects are meant to be serialized and transmitted, they are called messages.\\nCompressed TFRecord Files\\nIt can sometimes be useful to compress your TFRecord files, especially if they need to\\nbe loaded via a network connection. You can create a compressed TFRecord file by\\nsetting the options argument:\\noptions = tf.io.TFRecordOptions(compression_type=\"GZIP\")\\nwith tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\\n  [...]\\nWhen reading a compressed TFRecord file, you need to specify the compression type:\\ndataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\\n                                  compression_type=\"GZIP\")\\nA Brief Introduction to Protocol Buffers\\nEven though each record can use any binary format you want, TFRecord files usually\\ncontain serialized Protocol Buffers (also called protobufs). This is a portable, extensi‐\\nble and efficient binary format developed at Google back in 2001 and Open Sourced\\nin 2008, and they are now widely used, in particular in gRPC, Google’s remote proce‐\\ndure call system. Protocol Buffers are defined using a simple language that looks like\\nthis:\\nsyntax = \"proto3\";\\nmessage Person {\\n  string name = 1;\\n  int32 id = 2;\\n  repeated string email = 3;\\n}\\nThis definition says we are using the protobuf format version 3, and it specifies that\\neach Person object6 may (optionally) have a name of type string, an id of type int32,\\nand zero or more email fields, each of type string. The numbers 1, 2 and 3 are the\\nfield identifiers: they will be used in each record’s binary representation. Once you\\nhave a definition in a .proto file, you can compile it. This requires protoc, the proto‐\\nbuf compiler, to generate access classes in Python (or some other language). Note that\\nthe protobuf definitions we will use have already been compiled for you, and their\\nPython classes are part of TensorFlow, so you will not need to use protoc. All you\\nneed to know is how to use protobuf access classes in Python. To illustrate the basics,\\nlet’s look at a simple example that uses the access classes generated for the Person\\nprotobuf (the code is explained in the comments):\\n>>> from person_pb2 import Person  # import the generated access class\\n>>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\"])  # create a Person\\n>>> print(person)  # display the Person\\nThe TFRecord Format \\n| \\n415'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 441}, page_content='7 This chapter contains the bare minimum you need to know about protobufs to use TFRecords. To learn more\\nabout protobufs, please visit https://homl.info/protobuf.\\nname: \"Al\"\\nid: 123\\nemail: \"a@b.com\"\\n>>> person.name  # read a field\\n\"Al\"\\n>>> person.name = \"Alice\"  # modify a field\\n>>> person.email[0]  # repeated fields can be accessed like arrays\\n\"a@b.com\"\\n>>> person.email.append(\"c@d.com\")  # add an email address\\n>>> s = person.SerializeToString()  # serialize the object to a byte string\\n>>> s\\nb\\'\\\\n\\\\x05Alice\\\\x10{\\\\x1a\\\\x07a@b.com\\\\x1a\\\\x07c@d.com\\'\\n>>> person2 = Person()  # create a new Person\\n>>> person2.ParseFromString(s)  # parse the byte string (27 bytes long)\\n27\\n>>> person == person2  # now they are equal\\nTrue\\nIn short, we import the Person class generated by protoc, we create an instance and\\nwe play with it, visualizing it, reading and writing some fields, then we serialize it\\nusing the SerializeToString() method. This is the binary data that is ready to be\\nsaved or transmitted over the network. When reading or receiving this binary data,\\nwe can parse it using the ParseFromString() method, and we get a copy of the object\\nthat was serialized.7\\nWe could save the serialized Person object to a TFRecord file, then we could load and\\nparse it: everything would work fine. However, SerializeToString() and ParseFrom\\nString() are not TensorFlow operations (and neither are the other operations in this\\ncode), so they cannot be included in a TensorFlow Function (except by wrapping\\nthem in a tf.py_function() operation, which would make the code slower and less\\nportable, as we saw in Chapter 12). Fortunately, TensorFlow does include special pro‐\\ntobuf definitions for which it provides parsing operations.\\nTensorFlow Protobufs\\nThe main protobuf typically used in a TFRecord file is the Example protobuf, which\\nrepresents one instance in a dataset. It contains a list of named features, where each\\nfeature can either be a list of byte strings, a list of floats or a list of integers. Here is the\\nprotobuf definition:\\nsyntax = \"proto3\";\\nmessage BytesList { repeated bytes value = 1; }\\nmessage FloatList { repeated float value = 1 [packed = true]; }\\nmessage Int64List { repeated int64 value = 1 [packed = true]; }\\n416 \\n| \\nChapter 13: Loading and Preprocessing Data with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 442}, page_content='8 Why was Example even defined since it contains no more than a Features object? Well, TensorFlow may one\\nday decide to add more fields to it. As long as the new Example definition still contains the features field,\\nwith the same id, it will be backward compatible. This extensibility is one of the great features of protobufs.\\nmessage Feature {\\n    oneof kind {\\n        BytesList bytes_list = 1;\\n        FloatList float_list = 2;\\n        Int64List int64_list = 3;\\n    }\\n};\\nmessage Features { map<string, Feature> feature = 1; };\\nmessage Example { Features features = 1; };\\nThe definitions of BytesList, FloatList and Int64List are straightforward enough\\n([packed = true] is used for repeated numerical fields, for a more efficient encod‐\\ning). A Feature either contains a BytesList, a FloatList or an Int64List. A Fea\\ntures (with an s) contains a dictionary that maps a feature name to the\\ncorresponding feature value. And finally, an Example just contains a Features object.8\\nHere is how you could create a tf.train.Example representing the same person as\\nearlier, and write it to TFRecord file:\\nfrom tensorflow.train import BytesList, FloatList, Int64List\\nfrom tensorflow.train import Feature, Features, Example\\nperson_example = Example(\\n    features=Features(\\n        feature={\\n            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\\n            \"id\": Feature(int64_list=Int64List(value=[123])),\\n            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\",\\n                                                          b\"c@d.com\"]))\\n        }))\\nThe code is a bit verbose and repetitive, but it’s rather straightforward (and you could\\neasily wrap it inside a small helper function). Now that we have an Example protobuf,\\nwe can serialize it by calling its SerializeToString() method, then write the result‐\\ning data to a TFRecord file:\\nwith tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\\n    f.write(person_example.SerializeToString())\\nNormally you would write much more than just one example! Typically, you would\\ncreate a conversion script that reads from your current format (say, CSV files), creates\\nan Example protobuf for each instance, serializes them and saves them to several\\nTFRecord files, ideally shuffling them in the process. This requires a bit of work, so\\nonce again make sure it is really necessary (perhaps your pipeline works fine with\\nCSV files).\\nThe TFRecord Format \\n| \\n417'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 443}, page_content='Now that we have a nice TFRecord file containing a serialized Example, let’s try to\\nload it.\\nLoading and Parsing Examples\\nTo load the serialized Example protobufs, we will use a tf.data.TFRecordDataset\\nonce again, and we will parse each Example using tf.io.parse_single_example().\\nThis is a TensorFlow operation so it can be included in a TF Function. It requires at\\nleast two arguments: a string scalar tensor containing the serialized data, and a\\ndescription of each feature. The description is a dictionary that maps each feature\\nname to either a tf.io.FixedLenFeature descriptor indicating the feature’s shape,\\ntype and default value, or a tf.io.VarLenFeature descriptor indicating only the type\\n(if the length may vary, such as for the \"emails\" feature). For example:\\nfeature_description = {\\n    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\\n    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\\n    \"emails\": tf.io.VarLenFeature(tf.string),\\n}\\nfor serialized_example in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]):\\n    parsed_example = tf.io.parse_single_example(serialized_example,\\n                                                feature_description)\\nThe fixed length features are parsed as regular tensors, but the variable length fea‐\\ntures are parsed as sparse tensors. You can convert a sparse tensor to a dense tensor\\nusing tf.sparse.to_dense(), but in this case it is simpler to just access its values:\\n>>> tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")\\n<tf.Tensor: [...] dtype=string, numpy=array([b\\'a@b.com\\', b\\'c@d.com\\'], [...])>\\n>>> parsed_example[\"emails\"].values\\n<tf.Tensor: [...] dtype=string, numpy=array([b\\'a@b.com\\', b\\'c@d.com\\'], [...])>\\nA BytesList can contain any binary data you want, including any serialized object.\\nFor example, you can use tf.io.encode_jpeg() to encode an image using the JPEG\\nformat, and put this binary data in a BytesList. Later, when your code reads the\\nTFRecord, it will start by parsing the Example, then you will need to call\\ntf.io.decode_jpeg() to parse the data and get the original image (or you can use\\ntf.io.decode_image(), which can decode any BMP, GIF, JPEG or PNG image). You\\ncan also store any tensor you want in a BytesList by serializing the tensor using\\ntf.io.serialize_tensor(), then putting the resulting byte string in a BytesList\\nfeature. Later, when you parse the TFRecord, you can parse this data using\\ntf.io.parse_tensor().\\nInstead of parsing examples one by one using tf.io.parse_single_example(), you\\nmay want to parse them batch by batch using tf.io.parse_example():\\n418 \\n| \\nChapter 13: Loading and Preprocessing Data with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 444}, page_content='dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(10)\\nfor serialized_examples in dataset:\\n    parsed_examples = tf.io.parse_example(serialized_examples,\\n                                          feature_description)\\nAs you can see, the Example proto will probably be sufficient for most use cases.\\nHowever, it may be a bit cumbersome to use when you are dealing with lists of lists.\\nFor example, suppose you want to classify text documents. Each document may be\\nrepresented as a list of sentences, where each sentence is represented as a list of\\nwords. And perhaps each document also has a list of comments, where each com‐\\nment is also represented as a list of words. Moreover, there may be some contextual\\ndata as well, such as the document’s author, title and publication date. TensorFlow’s\\nSequenceExample protobuf is designed for such use cases.\\nHandling Lists of Lists Using the SequenceExample Protobuf\\nHere is the definition of the SequenceExample protobuf:\\nmessage FeatureList { repeated Feature feature = 1; };\\nmessage FeatureLists { map<string, FeatureList> feature_list = 1; };\\nmessage SequenceExample {\\n    Features context = 1;\\n    FeatureLists feature_lists = 2;\\n};\\nA SequenceExample contains a Features object for the contextual data and a Fea\\ntureLists object which contains one or more named FeatureList objects (e.g., a\\nFeatureList named \"content\" and another named \"comments\"). Each FeatureList\\njust contains a list of Feature objects, each of which may be a list of byte strings, a list\\nof 64-bit integers or a list of floats (in this example, each Feature would represent a\\nsentence or a comment, perhaps in the form of a list of word identifiers). Building a\\nSequenceExample, serializing it and parsing it is very similar to building, serializing\\nand parsing an Example, but you must use tf.io.parse_single_sequence_exam\\nple() to parse a single SequenceExample or tf.io.parse_sequence_example() to\\nparse a batch, and both functions return a tuple containing the context features (as a\\ndictionary) and the feature lists (also as a dictionary). If the feature lists contain\\nsequences of varying sizes (as in the example above), you may want to convert them\\nto ragged tensors using tf.RaggedTensor.from_sparse() (see the notebook for the\\nfull code):\\nparsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\\n    serialized_sequence_example, context_feature_descriptions,\\n    sequence_feature_descriptions)\\nparsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"])\\nNow that you know how to efficiently store, load and parse data, the next step is to\\nprepare it so that it can be fed to a neural network. This means converting all features\\nThe TFRecord Format \\n| \\n419'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 445}, page_content='into numerical features (ideally not too sparse), scaling them, and more. In particular,\\nif your data contains categorical features or text features, they need to be converted to\\nnumbers. For this, the Features API can help.\\nThe Features API\\nPreprocessing your data can be performed in many ways: it can be done ahead of\\ntime when preparing your data files, using any tool you like. Or you can preprocess\\nyour data on the fly when loading it with the Data API (e.g., using the dataset’s map()\\nmethod, as we saw earlier). Or you can include a preprocessing layer directly in your\\nmodel. Whichever solution you prefer, the Features API can help you: it is a set of\\nfunctions available in the tf.feature_column package, which let you define how\\neach feature (or group of features) in your data should be preprocessed (therefore you\\ncan think of this API as the analog of Scikit-Learn’s ColumnTransformer class). We\\nwill start by looking at the different types of columns available, and then we will look\\nat how to use them.\\nLet’s go back to the variant of the California housing dataset that we used in Chap‐\\nter 2, since it includes a categorical feature and missing data. Here is a simple numeri‐\\ncal column named \"housing_median_age\":\\nhousing_median_age = tf.feature_column.numeric_column(\"housing_median_age\")\\nNumeric columns let you specify a normalization function using the normalizer_fn\\nargument. For example, let’s tweak the \"housing_median_age\" column to define how\\nit should be scaled. Note that this requires computing ahead of time the mean and\\nstandard deviation of this feature in the training set:\\nage_mean, age_std = X_mean[1], X_std[1]  # The median age is column in 1\\nhousing_median_age = tf.feature_column.numeric_column(\\n    \"housing_median_age\", normalizer_fn=lambda x: (x - age_mean) / age_std)\\nIn some cases, it might improve performance to bucketize some numerical features,\\neffectively transforming a numerical feature into a categorical feature. For example,\\nlet’s create a bucketized column based on the median_income column, with 5 buckets:\\nless than 1.5 ($15,000), then 1.5 to 3, 3 to 4.5, 4.5 to 6., and above 6. (notice that when\\nyou specify 4 boundaries, there are actually 5 buckets):\\nmedian_income = tf.feature_column.numeric_column(\"median_income\")\\nbucketized_income = tf.feature_column.bucketized_column(\\n    median_income, boundaries=[1.5, 3., 4.5, 6.])\\nIf the median_income feature is equal to, say, 3.2, then the bucketized_income feature\\nwill automatically be equal to 2 (i.e., the index of the corresponding income bucket).\\nChoosing the right boundaries can be somewhat of an art, but one approach is to just\\nuse percentiles of the data (e.g., the 10th percentile, the 20th percentile, and so on). If\\na feature is multimodal, meaning it has separate peaks in its distribution, you may\\n420 \\n| \\nChapter 13: Loading and Preprocessing Data with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 446}, page_content='want to define a bucket for each mode, placing the boundaries in between the peaks.\\nWhether you use the percentiles or the modes, you need to analyze the distribution of\\nyour data ahead of time, just like we had to measure the mean and standard deviation\\nahead of time to normalize the housing_median_age column.\\nCategorical Features\\nFor categorical features such as ocean_proximity, there are several options. If it is\\nalready represented as a category ID (i.e., an integer from 0 to the max ID), then you\\ncan use the categorical_column_with_identity() function (specifying the max\\nID). If not, and you know the list of all possible categories, then you can use categori\\ncal_column_with_vocabulary_list():\\nocean_prox_vocab = [\\'<1H OCEAN\\', \\'INLAND\\', \\'ISLAND\\', \\'NEAR BAY\\', \\'NEAR OCEAN\\']\\nocean_proximity = tf.feature_column.categorical_column_with_vocabulary_list(\\n    \"ocean_proximity\", ocean_prox_vocab)\\nIf you prefer to have TensorFlow load the vocabulary from a file, you can call catego\\nrical_column_with_vocabulary_file() instead. As you might expect, these two\\nfunctions will simply map each category to its index in the vocabulary (e.g., NEAR\\nBAY will be mapped to 3), and unknown categories will be mapped to -1.\\nFor categorical columns with a large vocabulary (e.g., for zipcodes, cities, words,\\nproducts, users, etc.), it may not be convenient to get the full list of possible cate‐\\ngories, or perhaps categories may be added or removed so frequently that using cate‐\\ngory indices would be too unreliable. In this case, you may prefer to use a\\ncategorical_column_with_hash_bucket(). If we had a \"city\" feature in the dataset,\\nwe could encode it like this:\\ncity_hash = tf.feature_column.categorical_column_with_hash_bucket(\\n    \"city\", hash_bucket_size=1000)\\nThis feature will compute a hash for each category (i.e., for each city), modulo the\\nnumber of hash buckets (hash_bucket_size). You must set the number of buckets\\nhigh enough to avoid getting too many collisions (i.e., different categories ending up\\nin the same bucket), but the higher you set it, the more RAM will be used (by the\\nembedding table, as we will see shortly).\\nCrossed Categorical Features\\nIf you suspect that two (or more) categorical features are more meaningful when used\\njointly, then you can create a crossed column. For example, suppose people are partic‐\\nularly fond of old houses inland and new houses near the ocean, then it might help to\\nThe Features API \\n| \\n421'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 447}, page_content='9 Since the housing_median_age feature was normalized, the boundaries are for normalized ages.\\ncreate a bucketized column for the housing_median_age feature9, and cross it with\\nthe ocean_proximity column. The crossed column will compute a hash of every age\\n& ocean proximity combination it comes across, modulo the hash_bucket_size, and\\nthis will give it the cross category ID. You may then choose to use only this crossed\\ncolumn in your model, or also include the individual columns.\\nbucketized_age = tf.feature_column.bucketized_column(\\n    housing_median_age, boundaries=[-1., -0.5, 0., 0.5, 1.]) # age was scaled\\nage_and_ocean_proximity = tf.feature_column.crossed_column(\\n    [bucketized_age, ocean_proximity], hash_bucket_size=100)\\nAnother common use case for crossed columns is to cross latitude and longitude into\\na single categorical feature: you start by bucketizing the latitude and longitude, for\\nexample into 20 buckets each, then you cross these bucketized features into a loca\\ntion column. This will create a 20×20 grid over California, and each cell in the grid\\nwill correspond to one category:\\nlatitude = tf.feature_column.numeric_column(\"latitude\")\\nlongitude = tf.feature_column.numeric_column(\"longitude\")\\nbucketized_latitude = tf.feature_column.bucketized_column(\\n    latitude, boundaries=list(np.linspace(32., 42., 20 - 1)))\\nbucketized_longitude = tf.feature_column.bucketized_column(\\n    longitude, boundaries=list(np.linspace(-125., -114., 20 - 1)))\\nlocation = tf.feature_column.crossed_column(\\n    [bucketized_latitude, bucketized_longitude], hash_bucket_size=1000)\\nEncoding Categorical Features Using One-Hot Vectors\\nNo matter which option you choose to build a categorical feature (categorical col‐\\numns, bucketized columns or crossed columns), it must be encoded before you can\\nfeed it to a neural network. There are two options to encode a categorical feature:\\none-hot vectors or embeddings. For the first option, simply use the indicator_col\\numn() function:\\nocean_proximity_one_hot = tf.feature_column.indicator_column(ocean_proximity)\\nA one-hot vector encoding has the size of the vocabulary length, which is fine if there\\nare just a few possible categories, but if the vocabulary is large, you will end up with\\ntoo many inputs fed to your neural network: it will have too many weights to learn\\nand it will probably not perform very well. In particular, this will typically be the case\\nwhen you use hash buckets. In this case, you should probably encode them using\\nembeddings instead.\\n422 \\n| \\nChapter 13: Loading and Preprocessing Data with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 448}, page_content='As a rule of thumb (but your mileage may vary!), if the number of\\ncategories is lower than 10, then one-hot encoding is generally the\\nway to go. If the number of categories is greater than 50 (which is\\noften the case when you use hash buckets), then embeddings are\\nusually preferable. In between 10 and 50 categories, you may want\\nto experiment with both options and see which one works best for\\nyour use case. Also, embeddings typically require more training\\ndata, unless you can reuse pretrained embeddings.\\nEncoding Categorical Features Using Embeddings\\nAn embedding is a trainable dense vector that represents a category. By default,\\nembeddings are initialized randomly, so for example the \"NEAR BAY\" category could\\nbe represented initially by a random vector such as [0.131, 0.890], while the \"NEAR\\nOCEAN\" category may be represented by another random vector such as [0.631,\\n0.791] (in this example, we are using 2D embeddings, but the number of dimensions\\nis a hyperparameter you can tweak). Since these embeddings are trainable, they will\\ngradually improve during training, and as they represent fairly similar categories,\\nGradient Descent will certainly end up pushing them closer together, while it will\\ntend to move them away from the \"INLAND\" category’s embedding (see Figure 13-4).\\nIndeed, the better the representation, the easier it will be for the neural network to\\nmake accurate predictions, so training tends to make embeddings useful representa‐\\ntions of the categories. This is called representation learning (we will see other types of\\nrepresentation learning in ???).\\nThe Features API \\n| \\n423'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 449}, page_content='10 “Distributed Representations of Words and Phrases and their Compositionality”, T. Mikolov et al. (2013).\\nFigure 13-4. Embeddings Will Gradually Improve During Training\\nWord Embeddings\\nNot only will embeddings generally be useful representations for the task at hand, but\\nquite often these same embeddings can be reused successfully for other tasks as well.\\nThe most common example of this is word embeddings (i.e., embeddings of individual\\nwords): when you are working on a natural language processing task, you are often\\nbetter off reusing pretrained word embeddings than training your own. The idea of\\nusing vectors to represent words dates back to the 1960s, and many sophisticated\\ntechniques have been used to generate useful vectors, including using neural net‐\\nworks, but things really took off in 2013, when Tomáš Mikolov and other Google\\nresearchers published a paper10 describing how to learn word embeddings using deep\\nneural networks, much faster than previous attempts. This allowed them to learn\\nembeddings on a very large corpus of text: they trained a deep neural network to pre‐\\ndict the words near any given word. This allowed them to obtain astounding word\\nembeddings. For example, synonyms had very close embeddings, and semantically\\nrelated words such as France, Spain, Italy, and so on, ended up clustered together. But\\nit’s not just about proximity: word embeddings were also organized along meaningful\\naxes in the embedding space. Here is a famous example: if you compute King – Man\\n+ Woman (adding and subtracting the embedding vectors of these words), then the\\nresult will be very close to the embedding of the word Queen (see Figure 13-5). In\\nother words, the word embeddings encode the concept of gender! Similarly, you can\\ncompute Madrid – Spain + France, and of course the result is close to Paris, which\\nseems to show that the notion of capital city was also encoded in the embeddings.\\n424 \\n| \\nChapter 13: Loading and Preprocessing Data with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 450}, page_content='Figure 13-5. Word Embeddings\\nLet’s go back to the Features API. Here is how you could encode the ocean_proxim\\nity categories as 2D embeddings:\\nocean_proximity_embed = tf.feature_column.embedding_column(ocean_proximity,\\n                                                           dimension=2)\\nEach of the five ocean_proximity categories will now be represented as a 2D vector.\\nThese vectors are stored in an embedding matrix with one row per category, and one\\ncolumn per embedding dimension, so in this example it is a 5×2 matrix. When an\\nembedding column is given a category index as input (say, 3, which corresponds to\\nthe category \"NEAR BAY\"), it just performs a lookup in the embedding matrix and\\nreturns the corresponding row (say, [0.331, 0.190]). Unfortunately, the embedding\\nmatrix can be quite large, especially when you have a large vocabulary: if this is the\\ncase, the model can only learn good representations for the categories for which it has\\nsufficient training data. To reduce the size of the embedding matrix, you can of\\ncourse try lowering the dimension hyperparameter, but if you reduce this parameter\\ntoo much, the representations may not be as good. Another option is to reduce the\\nvocabulary size (e.g., if you are dealing with text, you can try dropping the rare words\\nfrom the vocabulary, and replace them all with a token like \"<unknown>\" or \"<UNK>\").\\nIf you are using hash buckets, you can also try reducing the hash_bucket_size (but\\nnot too much, or else you will get collisions).\\nThe Features API \\n| \\n425'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 451}, page_content='If there are no pretrained embeddings that you can reuse for the\\ntask you are trying to tackle, and if you do not have enough train‐\\ning data to learn them, then you can try to learn them on some\\nauxiliary task for which it is easier to obtain plenty of training data.\\nAfter that, you can reuse the trained embeddings for your main\\ntask.\\nUsing Feature Columns for Parsing\\nLet’s suppose you have created feature columns for each of your input features, as well\\nas for the target. What can you do with them? Well, for one you can pass them to the\\nmake_parse_example_spec() function to generate feature descriptions (so you don’t\\nhave to do it manually, as we did earlier):\\ncolumns = [bucketized_age, ....., median_house_value] # all features + target\\nfeature_descriptions = tf.feature_column.make_parse_example_spec(columns)\\nYou don’t always have to create a separate feature column for each\\nand every feature. For example, instead of having 2 numerical fea‐\\nture columns, you could choose to have a single 2D column: just\\nset shape=[2] when calling numerical_column().\\nYou can then create a function that parses serialized examples using these feature\\ndescriptions, and separates the target column from the input features:\\ndef parse_examples(serialized_examples):\\n    examples = tf.io.parse_example(serialized_examples, feature_descriptions)\\n    targets = examples.pop(\"median_house_value\") # separate the targets\\n    return examples, targets\\nNext, you can create a TFRecordDataset that will read batches of serialized examples\\n(assuming the TFRecord file contains serialized Example protobufs with the appropri‐\\nate features):\\nbatch_size = 32\\ndataset = tf.data.TFRecordDataset([\"my_data_with_features.tfrecords\"])\\ndataset = dataset.repeat().shuffle(10000).batch(batch_size).map(parse_examples)\\nUsing Feature Columns in Your Models\\nFeature columns can also be used directly in your model, to convert all your input\\nfeatures into a single dense vector which the neural network can then process. For\\nthis, all you need to do is add a keras.layers.DenseFeatures layer as the first layer\\nin your model, passing it the list of feature columns (excluding the target column):\\ncolumns_without_target = columns[:-1]\\nmodel = keras.models.Sequential([\\n    keras.layers.DenseFeatures(feature_columns=columns_without_target),\\n426 \\n| \\nChapter 13: Loading and Preprocessing Data with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 452}, page_content='keras.layers.Dense(1)\\n])\\nmodel.compile(loss=\"mse\", optimizer=\"sgd\", metrics=[\"accuracy\"])\\nsteps_per_epoch = len(X_train) // batch_size\\nhistory = model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=5)\\nThe DenseFeatures layer will take care of converting every input feature to a dense\\nrepresentation, and it will also apply any extra transformation we specified, such as\\nscaling the housing_median_age using the normalizer_fn function we provided. You\\ncan take a closer look at what the DenseFeatures layer does by calling it directly:\\n>>> some_columns = [ocean_proximity_embed, bucketized_income]\\n>>> dense_features = keras.layers.DenseFeatures(some_columns)\\n>>> dense_features({\\n...     \"ocean_proximity\": [[\"NEAR OCEAN\"], [\"INLAND\"], [\"INLAND\"]],\\n...     \"median_income\": [[3.], [7.2], [1.]]\\n... })\\n...\\n<tf.Tensor: id=559790, shape=(3, 7), dtype=float32, numpy=\\narray([[ 0. , 0. , 1. , 0. , 0. ,-0.36277947 , 0.30109018],\\n       [ 0. , 0. , 0. , 0. , 1. , 0.22548223 , 0.33142096],\\n       [ 1. , 0. , 0. , 0. , 0. , 0.22548223 , 0.33142096]], dtype=float32)>\\nIn this example, we create a DenseFeatures layer with just two columns, and we call\\nit with some data, in the form of a dictionary of features. In this case, since the bucke\\ntized_income column relies on the median_income column, the dictionary must\\ninclude the \"median_income\" key, and similarly since the ocean_proximity_embed\\ncolumn is based on the ocean_proximity column, the dictionary must include the\\n\"ocean_proximity\" key. Columns are handled in alphabetical order, so first we look\\nat the bucketized income column (its name is the same as the median_income column\\nname, plus \"_bucketized\"). The incomes 3, 7.2 and 1 get mapped respectively to cat‐\\negory 2 (for incomes between 1.5 and 3), category 0 (for incomes below 1.5), and cat‐\\negory 4 (for incomes greater than 6). Then these category IDs get one-hot encoded:\\ncategory 2 gets encoded as [0., 0., 1., 0., 0.] and so on (note that bucketized\\ncolumns get one-hot encoded by default, no need to call indicator_column()). Now\\non to the ocean_proximity_embed column. The \"NEAR OCEAN\" and \"INLAND\" cate‐\\ngories just get mapped to their respective embeddings (which were initialized ran‐\\ndomly). The resulting tensor is the concatenation of the one-hot vectors and the\\nembeddings.\\nNow you can feed all kinds of features to a neural network, including numerical fea‐\\ntures, categorical features, and even text (by splitting the text into words, then using\\nword embedding)! However, performing all the preprocessing on the fly can slow\\ndown training. Let’s see how this can be improved.\\nThe Features API \\n| \\n427'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 453}, page_content='TF Transform\\nIf preprocessing is computationally expensive, then handling it before training rather\\nthan on the fly may give you a significant speedup: the data will be preprocessed just\\nonce per instance before training, rather than once per instance and per epoch during\\ntraining. Tools like Apache Beam let you run efficient data processing pipelines over\\nlarge amounts of data, even distributed across multiple servers, so why not use it to\\npreprocess all the training data? This works great and indeed can speed up training,\\nbut there is one problem: once your model is trained, suppose you want to deploy it\\nto a mobile app: you will need to write some code in your app to take care of prepro‐\\ncessing the data before it is fed to the model. And suppose you also want to deploy\\nthe model to TensorFlow.js so it runs in a web browser? Once again, you will need to\\nwrite some preprocessing code. This can become a maintenance nightmare: when‐\\never you want to change the preprocessing logic, you will need to update your Apache\\nBeam code, your mobile app code and your Javascript code. It is not only time con‐\\nsuming, but also error prone: you may end up with subtle differences between the\\npreprocessing operations performed before training and the ones performed in your\\napp or in the browser. This training/serving skew will lead to bugs or degraded perfor‐\\nmance.\\nOne improvement would be to take the trained model (trained on data that was pre‐\\nprocessed by your Apache Beam code), and before deploying it to your app or the\\nbrowser, add an extra input layer to take care of preprocessing on the fly (either by\\nwriting a custom layer or by using a DenseFeatures layer). That’s definitely better,\\nsince now you just have two versions of your preprocessing code: the Apache Beam\\ncode and the preprocessing layer’s code.\\nBut what if you could define your preprocessing operations just once? This is what\\nTF Transform was designed for. It is part of TensorFlow Extended (TFX), an end-to-\\nend platform for productionizing TensorFlow models. First, to use a TFX component,\\nsuch as TF Transform, you must install it, it does not come bundled with TensorFlow.\\nYou define your preprocessing function just once (in Python), by using TF Transform\\nfunctions for scaling, bucketizing, crossing features, and more. You can also use any\\nTensorFlow operation you need. Here is what this preprocessing function might look\\nlike if we just had two features:\\nimport tensorflow_transform as tft\\ndef preprocess(inputs):  # inputs is a batch of input features\\n    median_age = inputs[\"housing_median_age\"]\\n    ocean_proximity = inputs[\"ocean_proximity\"]\\n    standardized_age = tft.scale_to_z_score(median_age - tft.mean(median_age))\\n    ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\\n    return {\\n        \"standardized_median_age\": standardized_age,\\n428 \\n| \\nChapter 13: Loading and Preprocessing Data with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 454}, page_content='11 At the time of writing, TFDS requires you to download a few files manually for ImageNet (for legal reasons),\\nbut this will hopefully get resolved soon.\\n        \"ocean_proximity_id\": ocean_proximity_id\\n    }\\nNext, TF Transform lets you apply this preprocess() function to the whole training\\nset using Apache Beam (it provides an AnalyzeAndTransformDataset class that you\\ncan use for this purpose in your Apache Beam pipeline). In the process, it will also\\ncompute all the necessary statistics over the whole training set: in this example, the\\nmean and standard deviation of the housing_median_age feature, and the vocabulary\\nfor the ocean_proximity feature. The components that compute these statistics are\\ncalled analyzers.\\nImportantly, TF Transform will also generate an equivalent TensorFlow Function that\\nyou can plug into the model you deploy. This TF Function contains all the necessary\\nstatistics computed by Apache Beam (the mean, standard deviation, and vocabulary),\\nsimply included as constants.\\nAt the time of this writing, TF Transform only supports Tensor‐\\nFlow 1. Moreover, Apache Beam only has partial support for\\nPython 3. That said, both these limitations will likely be fixed by\\nthe time your read this.\\nWith the Data API, TFRecords, the Features API and TF Transform, you can build\\nhighly scalable input pipelines for training, and also benefit from fast and portable\\ndata preprocessing in production.\\nBut what if you just wanted to use a standard dataset? Well in that case, things are\\nmuch simpler: just use TFDS!\\nThe TensorFlow Datasets (TFDS) Project\\nThe TensorFlow Datasets project makes it trivial to download common datasets, from\\nsmall ones like MNIST or Fashion MNIST, to huge datasets like ImageNet11 (you will\\nneed quite a bit of disk space!). The list includes image datasets, text datasets (includ‐\\ning translation datasets), audio and video datasets, and more. You can visit https://\\nhoml.info/tfds to view the full list, along with a description of each dataset.\\nTFDS is not bundled with TensorFlow, so you need to install the tensorflow-\\ndatasets library (e.g., using pip). Then all you need to do is call the tfds.load()\\nfunction, and it will download the data you want (unless it was already downloaded\\nearlier), and return the data as a dictionary of Datasets (typically one for training,\\nThe TensorFlow Datasets (TFDS) Project \\n| \\n429'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 455}, page_content='and one for testing, but this depends on the dataset you choose). For example, let’s\\ndownload MNIST:\\nimport tensorflow_datasets as tfds\\ndataset = tfds.load(name=\"mnist\")\\nmnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]\\nYou can then apply any transformation you want (typically repeating, batching and\\nprefetching), and you’re ready to train your model. Here is a simple example:\\nmnist_train = mnist_train.repeat(5).batch(32).prefetch(1)\\nfor item in mnist_train:\\n    images = item[\"image\"]\\n    labels = item[\"label\"]\\n    [...]\\nIn general, load() returns a shuffled training set, so there’s no need\\nto shuffle it some more.\\nNote that each item in the dataset is a dictionary containing both the features and the\\nlabels. But Keras expects each item to be a tuple containing 2 elements (again, the fea‐\\ntures and the labels). You could transform the dataset using the map() method, like\\nthis:\\nmnist_train = mnist_train.repeat(5).batch(32)\\nmnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\\nmnist_train = mnist_train.prefetch(1)\\nOr you can just ask the load() function to do this for you by setting as_super\\nvised=True (obviously this works only for labeled datasets). You can also specify the\\nbatch size if you want. Then the dataset can be passed directly to your tf.keras model:\\ndataset = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\\nmnist_train = dataset[\"train\"].repeat().prefetch(1)\\nmodel = keras.models.Sequential([...])\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\\nmodel.fit(mnist_train, steps_per_epoch=60000 // 32, epochs=5)\\nThis was quite a technical chapter, and you may feel that it is a bit far from the\\nabstract beauty of neural networks, but the fact is deep learning often involves large\\namounts of data, and knowing how to load, parse and preprocess it efficiently is a\\ncrucial skill to have. In the next chapter, we will look at Convolutional Neural Net‐\\nworks, which are among the most successful neural net architectures for image pro‐\\ncessing, and many other applications.\\n430 \\n| \\nChapter 13: Loading and Preprocessing Data with TensorFlow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 456}, page_content='CHAPTER 14\\nDeep Computer Vision Using Convolutional\\nNeural Networks\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 14 in the final\\nrelease of the book.\\nAlthough IBM’s Deep Blue supercomputer beat the chess world champion Garry Kas‐\\nparov back in 1996, it wasn’t until fairly recently that computers were able to reliably\\nperform seemingly trivial tasks such as detecting a puppy in a picture or recognizing\\nspoken words. Why are these tasks so effortless to us humans? The answer lies in the\\nfact that perception largely takes place outside the realm of our consciousness, within\\nspecialized visual, auditory, and other sensory modules in our brains. By the time\\nsensory information reaches our consciousness, it is already adorned with high-level\\nfeatures; for example, when you look at a picture of a cute puppy, you cannot choose\\nnot to see the puppy, or not to notice its cuteness. Nor can you explain how you rec‐\\nognize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective\\nexperience: perception is not trivial at all, and to understand it we must look at how\\nthe sensory modules work.\\nConvolutional neural networks (CNNs) emerged from the study of the brain’s visual\\ncortex, and they have been used in image recognition since the 1980s. In the last few\\nyears, thanks to the increase in computational power, the amount of available training\\ndata, and the tricks presented in Chapter 11 for training deep nets, CNNs have man‐\\naged to achieve superhuman performance on some complex visual tasks. They power\\nimage search services, self-driving cars, automatic video classification systems, and\\nmore. Moreover, CNNs are not restricted to visual perception: they are also successful\\n431'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 457}, page_content='1 “Single Unit Activity in Striate Cortex of Unrestrained Cats,” D. Hubel and T. Wiesel (1958).\\n2 “Receptive Fields of Single Neurones in the Cat’s Striate Cortex,” D. Hubel and T. Wiesel (1959).\\n3 “Receptive Fields and Functional Architecture of Monkey Striate Cortex,” D. Hubel and T. Wiesel (1968).\\nat many other tasks, such as voice recognition or natural language processing (NLP);\\nhowever, we will focus on visual applications for now.\\nIn this chapter we will present where CNNs came from, what their building blocks\\nlook like, and how to implement them using TensorFlow and Keras. Then we will dis‐\\ncuss some of the best CNN architectures, and discuss other visual tasks, including\\nobject detection (classifying multiple objects in an image and placing bounding boxes\\naround them) and semantic segmentation (classifying each pixel according to the class\\nof the object it belongs to).\\nThe Architecture of the Visual Cortex\\nDavid H. Hubel and Torsten Wiesel performed a series of experiments on cats in\\n19581 and 19592 (and a few years later on monkeys3), giving crucial insights on the\\nstructure of the visual cortex (the authors received the Nobel Prize in Physiology or\\nMedicine in 1981 for their work). In particular, they showed that many neurons in\\nthe visual cortex have a small local receptive field, meaning they react only to visual\\nstimuli located in a limited region of the visual field (see Figure 14-1, in which the\\nlocal receptive fields of five neurons are represented by dashed circles). The receptive\\nfields of different neurons may overlap, and together they tile the whole visual field.\\nMoreover, the authors showed that some neurons react only to images of horizontal\\nlines, while others react only to lines with different orientations (two neurons may\\nhave the same receptive field but react to different line orientations). They also\\nnoticed that some neurons have larger receptive fields, and they react to more com‐\\nplex patterns that are combinations of the lower-level patterns. These observations\\nled to the idea that the higher-level neurons are based on the outputs of neighboring\\nlower-level neurons (in Figure 14-1, notice that each neuron is connected only to a\\nfew neurons from the previous layer). This powerful architecture is able to detect all\\nsorts of complex patterns in any area of the visual field.\\n432 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 458}, page_content='4 “Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected\\nby Shift in Position,” K. Fukushima (1980).\\n5 “Gradient-Based Learning Applied to Document Recognition,” Y. LeCun et al. (1998).\\nFigure 14-1. Local receptive fields in the visual cortex\\nThese studies of the visual cortex inspired the neocognitron, introduced in 1980,4\\nwhich gradually evolved into what we now call convolutional neural networks. An\\nimportant milestone was a 1998 paper5 by Yann LeCun, Léon Bottou, Yoshua Bengio,\\nand Patrick Haffner, which introduced the famous LeNet-5 architecture, widely used\\nto recognize handwritten check numbers. This architecture has some building blocks\\nthat you already know, such as fully connected layers and sigmoid activation func‐\\ntions, but it also introduces two new building blocks: convolutional layers and pooling\\nlayers. Let’s look at them now.\\nWhy not simply use a regular deep neural network with fully con‐\\nnected layers for image recognition tasks? Unfortunately, although\\nthis works fine for small images (e.g., MNIST), it breaks down for\\nlarger images because of the huge number of parameters it\\nrequires. For example, a 100 × 100 image has 10,000 pixels, and if\\nthe first layer has just 1,000 neurons (which already severely\\nrestricts the amount of information transmitted to the next layer),\\nthis means a total of 10 million connections. And that’s just the first\\nlayer. CNNs solve this problem using partially connected layers and\\nweight sharing.\\nThe Architecture of the Visual Cortex \\n| \\n433'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 459}, page_content='6 A convolution is a mathematical operation that slides one function over another and measures the integral of\\ntheir pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform,\\nand is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very\\nsimilar to convolutions (see https://homl.info/76 for more details).\\nConvolutional Layer\\nThe most important building block of a CNN is the convolutional layer:6 neurons in\\nthe first convolutional layer are not connected to every single pixel in the input image\\n(like they were in previous chapters), but only to pixels in their receptive fields (see\\nFigure 14-2). In turn, each neuron in the second convolutional layer is connected\\nonly to neurons located within a small rectangle in the first layer. This architecture\\nallows the network to concentrate on small low-level features in the first hidden layer,\\nthen assemble them into larger higher-level features in the next hidden layer, and so\\non. This hierarchical structure is common in real-world images, which is one of the\\nreasons why CNNs work so well for image recognition.\\nFigure 14-2. CNN layers with rectangular local receptive fields\\nUntil now, all multilayer neural networks we looked at had layers\\ncomposed of a long line of neurons, and we had to flatten input\\nimages to 1D before feeding them to the neural network. Now each\\nlayer is represented in 2D, which makes it easier to match neurons\\nwith their corresponding inputs.\\nA neuron located in row i, column j of a given layer is connected to the outputs of the\\nneurons in the previous layer located in rows i to i + fh – 1, columns j to j + fw – 1,\\nwhere fh and fw are the height and width of the receptive field (see Figure 14-3). In\\norder for a layer to have the same height and width as the previous layer, it is com‐\\n434 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 460}, page_content='mon to add zeros around the inputs, as shown in the diagram. This is called zero pad‐\\nding.\\nFigure 14-3. Connections between layers and zero padding\\nIt is also possible to connect a large input layer to a much smaller layer by spacing out\\nthe receptive fields, as shown in Figure 14-4. The shift from one receptive field to the\\nnext is called the stride. In the diagram, a 5 × 7 input layer (plus zero padding) is con‐\\nnected to a 3 × 4 layer, using 3 × 3 receptive fields and a stride of 2 (in this example\\nthe stride is the same in both directions, but it does not have to be so). A neuron loca‐\\nted in row i, column j in the upper layer is connected to the outputs of the neurons in\\nthe previous layer located in rows i × sh to i × sh + fh – 1, columns j × sw to j × sw + fw –\\n1, where sh and sw are the vertical and horizontal strides.\\nConvolutional Layer \\n| \\n435'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 461}, page_content='Figure 14-4. Reducing dimensionality using a stride of 2\\nFilters\\nA neuron’s weights can be represented as a small image the size of the receptive field.\\nFor example, Figure 14-5 shows two possible sets of weights, called filters (or convolu‐\\ntion kernels). The first one is represented as a black square with a vertical white line in\\nthe middle (it is a 7 × 7 matrix full of 0s except for the central column, which is full of\\n1s); neurons using these weights will ignore everything in their receptive field except\\nfor the central vertical line (since all inputs will get multiplied by 0, except for the\\nones located in the central vertical line). The second filter is a black square with a\\nhorizontal white line in the middle. Once again, neurons using these weights will\\nignore everything in their receptive field except for the central horizontal line.\\nNow if all neurons in a layer use the same vertical line filter (and the same bias term),\\nand you feed the network the input image shown in Figure 14-5 (bottom image), the\\nlayer will output the top-left image. Notice that the vertical white lines get enhanced\\nwhile the rest gets blurred. Similarly, the upper-right image is what you get if all neu‐\\nrons use the same horizontal line filter; notice that the horizontal white lines get\\nenhanced while the rest is blurred out. Thus, a layer full of neurons using the same\\nfilter outputs a feature map, which highlights the areas in an image that activate the\\nfilter the most. Of course you do not have to define the filters manually: instead, dur‐\\ning training the convolutional layer will automatically learn the most useful filters for\\nits task, and the layers above will learn to combine them into more complex patterns.\\n436 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 462}, page_content='Figure 14-5. Applying two different filters to get two feature maps\\nStacking Multiple Feature Maps\\nUp to now, for simplicity, I have represented the output of each convolutional layer as\\na thin 2D layer, but in reality a convolutional layer has multiple filters (you decide\\nhow many), and it outputs one feature map per filter, so it is more accurately repre‐\\nsented in 3D (see Figure 14-6). To do so, it has one neuron per pixel in each feature\\nmap, and all neurons within a given feature map share the same parameters (i.e., the\\nsame weights and bias term). However, neurons in different feature maps use differ‐\\nent parameters. A neuron’s receptive field is the same as described earlier, but it\\nextends across all the previous layers’ feature maps. In short, a convolutional layer\\nsimultaneously applies multiple trainable filters to its inputs, making it capable of\\ndetecting multiple features anywhere in its inputs.\\nThe fact that all neurons in a feature map share the same parame‐\\nters dramatically reduces the number of parameters in the model.\\nMoreover, once the CNN has learned to recognize a pattern in one\\nlocation, it can recognize it in any other location. In contrast, once\\na regular DNN has learned to recognize a pattern in one location, it\\ncan recognize it only in that particular location.\\nMoreover, input images are also composed of multiple sublayers: one per color chan‐\\nnel. There are typically three: red, green, and blue (RGB). Grayscale images have just\\nConvolutional Layer \\n| \\n437'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 463}, page_content='one channel, but some images may have much more—for example, satellite images\\nthat capture extra light frequencies (such as infrared).\\nFigure 14-6. Convolution layers with multiple feature maps, and images with three color\\nchannels\\nSpecifically, a neuron located in row i, column j of the feature map k in a given convo‐\\nlutional layer l is connected to the outputs of the neurons in the previous layer l – 1,\\nlocated in rows i × sh to i × sh + fh – 1 and columns j × sw to j × sw + fw – 1, across all\\nfeature maps (in layer l – 1). Note that all neurons located in the same row i and col‐\\numn j but in different feature maps are connected to the outputs of the exact same\\nneurons in the previous layer.\\nEquation 14-1 summarizes the preceding explanations in one big mathematical equa‐\\ntion: it shows how to compute the output of a given neuron in a convolutional layer.\\n438 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 464}, page_content='It is a bit ugly due to all the different indices, but all it does is calculate the weighted\\nsum of all the inputs, plus the bias term.\\nEquation 14-1. Computing the output of a neuron in a convolutional layer\\nzi, j, k = bk + ∑\\nu = 0\\nf h −1\\n∑\\nv = 0\\nf w −1\\n∑\\nk′ = 0\\nf n′ −1\\nxi′, j′, k′ . wu, v, k′, k\\nwith\\ni′ = i × sh + u\\nj′ = j × sw + v\\n• zi, j, k is the output of the neuron located in row i, column j in feature map k of the\\nconvolutional layer (layer l).\\n• As explained earlier, sh and sw are the vertical and horizontal strides, fh and fw are\\nthe height and width of the receptive field, and fn′ is the number of feature maps\\nin the previous layer (layer l – 1).\\n• xi′, j′, k′ is the output of the neuron located in layer l – 1, row i′, column j′, feature\\nmap k′ (or channel k′ if the previous layer is the input layer).\\n• bk is the bias term for feature map k (in layer l). You can think of it as a knob that\\ntweaks the overall brightness of the feature map k.\\n• wu, v, k′ ,k is the connection weight between any neuron in feature map k of the layer\\nl and its input located at row u, column v (relative to the neuron’s receptive field),\\nand feature map k′.\\nTensorFlow Implementation\\nIn TensorFlow, each input image is typically represented as a 3D tensor of shape\\n[height, width, channels]. A mini-batch is represented as a 4D tensor of shape\\n[mini-batch size, height, width, channels]. The weights of a convolutional\\nlayer are represented as a 4D tensor of shape [fh, fw, fn′, fn]. The bias terms of a convo‐\\nlutional layer are simply represented as a 1D tensor of shape [fn].\\nLet’s look at a simple example. The following code loads two sample images, using\\nScikit-Learn’s load_sample_images() (which loads two color images, one of a Chi‐\\nnese temple, and the other of a flower). The pixel intensities (for each color channel)\\nis represented as a byte from 0 to 255, so we scale these features simply by dividing by\\n255, to get floats ranging from 0 to 1. Then we create two 7 × 7 filters (one with a\\nvertical white line in the middle, and the other with a horizontal white line in the\\nmiddle), and we apply them to both images using the tf.nn.conv2d() function,\\nwhich is part of TensorFlow’s low-level Deep Learning API. In this example, we use\\nzero padding (padding=\"SAME\") and a stride of 2. Finally, we plot one of the resulting\\nfeature maps (similar to the top-right image in Figure 14-5).\\nConvolutional Layer \\n| \\n439'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 465}, page_content='from sklearn.datasets import load_sample_image\\n# Load sample images\\nchina = load_sample_image(\"china.jpg\") / 255\\nflower = load_sample_image(\"flower.jpg\") / 255\\nimages = np.array([china, flower])\\nbatch_size, height, width, channels = images.shape\\n# Create 2 filters\\nfilters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\\nfilters[:, 3, :, 0] = 1  # vertical line\\nfilters[3, :, :, 1] = 1  # horizontal line\\noutputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\\nplt.imshow(outputs[0, :, :, 1], cmap=\"gray\") # plot 1st image\\'s 2nd feature map\\nplt.show()\\nMost of this code is self-explanatory, but the tf.nn.conv2d() line deserves a bit of\\nexplanation:\\n• images is the input mini-batch (a 4D tensor, as explained earlier).\\n• filters is the set of filters to apply (also a 4D tensor, as explained earlier).\\n• strides is equal to 1, but it could also be a 1D array with 4 elements, where the\\ntwo central elements are the vertical and horizontal strides (sh and sw). The first\\nand last elements must currently be equal to 1. They may one day be used to\\nspecify a batch stride (to skip some instances) and a channel stride (to skip some\\nof the previous layer’s feature maps or channels).\\n• padding must be either \"VALID\" or \"SAME\":\\n— If set to \"VALID\", the convolutional layer does not use zero padding, and may\\nignore some rows and columns at the bottom and right of the input image,\\ndepending on the stride, as shown in Figure 14-7 (for simplicity, only the hor‐\\nizontal dimension is shown here, but of course the same logic applies to the\\nvertical dimension).\\n— If set to \"SAME\", the convolutional layer uses zero padding if necessary. In this\\ncase, the number of output neurons is equal to the number of input neurons\\ndivided by the stride, rounded up (in this example, 13 / 5 = 2.6, rounded up to\\n3). Then zeros are added as evenly as possible around the inputs.\\n440 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 466}, page_content='Figure 14-7. Padding options—input width: 13, filter width: 6, stride: 5\\nIn this example, we manually defined the filters, but in a real CNN you would nor‐\\nmally define filters as trainable variables, so the neural net can learn which filters\\nwork best, as explained earlier. Instead of manually creating the variables, however,\\nyou can simply use the keras.layers.Conv2D layer:\\nconv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1,\\n                           padding=\"SAME\", activation=\"relu\")\\nThis code creates a Conv2D layer with 32 filters, each 3 × 3, using a stride of 1 (both\\nhorizontally and vertically), SAME padding, and applying the ReLU activation func‐\\ntion to its outputs. As you can see, convolutional layers have quite a few hyperpara‐\\nmeters: you must choose the number of filters, their height and width, the strides, and\\nthe padding type. As always, you can use cross-validation to find the right hyperpara‐\\nmeter values, but this is very time-consuming. We will discuss common CNN archi‐\\ntectures later, to give you some idea of what hyperparameter values work best in \\npractice.\\nMemory Requirements\\nAnother problem with CNNs is that the convolutional layers require a huge amount\\nof RAM. This is especially true during training, because the reverse pass of backpro‐\\npagation requires all the intermediate values computed during the forward pass.\\nFor example, consider a convolutional layer with 5 × 5 filters, outputting 200 feature\\nmaps of size 150 × 100, with stride 1 and SAME padding. If the input is a 150 × 100\\nConvolutional Layer \\n| \\n441'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 467}, page_content='7 A fully connected layer with 150 × 100 neurons, each connected to all 150 × 100 × 3 inputs, would have 1502\\n× 1002 × 3 = 675 million parameters!\\n8 In the international system of units (SI), 1 MB = 1,000 kB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits.\\nRGB image (three channels), then the number of parameters is (5 × 5 × 3 + 1) × 200\\n= 15,200 (the +1 corresponds to the bias terms), which is fairly small compared to a\\nfully connected layer.7 However, each of the 200 feature maps contains 150 × 100 neu‐\\nrons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 =\\n75 inputs: that’s a total of 225 million float multiplications. Not as bad as a fully con‐\\nnected layer, but still quite computationally intensive. Moreover, if the feature maps\\nare represented using 32-bit floats, then the convolutional layer’s output will occupy\\n200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM.8 And that’s just for one\\ninstance! If a training batch contains 100 instances, then this layer will use up 1.2 GB\\nof RAM!\\nDuring inference (i.e., when making a prediction for a new instance) the RAM occu‐\\npied by one layer can be released as soon as the next layer has been computed, so you\\nonly need as much RAM as required by two consecutive layers. But during training\\neverything computed during the forward pass needs to be preserved for the reverse\\npass, so the amount of RAM needed is (at least) the total amount of RAM required by\\nall layers.\\nIf training crashes because of an out-of-memory error, you can try\\nreducing the mini-batch size. Alternatively, you can try reducing\\ndimensionality using a stride, or removing a few layers. Or you can\\ntry using 16-bit floats instead of 32-bit floats. Or you could distrib‐\\nute the CNN across multiple devices.\\nNow let’s look at the second common building block of CNNs: the pooling layer.\\nPooling Layer\\nOnce you understand how convolutional layers work, the pooling layers are quite\\neasy to grasp. Their goal is to subsample (i.e., shrink) the input image in order to\\nreduce the computational load, the memory usage, and the number of parameters\\n(thereby limiting the risk of overfitting).\\nJust like in convolutional layers, each neuron in a pooling layer is connected to the\\noutputs of a limited number of neurons in the previous layer, located within a small\\nrectangular receptive field. You must define its size, the stride, and the padding type,\\njust like before. However, a pooling neuron has no weights; all it does is aggregate the\\ninputs using an aggregation function such as the max or mean. Figure 14-8 shows a\\nmax pooling layer, which is the most common type of pooling layer. In this example,\\n442 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 468}, page_content='9 Other kernels we discussed so far had weights, but pooling kernels do not: they are just stateless sliding win‐\\ndows.\\nwe use a 2 × 2 _pooling kernel_9, with a stride of 2, and no padding. Only the max\\ninput value in each receptive field makes it to the next layer, while the other inputs\\nare dropped. For example, in the lower left receptive field in Figure 14-8, the input\\nvalues are 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because\\nof the stride of 2, the output image has half the height and half the width of the input\\nimage (rounded down since we use no padding).\\nFigure 14-8. Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)\\nA pooling layer typically works on every input channel independ‐\\nently, so the output depth is the same as the input depth.\\nOther than reducing computations, memory usage and the number of parameters, a\\nmax pooling layer also introduces some level of invariance to small translations, as\\nshown in Figure 14-9. Here we assume that the bright pixels have a lower value than\\ndark pixels, and we consider 3 images (A, B, C) going through a max pooling layer\\nwith a 2 × 2 kernel and stride 2. Images B and C are the same as image A, but shifted\\nby one and two pixels to the right. As you can see, the outputs of the max pooling\\nlayer for images A and B are identical. This is what translation invariance means.\\nHowever, for image C, the output is different: it is shifted by one pixel to the right\\n(but there is still 75% invariance). By inserting a max pooling layer every few layers in\\na CNN, it is possible to get some level of translation invariance at a larger scale.\\nMoreover, max pooling also offers a small amount of rotational invariance and a\\nslight scale invariance. Such invariance (even if it is limited) can be useful in cases\\nwhere the prediction should not depend on these details, such as in classification\\ntasks.\\nPooling Layer \\n| \\n443'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 469}, page_content='Figure 14-9. Invariance to small translations\\nBut max pooling has some downsides: firstly, it is obviously very destructive: even\\nwith a tiny 2 × 2 kernel and a stride of 2, the output will be two times smaller in both\\ndirections (so its area will be four times smaller), simply dropping 75% of the input\\nvalues. And in some applications, invariance is not desirable, for example for seman‐\\ntic segmentation: this is the task of classifying each pixel in an image depending on the\\nobject that pixel belongs to: obviously, if the input image is translated by 1 pixel to the\\nright, the output should also be translated by 1 pixel to the right. The goal in this case\\nis equivariance, not invariance: a small change to the inputs should lead to a corre‐\\nsponding small change in the output.\\nTensorFlow Implementation\\nImplementing a max pooling layer in TensorFlow is quite easy. The following code\\ncreates a max pooling layer using a 2 × 2 kernel. The strides default to the kernel size,\\nso this layer will use a stride of 2 (both horizontally and vertically). By default, it uses\\nVALID padding (i.e., no padding at all):\\nmax_pool = keras.layers.MaxPool2D(pool_size=2)\\nTo create an average pooling layer, just use AvgPool2D instead of MaxPool2D. As you\\nmight expect, it works exactly like a max pooling layer, except it computes the mean\\nrather than the max. Average pooling layers used to be very popular, but people\\n444 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 470}, page_content='mostly use max pooling layers now, as they generally perform better. This may seem\\nsurprising, since computing the mean generally loses less information than comput‐\\ning the max. But on the other hand, max pooling preserves only the strongest feature,\\ngetting rid of all the meaningless ones, so the next layers get a cleaner signal to work\\nwith. Moreover, max pooling offers stronger translation invariance than average\\npooling.\\nNote that max pooling and average pooling can be performed along the depth dimen‐\\nsion rather than the spatial dimensions, although this is not as common. This can\\nallow the CNN to learn to be invariant to various features. For example, it could learn\\nmultiple filters, each detecting a different rotation of the same pattern, such as hand-\\nwritten digits (see Figure 14-10), and the depth-wise max pooling layer would ensure\\nthat the output is the same regardless of the rotation. The CNN could similarly learn\\nto be invariant to anything else: thickness, brightness, skew, color, and so on.\\nFigure 14-10. Depth-wise max pooling can help the CNN learn any invariance\\nPooling Layer \\n| \\n445'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 471}, page_content='Keras does not include a depth-wise max pooling layer, but TensorFlow’s low-level\\nDeep Learning API does: just use the tf.nn.max_pool() function, and specify the\\nkernel size and strides as 4-tuples. The first three values of each should be 1: this indi‐\\ncates that the kernel size and stride along the batch, height and width dimensions\\nshoud be 1. The last value should be whatever kernel size and stride you want along\\nthe depth dimension, for example 3 (this must be a divisor of the input depth; for\\nexample, it will not work if the previous layer outputs 20 feature maps, since 20 is not\\na multiple of 3):\\noutput = tf.nn.max_pool(images,\\n                        ksize=(1, 1, 1, 3),\\n                        strides=(1, 1, 1, 3),\\n                        padding=\"VALID\")\\nIf you want to include this as a layer in your Keras models, you can simply wrap it in\\na Lambda layer (or create a custom Keras layer):\\ndepth_pool = keras.layers.Lambda(\\n    lambda X: tf.nn.max_pool(X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3),\\n                             padding=\"VALID\"))\\nOne last type of pooling layer that you will often see in modern architectures is the\\nglobal average pooling layer. It works very differently: all it does is compute the mean\\nof each entire feature map (it’s like an average pooling layer using a pooling kernel\\nwith the same spatial dimensions as the inputs). This means that it just outputs a sin‐\\ngle number per feature map and per instance. Although this is of course extremely\\ndestructive (most of the information in the feature map is lost), it can be useful as the\\noutput layer, as we will see later in this chapter. To create such a layer, simply use the\\nkeras.layers.GlobalAvgPool2D class:\\nglobal_avg_pool = keras.layers.GlobalAvgPool2D()\\nIt is actually equivalent to this simple Lamba layer, which computes the mean over the\\nspatial dimensions (height and width):\\nglobal_avg_pool = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))\\nNow you know all the building blocks to create a convolutional neural network. Let’s\\nsee how to assemble them.\\nCNN Architectures\\nTypical CNN architectures stack a few convolutional layers (each one generally fol‐\\nlowed by a ReLU layer), then a pooling layer, then another few convolutional layers\\n(+ReLU), then another pooling layer, and so on. The image gets smaller and smaller\\nas it progresses through the network, but it also typically gets deeper and deeper (i.e.,\\nwith more feature maps) thanks to the convolutional layers (see Figure 14-11). At the\\ntop of the stack, a regular feedforward neural network is added, composed of a few\\n446 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 472}, page_content='fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a\\nsoftmax layer that outputs estimated class probabilities).\\nFigure 14-11. Typical CNN architecture\\nA common mistake is to use convolution kernels that are too large.\\nFor example, instead of using a convolutional layer with a 5 × 5\\nkernel, it is generally preferable to stack two layers with 3 × 3 ker‐\\nnels: it will use less parameters and require less computations, and\\nit will usually perform better. One exception to this recommenda‐\\ntion is for the first convolutional layer: it can typically have a large\\nkernel (e.g., 5 × 5), usually with stride of 2 or more: this will reduce\\nthe spatial dimension of the image without losing too much infor‐\\nmation, and since the input image only has 3 channels in general, it\\nwill not be too costly.\\nHere is how you can implement a simple CNN to tackle the fashion MNIST dataset\\n(introduced in Chapter 10):\\nfrom functools import partial\\nDefaultConv2D = partial(keras.layers.Conv2D,\\n                        kernel_size=3, activation=\\'relu\\', padding=\"SAME\")\\nmodel = keras.models.Sequential([\\n    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\\n    keras.layers.MaxPooling2D(pool_size=2),\\n    DefaultConv2D(filters=128),\\n    DefaultConv2D(filters=128),\\n    keras.layers.MaxPooling2D(pool_size=2),\\n    DefaultConv2D(filters=256),\\n    DefaultConv2D(filters=256),\\n    keras.layers.MaxPooling2D(pool_size=2),\\n    keras.layers.Flatten(),\\n    keras.layers.Dense(units=128, activation=\\'relu\\'),\\n    keras.layers.Dropout(0.5),\\n    keras.layers.Dense(units=64, activation=\\'relu\\'),\\n    keras.layers.Dropout(0.5),\\n    keras.layers.Dense(units=10, activation=\\'softmax\\'),\\n])\\nCNN Architectures \\n| \\n447'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 473}, page_content='• In this code, we start by using the partial() function to define a thin wrapper\\naround the Conv2D class, called DefaultConv2D: it simply avoids having to repeat\\nthe same hyperparameter values over and over again.\\n• The first layer uses a large kernel size, but no stride because the input images are\\nnot very large. It also sets input_shape=[28, 28, 1], which means the images\\nare 28 × 28 pixels, with a single color channel (i.e., grayscale).\\n• Next, we have a max pooling layer, which divides each spatial dimension by a fac‐\\ntor of two (since pool_size=2).\\n• Then we repeat the same structure twice: two convolutional layers followed by a\\nmax pooling layer. For larger images, we could repeat this structure several times\\n(the number of repetitions is a hyperparameter you can tune).\\n• Note that the number of filters grows as we climb up the CNN towards the out‐\\nput layer (it is initially 64, then 128, then 256): it makes sense for it to grow, since\\nthe number of low level features is often fairly low (e.g., small circles, horizontal\\nlines, etc.), but there are many different ways to combine them into higher level\\nfeatures. It is a common practice to double the number of filters after each pool‐\\ning layer: since a pooling layer divides each spatial dimension by a factor of 2, we\\ncan afford doubling the number of feature maps in the next layer, without fear of\\nexploding the number of parameters, memory usage, or computational load.\\n• Next is the fully connected network, composed of 2 hidden dense layers and a\\ndense output layer. Note that we must flatten its inputs, since a dense network\\nexpects a 1D array of features for each instance. We also add two dropout layers,\\nwith a dropout rate of 50% each, to reduce overfitting.\\nThis CNN reaches over 92% accuracy on the test set. It’s not the state of the art, but it\\nis pretty good, and clearly much better than what we achieved with dense networks in\\nChapter 10.\\nOver the years, variants of this fundamental architecture have been developed, lead‐\\ning to amazing advances in the field. A good measure of this progress is the error rate\\nin competitions such as the ILSVRC ImageNet challenge. In this competition the\\ntop-5 error rate for image classification fell from over 26% to less than 2.3% in just six\\nyears. The top-five error rate is the number of test images for which the system’s top 5\\npredictions did not include the correct answer. The images are large (256 pixels high)\\nand there are 1,000 classes, some of which are really subtle (try distinguishing 120\\ndog breeds). Looking at the evolution of the winning entries is a good way to under‐\\nstand how CNNs work.\\nWe will first look at the classical LeNet-5 architecture (1998), then three of the win‐\\nners of the ILSVRC challenge: AlexNet (2012), GoogLeNet (2014), and ResNet\\n(2015).\\n448 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 474}, page_content='10 “Gradient-Based Learning Applied to Document Recognition”, Y. LeCun, L. Bottou, Y. Bengio and P. Haffner\\n(1998).\\nLeNet-5\\nThe LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As\\nmentioned earlier, it was created by Yann LeCun in 1998 and widely used for hand‐\\nwritten digit recognition (MNIST). It is composed of the layers shown in Table 14-1.\\nTable 14-1. LeNet-5 architecture\\nLayer\\nType\\nMaps Size\\nKernel size Stride\\nActivation\\nOut\\nFully Connected\\n–\\n10\\n–\\n–\\nRBF\\nF6\\nFully Connected\\n–\\n84\\n–\\n–\\ntanh\\nC5\\nConvolution\\n120\\n1 × 1\\n5 × 5\\n1\\ntanh\\nS4\\nAvg Pooling\\n16\\n5 × 5\\n2 × 2\\n2\\ntanh\\nC3\\nConvolution\\n16\\n10 × 10 5 × 5\\n1\\ntanh\\nS2\\nAvg Pooling\\n6\\n14 × 14 2 × 2\\n2\\ntanh\\nC1\\nConvolution\\n6\\n28 × 28 5 × 5\\n1\\ntanh\\nIn\\nInput\\n1\\n32 × 32 –\\n–\\n–\\nThere are a few extra details to be noted:\\n• MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and\\nnormalized before being fed to the network. The rest of the network does not use\\nany padding, which is why the size keeps shrinking as the image progresses\\nthrough the network.\\n• The average pooling layers are slightly more complex than usual: each neuron\\ncomputes the mean of its inputs, then multiplies the result by a learnable coeffi‐\\ncient (one per map) and adds a learnable bias term (again, one per map), then\\nfinally applies the activation function.\\n• Most neurons in C3 maps are connected to neurons in only three or four S2\\nmaps (instead of all six S2 maps). See table 1 (page 8) in the original paper10 for\\ndetails.\\n• The output layer is a bit special: instead of computing the matrix multiplication\\nof the inputs and the weight vector, each neuron outputs the square of the Eucli‐\\ndian distance between its input vector and its weight vector. Each output meas‐\\nures how much the image belongs to a particular digit class. The cross entropy \\ncost function is now preferred, as it penalizes bad predictions much more, pro‐\\nducing larger gradients and converging faster.\\nCNN Architectures \\n| \\n449'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 475}, page_content='11 “ImageNet Classification with Deep Convolutional Neural Networks,” A. Krizhevsky et al. (2012).\\nYann LeCun’s website (“LENET” section) features great demos of LeNet-5 classifying \\ndigits.\\nAlexNet\\nThe AlexNet CNN architecture11 won the 2012 ImageNet ILSVRC challenge by a\\nlarge margin: it achieved 17% top-5 error rate while the second best achieved only\\n26%! It was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and\\nGeoffrey Hinton. It is quite similar to LeNet-5, only much larger and deeper, and it\\nwas the first to stack convolutional layers directly on top of each other, instead of\\nstacking a pooling layer on top of each convolutional layer. Table 14-2 presents this\\narchitecture.\\nTable 14-2. AlexNet architecture\\nLayer\\nType\\nMaps\\nSize\\nKernel size\\nStride\\nPadding\\nActivation\\nOut\\nFully Connected\\n–\\n1,000\\n–\\n–\\n–\\nSoftmax\\nF9\\nFully Connected\\n–\\n4,096\\n–\\n–\\n–\\nReLU\\nF8\\nFully Connected\\n–\\n4,096\\n–\\n–\\n–\\nReLU\\nC7\\nConvolution\\n256\\n13 × 13\\n3 × 3\\n1\\nSAME\\nReLU\\nC6\\nConvolution\\n384\\n13 × 13\\n3 × 3\\n1\\nSAME\\nReLU\\nC5\\nConvolution\\n384\\n13 × 13\\n3 × 3\\n1\\nSAME\\nReLU\\nS4\\nMax Pooling\\n256\\n13 × 13\\n3 × 3\\n2\\nVALID\\n–\\nC3\\nConvolution\\n256\\n27 × 27\\n5 × 5\\n1\\nSAME\\nReLU\\nS2\\nMax Pooling\\n96\\n27 × 27\\n3 × 3\\n2\\nVALID\\n–\\nC1\\nConvolution\\n96\\n55 × 55\\n11 × 11\\n4\\nVALID\\nReLU\\nIn\\nInput\\n3 (RGB)\\n227 × 227 –\\n–\\n–\\n–\\nTo reduce overfitting, the authors used two regularization techniques: first they\\napplied dropout (introduced in Chapter 11) with a 50% dropout rate during training\\nto the outputs of layers F8 and F9. Second, they performed data augmentation by ran‐\\ndomly shifting the training images by various offsets, flipping them horizontally, and\\nchanging the lighting conditions.\\nData Augmentation\\nData augmentation artificially increases the size of the training set by generating\\nmany realistic variants of each training instance. This reduces overfitting, making this\\na regularization technique. The generated instances should be as realistic as possible:\\n450 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 476}, page_content='ideally, given an image from the augmented training set, a human should not be able\\nto tell whether it was augmented or not. Moreover, simply adding white noise will not\\nhelp; the modifications should be learnable (white noise is not).\\nFor example, you can slightly shift, rotate, and resize every picture in the training set\\nby various amounts and add the resulting pictures to the training set (see\\nFigure 14-12). This forces the model to be more tolerant to variations in the position,\\norientation, and size of the objects in the pictures. If you want the model to be more\\ntolerant to different lighting conditions, you can similarly generate many images with\\nvarious contrasts. In general, you can also flip the pictures horizontally (except for\\ntext, and other non-symmetrical objects). By combining these transformations you\\ncan greatly increase the size of your training set.\\nFigure 14-12. Generating new training instances from existing ones\\nAlexNet also uses a competitive normalization step immediately after the ReLU step\\nof layers C1 and C3, called local response normalization. The most strongly activated\\nneurons inhibit other neurons located at the same position in neighboring feature\\nmaps (such competitive activation has been observed in biological neurons). This\\nencourages different feature maps to specialize, pushing them apart and forcing them\\nCNN Architectures \\n| \\n451'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 477}, page_content='12 “Going Deeper with Convolutions,” C. Szegedy et al. (2015).\\n13 In the 2010 movie Inception, the characters keep going deeper and deeper into multiple layers of dreams,\\nhence the name of these modules.\\nto explore a wider range of features, ultimately improving generalization. Equation\\n14-2 shows how to apply LRN.\\nEquation 14-2. Local response normalization\\nbi = ai k + α ∑\\nj = jlow\\njhigh\\naj\\n2\\n−β\\nwith\\njhigh = min i + r\\n2, f n −1\\njlow = max 0, i −r\\n2\\n• bi is the normalized output of the neuron located in feature map i, at some row u\\nand column v (note that in this equation we consider only neurons located at this\\nrow and column, so u and v are not shown).\\n• ai is the activation of that neuron after the ReLU step, but before normalization.\\n• k, α, β, and r are hyperparameters. k is called the bias, and r is called the depth\\nradius.\\n• fn is the number of feature maps.\\nFor example, if r = 2 and a neuron has a strong activation, it will inhibit the activation\\nof the neurons located in the feature maps immediately above and below its own.\\nIn AlexNet, the hyperparameters are set as follows: r = 2, α = 0.00002, β = 0.75, and k\\n= 1. This step can be implemented using the tf.nn.local_response_normaliza\\ntion() function (which you can wrap in a Lambda layer if you want to use it in a\\nKeras model).\\nA variant of AlexNet called ZF Net was developed by Matthew Zeiler and Rob Fergus\\nand won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked \\nhyperparameters (number of feature maps, kernel size, stride, etc.).\\nGoogLeNet\\nThe GoogLeNet architecture was developed by Christian Szegedy et al. from Google\\nResearch,12 and it won the ILSVRC 2014 challenge by pushing the top-5 error rate\\nbelow 7%. This great performance came in large part from the fact that the network\\nwas much deeper than previous CNNs (see Figure 14-14). This was made possible by\\nsub-networks called inception modules,13 which allow GoogLeNet to use parameters\\n452 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 478}, page_content='much more efficiently than previous architectures: GoogLeNet actually has 10 times\\nfewer parameters than AlexNet (roughly 6 million instead of 60 million).\\nFigure 14-13 shows the architecture of an inception module. The notation “3 × 3 +\\n1(S)” means that the layer uses a 3 × 3 kernel, stride 1, and SAME padding. The input\\nsignal is first copied and fed to four different layers. All convolutional layers use the\\nReLU activation function. Note that the second set of convolutional layers uses differ‐\\nent kernel sizes (1 × 1, 3 × 3, and 5 × 5), allowing them to capture patterns at different\\nscales. Also note that every single layer uses a stride of 1 and SAME padding (even\\nthe max pooling layer), so their outputs all have the same height and width as their\\ninputs. This makes it possible to concatenate all the outputs along the depth dimen‐\\nsion in the final depth concat layer (i.e., stack the feature maps from all four top con‐\\nvolutional layers). This concatenation layer can be implemented in TensorFlow using\\nthe tf.concat() operation, with axis=3 (axis 3 is the depth).\\nFigure 14-13. Inception module\\nYou may wonder why inception modules have convolutional layers with 1 × 1 ker‐\\nnels. Surely these layers cannot capture any features since they look at only one pixel\\nat a time? In fact, these layers serve three purposes:\\n• First, although they cannot capture spatial patterns, they can capture patterns\\nalong the depth dimension.\\n• Second, they are configured to output fewer feature maps than their inputs, so\\nthey serve as bottleneck layers, meaning they reduce dimensionality. This cuts the\\ncomputational cost and the number of parameters, speeding up training and\\nimproving generalization.\\n• Lastly, each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) acts like\\na single, powerful convolutional layer, capable of capturing more complex pat‐\\nterns. Indeed, instead of sweeping a simple linear classifier across the image (as a\\nCNN Architectures \\n| \\n453'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 479}, page_content='single convolutional layer does), this pair of convolutional layers sweeps a two-\\nlayer neural network across the image.\\nIn short, you can think of the whole inception module as a convolutional layer on\\nsteroids, able to output feature maps that capture complex patterns at various scales.\\nThe number of convolutional kernels for each convolutional layer\\nis a hyperparameter. Unfortunately, this means that you have six\\nmore hyperparameters to tweak for every inception layer you add.\\nNow let’s look at the architecture of the GoogLeNet CNN (see Figure 14-14). The\\nnumber of feature maps output by each convolutional layer and each pooling layer is\\nshown before the kernel size. The architecture is so deep that it has to be represented\\nin three columns, but GoogLeNet is actually one tall stack, including nine inception\\nmodules (the boxes with the spinning tops). The six numbers in the inception mod‐\\nules represent the number of feature maps output by each convolutional layer in the\\nmodule (in the same order as in Figure 14-13). Note that all the convolutional layers\\nuse the ReLU activation function.\\n454 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 480}, page_content='Figure 14-14. GoogLeNet architecture\\nLet’s go through this network:\\n• The first two layers divide the image’s height and width by 4 (so its area is divided\\nby 16), to reduce the computational load. The first layer uses a large kernel size,\\nso that much of the information is still preserved.\\n• Then the local response normalization layer ensures that the previous layers learn\\na wide variety of features (as discussed earlier).\\n• Two convolutional layers follow, where the first acts like a bottleneck layer. As\\nexplained earlier, you can think of this pair as a single smarter convolutional\\nlayer.\\n• Again, a local response normalization layer ensures that the previous layers cap‐\\nture a wide variety of patterns.\\nCNN Architectures \\n| \\n455'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 481}, page_content='14 “Very Deep Convolutional Networks for Large-Scale Image Recognition,” K. Simonyan and A. Zisserman\\n(2015).\\n• Next a max pooling layer reduces the image height and width by 2, again to speed\\nup computations.\\n• Then comes the tall stack of nine inception modules, interleaved with a couple\\nmax pooling layers to reduce dimensionality and speed up the net.\\n• Next, the global average pooling layer simply outputs the mean of each feature\\nmap: this drops any remaining spatial information, which is fine since there was\\nnot much spatial information left at that point. Indeed, GoogLeNet input images\\nare typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each\\ndividing the height and width by 2, the feature maps are down to 7 × 7. More‐\\nover, it is a classification task, not localization, so it does not matter where the\\nobject is. Thanks to the dimensionality reduction brought by this layer, there is\\nno need to have several fully connected layers at the top of the CNN (like in\\nAlexNet), and this considerably reduces the number of parameters in the net‐\\nwork and limits the risk of overfitting.\\n• The last layers are self-explanatory: dropout for regularization, then a fully con‐\\nnected layer with 1,000 units, since there are a 1,000 classes, and a softmax acti‐\\nvation function to output estimated class probabilities.\\nThis diagram is slightly simplified: the original GoogLeNet architecture also included\\ntwo auxiliary classifiers plugged on top of the third and sixth inception modules.\\nThey were both composed of one average pooling layer, one convolutional layer, two\\nfully connected layers, and a softmax activation layer. During training, their loss\\n(scaled down by 70%) was added to the overall loss. The goal was to fight the vanish‐\\ning gradients problem and regularize the network. However, it was later shown that\\ntheir effect was relatively minor.\\nSeveral variants of the GoogLeNet architecture were later proposed by Google\\nresearchers, including Inception-v3 and Inception-v4, using slightly different incep‐\\ntion modules, and reaching even better performance.\\nVGGNet\\nThe runner up in the ILSVRC 2014 challenge was VGGNet14, developed by K. Simon‐\\nyan and A. Zisserman. It had a very simple and classical architecture, with 2 or 3 con‐\\nvolutional layers, a pooling layer, then again 2 or 3 convolutional layers, a pooling\\nlayer, and so on (with a total of just 16 convolutional layers), plus a final dense net‐\\nwork with 2 hidden layers and the output layer. It used only 3 × 3 filters, but many\\nfilters.\\n456 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 482}, page_content='15 “Deep Residual Learning for Image Recognition,” K. He (2015).\\nResNet\\nThe ILSVRC 2015 challenge was won using a Residual Network (or ResNet), devel‐\\noped by Kaiming He et al.,15 which delivered an astounding top-5 error rate under\\n3.6%, using an extremely deep CNN composed of 152 layers. It confirmed the general\\ntrend: models are getting deeper and deeper, with fewer and fewer parameters. The\\nkey to being able to train such a deep network is to use skip connections (also called\\nshortcut connections): the signal feeding into a layer is also added to the output of a\\nlayer located a bit higher up the stack. Let’s see why this is useful.\\nWhen training a neural network, the goal is to make it model a target function h(x).\\nIf you add the input x to the output of the network (i.e., you add a skip connection),\\nthen the network will be forced to model f(x) = h(x) – x rather than h(x). This is\\ncalled residual learning (see Figure 14-15).\\nFigure 14-15. Residual learning\\nWhen you initialize a regular neural network, its weights are close to zero, so the net‐\\nwork just outputs values close to zero. If you add a skip connection, the resulting net‐\\nwork just outputs a copy of its inputs; in other words, it initially models the identity\\nfunction. If the target function is fairly close to the identity function (which is often\\nthe case), this will speed up training considerably.\\nMoreover, if you add many skip connections, the network can start making progress\\neven if several layers have not started learning yet (see Figure 14-16). Thanks to skip\\nconnections, the signal can easily make its way across the whole network. The deep\\nresidual network can be seen as a stack of residual units, where each residual unit is a\\nsmall neural network with a skip connection.\\nCNN Architectures \\n| \\n457'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 483}, page_content='Figure 14-16. Regular deep neural network (left) and deep residual network (right)\\nNow let’s look at ResNet’s architecture (see Figure 14-17). It is actually surprisingly\\nsimple. It starts and ends exactly like GoogLeNet (except without a dropout layer),\\nand in between is just a very deep stack of simple residual units. Each residual unit is\\ncomposed of two convolutional layers (and no pooling layer!), with Batch Normaliza‐\\ntion (BN) and ReLU activation, using 3 × 3 kernels and preserving spatial dimensions\\n(stride 1, SAME padding).\\nFigure 14-17. ResNet architecture\\nNote that the number of feature maps is doubled every few residual units, at the same\\ntime as their height and width are halved (using a convolutional layer with stride 2).\\nWhen this happens the inputs cannot be added directly to the outputs of the residual\\nunit since they don’t have the same shape (for example, this problem affects the skip\\n458 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 484}, page_content='16 “Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning,” C. Szegedy et al.\\n(2016).\\n17 “Xception: Deep Learning with Depthwise Separable Convolutions,” François Chollet (2016)\\nconnection represented by the dashed arrow in Figure 14-17). To solve this problem,\\nthe inputs are passed through a 1 × 1 convolutional layer with stride 2 and the right\\nnumber of output feature maps (see Figure 14-18).\\nFigure 14-18. Skip connection when changing feature map size and depth\\nResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and\\nthe fully connected layer) containing three residual units that output 64 feature maps,\\n4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple‐\\nment this architecture later in this chapter.\\nResNets deeper than that, such as ResNet-152, use slightly different residual units.\\nInstead of two 3 × 3 convolutional layers with (say) 256 feature maps, they use three\\nconvolutional layers: first a 1 × 1 convolutional layer with just 64 feature maps (4\\ntimes less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer\\nwith 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature\\nmaps (4 times 64) that restores the original depth. ResNet-152 contains three such\\nRUs that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024\\nmaps, and finally 3 RUs with 2,048 maps.\\nGoogle’s Inception-v416 architecture merged the ideas of GoogLe‐\\nNet and ResNet and achieved close to 3% top-5 error rate on\\nImageNet classification.\\nXception\\nAnother variant of the GoogLeNet architecture is also worth noting: Xception17\\n(which stands for Extreme Inception) was proposed in 2016 by François Chollet (the\\nCNN Architectures \\n| \\n459'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 485}, page_content='18 This name can sometimes be ambiguous, since spatially separable convolutions are often called “separable\\nconvolutions” as well.\\nauthor of Keras), and it significantly outperformed Inception-v3 on a huge vision task\\n(350 million images and 17,000 classes). Just like Inception-v4, it also merges the\\nideas of GoogLeNet and ResNet, but it replaces the inception modules with a special\\ntype of layer called a depthwise separable convolution (or separable convolution for\\nshort18). These layers had been used before in some CNN architectures, but they were\\nnot as central as in the Xception architecture. While a regular convolutional layer\\nuses filters that try to simultaneously capture spatial patterns (e.g., an oval) and cross-\\nchannel patterns (e.g., mouth + nose + eyes = face), a separable convolutional layer\\nmakes the strong assumption that spatial patterns and cross-channel patterns can be\\nmodeled separately (see Figure 14-19). Thus, it is composed of two parts: the first part\\napplies a single spatial filter for each input feature map, then the second part looks\\nexclusively for cross-channel patterns—it is just a regular convolutional layer with 1 ×\\n1 filters.\\nFigure 14-19. Depthwise Separable Convolutional Layer\\nSince separable convolutional layers only have one spatial filter per input channel,\\nyou should avoid using them after layers that have too few channels, such as the input\\nlayer (granted, that’s what Figure 14-19 represents, but it is just for illustration pur‐\\nposes). For this reason, the Xception architecture starts with 2 regular convolutional\\nlayers, but then the rest of the architecture uses only separable convolutions (34 in\\n460 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 486}, page_content='19 “Crafting GBD-Net for Object Detection,” X. Zeng et al. (2016).\\n20 “Squeeze-and-Excitation Networks,” Jie Hu et al. (2017)\\nall), plus a few max pooling layers and the usual final layers (a global average pooling\\nlayer, and a dense output layer).\\nYou might wonder why Xception is considered a variant of GoogLeNet, since it con‐\\ntains no inception module at all? Well, as we discussed earlier, an Inception module\\ncontains convolutional layers with 1 × 1 filters: these look exclusively for cross-\\nchannel patterns. However, the convolution layers that sit on top of them are regular\\nconvolutional layers that look both for spatial and cross-channel patterns. So you can\\nthink of an Inception module as an intermediate between a regular convolutional\\nlayer (which considers spatial patterns and cross-channel patterns jointly) and a sepa‐\\nrable convolutional layer (which considers them separately). In practice, it seems that\\nseparable convolutions generally perform better.\\nSeparable convolutions use less parameters, less memory and less\\ncomputations than regular convolutional layers, and in general\\nthey even perform better, so you should consider using them by\\ndefault (except after layers with few channels).\\nThe ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni‐\\nversity of Hong Kong. They used an ensemble of many different techniques, includ‐\\ning a sophisticated object-detection system called GBD-Net19, to achieve a top-5 error\\nrate below 3%. Although this result is unquestionably impressive, the complexity of\\nthe solution contrasted with the simplicity of ResNets. Moreover, one year later\\nanother fairly simple architecture performed even better, as we will see now.\\nSENet\\nThe winning architecture in the ILSVRC 2017 challenge was the Squeeze-and-\\nExcitation Network (SENet)20. This architecture extends existing architectures such as\\ninception networks or ResNets, and boosts their performance. This allowed SENet to\\nwin the competition with an astonishing 2.25% top-5 error rate! The extended ver‐\\nsions of inception networks and ResNet are called SE-Inception and SE-ResNet respec‐\\ntively. The boost comes from the fact that a SENet adds a small neural network, called\\na SE Block, to every unit in the original architecture (i.e., every inception module or\\nevery residual unit), as shown in Figure 14-20.\\nCNN Architectures \\n| \\n461'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 487}, page_content='Figure 14-20. SE-Inception Module (left) and SE-ResNet Unit (right)\\nA SE Block analyzes the output of the unit it is attached to, focusing exclusively on\\nthe depth dimension (it does not look for any spatial pattern), and it learns which fea‐\\ntures are usually most active together. It then uses this information to recalibrate the\\nfeature maps, as shown in Figure 14-21. For example, a SE Block may learn that\\nmouths, noses and eyes usually appear together in pictures: if you see a mouth and a\\nnose, you should expect to see eyes as well. So if a SE Block sees a strong activation in\\nthe mouth and nose feature maps, but only mild activation in the eye feature map, it\\nwill boost the eye feature map (more accurately, it will reduce irrelevant feature\\nmaps). If the eyes were somewhat confused with something else, this feature map\\nrecalibration will help resolve the ambiguity.\\n462 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 488}, page_content='Figure 14-21. An SE Block Performs Feature Map Recalibration\\nA SE Block is composed of just 3 layers: a global average pooling layer, a hidden dense\\nlayer using the ReLU activation function, and a dense output layer using the sigmoid\\nactivation function (see Figure 14-22):\\nFigure 14-22. SE Block Architecture\\nCNN Architectures \\n| \\n463'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 489}, page_content='As earlier, the global average pooling layer computes the mean activation for each fea‐\\nture map: for example, if its input contains 256 feature maps, it will output 256 num‐\\nbers representing the overall level of response for each filter. The next layer is where\\nthe “squeeze” happens: this layer has much less than 256 neurons, typically 16 times\\nless than the number of feature maps (e.g., 16 neurons), so the 256 numbers get com‐\\npressed into a small vector (e.g., 16 dimensional). This is a low-dimensional vector\\nrepresentation (i.e., an embedding) of the distribution of feature responses. This bot‐\\ntleneck step forces the SE Block to learn a general representation of the feature com‐\\nbinations (we will see this principle in action again when we discuss autoencoders\\nin ???). Finally, the output layer takes the embedding and outputs a recalibration vec‐\\ntor containing one number per feature map (e.g., 256), each between 0 and 1. The\\nfeature maps are then multiplied by this recalibration vector, so irrelevant features\\n(with a low recalibration score) get scaled down while relevant features (with a recali‐\\nbration score close to 1) are left alone.\\nImplementing a ResNet-34 CNN Using Keras\\nMost CNN architectures described so far are fairly straightforward to implement\\n(although generally you would load a pretrained network instead, as we will see). To\\nillustrate the process, let’s implement a ResNet-34 from scratch using Keras. First, let’s\\ncreate a ResidualUnit layer:\\nDefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,\\n                        padding=\"SAME\", use_bias=False)\\nclass ResidualUnit(keras.layers.Layer):\\n    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\\n        super().__init__(**kwargs)\\n        self.activation = keras.activations.get(activation)\\n        self.main_layers = [\\n            DefaultConv2D(filters, strides=strides),\\n            keras.layers.BatchNormalization(),\\n            self.activation,\\n            DefaultConv2D(filters),\\n            keras.layers.BatchNormalization()]\\n        self.skip_layers = []\\n        if strides > 1:\\n            self.skip_layers = [\\n                DefaultConv2D(filters, kernel_size=1, strides=strides),\\n                keras.layers.BatchNormalization()]\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.main_layers:\\n            Z = layer(Z)\\n        skip_Z = inputs\\n        for layer in self.skip_layers:\\n464 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 490}, page_content='skip_Z = layer(skip_Z)\\n        return self.activation(Z + skip_Z)\\nAs you can see, this code matches Figure 14-18 pretty closely. In the constructor, we\\ncreate all the layers we will need: the main layers are the ones on the right side of the\\ndiagram, and the skip layers are the ones on the left (only needed if the stride is\\ngreater than 1). Then in the call() method, we simply make the inputs go through\\nthe main layers, and the skip layers (if any), then we add both outputs and we apply\\nthe activation function.\\nNext, we can build the ResNet-34 simply using a Sequential model, since it is really\\njust a long sequence of layers (we can treat each residual unit as a single layer now\\nthat we have the ResidualUnit class):\\nmodel = keras.models.Sequential()\\nmodel.add(DefaultConv2D(64, kernel_size=7, strides=2,\\n                        input_shape=[224, 224, 3]))\\nmodel.add(keras.layers.BatchNormalization())\\nmodel.add(keras.layers.Activation(\"relu\"))\\nmodel.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\"))\\nprev_filters = 64\\nfor filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\\n    strides = 1 if filters == prev_filters else 2\\n    model.add(ResidualUnit(filters, strides=strides))\\n    prev_filters = filters\\nmodel.add(keras.layers.GlobalAvgPool2D())\\nmodel.add(keras.layers.Flatten())\\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\\nThe only slightly tricky part in this code is the loop that adds the ResidualUnit layers\\nto the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs\\nhave 128 filters, and so on. We then set the strides to 1 when the number of filters is\\nthe same as in the previous RU, or else we set it to 2. Then we add the ResidualUnit,\\nand finally we update prev_filters.\\nIt is quite amazing that in less than 40 lines of code, we can build the model that won\\nthe ILSVRC 2015 challenge! It demonstrates both the elegance of the ResNet model,\\nand the expressiveness of the Keras API. Implementing the other CNN architectures\\nis not much harder. However, Keras comes with several of these architectures built in,\\nso why not use them instead?\\nUsing Pretrained Models From Keras\\nIn general, you won’t have to implement standard models like GoogLeNet or ResNet\\nmanually, since pretrained networks are readily available with a single line of code, in\\nthe keras.applications package. For example:\\nmodel = keras.applications.resnet50.ResNet50(weights=\"imagenet\")\\nUsing Pretrained Models From Keras \\n| \\n465'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 491}, page_content='21 In the ImageNet dataset, each image is associated to a word in the WordNet dataset: the class ID is just a\\nWordNet ID.\\nThat’s all! This will create a ResNet-50 model and download weights pretrained on\\nthe ImageNet dataset. To use it, you first need to ensure that the images have the right\\nsize. A ResNet-50 model expects 224 × 224 images (other models may expect other\\nsizes, such as 299 × 299), so let’s use TensorFlow’s tf.image.resize() function to\\nresize the images we loaded earlier:\\nimages_resized = tf.image.resize(images, [224, 224])\\nThe tf.image.resize() will not preserve the aspect ratio. If this is\\na problem, you can try cropping the images to the appropriate\\naspect ratio before resizing. Both operations can be done in one\\nshot with tf.image.crop_and_resize().\\nThe pretrained models assume that the images are preprocessed in a specific way. In\\nsome cases they may expect the inputs to be scaled from 0 to 1, or -1 to 1, and so on.\\nEach model provides a preprocess_input() function that you can use to preprocess\\nyour images. These functions assume that the pixel values range from 0 to 255, so we\\nmust multiply them by 255 (since earlier we scaled them to the 0–1 range):\\ninputs = keras.applications.resnet50.preprocess_input(images_resized * 255)\\nNow we can use the pretrained model to make predictions:\\nY_proba = model.predict(inputs)\\nAs usual, the output Y_proba is a matrix with one row per image and one column per\\nclass (in this case, there are 1,000 classes). If you want to display the top K predic‐\\ntions, including the class name and the estimated probability of each predicted class,\\nyou can use the decode_predictions() function. For each image, it returns an array\\ncontaining the top K predictions, where each prediction is represented as an array\\ncontaining the class identifier21, its name and the corresponding confidence score:\\ntop_K = keras.applications.resnet50.decode_predictions(Y_proba, top=3)\\nfor image_index in range(len(images)):\\n    print(\"Image #{}\".format(image_index))\\n    for class_id, name, y_proba in top_K[image_index]:\\n        print(\"  {} - {:12s} {:.2f}%\".format(class_id, name, y_proba * 100))\\n    print()\\nThe output looks like this:\\nImage #0\\n  n03877845 - palace       42.87%\\n  n02825657 - bell_cote    40.57%\\n  n03781244 - monastery    14.56%\\n466 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 492}, page_content='Image #1\\n  n04522168 - vase         46.83%\\n  n07930864 - cup          7.78%\\n  n11939491 - daisy        4.87%\\nThe correct classes (monastery and daisy) appear in the top 3 results for both images.\\nThat’s pretty good considering that the model had to choose among 1,000 classes.\\nAs you can see, it is very easy to create a pretty good image classifier using a pre‐\\ntrained model. Other vision models are available in keras.applications, including\\nseveral ResNet variants, GoogLeNet variants like InceptionV3 and Xception,\\nVGGNet variants, MobileNet and MobileNetV2 (lightweight models for use in\\nmobile applications), and more.\\nBut what if you want to use an image classifier for classes of images that are not part\\nof ImageNet? In that case, you may still benefit from the pretrained models to per‐\\nform transfer learning.\\nPretrained Models for Transfer Learning\\nIf you want to build an image classifier, but you do not have enough training data,\\nthen it is often a good idea to reuse the lower layers of a pretrained model, as we dis‐\\ncussed in Chapter 11. For example, let’s train a model to classify pictures of flowers,\\nreusing a pretrained Xception model. First, let’s load the dataset using TensorFlow\\nDatasets (see Chapter 13):\\nimport tensorflow_datasets as tfds\\ndataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\\ndataset_size = info.splits[\"train\"].num_examples # 3670\\nclass_names = info.features[\"label\"].names # [\"dandelion\", \"daisy\", ...]\\nn_classes = info.features[\"label\"].num_classes # 5\\nNote that you can get information about the dataset by setting with_info=True. Here,\\nwe get the dataset size and the names of the classes. Unfortunately, there is only a\\n\"train\" dataset, no test set or validation set, so we need to split the training set. The\\nTF Datasets project provides an API for this. For example, let’s take the first 10% of\\nthe dataset for testing, the next 15% for validation, and the remaining 75% for train‐\\ning:\\ntest_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 15, 75])\\ntest_set = tfds.load(\"tf_flowers\", split=test_split, as_supervised=True)\\nvalid_set = tfds.load(\"tf_flowers\", split=valid_split, as_supervised=True)\\ntrain_set = tfds.load(\"tf_flowers\", split=train_split, as_supervised=True)\\nPretrained Models for Transfer Learning \\n| \\n467'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 493}, page_content='Next we must preprocess the images. The CNN expects 224 × 224 images, so we need\\nto resize them. We also need to run the image through Xception’s prepro\\ncess_input() function:\\ndef preprocess(image, label):\\n    resized_image = tf.image.resize(image, [224, 224])\\n    final_image = keras.applications.xception.preprocess_input(resized_image)\\n    return final_image, label\\nLet’s apply this preprocessing function to all 3 datasets, and let’s also shuffle & repeat\\nthe training set, and add batching & prefetching to all datasets:\\nbatch_size = 32\\ntrain_set = train_set.shuffle(1000).repeat()\\ntrain_set = train_set.map(preprocess).batch(batch_size).prefetch(1)\\nvalid_set = valid_set.map(preprocess).batch(batch_size).prefetch(1)\\ntest_set = test_set.map(preprocess).batch(batch_size).prefetch(1)\\nIf you want to perform some data augmentation, you can just change the preprocess‐\\ning function for the training set, adding some random transformations to the training\\nimages. For example, use tf.image.random_crop() to randomly crop the images, use\\ntf.image.random_flip_left_right() to randomly flip the images horizontally, and\\nso on (see the notebook for an example).\\nNext let’s load an Xception model, pretrained on ImageNet. We exclude the top of the\\nnetwork (by setting include_top=False): this excludes the global average pooling\\nlayer and the dense output layer. We then add our own global average pooling layer,\\nbased on the output of the base model, followed by a dense output layer with 1 unit\\nper class, using the softmax activation function. Finally, we create the Keras Model:\\nbase_model = keras.applications.xception.Xception(weights=\"imagenet\",\\n                                                  include_top=False)\\navg = keras.layers.GlobalAveragePooling2D()(base_model.output)\\noutput = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\\nmodel = keras.models.Model(inputs=base_model.input, outputs=output)\\nAs explained in Chapter 11, it’s usually a good idea to freeze the weights of the pre‐\\ntrained layers, at least at the beginning of training:\\nfor layer in base_model.layers:\\n    layer.trainable = False\\nSince our model uses the base model’s layers directly, rather than\\nthe base_model object itself, setting base_model.trainable=False\\nwould have no effect.\\nFinally, we can compile the model and start training:\\n468 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 494}, page_content='optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\\n              metrics=[\"accuracy\"])\\nhistory = model.fit(train_set,\\n                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\\n                    validation_data=valid_set,\\n                    validation_steps=int(0.15 * dataset_size / batch_size),\\n                    epochs=5)\\nThis will be very slow, unless you have a GPU. If you do not, then\\nyou should run this chapter’s notebook in Colab, using a GPU run‐\\ntime (it’s free!). See the instructions at https://github.com/ageron/\\nhandson-ml2.\\nAfter training the model for a few epochs, its validation accuracy should reach about\\n75-80%, and stop making much progress. This means that the top layers are now\\npretty well trained, so we are ready to unfreeze all layers (or you could try unfreezing\\njust the top ones), and continue training (don’t forget to compile the model when you\\nfreeze or unfreeze layers). This time we use a much lower learning rate to avoid dam‐\\naging the pretrained weights:\\nfor layer in base_model.layers:\\n    layer.trainable = True\\noptimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)\\nmodel.compile(...)\\nhistory = model.fit(...)\\nIt will take a while, but this model should reach around 95% accuracy on the test set.\\nWith that, you can start training amazing image classifiers! But there’s more to com‐\\nputer vision than just classification. For example, what if you also want to know where\\nthe flower is in the picture? Let’s look at this now.\\nClassification and Localization\\nLocalizing an object in a picture can be expressed as a regression task, as discussed in\\nChapter 10: to predict a bounding box around the object, a common approach is to\\npredict the horizontal and vertical coordinates of the object’s center, as well as its\\nheight and width. This means we have 4 numbers to predict. It does not require much\\nchange to the model, we just need to add a second dense output layer with 4 units\\n(typically on top of the global average pooling layer), and it can be trained using the\\nMSE loss:\\nbase_model = keras.applications.xception.Xception(weights=\"imagenet\",\\n                                                  include_top=False)\\navg = keras.layers.GlobalAveragePooling2D()(base_model.output)\\nclass_output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\\nClassification and Localization \\n| \\n469'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 495}, page_content='22 “Crowdsourcing in Computer Vision,” A. Kovashka et al. (2016).\\nloc_output = keras.layers.Dense(4)(avg)\\nmodel = keras.models.Model(inputs=base_model.input,\\n                           outputs=[class_output, loc_output])\\nmodel.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\\n              loss_weights=[0.8, 0.2], # depends on what you care most about\\n              optimizer=optimizer, metrics=[\"accuracy\"])\\nBut now we have a problem: the flowers dataset does not have bounding boxes\\naround the flowers. So we need to add them ourselves. This is often one of the hard‐\\nest and most costly part of a Machine Learning project: getting the labels. It’s a good\\nidea to spend time looking for the right tools. To annotate images with bounding\\nboxes, you may want to use an open source image labeling tool like VGG Image\\nAnnotator, LabelImg, OpenLabeler or ImgLab, or perhaps a commercial tool like\\nLabelBox or Supervisely. You may also want to consider crowdsourcing platforms\\nsuch as Amazon Mechanical Turk or CrowdFlower if you have a very large number of\\nimages to annotate. However, it is quite a lot of work to setup a crowdsourcing plat‐\\nform, prepare the form to be sent to the workers, to supervise them and ensure the\\nquality of the bounding boxes they produce is good, so make sure it is worth the\\neffort: if there are just a few thousand images to label, and you don’t plan to do this\\nfrequently, it may be preferable to do it yourself. Adriana Kovashka et al. wrote a very\\npractical paper22 about crowdsourcing in Computer Vision, I recommend you check\\nit out, even if you do not plan to use crowdsourcing.\\nSo let’s suppose you obtained the bounding boxes for every image in the flowers data‐\\nset (for now we will assume there is a single bounding box per image), you then need\\nto create a dataset whose items will be batches of preprocessed images along with\\ntheir class labels and their bounding boxes. Each item should be a tuple of the form:\\n(images, (class_labels, bounding_boxes)). Then you are ready to train your\\nmodel!\\nThe bounding boxes should be normalized so that the horizontal\\nand vertical coordinates, as well as the height and width all range\\nfrom 0 to 1. Also, it is common to predict the square root of the\\nheight and width rather than the height and width directly: this\\nway, a 10 pixel error for a large bounding box will not be penalized\\nas much as a 10 pixel error for a small bounding box.\\nThe MSE often works fairly well as a cost function to train the model, but it is not a\\ngreat metric to evaluate how well the model can predict bounding boxes. The most\\ncommon metric for this is the Intersection over Union (IoU): it is the area of overlap\\nbetween the predicted bounding box and the target bounding box, divided by the\\n470 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 496}, page_content='area of their union (see Figure 14-23). In tf.keras, it is implemented by the\\ntf.keras.metrics.MeanIoU class.\\nFigure 14-23. Intersection over Union (IoU) Metric for Bounding Boxes\\nClassifying and localizing a single object is nice, but what if the images contain multi‐\\nple objects (as is often the case in the flowers dataset)?\\nObject Detection\\nThe task of classifying and localizing multiple objects in an image is called object\\ndetection. Until a few years ago, a common approach was to take a CNN that was\\ntrained to classify and locate a single object, then slide it across the image, as shown\\nin Figure 14-24. In this example, the image was chopped into a 6 × 8 grid, and we\\nshow a CNN (the thick black rectangle) sliding across all 3 × 3 regions. When the\\nCNN was looking at the top left of the image, it detected part of the left-most rose,\\nand then it detected that same rose again when it was first shifted one step to the\\nright. At the next step, it started detecting part of the top-most rose, and then it detec‐\\nted it again once it was shifted one more step to the right. You would then continue to\\nslide the CNN through the whole image, looking at all 3 × 3 regions. Moreover, since\\nobjects can have varying sizes, you would also slide the CNN across regions of differ‐\\nent sizes. For example, once you are done with the 3 × 3 regions, you might want to\\nslide the CNN across all 4 × 4 regions as well.\\nObject Detection \\n| \\n471'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 497}, page_content='Figure 14-24. Detecting Multiple Objects by Sliding a CNN Across the Image\\nThis technique is fairly straightforward, but as you can see it will detect the same\\nobject multiple times, at slightly different positions. Some post-processing will then\\nbe needed to get rid of all the unnecessary bounding boxes. A common approach for\\nthis is called non-max suppression:\\n• First, you need to add an extra objectness output to your CNN, to estimate the\\nprobability that a flower is indeed present in the image (alternatively, you could\\nadd a “no-flower” class, but this usually does not work as well). It must use the\\nsigmoid activation function and you can train it using the \"binary_crossen\\ntropy\" loss. Then just get rid of all the bounding boxes for which the objectness\\nscore is below some threshold: this will drop all the bounding boxes that don’t\\nactually contain a flower.\\n• Second, find the bounding box with the highest objectness score, and get rid of\\nall the other bounding boxes that overlap a lot with it (e.g., with an IoU greater\\nthan 60%). For example, in Figure 14-24, the bounding box with the max object‐\\nness score is the thick bounding box over the top-most rose (the objectness score\\nis represented by the thickness of the bounding boxes). The other bounding box\\nover that same rose overlaps a lot with the max bounding box, so we will get rid\\nof it.\\n472 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 498}, page_content='23 “Fully Convolutional Networks for Semantic Segmentation,” J. Long, E. Shelhamer, T. Darrell (2015).\\n24 There is one small exception: a convolutional layer using VALID padding will complain if the input size is\\nsmaller than the kernel size.\\n• Third, repeat step two until there are no more bounding boxes to get rid of.\\nThis simple approach to object detection works pretty well, but it requires running\\nthe CNN many times, so it is quite slow. Fortunately, there is a much faster way to\\nslide a CNN across an image: using a Fully Convolutional Network.\\nFully Convolutional Networks (FCNs)\\nThe idea of FCNs was first introduced in a 2015 paper23 by Jonathan Long et al., for\\nsemantic segmentation (the task of classifying every pixel in an image according to\\nthe class of the object it belongs to). They pointed out that you could replace the\\ndense layers at the top of a CNN by convolutional layers. To understand this, let’s look\\nat an example: suppose a dense layer with 200 neurons sits on top of a convolutional\\nlayer that outputs 100 feature maps, each of size 7 × 7 (this is the feature map size, not\\nthe kernel size). Each neuron will compute a weighted sum of all 100 × 7 × 7 activa‐\\ntions from the convolutional layer (plus a bias term). Now let’s see what happens if we\\nreplace the dense layer with a convolution layer using 200 filters, each 7 × 7, and with\\nVALID padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel\\nis exactly the size of the input feature maps and we are using VALID padding). In\\nother words, it will output 200 numbers, just like the dense layer did, and if you look\\nclosely at the computations performed by a convolutional layer, you will notice that\\nthese numbers will be precisely the same as the dense layer produced. The only differ‐\\nence is that the dense layer’s output was a tensor of shape [batch size, 200] while the\\nconvolutional layer will output a tensor of shape [batch size, 1, 1, 200].\\nTo convert a dense layer to a convolutional layer, the number of fil‐\\nters in the convolutional layer must be equal to the number of units\\nin the dense layer, the filter size must be equal to the size of the\\ninput feature maps, and you must use VALID padding. The stride\\nmay be set to 1 or more, as we will see shortly.\\nWhy is this important? Well, while a dense layer expects a specific input size (since it\\nhas one weight per input feature), a convolutional layer will happily process images of\\nany size24 (however, it does expect its inputs to have a specific number of channels,\\nsince each kernel contains a different set of weights for each input channel). Since an\\nFCN contains only convolutional layers (and pooling layers, which have the same\\nproperty), it can be trained and executed on images of any size!\\nObject Detection \\n| \\n473'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 499}, page_content='25 This assumes we used only SAME padding in the network: indeed, VALID padding would reduce the size of\\nthe feature maps. Moreover, 448 can be neatly divided by 2 several times until we reach 7, without any round‐\\ning error. If any layer uses a different stride than 1 or 2, then there may be some rounding error, so again the\\nfeature maps may end up being smaller.\\nFor example, suppose we already trained a CNN for flower classification and localiza‐\\ntion. It was trained on 224 × 224 images and it outputs 10 numbers: outputs 0 to 4 are\\nsent through the softmax activation function, and this gives the class probabilities\\n(one per class); output 5 is sent through the logistic activation function, and this gives\\nthe objectness score; outputs 6 to 9 do not use any activation function, and they rep‐\\nresent the bounding box’s center coordinates, and its height and width. We can now\\nconvert its dense layers to convolutional layers. In fact, we don’t even need to retrain\\nit, we can just copy the weights from the dense layers to the convolutional layers!\\nAlternatively, we could have converted the CNN into an FCN before training.\\nNow suppose the last convolutional layer before the output layer (also called the bot‐\\ntleneck layer) outputs 7 × 7 feature maps when the network is fed a 224 × 224 image\\n(see the left side of Figure 14-25). If we feed the FCN a 448 × 448 image (see the right\\nside of Figure 14-25), the bottleneck layer will now output 14 × 14 feature maps.25\\nSince the dense output layer was replaced by a convolutional layer using 10 filters of\\nsize 7 × 7, VALID padding and stride 1, the output will be composed of 10 features\\nmaps, each of size 8 × 8 (since 14 - 7 + 1 = 8). In other words, the FCN will process\\nthe whole image only once and it will output an 8 × 8 grid where each cell contains 10\\nnumbers (5 class probabilities, 1 objectness score and 4 bounding box coordinates).\\nIt’s exactly like taking the original CNN and sliding it across the image using 8 steps\\nper row and 8 steps per column: to visualize this, imagine chopping the original\\nimage into a 14 × 14 grid, then sliding a 7 × 7 window across this grid: there will be 8\\n× 8 = 64 possible locations for the window, hence 8 × 8 predictions. However, the\\nFCN approach is much more efficient, since the network only looks at the image\\nonce. In fact, You Only Look Once (YOLO) is the name of a very popular object detec‐\\ntion architecture!\\n474 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 500}, page_content='26 “You Only Look Once: Unified, Real-Time Object Detection,” J. Redmon, S. Divvala, R. Girshick, A. Farhadi\\n(2015).\\n27 “YOLO9000: Better, Faster, Stronger,” J. Redmon, A. Farhadi (2016).\\n28 “YOLOv3: An Incremental Improvement,” J. Redmon, A. Farhadi (2018).\\nFigure 14-25. A Fully Convolutional Network Processing a Small Image (left) and a\\nLarge One (right)\\nYou Only Look Once (YOLO)\\nYOLO is an extremely fast and accurate object detection architecture proposed by\\nJoseph Redmon et al. in a 2015 paper26, and subsequently improved in 201627\\n(YOLOv2) and in 201828 (YOLOv3). It is so fast that it can run in realtime on a video\\n(check out this nice demo).\\nYOLOv3’s architecture is quite similar to the one we just discussed, but with a few\\nimportant differences:\\nObject Detection \\n| \\n475'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 501}, page_content='• First, it outputs 5 bounding boxes for each grid cell (instead of just 1), and each\\nbounding box comes with an objectness score. It also outputs 20 class probabili‐\\nties per grid cell, as it was trained on the PASCAL VOC dataset, which contains\\n20 classes. That’s a total of 45 numbers per grid cell (5 * 4 bounding box coordi‐\\nnates, plus 5 objectness scores, plus 20 class probabilities).\\n• Second, instead of predicting the absolute coordinates of the bounding box cen‐\\nters, YOLOv3 predicts an offset relative to the coordinates of the grid cell, where\\n(0, 0) means the top left of that cell, and (1, 1) means the bottom right. For each\\ngrid cell, YOLOv3 is trained to predict only bounding boxes whose center lies in\\nthat cell (but the bounding box itself generally extends well beyond the grid cell).\\nYOLOv3 applies the logistic activation function to the bounding box coordinates\\nto ensure they remain in the 0 to 1 range.\\n• Third, before training the neural net, YOLOv3 finds 5 representative bounding\\nbox dimensions, called anchor boxes (or bounding box priors): it does this by\\napplying the K-Means algorithm (see ???) to the height and width of the training\\nset bounding boxes. For example, if the training images contain many pedes‐\\ntrians, then one of the anchor boxes will likely have the dimensions of a typical\\npedestrian. Then when the neural net predicts 5 bounding boxes per grid cell, it\\nactually predicts how much to rescale each of the anchor boxes. For example,\\nsuppose one anchor box is 100 pixels tall and 50 pixels wide, and the network\\npredicts, say, a vertical rescaling factor of 1.5 and a horizontal rescaling of 0.9 (for\\none of the grid cells), this will result in a predicted bounding box of size 150 × 45\\npixels. To be more precise, for each grid cell and each anchor box, the network\\npredicts the log of the vertical and horizontal rescaling factors. Having these pri‐\\nors makes the network more likely to predict bounding boxes of the appropriate\\ndimensions, and it also speeds up training since it will more quickly learn what\\nreasonable bounding boxes look like.\\n• Fourth, the network is trained using images of different scales: every few batches\\nduring training, the network randomly chooses a new image dimension (from\\n330 × 330 to 608 × 608 pixels). This allows the network to learn to detect objects\\nat different scales. Moreover, it makes it possible to use YOLOv3 at different\\nscales: the smaller scale will be less accurate but faster than the larger scale, so\\nyou can choose the right tradeoff for your use case.\\nThere are a few more innovations you might be interested in, such as the use of skip\\nconnections to recover some of the spatial resolution that is lost in the CNN (we will\\ndiscuss this shortly when we look at semantic segmentation). Moreover, in the 2016\\npaper, the authors introduce the YOLO9000 model that uses hierarchical classifica‐\\ntion: the model predicts a probability for each node in a visual hierarchy called Word‐\\nTree. This makes it possible for the network to predict with high confidence that an\\nimage represents, say, a dog, even though it is unsure what specific type of dog it is.\\n476 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 502}, page_content='So I encourage you to go ahead and read all three papers: they are quite pleasant to\\nread, and it is an excellent example of how Deep Learning systems can be incremen‐\\ntally improved.\\nMean Average Precision (mAP)\\nA very common metric used in object detection tasks is the mean Average Precision\\n(mAP). “Mean Average” sounds a bit redundant, doesn’t it? To understand this met‐\\nric, let’s go back to two classification metrics we discussed in Chapter 3: precision and\\nrecall. Remember the tradeoff: the higher the recall, the lower the precision. You can\\nvisualize this in a Precision/Recall curve (see Figure 3-5). To summarize this curve\\ninto a single number, we could compute its Area Under the Curve (AUC). But note\\nthat the Precision/Recall curve may contain a few sections where precision actually\\ngoes up when recall increases, especially at low recall values (you can see this at the\\ntop left of Figure 3-5). This is one of the motivations for the mAP metric.\\nSuppose the classifier has a 90% precision at 10% recall, but a 96% precision at 20%\\nrecall: there’s really no tradeoff here: it simply makes more sense to use the classifier\\nat 20% recall rather than at 10% recall, as you will get both higher recall and higher\\nprecision. So instead of looking at the precision at 10% recall, we should really be\\nlooking at the maximum precision that the classifier can offer with at least 10% recall.\\nIt would be 96%, not 90%. So one way to get a fair idea of the model’s performance is\\nto compute the maximum precision you can get with at least 0% recall, then 10%\\nrecall, 20%, and so on up to 100%, and then calculate the mean of these maximum\\nprecisions. This is called the Average Precision (AP) metric. Now when there are more\\nthan 2 classes, we can compute the AP for each class, and then compute the mean AP\\n(mAP). That’s it!\\nHowever, in an object detection systems, there is an additional level of complexity:\\nwhat if the system detected the correct class, but at the wrong location (i.e., the\\nbounding box is completely off)? Surely we should not count this as a positive predic‐\\ntion. So one approach is to define an IOU threshold: for example, we may consider\\nthat a prediction is correct only if the IOU is greater than, say, 0.5, and the predicted\\nclass is correct. The corresponding mAP is generally noted mAP@0.5 (or mAP@50%,\\nor sometimes just AP50). In some competitions (such as the Pascal VOC challenge),\\nthis is what is done. In others (such as the COCO competition), the mAP is computed\\nfor different IOU thresholds (0.50, 0.55, 0.60, …, 0.95), and the final metric is the\\nmean of all these mAPs (noted AP@[.50:.95] or AP@[.50:0.05:.95]). Yes, that’s a mean\\nmean average.\\nSeveral YOLO implementations built using TensorFlow are available on github, some\\nwith pretrained weights. At the time of writing, they are based on TensorFlow 1, but\\nby the time you read this, TF 2 implementations will certainly be available. Moreover,\\nother object detection models are available in the TensorFlow Models project, many\\nObject Detection \\n| \\n477'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 503}, page_content='29 “SSD: Single Shot MultiBox Detector,” Wei Liu et al. (2015).\\n30 “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,” Shaoqing Ren et al.\\n(2015).\\nwith pretrained weights, and some have even been ported to TF Hub, making them\\nextremely easy to use, such as SSD29 and Faster-RCNN.30, which are both quite popu‐\\nlar. SSD is also a “single shot” detection model, quite similar to YOLO, while Faster R-\\nCNN is more complex: the image first goes through a CNN, and the output is passed\\nto a Region Proposal Network (RPN) which proposes bounding boxes that are most\\nlikely to contain an object, and a classifier is run for each bounding box, based on the\\ncropped output of the CNN.\\nThe choice of detection system depends on many factors: speed, accuracy, available\\npretrained models, training time, complexity, etc. The papers contain tables of met‐\\nrics, but there is quite a lot of variability in the testing environments, and the technol‐\\nogies evolve so fast that it is difficulty to make a fair comparison that will be useful for\\nmost people and remain valid for more than a few months.\\nGreat! So we can locate objects by drawing bounding boxes around them. But per‐\\nhaps you might want to be a bit more precise. Let’s see how to go down to the pixel\\nlevel.\\nSemantic Segmentation\\nIn semantic segmentation, each pixel is classified according to the class of the object it\\nbelongs to (e.g., road, car, pedestrian, building, etc.), as shown in Figure 14-26. Note\\nthat different objects of the same class are not distinguished. For example, all the bicy‐\\ncles on the right side of the segmented image end up as one big lump of pixels. The\\nmain difficulty in this task is that when images go through a regular CNN, they grad‐\\nually lose their spatial resolution (due to the layers with strides greater than 1): so a\\nregular CNN may end up knowing that there’s a person in the image, somewhere in\\nthe bottom left of the image, but it will not be much more precise than that.\\n478 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 504}, page_content='31 This type of layer is sometimes referred to as a deconvolution layer, but it does not perform what mathemati‐\\ncians call a deconvolution, so this name should be avoided.\\nFigure 14-26. Semantic segmentation\\nJust like for object detection, there are many different approaches to tackle this prob‐\\nlem, some quite complex. However, a fairly simple solution was proposed in the 2015\\npaper by Jonathan Long et al. we discussed earlier. They start by taking a pretrained\\nCNN and turning into an FCN, as discussed earlier. The CNN applies a stride of 32 to\\nthe input image overall (i.e., if you add up all the strides greater than 1), meaning the\\nlast layer outputs feature maps that are 32 times smaller than the input image. This is\\nclearly too coarse, so they add a single upsampling layer that multiplies the resolution\\nby 32. There are several solutions available for upsampling (increasing the size of an\\nimage), such as bilinear interpolation, but it only works reasonably well up to ×4 or\\n×8. Instead, they used a transposed convolutional layer:31 it is equivalent to first\\nstretching the image by inserting empty rows and columns (full of zeros), then per‐\\nforming a regular convolution (see Figure 14-27). Alternatively, some people prefer to\\nthink of it as a regular convolutional layer that uses fractional strides (e.g., 1/2 in\\nFigure 14-27). The transposed convolutional layer can be initialized to perform some‐\\nthing close to linear interpolation, but since it is a trainable layer, it will learn to do\\nbetter during training.\\nSemantic Segmentation \\n| \\n479'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 505}, page_content='Figure 14-27. Upsampling Using a Transpose Convolutional Layer\\nIn a transposed convolution layer, the stride defines how much the\\ninput will be stretched, not the size of the filter steps, so the larger\\nthe stride, the larger the output (unlike for convolutional layers or\\npooling layers).\\nTensorFlow Convolution Operations\\nTensorFlow also offers a few other kinds of convolutional layers:\\n• keras.layers.Conv1D creates a convolutional layer for 1D inputs, such as time\\nseries or text (sequences of letters or words), as we will see in ???.\\n• keras.layers.Conv3D creates a convolutional layer for 3D inputs, such as 3D\\nPET scan.\\n• Setting the dilation_rate hyperparameter of any convolutional layer to a value\\nof 2 or more creates an à-trous convolutional layer (“à trous” is French for “with\\nholes”). This is equivalent to using a regular convolutional layer with a filter dila‐\\nted by inserting rows and columns of zeros (i.e., holes). For example, a 1 × 3 filter\\nequal to [[1,2,3]] may be dilated with a dilation rate of 4, resulting in a dilated\\nfilter [[1, 0, 0, 0, 2, 0, 0, 0, 3]]. This allows the convolutional layer to\\n480 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 506}, page_content='have a larger receptive field at no computational price and using no extra param‐\\neters.\\n• tf.nn.depthwise_conv2d() can be used to create a depthwise convolutional layer\\n(but you need to create the variables yourself). It applies every filter to every\\nindividual input channel independently. Thus, if there are fn filters and fn′ input\\nchannels, then this will output fn × fn′ feature maps.\\nThis solution is okay, but still too imprecise. To do better, the authors added skip con‐\\nnections from lower layers: for example, they upsampled the output image by a factor\\nof 2 (instead of 32), and they added the output of a lower layer that had this double\\nresolution. Then they upsampled the result by a factor of 16, leading to a total upsam‐\\npling factor of 32 (see Figure 14-28). This recovered some of the spatial resolution\\nthat was lost in earlier pooling layers. In their best architecture, they used a second\\nsimilar skip connection to recover even finer details from an even lower layer: in\\nshort, the output of the original CNN goes through the following extra steps: upscale\\n×2, add the output of a lower layer (of the appropriate scale), upscale ×2, add the out‐\\nput of an even lower layer, and finally upscale ×8. It is even possible to scale up\\nbeyond the size of the original image: this can be used to increase the resolution of an\\nimage, which is a technique called super-resolution.\\nFigure 14-28. Skip layers recover some spatial resolution from lower layers\\nOnce again, many github repositories provide TensorFlow implementations of\\nsemantic segmentation (TensorFlow 1 for now), and you will even find a pretrained\\ninstance segmentation model in the TensorFlow Models project. Instance segmenta‐\\ntion is similar to semantic segmentation, but instead of merging all objects of the\\nsame class into one big lump, each object is distinguished from the others (e.g., it\\nidentifies each individual bicycle). At the present, they provide multiple implementa‐\\ntions of the Mask R-CNN architecture, which was proposed in a 2017 paper: it\\nextends the Faster R-CNN model by additionally producing a pixel-mask for each\\nbounding box. So not only do you get a bounding box around each object, with a set\\nof estimated class probabilities, you also get a pixel mask that locates pixels in the\\nbounding box that belong to the object.\\nSemantic Segmentation \\n| \\n481'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 507}, page_content='32 “Matrix Capsules with EM Routing,” G. Hinton, S. Sabour, N. Frosst (2018).\\nAs you can see, the field of Deep Computer Vision is vast and moving fast, with all\\nsorts of architectures popping out every year, all based on Convolutional Neural Net‐\\nworks. The progress made in just a few years has been astounding, and researchers\\nare now focusing on harder and harder problems, such as adversarial learning (which\\nattempts to make the network more resistant to images designed to fool it), explaina‐\\nbility (understanding why the network makes a specific classification), realistic image\\ngeneration (which we will come back to in ???), single-shot learning (a system that can\\nrecognize an object after it has seen it just once), and much more. Some even explore\\ncompletely novel architectures, such as Geoffrey Hinton’s capsule networks32 (I pre‐\\nsented them in a couple videos, with the corresponding code in a notebook). Now on\\nto the next chapter, where we will look at how to process sequential data such as time\\nseries using Recurrent Neural Networks and Convolutional Neural Networks.\\nExercises\\n1. What are the advantages of a CNN over a fully connected DNN for image classi‐\\nfication?\\n2. Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,\\na stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the\\nmiddle one outputs 200, and the top one outputs 400. The input images are RGB\\nimages of 200 × 300 pixels. What is the total number of parameters in the CNN?\\nIf we are using 32-bit floats, at least how much RAM will this network require\\nwhen making a prediction for a single instance? What about when training on a\\nmini-batch of 50 images?\\n3. If your GPU runs out of memory while training a CNN, what are five things you\\ncould try to solve the problem?\\n4. Why would you want to add a max pooling layer rather than a convolutional\\nlayer with the same stride?\\n5. When would you want to add a local response normalization layer?\\n6. Can you name the main innovations in AlexNet, compared to LeNet-5? What\\nabout the main innovations in GoogLeNet, ResNet, SENet and Xception?\\n7. What is a Fully Convolutional Network? How can you convert a dense layer into\\na convolutional layer?\\n8. What is the main technical difficulty of semantic segmentation?\\n9. Build your own CNN from scratch and try to achieve the highest possible accu‐\\nracy on MNIST.\\n482 \\n| \\nChapter 14: Deep Computer Vision Using Convolutional Neural Networks'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 508}, page_content='10. Use transfer learning for large image classification.\\na. Create a training set containing at least 100 images per class. For example, you\\ncould classify your own pictures based on the location (beach, mountain, city,\\netc.), or alternatively you can just use an existing dataset (e.g., from Tensor‐\\nFlow Datasets).\\nb. Split it into a training set, a validation set and a test set.\\nc. Build the input pipeline, including the appropriate preprocessing operations,\\nand optionally add data augmentation.\\nd. Fine-tune a pretrained model on this dataset.\\n11. Go through TensorFlow’s DeepDream tutorial. It is a fun way to familiarize your‐\\nself with various ways of visualizing the patterns learned by a CNN, and to gener‐\\nate art using Deep Learning.\\nSolutions to these exercises are available in ???.\\nExercises \\n| \\n483'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2019-05-07T15:51:31+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Hands-on-Machine-Learning.pdf', 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-06-13T02:46:20+00:00', 'trapped': '', 'modDate': 'D:20190613024620Z', 'creationDate': \"D:20190507155131+00'00'\", 'page': 509}, page_content='About the Author\\nAurélien Géron is a Machine Learning consultant. A former Googler, he led the You‐\\nTube video classification team from 2013 to 2016. He was also a founder and CTO of\\nWifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\\nof Polyconseil in 2001, the firm that now manages the electric car sharing service\\nAutolib’.\\nBefore this he worked as an engineer in a variety of domains: finance (JP Morgan and\\nSociété Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He\\npublished a few technical books (on C++, WiFi, and internet architectures), and was\\na Computer Science lecturer in a French engineering school.\\nA few fun facts: he taught his three children to count in binary with their fingers (up\\nto 1023), he studied microbiology and evolutionary genetics before going into soft‐\\nware engineering, and his parachute didn’t open on the second jump.\\nColophon\\nThe animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\\nsorFlow is the fire salamander (Salamandra salamandra), an amphibian found across\\nmost of Europe. Its black, glossy skin features large yellow spots on the head and\\nback, signaling the presence of alkaloid toxins. This is a possible source of this\\namphibian’s common name: contact with these toxins (which they can also spray\\nshort distances) causes convulsions and hyperventilation. Either the painful poisons\\nor the moistness of the salamander’s skin (or both) led to a misguided belief that these\\ncreatures not only could survive being placed in fire but could extinguish it as well.\\nFire salamanders live in shaded forests, hiding in moist crevices and under logs near\\nthe pools or other freshwater bodies that facilitate their breeding. Though they spend\\nmost of their life on land, they give birth to their young in water. They subsist mostly\\non a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\\nin length, and in captivity, may live as long as 50 years.\\nThe fire salamander’s numbers have been reduced by destruction of their forest habi‐\\ntat and capture for the pet trade, but the greatest threat is the susceptibility of their\\nmoisture-permeable skin to pollutants and microbes. Since 2014, they have become\\nextinct in parts of the Netherlands and Belgium due to an introduced fungus.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com.\\nThe cover image is from Wood’s Illustrated Natural History. The cover fonts are URW\\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\\nis Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 0}, page_content='Andreas C. Müller & Sarah Guido\\nIntroduction to \\nMachine \\nLearning  \\nwith Python  \\nA GUIDE FOR DATA SCIENTISTS'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 1}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 2}, page_content='Andreas C. Müller and Sarah Guido\\nIntroduction to Machine Learning\\nwith Python\\nA Guide for Data Scientists\\nBoston\\nFarnham\\nSebastopol\\nTokyo\\nBeijing\\nBoston\\nFarnham\\nSebastopol\\nTokyo\\nBeijing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 3}, page_content='978-1-449-36941-5\\n[LSI]\\nIntroduction to Machine Learning with Python\\nby Andreas C. Müller and Sarah Guido\\nCopyright © 2017 Sarah Guido, Andreas Müller. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles (http://safaribooksonline.com). For more information, contact our corporate/\\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\\nEditor: Dawn Schanafelt\\nProduction Editor: Kristen Brown\\nCopyeditor: Rachel Head\\nProofreader: Jasmine Kwityn\\nIndexer: Judy McConville\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Rebecca Demarest\\nOctober 2016:\\n First Edition\\nRevision History for the First Edition\\n2016-09-22: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introduction to Machine Learning with\\nPython, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 4}, page_content='Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  vii\\n1. Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nWhy Machine Learning?                                                                                                   1\\nProblems Machine Learning Can Solve                                                                      2\\nKnowing Your Task and Knowing Your Data                                                            4\\nWhy Python?                                                                                                                      5\\nscikit-learn                                                                                                                          5\\nInstalling scikit-learn                                                                                                     6\\nEssential Libraries and Tools                                                                                            7\\nJupyter Notebook                                                                                                           7\\nNumPy                                                                                                                             7\\nSciPy                                                                                                                                 8\\nmatplotlib                                                                                                                        9\\npandas                                                                                                                            10\\nmglearn                                                                                                                          11\\nPython 2 Versus Python 3                                                                                               12\\nVersions Used in this Book                                                                                             12\\nA First Application: Classifying Iris Species                                                                13\\nMeet the Data                                                                                                                14\\nMeasuring Success: Training and Testing Data                                                        17\\nFirst Things First: Look at Your Data                                                                        19\\nBuilding Your First Model: k-Nearest Neighbors                                                    20\\nMaking Predictions                                                                                                      22\\nEvaluating the Model                                                                                                   22\\nSummary and Outlook                                                                                                   23\\niii'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 5}, page_content='2. Supervised Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  25\\nClassification and Regression                                                                                         25\\nGeneralization, Overfitting, and Underfitting                                                             26\\nRelation of Model Complexity to Dataset Size                                                         29\\nSupervised Machine Learning Algorithms                                                                  29\\nSome Sample Datasets                                                                                                 30\\nk-Nearest Neighbors                                                                                                    35\\nLinear Models                                                                                                               45\\nNaive Bayes Classifiers                                                                                                 68\\nDecision Trees                                                                                                               70\\nEnsembles of Decision Trees                                                                                      83\\nKernelized Support Vector Machines                                                                        92\\nNeural Networks (Deep Learning)                                                                          104\\nUncertainty Estimates from Classifiers                                                                      119\\nThe Decision Function                                                                                              120\\nPredicting Probabilities                                                                                             122\\nUncertainty in Multiclass Classification                                                                 124\\nSummary and Outlook                                                                                                 127\\n3. Unsupervised Learning and Preprocessing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  131\\nTypes of Unsupervised Learning                                                                                 131\\nChallenges in Unsupervised Learning                                                                        132\\nPreprocessing and Scaling                                                                                            132\\nDifferent Kinds of Preprocessing                                                                             133\\nApplying Data Transformations                                                                               134\\nScaling Training and Test Data the Same Way                                                       136\\nThe Effect of Preprocessing on Supervised Learning                                           138\\nDimensionality Reduction, Feature Extraction, and Manifold Learning              140\\nPrincipal Component Analysis (PCA)                                                                    140\\nNon-Negative Matrix Factorization (NMF)                                                           156\\nManifold Learning with t-SNE                                                                                 163\\nClustering                                                                                                                        168\\nk-Means Clustering                                                                                                    168\\nAgglomerative Clustering                                                                                         182\\nDBSCAN                                                                                                                     187\\nComparing and Evaluating Clustering Algorithms                                              191\\nSummary of Clustering Methods                                                                             207\\nSummary and Outlook                                                                                                 208\\n4. Representing Data and Engineering Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  211\\nCategorical Variables                                                                                                     212\\nOne-Hot-Encoding (Dummy Variables)                                                                213\\niv \\n| \\nTable of Contents'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 6}, page_content='Numbers Can Encode Categoricals                                                                         218\\nBinning, Discretization, Linear Models, and Trees                                                   220\\nInteractions and Polynomials                                                                                      224\\nUnivariate Nonlinear Transformations                                                                      232\\nAutomatic Feature Selection                                                                                        236\\nUnivariate Statistics                                                                                                    236\\nModel-Based Feature Selection                                                                                238\\nIterative Feature Selection                                                                                         240\\nUtilizing Expert Knowledge                                                                                         242\\nSummary and Outlook                                                                                                 250\\n5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\\nCross-Validation                                                                                                            252\\nCross-Validation in scikit-learn                                                                               253\\nBenefits of Cross-Validation                                                                                     254\\nStratified k-Fold Cross-Validation and Other Strategies                                      254\\nGrid Search                                                                                                                     260\\nSimple Grid Search                                                                                                    261\\nThe Danger of Overfitting the Parameters and the Validation Set                     261\\nGrid Search with Cross-Validation                                                                          263\\nEvaluation Metrics and Scoring                                                                                   275\\nKeep the End Goal in Mind                                                                                      275\\nMetrics for Binary Classification                                                                             276\\nMetrics for Multiclass Classification                                                                       296\\nRegression Metrics                                                                                                     299\\nUsing Evaluation Metrics in Model Selection                                                        300\\nSummary and Outlook                                                                                                 302\\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\\nParameter Selection with Preprocessing                                                                    306\\nBuilding Pipelines                                                                                                          308\\nUsing Pipelines in Grid Searches                                                                                 309\\nThe General Pipeline Interface                                                                                    312\\nConvenient Pipeline Creation with make_pipeline                                              313\\nAccessing Step Attributes                                                                                          314\\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\\nGrid-Searching Preprocessing Steps and Model Parameters                                  317\\nGrid-Searching Which Model To Use                                                                        319\\nSummary and Outlook                                                                                                 320\\n7. Working with Text Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  323\\nTypes of Data Represented as Strings                                                                         323\\nTable of Contents \\n| \\nv'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 7}, page_content='Example Application: Sentiment Analysis of Movie Reviews                                 325\\nRepresenting Text Data as a Bag of Words                                                                 327\\nApplying Bag-of-Words to a Toy Dataset                                                               329\\nBag-of-Words for Movie Reviews                                                                            330\\nStopwords                                                                                                                       334\\nRescaling the Data with tf–idf                                                                                      336\\nInvestigating Model Coefficients                                                                                 338\\nBag-of-Words with More Than One Word (n-Grams)                                            339\\nAdvanced Tokenization, Stemming, and Lemmatization                                        344\\nTopic Modeling and Document Clustering                                                               347\\nLatent Dirichlet Allocation                                                                                       348\\nSummary and Outlook                                                                                                 355\\n8. Wrapping Up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357\\nApproaching a Machine Learning Problem                                                               357\\nHumans in the Loop                                                                                                  358\\nFrom Prototype to Production                                                                                    359\\nTesting Production Systems                                                                                         359\\nBuilding Your Own Estimator                                                                                     360\\nWhere to Go from Here                                                                                                361\\nTheory                                                                                                                          361\\nOther Machine Learning Frameworks and Packages                                           362\\nRanking, Recommender Systems, and Other Kinds of Learning                       363\\nProbabilistic Modeling, Inference, and Probabilistic Programming                  363\\nNeural Networks                                                                                                        364\\nScaling to Larger Datasets                                                                                         364\\nHoning Your Skills                                                                                                     365\\nConclusion                                                                                                                      366\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  367\\nvi \\n| \\nTable of Contents'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 8}, page_content='Preface\\nMachine learning is an integral part of many commercial applications and research\\nprojects today, in areas ranging from medical diagnosis and treatment to finding your\\nfriends on social networks. Many people think that machine learning can only be\\napplied by large companies with extensive research teams. In this book, we want to\\nshow you how easy it can be to build machine learning solutions yourself, and how to\\nbest go about it. With the knowledge in this book, you can build your own system for\\nfinding out how people feel on Twitter, or making predictions about global warming.\\nThe applications of machine learning are endless and, with the amount of data avail‐\\nable today, mostly limited by your imagination.\\nWho Should Read This Book\\nThis book is for current and aspiring machine learning practitioners looking to\\nimplement solutions to real-world machine learning problems. This is an introduc‐\\ntory book requiring no previous knowledge of machine learning or artificial intelli‐\\ngence (AI). We focus on using Python and the scikit-learn library, and work\\nthrough all the steps to create a successful machine learning application. The meth‐\\nods we introduce will be helpful for scientists and researchers, as well as data scien‐\\ntists working on commercial applications. You will get the most out of the book if you\\nare somewhat familiar with Python and the NumPy and matplotlib libraries.\\nWe made a conscious effort not to focus too much on the math, but rather on the\\npractical aspects of using machine learning algorithms. As mathematics (probability\\ntheory, in particular) is the foundation upon which machine learning is built, we\\nwon’t go into the analysis of the algorithms in great detail. If you are interested in the\\nmathematics of machine learning algorithms, we recommend the book The Elements\\nof Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\\nFriedman, which is available for free at the authors’ website. We will also not describe\\nhow to write machine learning algorithms from scratch, and will instead focus on\\nvii'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 9}, page_content='how to use the large array of models already implemented in scikit-learn and other\\nlibraries.\\nWhy We Wrote This Book\\nThere are many books on machine learning and AI. However, all of them are meant\\nfor graduate students or PhD students in computer science, and they’re full of\\nadvanced mathematics. This is in stark contrast with how machine learning is being\\nused, as a commodity tool in research and commercial applications. Today, applying\\nmachine learning does not require a PhD. However, there are few resources out there\\nthat fully cover all the important aspects of implementing machine learning in prac‐\\ntice, without requiring you to take advanced math courses. We hope this book will\\nhelp people who want to apply machine learning without reading up on years’ worth\\nof calculus, linear algebra, and probability theory.\\nNavigating This Book\\nThis book is organized roughly as follows:\\n• Chapter 1 introduces the fundamental concepts of machine learning and its\\napplications, and describes the setup we will be using throughout the book.\\n• Chapters 2 and 3 describe the actual machine learning algorithms that are most\\nwidely used in practice, and discuss their advantages and shortcomings.\\n• Chapter 4 discusses the importance of how we represent data that is processed by\\nmachine learning, and what aspects of the data to pay attention to.\\n• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\\nwith a particular focus on cross-validation and grid search.\\n• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\\ning your workflow.\\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\\ndata, and introduces some text-specific processing techniques.\\n• Chapter 8 offers a high-level overview, and includes references to more advanced\\ntopics.\\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\\nalgorithms might not be necessary for a beginner. If you need to build a machine\\nlearning system ASAP, we suggest starting with Chapter 1 and the opening sections of\\nChapter 2, which introduce all the core concepts. You can then skip to “Summary and\\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models\\nthat we cover. Choose the model that best fits your needs and flip back to read the\\nviii \\n| \\nPreface'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 10}, page_content='section devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\\nuate and tune your model.\\nOnline Resources\\nWhile studying this book, definitely refer to the scikit-learn website for more in-\\ndepth documentation of the classes and functions, and many examples. There is also\\na video course created by Andreas Müller, “Advanced Machine Learning with scikit-\\nlearn,” \\nthat \\nsupplements \\nthis \\nbook. \\nYou \\ncan \\nfind \\nit \\nat \\nhttp://bit.ly/\\nadvanced_machine_learning_scikit-learn.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords. Also used for commands and module and\\npackage names.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nThis element signifies a tip or suggestion.\\nThis element signifies a general note.\\nPreface \\n| \\nix'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 11}, page_content='This icon indicates a warning or caution.\\nUsing Code Examples\\nSupplemental material (code examples, IPython notebooks, etc.) is available for\\ndownload at https://github.com/amueller/introduction_to_ml_with_python.\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. You do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “An Introduction to Machine Learning\\nwith Python by Andreas C. Müller and Sarah Guido (O’Reilly). Copyright 2017 Sarah\\nGuido and Andreas Müller, 978-1-449-36941-5.”\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com.\\nSafari® Books Online\\nSafari Books Online is an on-demand digital library that deliv‐\\ners expert content in both book and video form from the\\nworld’s leading authors in technology and business.\\nTechnology professionals, software developers, web designers, and business and crea‐\\ntive professionals use Safari Books Online as their primary resource for research,\\nproblem solving, learning, and certification training.\\nSafari Books Online offers a range of plans and pricing for enterprise, government,\\neducation, and individuals.\\nMembers have access to thousands of books, training videos, and prepublication\\nmanuscripts in one fully searchable database from publishers like O’Reilly Media,\\nPrentice Hall Professional, Addison-Wesley Professional, Microsoft Press, Sams, Que,\\nx \\n| \\nPreface'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 12}, page_content='Peachpit Press, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kauf‐\\nmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders,\\nMcGraw-Hill, Jones & Bartlett, Course Technology, and hundreds more. For more\\ninformation about Safari Books Online, please visit us online.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. You can access this page at http://bit.ly/intro-machine-learning-python.\\nTo comment or ask technical questions about this book, send email to bookques‐\\ntions@oreilly.com.\\nFor more information about our books, courses, conferences, and news, see our web‐\\nsite at http://www.oreilly.com.\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on YouTube: http://www.youtube.com/oreillymedia\\nAcknowledgments\\nFrom Andreas\\nWithout the help and support of a large group of people, this book would never have\\nexisted.\\nI would like to thank the editors, Meghan Blanchette, Brian MacDonald, and in par‐\\nticular Dawn Schanafelt, for helping Sarah and me make this book a reality.\\nI want to thank my reviewers, Thomas Caswell, Olivier Grisel, Stefan van der Walt,\\nand John Myles White, who took the time to read the early versions of this book and\\nprovided me with invaluable feedback—in addition to being some of the corner‐\\nstones of the scientific open source ecosystem.\\nPreface \\n| \\nxi'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 13}, page_content='I am forever thankful for the welcoming open source scientific Python community,\\nespecially the contributors to scikit-learn. Without the support and help from this\\ncommunity, in particular from Gael Varoquaux, Alex Gramfort, and Olivier Grisel, I\\nwould never have become a core contributor to scikit-learn or learned to under‐\\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\\nutors who donate their time to improve and maintain this package.\\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\\nped me understand the challenges of machine learning and gave me ideas for struc‐\\nturing a textbook. Among the people I talk to about machine learning, I specifically\\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\\nand Dan Cervone.\\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\\nof an early version of this book, and helped me shape it in many ways.\\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\\nMiriam, for their continuing support and encouragement. I also want to thank the\\nmany people in my life whose love and friendship gave me the energy and support to\\nundertake such a challenging task.\\nFrom Sarah\\nI would like to thank Meg Blanchette, without whose help and guidance this project\\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\\nto DTS, for your everlasting and endless support.\\nxii \\n| \\nPreface'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 14}, page_content='CHAPTER 1\\nIntroduction\\nMachine learning is about extracting knowledge from data. It is a research field at the\\nintersection of statistics, artificial intelligence, and computer science and is also\\nknown as predictive analytics or statistical learning. The application of machine\\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\\nmatic recommendations of which movies to watch, to what food to order or which\\nproducts to buy, to personalized online radio and recognizing your friends in your\\nphotos, many modern websites and devices have machine learning algorithms at their\\ncore. When you look at a complex website like Facebook, Amazon, or Netflix, it is\\nvery likely that every part of the site contains multiple machine learning models.\\nOutside of commercial applications, machine learning has had a tremendous influ‐\\nence on the way data-driven research is done today. The tools introduced in this book\\nhave been applied to diverse scientific problems such as understanding stars, finding\\ndistant planets, discovering new particles, analyzing DNA sequences, and providing\\npersonalized cancer treatments.\\nYour application doesn’t need to be as large-scale or world-changing as these exam‐\\nples in order to benefit from machine learning, though. In this chapter, we will\\nexplain why machine learning has become so popular and discuss what kinds of\\nproblems can be solved using machine learning. Then, we will show you how to build\\nyour first machine learning model, introducing important concepts along the way.\\nWhy Machine Learning?\\nIn the early days of “intelligent” applications, many systems used handcoded rules of\\n“if” and “else” decisions to process data or adjust to user input. Think of a spam filter\\nwhose job is to move the appropriate incoming email messages to a spam folder. You\\ncould make up a blacklist of words that would result in an email being marked as\\n1'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 15}, page_content='spam. This would be an example of using an expert-designed rule system to design an\\n“intelligent” application. Manually crafting decision rules is feasible for some applica‐\\ntions, particularly those in which humans have a good understanding of the process\\nto model. However, using handcoded rules to make decisions has two major disad‐\\nvantages:\\n• The logic required to make a decision is specific to a single domain and task.\\nChanging the task even slightly might require a rewrite of the whole system.\\n• Designing rules requires a deep understanding of how a decision should be made\\nby a human expert.\\nOne example of where this handcoded approach will fail is in detecting faces in\\nimages. Today, every smartphone can detect a face in an image. However, face detec‐\\ntion was an unsolved problem until as recently as 2001. The main problem is that the\\nway in which pixels (which make up an image in a computer) are “perceived” by the\\ncomputer is very different from how humans perceive a face. This difference in repre‐\\nsentation makes it basically impossible for a human to come up with a good set of\\nrules to describe what constitutes a face in a digital image.\\nUsing machine learning, however, simply presenting a program with a large collec‐\\ntion of images of faces is enough for an algorithm to determine what characteristics\\nare needed to identify a face.\\nProblems Machine Learning Can Solve\\nThe most successful kinds of machine learning algorithms are those that automate\\ndecision-making processes by generalizing from known examples. In this setting,\\nwhich is known as supervised learning, the user provides the algorithm with pairs of\\ninputs and desired outputs, and the algorithm finds a way to produce the desired out‐\\nput given an input. In particular, the algorithm is able to create an output for an input\\nit has never seen before without any help from a human. Going back to our example\\nof spam classification, using machine learning, the user provides the algorithm with a\\nlarge number of emails (which are the input), together with information about\\nwhether any of these emails are spam (which is the desired output). Given a new\\nemail, the algorithm will then produce a prediction as to whether the new email is\\nspam.\\nMachine learning algorithms that learn from input/output pairs are called supervised\\nlearning algorithms because a “teacher” provides supervision to the algorithms in the\\nform of the desired outputs for each example that they learn from. While creating a\\ndataset of inputs and outputs is often a laborious manual process, supervised learning\\nalgorithms are well understood and their performance is easy to measure. If your\\napplication can be formulated as a supervised learning problem, and you are able to\\n2 \\n| \\nChapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 16}, page_content='create a dataset that includes the desired outcome, machine learning will likely be\\nable to solve your problem.\\nExamples of supervised machine learning tasks include:\\nIdentifying the zip code from handwritten digits on an envelope\\nHere the input is a scan of the handwriting, and the desired output is the actual\\ndigits in the zip code. To create a dataset for building a machine learning model,\\nyou need to collect many envelopes. Then you can read the zip codes yourself\\nand store the digits as your desired outcomes.\\nDetermining whether a tumor is benign based on a medical image\\nHere the input is the image, and the output is whether the tumor is benign. To\\ncreate a dataset for building a model, you need a database of medical images. You\\nalso need an expert opinion, so a doctor needs to look at all of the images and\\ndecide which tumors are benign and which are not. It might even be necessary to\\ndo additional diagnosis beyond the content of the image to determine whether\\nthe tumor in the image is cancerous or not.\\nDetecting fraudulent activity in credit card transactions\\nHere the input is a record of the credit card transaction, and the output is\\nwhether it is likely to be fraudulent or not. Assuming that you are the entity dis‐\\ntributing the credit cards, collecting a dataset means storing all transactions and\\nrecording if a user reports any transaction as fraudulent.\\nAn interesting thing to note about these examples is that although the inputs and out‐\\nputs look fairly straightforward, the data collection process for these three tasks is\\nvastly different. While reading envelopes is laborious, it is easy and cheap. Obtaining\\nmedical imaging and diagnoses, on the other hand, requires not only expensive\\nmachinery but also rare and expensive expert knowledge, not to mention the ethical\\nconcerns and privacy issues. In the example of detecting credit card fraud, data col‐\\nlection is much simpler. Your customers will provide you with the desired output, as\\nthey will report fraud. All you have to do to obtain the input/output pairs of fraudu‐\\nlent and nonfraudulent activity is wait.\\nUnsupervised algorithms are the other type of algorithm that we will cover in this\\nbook. In unsupervised learning, only the input data is known, and no known output\\ndata is given to the algorithm. While there are many successful applications of these\\nmethods, they are usually harder to understand and evaluate.\\nExamples of unsupervised learning include:\\nIdentifying topics in a set of blog posts\\nIf you have a large collection of text data, you might want to summarize it and\\nfind prevalent themes in it. You might not know beforehand what these topics\\nare, or how many topics there might be. Therefore, there are no known outputs.\\nWhy Machine Learning? \\n| \\n3'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 17}, page_content='Segmenting customers into groups with similar preferences\\nGiven a set of customer records, you might want to identify which customers are\\nsimilar, and whether there are groups of customers with similar preferences. For\\na shopping site, these might be “parents,” “bookworms,” or “gamers.” Because you\\ndon’t know in advance what these groups might be, or even how many there are,\\nyou have no known outputs.\\nDetecting abnormal access patterns to a website\\nTo identify abuse or bugs, it is often helpful to find access patterns that are differ‐\\nent from the norm. Each abnormal pattern might be very different, and you\\nmight not have any recorded instances of abnormal behavior. Because in this\\nexample you only observe traffic, and you don’t know what constitutes normal\\nand abnormal behavior, this is an unsupervised problem.\\nFor both supervised and unsupervised learning tasks, it is important to have a repre‐\\nsentation of your input data that a computer can understand. Often it is helpful to\\nthink of your data as a table. Each data point that you want to reason about (each\\nemail, each customer, each transaction) is a row, and each property that describes that\\ndata point (say, the age of a customer or the amount or location of a transaction) is a\\ncolumn. You might describe users by their age, their gender, when they created an\\naccount, and how often they have bought from your online shop. You might describe\\nthe image of a tumor by the grayscale values of each pixel, or maybe by using the size,\\nshape, and color of the tumor.\\nEach entity or row here is known as a sample (or data point) in machine learning,\\nwhile the columns—the properties that describe these entities—are called features.\\nLater in this book we will go into more detail on the topic of building a good repre‐\\nsentation of your data, which is called feature extraction or feature engineering. You\\nshould keep in mind, however, that no machine learning algorithm will be able to\\nmake a prediction on data for which it has no information. For example, if the only\\nfeature that you have for a patient is their last name, no algorithm will be able to pre‐\\ndict their gender. This information is simply not contained in your data. If you add\\nanother feature that contains the patient’s first name, you will have much better luck,\\nas it is often possible to tell the gender by a person’s first name.\\nKnowing Your Task and Knowing Your Data\\nQuite possibly the most important part in the machine learning process is under‐\\nstanding the data you are working with and how it relates to the task you want to\\nsolve. It will not be effective to randomly choose an algorithm and throw your data at\\nit. It is necessary to understand what is going on in your dataset before you begin\\nbuilding a model. Each algorithm is different in terms of what kind of data and what\\nproblem setting it works best for. While you are building a machine learning solution,\\nyou should answer, or at least keep in mind, the following questions:\\n4 \\n| \\nChapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 18}, page_content='• What question(s) am I trying to answer? Do I think the data collected can answer\\nthat question?\\n• What is the best way to phrase my question(s) as a machine learning problem?\\n• Have I collected enough data to represent the problem I want to solve?\\n• What features of the data did I extract, and will these enable the right\\npredictions?\\n• How will I measure success in my application?\\n• How will the machine learning solution interact with other parts of my research\\nor business product?\\nIn a larger context, the algorithms and methods in machine learning are only one\\npart of a greater process to solve a particular problem, and it is good to keep the big\\npicture in mind at all times. Many people spend a lot of time building complex\\nmachine learning solutions, only to find out they don’t solve the right problem.\\nWhen going deep into the technical aspects of machine learning (as we will in this\\nbook), it is easy to lose sight of the ultimate goals. While we will not discuss the ques‐\\ntions listed here in detail, we still encourage you to keep in mind all the assumptions\\nthat you might be making, explicitly or implicitly, when you start building machine\\nlearning models.\\nWhy Python?\\nPython has become the lingua franca for many data science applications. It combines\\nthe power of general-purpose programming languages with the ease of use of\\ndomain-specific scripting languages like MATLAB or R. Python has libraries for data\\nloading, visualization, statistics, natural language processing, image processing, and\\nmore. This vast toolbox provides data scientists with a large array of general- and\\nspecial-purpose functionality. One of the main advantages of using Python is the abil‐\\nity to interact directly with the code, using a terminal or other tools like the Jupyter\\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\\nmentally iterative processes, in which the data drives the analysis. It is essential for\\nthese processes to have tools that allow quick iteration and easy interaction.\\nAs a general-purpose programming language, Python also allows for the creation of\\ncomplex graphical user interfaces (GUIs) and web services, and for integration into\\nexisting systems.\\nscikit-learn\\nscikit-learn is an open source project, meaning that it is free to use and distribute,\\nand anyone can easily obtain the source code to see what is going on behind the\\nWhy Python? \\n| \\n5'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 19}, page_content='scenes. The scikit-learn project is constantly being developed and improved, and it\\nhas a very active user community. It contains a number of state-of-the-art machine\\nlearning algorithms, as well as comprehensive documentation about each algorithm.\\nscikit-learn is a very popular tool, and the most prominent Python library for\\nmachine learning. It is widely used in industry and academia, and a wealth of tutori‐\\nals and code snippets are available online. scikit-learn works well with a number of\\nother scientific Python tools, which we will discuss later in this chapter.\\nWhile reading this, we recommend that you also browse the scikit-learn user guide \\nand API documentation for additional details on and many more options for each\\nalgorithm. The online documentation is very thorough, and this book will provide\\nyou with all the prerequisites in machine learning to understand it in detail.\\nInstalling scikit-learn\\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\\nting and interactive development, you should also install matplotlib, IPython, and\\nthe Jupyter Notebook. We recommend using one of the following prepackaged\\nPython distributions, which will provide the necessary packages:\\nAnaconda\\nA Python distribution made for large-scale data processing, predictive analytics,\\nand scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\\npandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\\nWindows, and Linux, it is a very convenient solution and is the one we suggest\\nfor people without an existing installation of the scientific Python packages. Ana‐\\nconda now also includes the commercial Intel MKL library for free. Using MKL\\n(which is done automatically when Anaconda is installed) can give significant\\nspeed improvements for many algorithms in scikit-learn.\\nEnthought Canopy\\nAnother Python distribution for scientific computing. This comes with NumPy,\\nSciPy, matplotlib, pandas, and IPython, but the free version does not come with\\nscikit-learn. If you are part of an academic, degree-granting institution, you\\ncan request an academic license and get free access to the paid subscription ver‐\\nsion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\\nworks on Mac OS, Windows, and Linux.\\nPython(x,y)\\nA free Python distribution for scientific computing, specifically for Windows.\\nPython(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\\nscikit-learn.\\n6 \\n| \\nChapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 20}, page_content='1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\\nture Notes.\\nIf you already have a Python installation set up, you can use pip to install all of these\\npackages:\\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\\nEssential Libraries and Tools\\nUnderstanding what scikit-learn is and how to use it is important, but there are a\\nfew other libraries that will enhance your experience. scikit-learn is built on top of\\nthe NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\\nwill be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\\nwhich is a browser-based interactive programming environment. Briefly, here is what\\nyou should know about these tools in order to get the most out of scikit-learn.1\\nJupyter Notebook\\nThe Jupyter Notebook is an interactive environment for running code in the browser.\\nIt is a great tool for exploratory data analysis and is widely used by data scientists.\\nWhile the Jupyter Notebook supports many programming languages, we only need\\nthe Python support. The Jupyter Notebook makes it easy to incorporate code, text,\\nand images, and all of this book was in fact written as a Jupyter Notebook. All of the\\ncode examples we include can be downloaded from GitHub.\\nNumPy\\nNumPy is one of the fundamental packages for scientific computing in Python. It\\ncontains functionality for multidimensional arrays, high-level mathematical func‐\\ntions such as linear algebra operations and the Fourier transform, and pseudorandom\\nnumber generators.\\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\\ntakes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\\nverted to a NumPy array. The core functionality of NumPy is the ndarray class, a\\nmultidimensional (n-dimensional) array. All elements of the array must be of the\\nsame type. A NumPy array looks like this:\\nIn[2]:\\nimport numpy as np\\nx = np.array([[1, 2, 3], [4, 5, 6]])\\nprint(\"x:\\\\n{}\".format(x))\\nEssential Libraries and Tools \\n| \\n7'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 21}, page_content='Out[2]:\\nx:\\n[[1 2 3]\\n [4 5 6]]\\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\\nndarray class as “NumPy arrays” or just “arrays.”\\nSciPy\\nSciPy is a collection of functions for scientific computing in Python. It provides,\\namong other functionality, advanced linear algebra routines, mathematical function\\noptimization, signal processing, special mathematical functions, and statistical distri‐\\nbutions. scikit-learn draws from SciPy’s collection of functions for implementing\\nits algorithms. The most important part of SciPy for us is scipy.sparse: this provides\\nsparse matrices, which are another representation that is used for data in scikit-\\nlearn. Sparse matrices are used whenever we want to store a 2D array that contains\\nmostly zeros:\\nIn[3]:\\nfrom scipy import sparse\\n# Create a 2D NumPy array with a diagonal of ones, and zeros everywhere else\\neye = np.eye(4)\\nprint(\"NumPy array:\\\\n{}\".format(eye))\\nOut[3]:\\nNumPy array:\\n[[ 1.  0.  0.  0.]\\n [ 0.  1.  0.  0.]\\n [ 0.  0.  1.  0.]\\n [ 0.  0.  0.  1.]]\\nIn[4]:\\n# Convert the NumPy array to a SciPy sparse matrix in CSR format\\n# Only the nonzero entries are stored\\nsparse_matrix = sparse.csr_matrix(eye)\\nprint(\"\\\\nSciPy sparse CSR matrix:\\\\n{}\".format(sparse_matrix))\\nOut[4]:\\nSciPy sparse CSR matrix:\\n  (0, 0)    1.0\\n  (1, 1)    1.0\\n  (2, 2)    1.0\\n  (3, 3)    1.0\\n8 \\n| \\nChapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 22}, page_content='Usually it is not possible to create dense representations of sparse data (as they would\\nnot fit into memory), so we need to create sparse representations directly. Here is a\\nway to create the same sparse matrix as before, using the COO format:\\nIn[5]:\\ndata = np.ones(4)\\nrow_indices = np.arange(4)\\ncol_indices = np.arange(4)\\neye_coo = sparse.coo_matrix((data, (row_indices, col_indices)))\\nprint(\"COO representation:\\\\n{}\".format(eye_coo))\\nOut[5]:\\nCOO representation:\\n  (0, 0)    1.0\\n  (1, 1)    1.0\\n  (2, 2)    1.0\\n  (3, 3)    1.0\\nMore details on SciPy sparse matrices can be found in the SciPy Lecture Notes.\\nmatplotlib\\nmatplotlib is the primary scientific plotting library in Python. It provides functions\\nfor making publication-quality visualizations such as line charts, histograms, scatter\\nplots, and so on. Visualizing your data and different aspects of your analysis can give\\nyou important insights, and we will be using matplotlib for all our visualizations.\\nWhen working inside the Jupyter Notebook, you can show figures directly in the\\nbrowser by using the %matplotlib notebook and %matplotlib inline commands.\\nWe recommend using %matplotlib notebook, which provides an interactive envi‐\\nronment (though we are using %matplotlib inline to produce this book). For\\nexample, this code produces the plot in Figure 1-1:\\nIn[6]:\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\n# Generate a sequence of numbers from -10 to 10 with 100 steps in between\\nx = np.linspace(-10, 10, 100)\\n# Create a second array using sine\\ny = np.sin(x)\\n# The plot function makes a line chart of one array against another\\nplt.plot(x, y, marker=\"x\")\\nEssential Libraries and Tools \\n| \\n9'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 23}, page_content='Figure 1-1. Simple line plot of the sine function using matplotlib\\npandas\\npandas is a Python library for data wrangling and analysis. It is built around a data\\nstructure called the DataFrame that is modeled after the R DataFrame. Simply put, a\\npandas DataFrame is a table, similar to an Excel spreadsheet. pandas provides a great\\nrange of methods to modify and operate on this table; in particular, it allows SQL-like\\nqueries and joins of tables. In contrast to NumPy, which requires that all entries in an\\narray be of the same type, pandas allows each column to have a separate type (for\\nexample, integers, dates, floating-point numbers, and strings). Another valuable tool\\nprovided by pandas is its ability to ingest from a great variety of file formats and data‐\\nbases, like SQL, Excel files, and comma-separated values (CSV) files. Going into\\ndetail about the functionality of pandas is out of the scope of this book. However,\\nPython for Data Analysis by Wes McKinney (O’Reilly, 2012) provides a great guide.\\nHere is a small example of creating a DataFrame using a dictionary:\\nIn[7]:\\nimport pandas as pd\\n# create a simple dataset of people\\ndata = {\\'Name\\': [\"John\", \"Anna\", \"Peter\", \"Linda\"],\\n        \\'Location\\' : [\"New York\", \"Paris\", \"Berlin\", \"London\"],\\n        \\'Age\\' : [24, 13, 53, 33]\\n       }\\ndata_pandas = pd.DataFrame(data)\\n# IPython.display allows \"pretty printing\" of dataframes\\n# in the Jupyter notebook\\ndisplay(data_pandas)\\n10 \\n| \\nChapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 24}, page_content='This produces the following output:\\nAge\\nLocation\\nName\\n0\\n24\\nNew York\\nJohn\\n1\\n13\\nParis\\nAnna\\n2\\n53\\nBerlin\\nPeter\\n3\\n33\\nLondon\\nLinda\\nThere are several possible ways to query this table. For example:\\nIn[8]:\\n# Select all rows that have an age column greater than 30\\ndisplay(data_pandas[data_pandas.Age > 30])\\nThis produces the following result:\\nAge\\nLocation\\nName\\n2\\n53\\nBerlin\\nPeter\\n3\\n33\\nLondon\\nLinda\\nmglearn\\nThis book comes with accompanying code, which you can find on GitHub. The\\naccompanying code includes not only all the examples shown in this book, but also\\nthe mglearn library. This is a library of utility functions we wrote for this book, so\\nthat we don’t clutter up our code listings with details of plotting and data loading. If\\nyou’re interested, you can look up all the functions in the repository, but the details of\\nthe mglearn module are not really important to the material in this book. If you see a\\ncall to mglearn in the code, it is usually a way to make a pretty picture quickly, or to\\nget our hands on some interesting data.\\nThroughout the book we make ample use of NumPy, matplotlib\\nand pandas. All the code will assume the following imports:\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport mglearn\\nWe also assume that you will run the code in a Jupyter Notebook\\nwith the %matplotlib notebook or %matplotlib inline magic\\nenabled to show plots. If you are not using the notebook or these\\nmagic commands, you will have to call plt.show to actually show\\nany of the figures.\\nEssential Libraries and Tools \\n| \\n11'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 25}, page_content='2 The six package can be very handy for that.\\nPython 2 Versus Python 3\\nThere are two major versions of Python that are widely used at the moment: Python 2\\n(more precisely, 2.7) and Python 3 (with the latest release being 3.5 at the time of\\nwriting). This sometimes leads to some confusion. Python 2 is no longer actively\\ndeveloped, but because Python 3 contains major changes, Python 2 code usually does\\nnot run on Python 3. If you are new to Python, or are starting a new project from\\nscratch, we highly recommend using the latest version of Python 3 without changes.\\nIf you have a large codebase that you rely on that is written for Python 2, you are\\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\\nsoon as possible. When writing any new code, it is for the most part quite easy to\\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with\\nlegacy software, you should definitely use Python 3. All the code in this book is writ‐\\nten in a way that works for both versions. However, the exact output might differ\\nslightly under Python 2.\\nVersions Used in this Book\\nWe are using the following versions of the previously mentioned libraries in this\\nbook:\\nIn[9]:\\nimport sys\\nprint(\"Python version: {}\".format(sys.version))\\nimport pandas as pd\\nprint(\"pandas version: {}\".format(pd.__version__))\\nimport matplotlib\\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\\nimport numpy as np\\nprint(\"NumPy version: {}\".format(np.__version__))\\nimport scipy as sp\\nprint(\"SciPy version: {}\".format(sp.__version__))\\nimport IPython\\nprint(\"IPython version: {}\".format(IPython.__version__))\\nimport sklearn\\nprint(\"scikit-learn version: {}\".format(sklearn.__version__))\\n12 \\n| \\nChapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 26}, page_content='Out[9]:\\nPython version: 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  2 2016, 17:53:06)\\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\\npandas version: 0.18.1\\nmatplotlib version: 1.5.1\\nNumPy version: 1.11.1\\nSciPy version: 0.17.1\\nIPython version: 5.1.0\\nscikit-learn version: 0.18\\nWhile it is not important to match these versions exactly, you should have a version\\nof scikit-learn that is as least as recent as the one we used.\\nNow that we have everything set up, let’s dive into our first application of machine\\nlearning.\\nThis book assumes that you have version 0.18 or later of scikit-\\nlearn. The model_selection module was added in 0.18, and if you\\nuse an earlier version of scikit-learn, you will need to adjust the\\nimports from this module.\\nA First Application: Classifying Iris Species\\nIn this section, we will go through a simple machine learning application and create\\nour first model. In the process, we will introduce some core concepts and terms.\\nLet’s assume that a hobby botanist is interested in distinguishing the species of some\\niris flowers that she has found. She has collected some measurements associated with\\neach iris: the length and width of the petals and the length and width of the sepals, all\\nmeasured in centimeters (see Figure 1-2).\\nShe also has the measurements of some irises that have been previously identified by\\nan expert botanist as belonging to the species setosa, versicolor, or virginica. For these\\nmeasurements, she can be certain of which species each iris belongs to. Let’s assume\\nthat these are the only species our hobby botanist will encounter in the wild.\\nOur goal is to build a machine learning model that can learn from the measurements\\nof these irises whose species is known, so that we can predict the species for a new\\niris.\\nA First Application: Classifying Iris Species \\n| \\n13'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 27}, page_content='Figure 1-2. Parts of the iris flower\\nBecause we have measurements for which we know the correct species of iris, this is a\\nsupervised learning problem. In this problem, we want to predict one of several\\noptions (the species of iris). This is an example of a classification problem. The possi‐\\nble outputs (different species of irises) are called classes. Every iris in the dataset\\nbelongs to one of three classes, so this problem is a three-class classification problem.\\nThe desired output for a single data point (an iris) is the species of this flower. For a\\nparticular data point, the species it belongs to is called its label.\\nMeet the Data\\nThe data we will use for this example is the Iris dataset, a classical dataset in machine\\nlearning and statistics. It is included in scikit-learn in the datasets module. We\\ncan load it by calling the load_iris function:\\nIn[10]:\\nfrom sklearn.datasets import load_iris\\niris_dataset = load_iris()\\nThe iris object that is returned by load_iris is a Bunch object, which is very similar\\nto a dictionary. It contains keys and values:\\n14 \\n| \\nChapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 28}, page_content='In[11]:\\nprint(\"Keys of iris_dataset: \\\\n{}\".format(iris_dataset.keys()))\\nOut[11]:\\nKeys of iris_dataset:\\ndict_keys([\\'target_names\\', \\'feature_names\\', \\'DESCR\\', \\'data\\', \\'target\\'])\\nThe value of the key DESCR is a short description of the dataset. We show the begin‐\\nning of the description here (feel free to look up the rest yourself):\\nIn[12]:\\nprint(iris_dataset[\\'DESCR\\'][:193] + \"\\\\n...\")\\nOut[12]:\\nIris Plants Database\\n====================\\nNotes\\n----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive att\\n...\\n----\\nThe value of the key target_names is an array of strings, containing the species of\\nflower that we want to predict:\\nIn[13]:\\nprint(\"Target names: {}\".format(iris_dataset[\\'target_names\\']))\\nOut[13]:\\nTarget names: [\\'setosa\\' \\'versicolor\\' \\'virginica\\']\\nThe value of feature_names is a list of strings, giving the description of each feature:\\nIn[14]:\\nprint(\"Feature names: \\\\n{}\".format(iris_dataset[\\'feature_names\\']))\\nOut[14]:\\nFeature names:\\n[\\'sepal length (cm)\\', \\'sepal width (cm)\\', \\'petal length (cm)\\',\\n \\'petal width (cm)\\']\\nThe data itself is contained in the target and data fields. data contains the numeric\\nmeasurements of sepal length, sepal width, petal length, and petal width in a NumPy\\narray:\\nA First Application: Classifying Iris Species \\n| \\n15'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 29}, page_content='In[15]:\\nprint(\"Type of data: {}\".format(type(iris_dataset[\\'data\\'])))\\nOut[15]:\\nType of data: <class \\'numpy.ndarray\\'>\\nThe rows in the data array correspond to flowers, while the columns represent the\\nfour measurements that were taken for each flower:\\nIn[16]:\\nprint(\"Shape of data: {}\".format(iris_dataset[\\'data\\'].shape))\\nOut[16]:\\nShape of data: (150, 4)\\nWe see that the array contains measurements for 150 different flowers. Remember\\nthat the individual items are called samples in machine learning, and their properties\\nare called features. The shape of the data array is the number of samples multiplied by\\nthe number of features. This is a convention in scikit-learn, and your data will\\nalways be assumed to be in this shape. Here are the feature values for the first five\\nsamples:\\nIn[17]:\\nprint(\"First five columns of data:\\\\n{}\".format(iris_dataset[\\'data\\'][:5]))\\nOut[17]:\\nFirst five columns of data:\\n[[ 5.1  3.5  1.4  0.2]\\n [ 4.9  3.   1.4  0.2]\\n [ 4.7  3.2  1.3  0.2]\\n [ 4.6  3.1  1.5  0.2]\\n [ 5.   3.6  1.4  0.2]]\\nFrom this data, we can see that all of the first five flowers have a petal width of 0.2 cm\\nand that the first flower has the longest sepal, at 5.1 cm.\\nThe target array contains the species of each of the flowers that were measured, also\\nas a NumPy array:\\nIn[18]:\\nprint(\"Type of target: {}\".format(type(iris_dataset[\\'target\\'])))\\nOut[18]:\\nType of target: <class \\'numpy.ndarray\\'>\\ntarget is a one-dimensional array, with one entry per flower:\\n16 \\n| \\nChapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 30}, page_content='In[19]:\\nprint(\"Shape of target: {}\".format(iris_dataset[\\'target\\'].shape))\\nOut[19]:\\nShape of target: (150,)\\nThe species are encoded as integers from 0 to 2:\\nIn[20]:\\nprint(\"Target:\\\\n{}\".format(iris_dataset[\\'target\\']))\\nOut[20]:\\nTarget:\\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\\n 2 2]\\nThe meanings of the numbers are given by the iris[\\'target_names\\'] array:\\n0 means setosa, 1 means versicolor, and 2 means virginica.\\nMeasuring Success: Training and Testing Data\\nWe want to build a machine learning model from this data that can predict the spe‐\\ncies of iris for a new set of measurements. But before we can apply our model to new\\nmeasurements, we need to know whether it actually works—that is, whether we\\nshould trust its predictions.\\nUnfortunately, we cannot use the data we used to build the model to evaluate it. This\\nis because our model can always simply remember the whole training set, and will\\ntherefore always predict the correct label for any point in the training set. This\\n“remembering” does not indicate to us whether our model will generalize well (in\\nother words, whether it will also perform well on new data).\\nTo assess the model’s performance, we show it new data (data that it hasn’t seen\\nbefore) for which we have labels. This is usually done by splitting the labeled data we\\nhave collected (here, our 150 flower measurements) into two parts. One part of the\\ndata is used to build our machine learning model, and is called the training data or\\ntraining set. The rest of the data will be used to assess how well the model works; this\\nis called the test data, test set, or hold-out set.\\nscikit-learn contains a function that shuffles the dataset and splits it for you: the\\ntrain_test_split function. This function extracts 75% of the rows in the data as the\\ntraining set, together with the corresponding labels for this data. The remaining 25%\\nof the data, together with the remaining labels, is declared as the test set. Deciding\\nA First Application: Classifying Iris Species \\n| \\n17'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 31}, page_content='how much data you want to put into the training and the test set respectively is some‐\\nwhat arbitrary, but using a test set containing 25% of the data is a good rule of thumb.\\nIn scikit-learn, data is usually denoted with a capital X, while labels are denoted by\\na lowercase y. This is inspired by the standard formulation f(x)=y in mathematics,\\nwhere x is the input to a function and y is the output. Following more conventions\\nfrom mathematics, we use a capital X because the data is a two-dimensional array (a\\nmatrix) and a lowercase y because the target is a one-dimensional array (a vector).\\nLet’s call train_test_split on our data and assign the outputs using this nomencla‐\\nture:\\nIn[21]:\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris_dataset[\\'data\\'], iris_dataset[\\'target\\'], random_state=0)\\nBefore making the split, the train_test_split function shuffles the dataset using a\\npseudorandom number generator. If we just took the last 25% of the data as a test set,\\nall the data points would have the label 2, as the data points are sorted by the label\\n(see the output for iris[\\'target\\'] shown earlier). Using a test set containing only\\none of the three classes would not tell us much about how well our model generalizes,\\nso we shuffle our data to make sure the test data contains data from all classes.\\nTo make sure that we will get the same output if we run the same function several\\ntimes, we provide the pseudorandom number generator with a fixed seed using the\\nrandom_state parameter. This will make the outcome deterministic, so this line will\\nalways have the same outcome. We will always fix the random_state in this way when\\nusing randomized procedures in this book.\\nThe output of the train_test_split function is X_train, X_test, y_train, and\\ny_test, which are all NumPy arrays. X_train contains 75% of the rows of the dataset,\\nand X_test contains the remaining 25%:\\nIn[22]:\\nprint(\"X_train shape: {}\".format(X_train.shape))\\nprint(\"y_train shape: {}\".format(y_train.shape))\\nOut[22]:\\nX_train shape: (112, 4)\\ny_train shape: (112,)\\n18 \\n| \\nChapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 32}, page_content='In[23]:\\nprint(\"X_test shape: {}\".format(X_test.shape))\\nprint(\"y_test shape: {}\".format(y_test.shape))\\nOut[23]:\\nX_test shape: (38, 4)\\ny_test shape: (38,)\\nFirst Things First: Look at Your Data\\nBefore building a machine learning model it is often a good idea to inspect the data,\\nto see if the task is easily solvable without machine learning, or if the desired infor‐\\nmation might not be contained in the data.\\nAdditionally, inspecting your data is a good way to find abnormalities and peculiari‐\\nties. Maybe some of your irises were measured using inches and not centimeters, for\\nexample. In the real world, inconsistencies in the data and unexpected measurements\\nare very common.\\nOne of the best ways to inspect data is to visualize it. One way to do this is by using a\\nscatter plot. A scatter plot of the data puts one feature along the x-axis and another\\nalong the y-axis, and draws a dot for each data point. Unfortunately, computer\\nscreens have only two dimensions, which allows us to plot only two (or maybe three)\\nfeatures at a time. It is difficult to plot datasets with more than three features this way.\\nOne way around this problem is to do a pair plot, which looks at all possible pairs of\\nfeatures. If you have a small number of features, such as the four we have here, this is\\nquite reasonable. You should keep in mind, however, that a pair plot does not show\\nthe interaction of all of features at once, so some interesting aspects of the data may\\nnot be revealed when visualizing it this way.\\nFigure 1-3 is a pair plot of the features in the training set. The data points are colored\\naccording to the species the iris belongs to. To create the plot, we first convert the\\nNumPy array into a pandas DataFrame. pandas has a function to create pair plots\\ncalled scatter_matrix. The diagonal of this matrix is filled with histograms of each\\nfeature:\\nIn[24]:\\n# create dataframe from data in X_train\\n# label the columns using the strings in iris_dataset.feature_names\\niris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\\n# create a scatter matrix from the dataframe, color by y_train\\ngrr = pd.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker=\\'o\\',\\n                        hist_kwds={\\'bins\\': 20}, s=60, alpha=.8, cmap=mglearn.cm3)\\nA First Application: Classifying Iris Species \\n| \\n19'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 33}, page_content='Figure 1-3. Pair plot of the Iris dataset, colored by class label\\nFrom the plots, we can see that the three classes seem to be relatively well separated\\nusing the sepal and petal measurements. This means that a machine learning model\\nwill likely be able to learn to separate them.\\nBuilding Your First Model: k-Nearest Neighbors\\nNow we can start building the actual machine learning model. There are many classi‐\\nfication algorithms in scikit-learn that we could use. Here we will use a k-nearest\\nneighbors classifier, which is easy to understand. Building this model only consists of\\nstoring the training set. To make a prediction for a new data point, the algorithm\\nfinds the point in the training set that is closest to the new point. Then it assigns the\\nlabel of this training point to the new data point.\\n20 \\n| \\nChapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 34}, page_content=\"The k in k-nearest neighbors signifies that instead of using only the closest neighbor\\nto the new data point, we can consider any fixed number k of neighbors in the train‐\\ning (for example, the closest three or five neighbors). Then, we can make a prediction\\nusing the majority class among these neighbors. We will go into more detail about\\nthis in Chapter 2; for now, we’ll use only a single neighbor.\\nAll machine learning models in scikit-learn are implemented in their own classes,\\nwhich are called Estimator classes. The k-nearest neighbors classification algorithm\\nis implemented in the KNeighborsClassifier class in the neighbors module. Before\\nwe can use the model, we need to instantiate the class into an object. This is when we\\nwill set any parameters of the model. The most important parameter of KNeighbor\\nsClassifier is the number of neighbors, which we will set to 1:\\nIn[25]:\\nfrom sklearn.neighbors import KNeighborsClassifier\\nknn = KNeighborsClassifier(n_neighbors=1)\\nThe knn object encapsulates the algorithm that will be used to build the model from\\nthe training data, as well the algorithm to make predictions on new data points. It will\\nalso hold the information that the algorithm has extracted from the training data. In\\nthe case of KNeighborsClassifier, it will just store the training set.\\nTo build the model on the training set, we call the fit method of the knn object,\\nwhich takes as arguments the NumPy array X_train containing the training data and\\nthe NumPy array y_train of the corresponding training labels:\\nIn[26]:\\nknn.fit(X_train, y_train)\\nOut[26]:\\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\\n           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\\n           weights='uniform')\\nThe fit method returns the knn object itself (and modifies it in place), so we get a\\nstring representation of our classifier. The representation shows us which parameters\\nwere used in creating the model. Nearly all of them are the default values, but you can\\nalso find n_neighbors=1, which is the parameter that we passed. Most models in\\nscikit-learn have many parameters, but the majority of them are either speed opti‐\\nmizations or for very special use cases. You don’t have to worry about the other\\nparameters shown in this representation. Printing a scikit-learn model can yield\\nvery long strings, but don’t be intimidated by these. We will cover all the important\\nparameters in Chapter 2. In the remainder of this book, we will not show the output\\nof fit because it doesn’t contain any new information.\\nA First Application: Classifying Iris Species \\n| \\n21\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 35}, page_content='Making Predictions\\nWe can now make predictions using this model on new data for which we might not\\nknow the correct labels. Imagine we found an iris in the wild with a sepal length of\\n5 cm, a sepal width of 2.9 cm, a petal length of 1 cm, and a petal width of 0.2 cm.\\nWhat species of iris would this be? We can put this data into a NumPy array, again by\\ncalculating the shape—that is, the number of samples (1) multiplied by the number of\\nfeatures (4):\\nIn[27]:\\nX_new = np.array([[5, 2.9, 1, 0.2]])\\nprint(\"X_new.shape: {}\".format(X_new.shape))\\nOut[27]:\\nX_new.shape: (1, 4)\\nNote that we made the measurements of this single flower into a row in a two-\\ndimensional NumPy array, as scikit-learn always expects two-dimensional arrays\\nfor the data.\\nTo make a prediction, we call the predict method of the knn object:\\nIn[28]:\\nprediction = knn.predict(X_new)\\nprint(\"Prediction: {}\".format(prediction))\\nprint(\"Predicted target name: {}\".format(\\n       iris_dataset[\\'target_names\\'][prediction]))\\nOut[28]:\\nPrediction: [0]\\nPredicted target name: [\\'setosa\\']\\nOur model predicts that this new iris belongs to the class 0, meaning its species is\\nsetosa. But how do we know whether we can trust our model? We don’t know the cor‐\\nrect species of this sample, which is the whole point of building the model!\\nEvaluating the Model\\nThis is where the test set that we created earlier comes in. This data was not used to\\nbuild the model, but we do know what the correct species is for each iris in the test\\nset.\\nTherefore, we can make a prediction for each iris in the test data and compare it\\nagainst its label (the known species). We can measure how well the model works by\\ncomputing the accuracy, which is the fraction of flowers for which the right species\\nwas predicted:\\n22 \\n| \\nChapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 36}, page_content='In[29]:\\ny_pred = knn.predict(X_test)\\nprint(\"Test set predictions:\\\\n {}\".format(y_pred))\\nOut[29]:\\nTest set predictions:\\n [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0 2]\\nIn[30]:\\nprint(\"Test set score: {:.2f}\".format(np.mean(y_pred == y_test)))\\nOut[30]:\\nTest set score: 0.97\\nWe can also use the score method of the knn object, which will compute the test set\\naccuracy for us:\\nIn[31]:\\nprint(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\\nOut[31]:\\nTest set score: 0.97\\nFor this model, the test set accuracy is about 0.97, which means we made the right\\nprediction for 97% of the irises in the test set. Under some mathematical assump‐\\ntions, this means that we can expect our model to be correct 97% of the time for new\\nirises. For our hobby botanist application, this high level of accuracy means that our\\nmodel may be trustworthy enough to use. In later chapters we will discuss how we\\ncan improve performance, and what caveats there are in tuning a model.\\nSummary and Outlook\\nLet’s summarize what we learned in this chapter. We started with a brief introduction\\nto machine learning and its applications, then discussed the distinction between\\nsupervised and unsupervised learning and gave an overview of the tools we’ll be\\nusing in this book. Then, we formulated the task of predicting which species of iris a\\nparticular flower belongs to by using physical measurements of the flower. We used a\\ndataset of measurements that was annotated by an expert with the correct species to\\nbuild our model, making this a supervised learning task. There were three possible\\nspecies, setosa, versicolor, or virginica, which made the task a three-class classification\\nproblem. The possible species are called classes in the classification problem, and the\\nspecies of a single iris is called its label.\\nThe Iris dataset consists of two NumPy arrays: one containing the data, which is\\nreferred to as X in scikit-learn, and one containing the correct or desired outputs,\\nSummary and Outlook \\n| \\n23'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 37}, page_content='which is called y. The array X is a two-dimensional array of features, with one row per\\ndata point and one column per feature. The array y is a one-dimensional array, which\\nhere contains one class label, an integer ranging from 0 to 2, for each of the samples.\\nWe split our dataset into a training set, to build our model, and a test set, to evaluate\\nhow well our model will generalize to new, previously unseen data.\\nWe chose the k-nearest neighbors classification algorithm, which makes predictions\\nfor a new data point by considering its closest neighbor(s) in the training set. This is\\nimplemented in the KNeighborsClassifier class, which contains the algorithm that\\nbuilds the model as well as the algorithm that makes a prediction using the model.\\nWe instantiated the class, setting parameters. Then we built the model by calling the\\nfit method, passing the training data (X_train) and training outputs (y_train) as\\nparameters. We evaluated the model using the score method, which computes the\\naccuracy of the model. We applied the score method to the test set data and the test\\nset labels and found that our model is about 97% accurate, meaning it is correct 97%\\nof the time on the test set.\\nThis gave us the confidence to apply the model to new data (in our example, new\\nflower measurements) and trust that the model will be correct about 97% of the time.\\nHere is a summary of the code needed for the whole training and evaluation\\nprocedure:\\nIn[32]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris_dataset[\\'data\\'], iris_dataset[\\'target\\'], random_state=0)\\nknn = KNeighborsClassifier(n_neighbors=1)\\nknn.fit(X_train, y_train)\\nprint(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\\nOut[32]:\\nTest set score: 0.97\\nThis snippet contains the core code for applying any machine learning algorithm\\nusing scikit-learn. The fit, predict, and score methods are the common inter‐\\nface to supervised models in scikit-learn, and with the concepts introduced in this\\nchapter, you can apply these models to many machine learning tasks. In the next\\nchapter, we will go into more depth about the different kinds of supervised models in\\nscikit-learn and how to apply them successfully.\\n24 \\n| \\nChapter 1: Introduction'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 38}, page_content='CHAPTER 2\\nSupervised Learning\\nAs we mentioned earlier, supervised machine learning is one of the most commonly\\nused and successful types of machine learning. In this chapter, we will describe super‐\\nvised learning in more detail and explain several popular supervised learning algo‐\\nrithms. We already saw an application of supervised machine learning in Chapter 1:\\nclassifying iris flowers into several species using physical measurements of the\\nflowers.\\nRemember that supervised learning is used whenever we want to predict a certain\\noutcome from a given input, and we have examples of input/output pairs. We build a\\nmachine learning model from these input/output pairs, which comprise our training\\nset. Our goal is to make accurate predictions for new, never-before-seen data. Super‐\\nvised learning often requires human effort to build the training set, but afterward\\nautomates and often speeds up an otherwise laborious or infeasible task.\\nClassification and Regression\\nThere are two major types of supervised machine learning problems, called classifica‐\\ntion and regression.\\nIn classification, the goal is to predict a class label, which is a choice from a predefined\\nlist of possibilities. In Chapter 1 we used the example of classifying irises into one of\\nthree possible species. Classification is sometimes separated into binary classification,\\nwhich is the special case of distinguishing between exactly two classes, and multiclass\\nclassification, which is classification between more than two classes. You can think of\\nbinary classification as trying to answer a yes/no question. Classifying emails as\\neither spam or not spam is an example of a binary classification problem. In this\\nbinary classification task, the yes/no question being asked would be “Is this email\\nspam?”\\n25'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 39}, page_content='1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\\nIn binary classification we often speak of one class being the posi‐\\ntive class and the other class being the negative class. Here, positive\\ndoesn’t represent having benefit or value, but rather what the object\\nof the study is. So, when looking for spam, “positive” could mean\\nthe spam class. Which of the two classes is called positive is often a\\nsubjective matter, and specific to the domain.\\nThe iris example, on the other hand, is an example of a multiclass classification prob‐\\nlem. Another example is predicting what language a website is in from the text on the\\nwebsite. The classes here would be a pre-defined list of possible languages.\\nFor regression tasks, the goal is to predict a continuous number, or a floating-point\\nnumber in programming terms (or real number in mathematical terms). Predicting a\\nperson’s annual income from their education, their age, and where they live is an\\nexample of a regression task. When predicting income, the predicted value is an\\namount, and can be any number in a given range. Another example of a regression\\ntask is predicting the yield of a corn farm given attributes such as previous yields,\\nweather, and number of employees working on the farm. The yield again can be an\\narbitrary number.\\nAn easy way to distinguish between classification and regression tasks is to ask\\nwhether there is some kind of continuity in the output. If there is continuity between\\npossible outcomes, then the problem is a regression problem. Think about predicting\\nannual income. There is a clear continuity in the output. Whether a person makes\\n$40,000 or $40,001 a year does not make a tangible difference, even though these are\\ndifferent amounts of money; if our algorithm predicts $39,999 or $40,001 when it\\nshould have predicted $40,000, we don’t mind that much.\\nBy contrast, for the task of recognizing the language of a website (which is a classifi‐\\ncation problem), there is no matter of degree. A website is in one language, or it is in\\nanother. There is no continuity between languages, and there is no language that is\\nbetween English and French.1\\nGeneralization, Overfitting, and Underfitting\\nIn supervised learning, we want to build a model on the training data and then be\\nable to make accurate predictions on new, unseen data that has the same characteris‐\\ntics as the training set that we used. If a model is able to make accurate predictions on\\nunseen data, we say it is able to generalize from the training set to the test set. We\\nwant to build a model that is able to generalize as accurately as possible.\\n26 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 40}, page_content='2 In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a\\nboat from us yet, they might have bought one from someone else, or they may still be saving and plan to buy\\none in the future.\\nUsually we build a model in such a way that it can make accurate predictions on the\\ntraining set. If the training and test sets have enough in common, we expect the\\nmodel to also be accurate on the test set. However, there are some cases where this\\ncan go wrong. For example, if we allow ourselves to build very complex models, we\\ncan always be as accurate as we like on the training set.\\nLet’s take a look at a made-up example to illustrate this point. Say a novice data scien‐\\ntist wants to predict whether a customer will buy a boat, given records of previous\\nboat buyers and customers who we know are not interested in buying a boat.2 The\\ngoal is to send out promotional emails to people who are likely to actually make a\\npurchase, but not bother those customers who won’t be interested.\\nSuppose we have the customer records shown in Table 2-1.\\nTable 2-1. Example data about customers\\nAge\\nNumber of \\ncars owned\\nOwns house\\nNumber of children\\nMarital status\\nOwns a dog\\nBought a boat\\n66\\n1\\nyes\\n2\\nwidowed\\nno\\nyes\\n52\\n2\\nyes\\n3\\nmarried\\nno\\nyes\\n22\\n0\\nno\\n0\\nmarried\\nyes\\nno\\n25\\n1\\nno\\n1\\nsingle\\nno\\nno\\n44\\n0\\nno\\n2\\ndivorced\\nyes\\nno\\n39\\n1\\nyes\\n2\\nmarried\\nyes\\nno\\n26\\n1\\nno\\n2\\nsingle\\nno\\nno\\n40\\n3\\nyes\\n1\\nmarried\\nyes\\nno\\n53\\n2\\nyes\\n2\\ndivorced\\nno\\nyes\\n64\\n2\\nyes\\n3\\ndivorced\\nno\\nno\\n58\\n2\\nyes\\n2\\nmarried\\nyes\\nyes\\n33\\n1\\nno\\n1\\nsingle\\nno\\nno\\nAfter looking at the data for a while, our novice data scientist comes up with the fol‐\\nlowing rule: “If the customer is older than 45, and has less than 3 children or is not\\ndivorced, then they want to buy a boat.” When asked how well this rule of his does,\\nour data scientist answers, “It’s 100 percent accurate!” And indeed, on the data that is\\nin the table, the rule is perfectly accurate. There are many possible rules we could\\ncome up with that would explain perfectly if someone in this dataset wants to buy a\\nboat. No age appears twice in the data, so we could say people who are 66, 52, 53, or\\nGeneralization, Overfitting, and Underfitting \\n| \\n27'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 41}, page_content='3 And also provably, with the right math.\\n58 years old want to buy a boat, while all others don’t. While we can make up many\\nrules that work well on this data, remember that we are not interested in making pre‐\\ndictions for this dataset; we already know the answers for these customers. We want\\nto know if new customers are likely to buy a boat. We therefore want to find a rule that\\nwill work well for new customers, and achieving 100 percent accuracy on the training\\nset does not help us there. We might not expect that the rule our data scientist came\\nup with will work very well on new customers. It seems too complex, and it is sup‐\\nported by very little data. For example, the “or is not divorced” part of the rule hinges\\non a single customer.\\nThe only measure of whether an algorithm will perform well on new data is the eval‐\\nuation on the test set. However, intuitively3 we expect simple models to generalize\\nbetter to new data. If the rule was “People older than 50 want to buy a boat,” and this\\nwould explain the behavior of all the customers, we would trust it more than the rule\\ninvolving children and marital status in addition to age. Therefore, we always want to\\nfind the simplest model. Building a model that is too complex for the amount of\\ninformation we have, as our novice data scientist did, is called overfitting. Overfitting\\noccurs when you fit a model too closely to the particularities of the training set and\\nobtain a model that works well on the training set but is not able to generalize to new\\ndata. On the other hand, if your model is too simple—say, “Everybody who owns a\\nhouse buys a boat”—then you might not be able to capture all the aspects of and vari‐\\nability in the data, and your model will do badly even on the training set. Choosing\\ntoo simple a model is called underfitting.\\nThe more complex we allow our model to be, the better we will be able to predict on\\nthe training data. However, if our model becomes too complex, we start focusing too\\nmuch on each individual data point in our training set, and the model will not gener‐\\nalize well to new data.\\nThere is a sweet spot in between that will yield the best generalization performance.\\nThis is the model we want to find.\\nThe trade-off between overfitting and underfitting is illustrated in Figure 2-1.\\n28 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 42}, page_content='Figure 2-1. Trade-off of model complexity against training and test accuracy\\nRelation of Model Complexity to Dataset Size\\nIt’s important to note that model complexity is intimately tied to the variation of\\ninputs contained in your training dataset: the larger variety of data points your data‐\\nset contains, the more complex a model you can use without overfitting. Usually, col‐\\nlecting more data points will yield more variety, so larger datasets allow building\\nmore complex models. However, simply duplicating the same data points or collect‐\\ning very similar data will not help.\\nGoing back to the boat selling example, if we saw 10,000 more rows of customer data,\\nand all of them complied with the rule “If the customer is older than 45, and has less\\nthan 3 children or is not divorced, then they want to buy a boat,” we would be much\\nmore likely to believe this to be a good rule than when it was developed using only\\nthe 12 rows in Table 2-1.\\nHaving more data and building appropriately more complex models can often work\\nwonders for supervised learning tasks. In this book, we will focus on working with\\ndatasets of fixed sizes. In the real world, you often have the ability to decide how\\nmuch data to collect, which might be more beneficial than tweaking and tuning your\\nmodel. Never underestimate the power of more data.\\nSupervised Machine Learning Algorithms\\nWe will now review the most popular machine learning algorithms and explain how\\nthey learn from data and how they make predictions. We will also discuss how the\\nconcept of model complexity plays out for each of these models, and provide an over‐\\nSupervised Machine Learning Algorithms \\n| \\n29'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 43}, page_content='4 Discussing all of them is beyond the scope of the book, and we refer you to the scikit-learn documentation\\nfor more details.\\nview of how each algorithm builds a model. We will examine the strengths and weak‐\\nnesses of each algorithm, and what kind of data they can best be applied to. We will\\nalso explain the meaning of the most important parameters and options.4 Many algo‐\\nrithms have a classification and a regression variant, and we will describe both.\\nIt is not necessary to read through the descriptions of each algorithm in detail, but\\nunderstanding the models will give you a better feeling for the different ways\\nmachine learning algorithms can work. This chapter can also be used as a reference\\nguide, and you can come back to it when you are unsure about the workings of any of\\nthe algorithms.\\nSome Sample Datasets\\nWe will use several datasets to illustrate the different algorithms. Some of the datasets\\nwill be small and synthetic (meaning made-up), designed to highlight particular\\naspects of the algorithms. Other datasets will be large, real-world examples.\\nAn example of a synthetic two-class classification dataset is the forge dataset, which\\nhas two features. The following code creates a scatter plot (Figure 2-2) visualizing all\\nof the data points in this dataset. The plot has the first feature on the x-axis and the\\nsecond feature on the y-axis. As is always the case in scatter plots, each data point is\\nrepresented as one dot. The color and shape of the dot indicates its class:\\nIn[2]:\\n# generate dataset\\nX, y = mglearn.datasets.make_forge()\\n# plot dataset\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nplt.legend([\"Class 0\", \"Class 1\"], loc=4)\\nplt.xlabel(\"First feature\")\\nplt.ylabel(\"Second feature\")\\nprint(\"X.shape: {}\".format(X.shape))\\nOut[2]:\\nX.shape: (26, 2)\\n30 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 44}, page_content='Figure 2-2. Scatter plot of the forge dataset\\nAs you can see from X.shape, this dataset consists of 26 data points, with 2 features.\\nTo illustrate regression algorithms, we will use the synthetic wave dataset. The wave\\ndataset has a single input feature and a continuous target variable (or response) that\\nwe want to model. The plot created here (Figure 2-3) shows the single feature on the\\nx-axis and the regression target (the output) on the y-axis:\\nIn[3]:\\nX, y = mglearn.datasets.make_wave(n_samples=40)\\nplt.plot(X, y, \\'o\\')\\nplt.ylim(-3, 3)\\nplt.xlabel(\"Feature\")\\nplt.ylabel(\"Target\")\\nSupervised Machine Learning Algorithms \\n| \\n31'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 45}, page_content='Figure 2-3. Plot of the wave dataset, with the x-axis showing the feature and the y-axis\\nshowing the regression target\\nWe are using these very simple, low-dimensional datasets because we can easily visu‐\\nalize them—a printed page has two dimensions, so data with more than two features\\nis hard to show. Any intuition derived from datasets with few features (also called\\nlow-dimensional datasets) might not hold in datasets with many features (high-\\ndimensional datasets). As long as you keep that in mind, inspecting algorithms on\\nlow-dimensional datasets can be very instructive.\\nWe will complement these small synthetic datasets with two real-world datasets that\\nare included in scikit-learn. One is the Wisconsin Breast Cancer dataset (cancer,\\nfor short), which records clinical measurements of breast cancer tumors. Each tumor\\nis labeled as “benign” (for harmless tumors) or “malignant” (for cancerous tumors),\\nand the task is to learn to predict whether a tumor is malignant based on the meas‐\\nurements of the tissue.\\nThe data can be loaded using the load_breast_cancer function from scikit-learn:\\nIn[4]:\\nfrom sklearn.datasets import load_breast_cancer\\ncancer = load_breast_cancer()\\nprint(\"cancer.keys(): \\\\n{}\".format(cancer.keys()))\\n32 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 46}, page_content='Out[4]:\\ncancer.keys():\\ndict_keys([\\'feature_names\\', \\'data\\', \\'DESCR\\', \\'target\\', \\'target_names\\'])\\nDatasets that are included in scikit-learn are usually stored as\\nBunch objects, which contain some information about the dataset\\nas well as the actual data. All you need to know about Bunch objects\\nis that they behave like dictionaries, with the added benefit that you\\ncan access values using a dot (as in bunch.key instead of\\nbunch[\\'key\\']).\\nThe dataset consists of 569 data points, with 30 features each:\\nIn[5]:\\nprint(\"Shape of cancer data: {}\".format(cancer.data.shape))\\nOut[5]:\\nShape of cancer data: (569, 30)\\nOf these 569 data points, 212 are labeled as malignant and 357 as benign:\\nIn[6]:\\nprint(\"Sample counts per class:\\\\n{}\".format(\\n      {n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))\\nOut[6]:\\nSample counts per class:\\n{\\'benign\\': 357, \\'malignant\\': 212}\\nTo get a description of the semantic meaning of each feature, we can have a look at\\nthe feature_names attribute:\\nIn[7]:\\nprint(\"Feature names:\\\\n{}\".format(cancer.feature_names))\\nOut[7]:\\nFeature names:\\n[\\'mean radius\\' \\'mean texture\\' \\'mean perimeter\\' \\'mean area\\'\\n \\'mean smoothness\\' \\'mean compactness\\' \\'mean concavity\\'\\n \\'mean concave points\\' \\'mean symmetry\\' \\'mean fractal dimension\\'\\n \\'radius error\\' \\'texture error\\' \\'perimeter error\\' \\'area error\\'\\n \\'smoothness error\\' \\'compactness error\\' \\'concavity error\\'\\n \\'concave points error\\' \\'symmetry error\\' \\'fractal dimension error\\'\\n \\'worst radius\\' \\'worst texture\\' \\'worst perimeter\\' \\'worst area\\'\\n \\'worst smoothness\\' \\'worst compactness\\' \\'worst concavity\\'\\n \\'worst concave points\\' \\'worst symmetry\\' \\'worst fractal dimension\\']\\nSupervised Machine Learning Algorithms \\n| \\n33'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 47}, page_content='5 This is called the binomial coefficient, which is the number of combinations of k elements that can be selected\\nfrom a set of n elements. Often this is written as n\\nk  and spoken as “n choose k”—in this case, “13 choose 2.”\\nYou can find out more about the data by reading cancer.DESCR if you are interested.\\nWe will also be using a real-world regression dataset, the Boston Housing dataset.\\nThe task associated with this dataset is to predict the median value of homes in sev‐\\neral Boston neighborhoods in the 1970s, using information such as crime rate, prox‐\\nimity to the Charles River, highway accessibility, and so on. The dataset contains 506\\ndata points, described by 13 features:\\nIn[8]:\\nfrom sklearn.datasets import load_boston\\nboston = load_boston()\\nprint(\"Data shape: {}\".format(boston.data.shape))\\nOut[8]:\\nData shape: (506, 13)\\nAgain, you can get more information about the dataset by reading the DESCR attribute\\nof boston. For our purposes here, we will actually expand this dataset by not only\\nconsidering these 13 measurements as input features, but also looking at all products\\n(also called interactions) between features. In other words, we will not only consider\\ncrime rate and highway accessibility as features, but also the product of crime rate\\nand highway accessibility. Including derived feature like these is called feature engi‐\\nneering, which we will discuss in more detail in Chapter 4. This derived dataset can be\\nloaded using the load_extended_boston function:\\nIn[9]:\\nX, y = mglearn.datasets.load_extended_boston()\\nprint(\"X.shape: {}\".format(X.shape))\\nOut[9]:\\nX.shape: (506, 104)\\nThe resulting 104 features are the 13 original features together with the 91 possible\\ncombinations of two features within those 13.5\\nWe will use these datasets to explain and illustrate the properties of the different\\nmachine learning algorithms. But for now, let’s get to the algorithms themselves.\\nFirst, we will revisit the k-nearest neighbors (k-NN) algorithm that we saw in the pre‐\\nvious chapter.\\n34 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 48}, page_content='k-Nearest Neighbors\\nThe k-NN algorithm is arguably the simplest machine learning algorithm. Building\\nthe model consists only of storing the training dataset. To make a prediction for a\\nnew data point, the algorithm finds the closest data points in the training dataset—its\\n“nearest neighbors.”\\nk-Neighbors classification\\nIn its simplest version, the k-NN algorithm only considers exactly one nearest neigh‐\\nbor, which is the closest training data point to the point we want to make a prediction\\nfor. The prediction is then simply the known output for this training point. Figure 2-4\\nillustrates this for the case of classification on the forge dataset:\\nIn[10]:\\nmglearn.plots.plot_knn_classification(n_neighbors=1)\\nFigure 2-4. Predictions made by the one-nearest-neighbor model on the forge dataset\\nHere, we added three new data points, shown as stars. For each of them, we marked\\nthe closest point in the training set. The prediction of the one-nearest-neighbor algo‐\\nrithm is the label of that point (shown by the color of the cross).\\nSupervised Machine Learning Algorithms \\n| \\n35'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 49}, page_content='Instead of considering only the closest neighbor, we can also consider an arbitrary\\nnumber, k, of neighbors. This is where the name of the k-nearest neighbors algorithm\\ncomes from. When considering more than one neighbor, we use voting to assign a\\nlabel. This means that for each test point, we count how many neighbors belong to\\nclass 0 and how many neighbors belong to class 1. We then assign the class that is\\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\\nfollowing example (Figure 2-5) uses the three closest neighbors:\\nIn[11]:\\nmglearn.plots.plot_knn_classification(n_neighbors=3)\\nFigure 2-5. Predictions made by the three-nearest-neighbors model on the forge dataset\\nAgain, the prediction is shown as the color of the cross. You can see that the predic‐\\ntion for the new data point at the top left is not the same as the prediction when we\\nused only one neighbor.\\nWhile this illustration is for a binary classification problem, this method can be\\napplied to datasets with any number of classes. For more classes, we count how many\\nneighbors belong to each class and again predict the most common class.\\nNow let’s look at how we can apply the k-nearest neighbors algorithm using scikit-\\nlearn. First, we split our data into a training and a test set so we can evaluate general‐\\nization performance, as discussed in Chapter 1:\\n36 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 50}, page_content='In[12]:\\nfrom sklearn.model_selection import train_test_split\\nX, y = mglearn.datasets.make_forge()\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nNext, we import and instantiate the class. This is when we can set parameters, like the\\nnumber of neighbors to use. Here, we set it to 3:\\nIn[13]:\\nfrom sklearn.neighbors import KNeighborsClassifier\\nclf = KNeighborsClassifier(n_neighbors=3)\\nNow, we fit the classifier using the training set. For KNeighborsClassifier this\\nmeans storing the dataset, so we can compute neighbors during prediction:\\nIn[14]:\\nclf.fit(X_train, y_train)\\nTo make predictions on the test data, we call the predict method. For each data point\\nin the test set, this computes its nearest neighbors in the training set and finds the\\nmost common class among these:\\nIn[15]:\\nprint(\"Test set predictions: {}\".format(clf.predict(X_test)))\\nOut[15]:\\nTest set predictions: [1 0 1 0 1 0 0]\\nTo evaluate how well our model generalizes, we can call the score method with the\\ntest data together with the test labels:\\nIn[16]:\\nprint(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))\\nOut[16]:\\nTest set accuracy: 0.86\\nWe see that our model is about 86% accurate, meaning the model predicted the class\\ncorrectly for 86% of the samples in the test dataset.\\nAnalyzing KNeighborsClassifier\\nFor two-dimensional datasets, we can also illustrate the prediction for all possible test\\npoints in the xy-plane. We color the plane according to the class that would be\\nassigned to a point in this region. This lets us view the decision boundary, which is the\\ndivide between where the algorithm assigns class 0 versus where it assigns class 1.\\nSupervised Machine Learning Algorithms \\n| \\n37'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 51}, page_content='The following code produces the visualizations of the decision boundaries for one,\\nthree, and nine neighbors shown in Figure 2-6:\\nIn[17]:\\nfig, axes = plt.subplots(1, 3, figsize=(10, 3))\\nfor n_neighbors, ax in zip([1, 3, 9], axes):\\n    # the fit method returns the object self, so we can instantiate\\n    # and fit in one line\\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\\n    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\\n    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\\n    ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\\n    ax.set_xlabel(\"feature 0\")\\n    ax.set_ylabel(\"feature 1\")\\naxes[0].legend(loc=3)\\nFigure 2-6. Decision boundaries created by the nearest neighbors model for different val‐\\nues of n_neighbors\\nAs you can see on the left in the figure, using a single neighbor results in a decision\\nboundary that follows the training data closely. Considering more and more neigh‐\\nbors leads to a smoother decision boundary. A smoother boundary corresponds to a\\nsimpler model. In other words, using few neighbors corresponds to high model com‐\\nplexity (as shown on the right side of Figure 2-1), and using many neighbors corre‐\\nsponds to low model complexity (as shown on the left side of Figure 2-1). If you\\nconsider the extreme case where the number of neighbors is the number of all data\\npoints in the training set, each test point would have exactly the same neighbors (all\\ntraining points) and all predictions would be the same: the class that is most frequent\\nin the training set.\\nLet’s investigate whether we can confirm the connection between model complexity\\nand generalization that we discussed earlier. We will do this on the real-world Breast\\nCancer dataset. We begin by splitting the dataset into a training and a test set. Then\\n38 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 52}, page_content='we evaluate training and test set performance with different numbers of neighbors.\\nThe results are shown in Figure 2-7:\\nIn[18]:\\nfrom sklearn.datasets import load_breast_cancer\\ncancer = load_breast_cancer()\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, stratify=cancer.target, random_state=66)\\ntraining_accuracy = []\\ntest_accuracy = []\\n# try n_neighbors from 1 to 10\\nneighbors_settings = range(1, 11)\\nfor n_neighbors in neighbors_settings:\\n    # build the model\\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\\n    clf.fit(X_train, y_train)\\n    # record training set accuracy\\n    training_accuracy.append(clf.score(X_train, y_train))\\n    # record generalization accuracy\\n    test_accuracy.append(clf.score(X_test, y_test))\\nplt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\\nplt.ylabel(\"Accuracy\")\\nplt.xlabel(\"n_neighbors\")\\nplt.legend()\\nThe plot shows the training and test set accuracy on the y-axis against the setting of\\nn_neighbors on the x-axis. While real-world plots are rarely very smooth, we can still\\nrecognize some of the characteristics of overfitting and underfitting (note that\\nbecause considering fewer neighbors corresponds to a more complex model, the plot\\nis horizontally flipped relative to the illustration in Figure 2-1). Considering a single\\nnearest neighbor, the prediction on the training set is perfect. But when more neigh‐\\nbors are considered, the model becomes simpler and the training accuracy drops. The\\ntest set accuracy for using a single neighbor is lower than when using more neigh‐\\nbors, indicating that using the single nearest neighbor leads to a model that is too\\ncomplex. On the other hand, when considering 10 neighbors, the model is too simple\\nand performance is even worse. The best performance is somewhere in the middle,\\nusing around six neighbors. Still, it is good to keep the scale of the plot in mind. The\\nworst performance is around 88% accuracy, which might still be acceptable.\\nSupervised Machine Learning Algorithms \\n| \\n39'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 53}, page_content='Figure 2-7. Comparison of training and test accuracy as a function of n_neighbors\\nk-neighbors regression\\nThere is also a regression variant of the k-nearest neighbors algorithm. Again, let’s\\nstart by using the single nearest neighbor, this time using the wave dataset. We’ve\\nadded three test data points as green stars on the x-axis. The prediction using a single\\nneighbor is just the target value of the nearest neighbor. These are shown as blue stars\\nin Figure 2-8:\\nIn[19]:\\nmglearn.plots.plot_knn_regression(n_neighbors=1)\\n40 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 54}, page_content='Figure 2-8. Predictions made by one-nearest-neighbor regression on the wave dataset\\nAgain, we can use more than the single closest neighbor for regression. When using\\nmultiple nearest neighbors, the prediction is the average, or mean, of the relevant\\nneighbors (Figure 2-9):\\nIn[20]:\\nmglearn.plots.plot_knn_regression(n_neighbors=3)\\nSupervised Machine Learning Algorithms \\n| \\n41'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 55}, page_content='Figure 2-9. Predictions made by three-nearest-neighbors regression on the wave dataset\\nThe k-nearest neighbors algorithm for regression is implemented in the KNeighbors\\nRegressor class in scikit-learn. It’s used similarly to KNeighborsClassifier:\\nIn[21]:\\nfrom sklearn.neighbors import KNeighborsRegressor\\nX, y = mglearn.datasets.make_wave(n_samples=40)\\n# split the wave dataset into a training and a test set\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n# instantiate the model and set the number of neighbors to consider to 3\\nreg = KNeighborsRegressor(n_neighbors=3)\\n# fit the model using the training data and training targets\\nreg.fit(X_train, y_train)\\nNow we can make predictions on the test set:\\nIn[22]:\\nprint(\"Test set predictions:\\\\n{}\".format(reg.predict(X_test)))\\n42 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 56}, page_content='Out[22]:\\nTest set predictions:\\n[-0.054  0.357  1.137 -1.894 -1.139 -1.631  0.357  0.912 -0.447 -1.139]\\nWe can also evaluate the model using the score method, which for regressors returns\\nthe R2 score. The R2 score, also known as the coefficient of determination, is a meas‐\\nure of goodness of a prediction for a regression model, and yields a score between 0\\nand 1. A value of 1 corresponds to a perfect prediction, and a value of 0 corresponds\\nto a constant model that just predicts the mean of the training set responses, y_train:\\nIn[23]:\\nprint(\"Test set R^2: {:.2f}\".format(reg.score(X_test, y_test)))\\nOut[23]:\\nTest set R^2: 0.83\\nHere, the score is 0.83, which indicates a relatively good model fit.\\nAnalyzing KNeighborsRegressor\\nFor our one-dimensional dataset, we can see what the predictions look like for all\\npossible feature values (Figure 2-10). To do this, we create a test dataset consisting of\\nmany points on the line:\\nIn[24]:\\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\\n# create 1,000 data points, evenly spaced between -3 and 3\\nline = np.linspace(-3, 3, 1000).reshape(-1, 1)\\nfor n_neighbors, ax in zip([1, 3, 9], axes):\\n    # make predictions using 1, 3, or 9 neighbors\\n    reg = KNeighborsRegressor(n_neighbors=n_neighbors)\\n    reg.fit(X_train, y_train)\\n    ax.plot(line, reg.predict(line))\\n    ax.plot(X_train, y_train, \\'^\\', c=mglearn.cm2(0), markersize=8)\\n    ax.plot(X_test, y_test, \\'v\\', c=mglearn.cm2(1), markersize=8)\\n    ax.set_title(\\n        \"{} neighbor(s)\\\\n train score: {:.2f} test score: {:.2f}\".format(\\n            n_neighbors, reg.score(X_train, y_train),\\n            reg.score(X_test, y_test)))\\n    ax.set_xlabel(\"Feature\")\\n    ax.set_ylabel(\"Target\")\\naxes[0].legend([\"Model predictions\", \"Training data/target\",\\n                \"Test data/target\"], loc=\"best\")\\nSupervised Machine Learning Algorithms \\n| \\n43'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 57}, page_content='Figure 2-10. Comparing predictions made by nearest neighbors regression for different\\nvalues of n_neighbors\\nAs we can see from the plot, using only a single neighbor, each point in the training\\nset has an obvious influence on the predictions, and the predicted values go through\\nall of the data points. This leads to a very unsteady prediction. Considering more\\nneighbors leads to smoother predictions, but these do not fit the training data as well.\\nStrengths, weaknesses, and parameters\\nIn principle, there are two important parameters to the KNeighbors classifier: the\\nnumber of neighbors and how you measure distance between data points. In practice,\\nusing a small number of neighbors like three or five often works well, but you should\\ncertainly adjust this parameter. Choosing the right distance measure is somewhat\\nbeyond the scope of this book. By default, Euclidean distance is used, which works\\nwell in many settings.\\nOne of the strengths of k-NN is that the model is very easy to understand, and often\\ngives reasonable performance without a lot of adjustments. Using this algorithm is a\\ngood baseline method to try before considering more advanced techniques. Building\\nthe nearest neighbors model is usually very fast, but when your training set is very\\nlarge (either in number of features or in number of samples) prediction can be slow.\\nWhen using the k-NN algorithm, it’s important to preprocess your data (see Chap‐\\nter 3). This approach often does not perform well on datasets with many features\\n(hundreds or more), and it does particularly badly with datasets where most features\\nare 0 most of the time (so-called sparse datasets).\\nSo, while the nearest k-neighbors algorithm is easy to understand, it is not often used\\nin practice, due to prediction being slow and its inability to handle many features.\\nThe method we discuss next has neither of these drawbacks.\\n44 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 58}, page_content='Linear Models\\nLinear models are a class of models that are widely used in practice and have been\\nstudied extensively in the last few decades, with roots going back over a hundred\\nyears. Linear models make a prediction using a linear function of the input features,\\nwhich we will explain shortly.\\nLinear models for regression\\nFor regression, the general prediction formula for a linear model looks as follows:\\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\\nHere, x[0] to x[p] denotes the features (in this example, the number of features is p)\\nof a single data point, w and b are parameters of the model that are learned, and ŷ is\\nthe prediction the model makes. For a dataset with a single feature, this is:\\nŷ = w[0] * x[0] + b\\nwhich you might remember from high school mathematics as the equation for a line.\\nHere, w[0] is the slope and b is the y-axis offset. For more features, w contains the\\nslopes along each feature axis. Alternatively, you can think of the predicted response\\nas being a weighted sum of the input features, with weights (which can be negative)\\ngiven by the entries of w.\\nTrying to learn the parameters w[0] and b on our one-dimensional wave dataset\\nmight lead to the following line (see Figure 2-11):\\nIn[25]:\\nmglearn.plots.plot_linear_regression_wave()\\nOut[25]:\\nw[0]: 0.393906  b: -0.031804\\nSupervised Machine Learning Algorithms \\n| \\n45'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 59}, page_content='Figure 2-11. Predictions of a linear model on the wave dataset\\nWe added a coordinate cross into the plot to make it easier to understand the line.\\nLooking at w[0] we see that the slope should be around 0.4, which we can confirm\\nvisually in the plot. The intercept is where the prediction line should cross the y-axis:\\nthis is slightly below zero, which you can also confirm in the image.\\nLinear models for regression can be characterized as regression models for which the\\nprediction is a line for a single feature, a plane when using two features, or a hyper‐\\nplane in higher dimensions (that is, when using more features).\\nIf you compare the predictions made by the straight line with those made by the\\nKNeighborsRegressor in Figure 2-10, using a straight line to make predictions seems\\nvery restrictive. It looks like all the fine details of the data are lost. In a sense, this is\\ntrue. It is a strong (and somewhat unrealistic) assumption that our target y is a linear\\n46 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 60}, page_content='6 This is easy to see if you know some linear algebra.\\ncombination of the features. But looking at one-dimensional data gives a somewhat\\nskewed perspective. For datasets with many features, linear models can be very pow‐\\nerful. In particular, if you have more features than training data points, any target y\\ncan be perfectly modeled (on the training set) as a linear function.6\\nThere are many different linear models for regression. The difference between these\\nmodels lies in how the model parameters w and b are learned from the training data,\\nand how model complexity can be controlled. We will now take a look at the most\\npopular linear models for regression.\\nLinear regression (aka ordinary least squares)\\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐\\near method for regression. Linear regression finds the parameters w and b that mini‐\\nmize the mean squared error between predictions and the true regression targets, y,\\non the training set. The mean squared error is the sum of the squared differences\\nbetween the predictions and the true values. Linear regression has no parameters,\\nwhich is a benefit, but it also has no way to control model complexity.\\nHere is the code that produces the model you can see in Figure 2-11:\\nIn[26]:\\nfrom sklearn.linear_model import LinearRegression\\nX, y = mglearn.datasets.make_wave(n_samples=60)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\nlr = LinearRegression().fit(X_train, y_train)\\nThe “slope” parameters (w), also called weights or coefficients, are stored in the coef_\\nattribute, while the offset or intercept (b) is stored in the intercept_ attribute:\\nIn[27]:\\nprint(\"lr.coef_: {}\".format(lr.coef_))\\nprint(\"lr.intercept_: {}\".format(lr.intercept_))\\nOut[27]:\\nlr.coef_: [ 0.394]\\nlr.intercept_: -0.031804343026759746\\nSupervised Machine Learning Algorithms \\n| \\n47'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 61}, page_content='You might notice the strange-looking trailing underscore at the end\\nof coef_ and intercept_. scikit-learn always stores anything\\nthat is derived from the training data in attributes that end with a\\ntrailing underscore. That is to separate them from parameters that\\nare set by the user.\\nThe intercept_ attribute is always a single float number, while the coef_ attribute is\\na NumPy array with one entry per input feature. As we only have a single input fea‐\\nture in the wave dataset, lr.coef_ only has a single entry.\\nLet’s look at the training set and test set performance:\\nIn[28]:\\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\\nOut[28]:\\nTraining set score: 0.67\\nTest set score: 0.66\\nAn R2 of around 0.66 is not very good, but we can see that the scores on the training\\nand test sets are very close together. This means we are likely underfitting, not over‐\\nfitting. For this one-dimensional dataset, there is little danger of overfitting, as the\\nmodel is very simple (or restricted). However, with higher-dimensional datasets\\n(meaning datasets with a large number of features), linear models become more pow‐\\nerful, and there is a higher chance of overfitting. Let’s take a look at how LinearRe\\ngression performs on a more complex dataset, like the Boston Housing dataset.\\nRemember that this dataset has 506 samples and 105 derived features. First, we load\\nthe dataset and split it into a training and a test set. Then we build the linear regres‐\\nsion model as before:\\nIn[29]:\\nX, y = mglearn.datasets.load_extended_boston()\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nlr = LinearRegression().fit(X_train, y_train)\\nWhen comparing training set and test set scores, we find that we predict very accu‐\\nrately on the training set, but the R2 on the test set is much worse:\\nIn[30]:\\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\\n48 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 62}, page_content='7 Mathematically, Ridge penalizes the L2 norm of the coefficients, or the Euclidean length of w.\\nOut[30]:\\nTraining set score: 0.95\\nTest set score: 0.61\\nThis discrepancy between performance on the training set and the test set is a clear\\nsign of overfitting, and therefore we should try to find a model that allows us to con‐\\ntrol complexity. One of the most commonly used alternatives to standard linear\\nregression is ridge regression, which we will look into next.\\nRidge regression\\nRidge regression is also a linear model for regression, so the formula it uses to make\\npredictions is the same one used for ordinary least squares. In ridge regression,\\nthough, the coefficients (w) are chosen not only so that they predict well on the train‐\\ning data, but also to fit an additional constraint. We also want the magnitude of coef‐\\nficients to be as small as possible; in other words, all entries of w should be close to\\nzero. Intuitively, this means each feature should have as little effect on the outcome as\\npossible (which translates to having a small slope), while still predicting well. This\\nconstraint is an example of what is called regularization. Regularization means explic‐\\nitly restricting a model to avoid overfitting. The particular kind used by ridge regres‐\\nsion is known as L2 regularization.7\\nRidge regression is implemented in linear_model.Ridge. Let’s see how well it does\\non the extended Boston Housing dataset:\\nIn[31]:\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge().fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\\nOut[31]:\\nTraining set score: 0.89\\nTest set score: 0.75\\nAs you can see, the training set score of Ridge is lower than for LinearRegression,\\nwhile the test set score is higher. This is consistent with our expectation. With linear\\nregression, we were overfitting our data. Ridge is a more restricted model, so we are\\nless likely to overfit. A less complex model means worse performance on the training\\nset, but better generalization. As we are only interested in generalization perfor‐\\nmance, we should choose the Ridge model over the LinearRegression model.\\nSupervised Machine Learning Algorithms \\n| \\n49'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 63}, page_content='The Ridge model makes a trade-off between the simplicity of the model (near-zero\\ncoefficients) and its performance on the training set. How much importance the\\nmodel places on simplicity versus training set performance can be specified by the\\nuser, using the alpha parameter. In the previous example, we used the default param‐\\neter alpha=1.0. There is no reason why this will give us the best trade-off, though.\\nThe optimum setting of alpha depends on the particular dataset we are using.\\nIncreasing alpha forces coefficients to move more toward zero, which decreases\\ntraining set performance but might help generalization. For example:\\nIn[32]:\\nridge10 = Ridge(alpha=10).fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))\\nOut[32]:\\nTraining set score: 0.79\\nTest set score: 0.64\\nDecreasing alpha allows the coefficients to be less restricted, meaning we move right\\nin Figure 2-1. For very small values of alpha, coefficients are barely restricted at all,\\nand we end up with a model that resembles LinearRegression:\\nIn[33]:\\nridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))\\nOut[33]:\\nTraining set score: 0.93\\nTest set score: 0.77\\nHere, alpha=0.1 seems to be working well. We could try decreasing alpha even more\\nto improve generalization. For now, notice how the parameter alpha corresponds to\\nthe model complexity as shown in Figure 2-1. We will discuss methods to properly\\nselect parameters in Chapter 5.\\nWe can also get a more qualitative insight into how the alpha parameter changes the\\nmodel by inspecting the coef_ attribute of models with different values of alpha. A\\nhigher alpha means a more restricted model, so we expect the entries of coef_ to\\nhave smaller magnitude for a high value of alpha than for a low value of alpha. This\\nis confirmed in the plot in Figure 2-12:\\n50 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 64}, page_content='In[34]:\\nplt.plot(ridge.coef_, \\'s\\', label=\"Ridge alpha=1\")\\nplt.plot(ridge10.coef_, \\'^\\', label=\"Ridge alpha=10\")\\nplt.plot(ridge01.coef_, \\'v\\', label=\"Ridge alpha=0.1\")\\nplt.plot(lr.coef_, \\'o\\', label=\"LinearRegression\")\\nplt.xlabel(\"Coefficient index\")\\nplt.ylabel(\"Coefficient magnitude\")\\nplt.hlines(0, 0, len(lr.coef_))\\nplt.ylim(-25, 25)\\nplt.legend()\\nFigure 2-12. Comparing coefficient magnitudes for ridge regression with different values\\nof alpha and linear regression\\nHere, the x-axis enumerates the entries of coef_: x=0 shows the coefficient associated\\nwith the first feature, x=1 the coefficient associated with the second feature, and so on\\nup to x=100. The y-axis shows the numeric values of the corresponding values of the\\ncoefficients. The main takeaway here is that for alpha=10, the coefficients are mostly\\nbetween around –3 and 3. The coefficients for the Ridge model with alpha=1 are\\nsomewhat larger. The dots corresponding to alpha=0.1 have larger magnitude still,\\nand many of the dots corresponding to linear regression without any regularization\\n(which would be alpha=0) are so large they are outside of the chart.\\nSupervised Machine Learning Algorithms \\n| \\n51'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 65}, page_content='Another way to understand the influence of regularization is to fix a value of alpha\\nbut vary the amount of training data available. For Figure 2-13, we subsampled the\\nBoston Housing dataset and evaluated LinearRegression and Ridge(alpha=1) on\\nsubsets of increasing size (plots that show model performance as a function of dataset\\nsize are called learning curves):\\nIn[35]:\\nmglearn.plots.plot_ridge_n_samples()\\nFigure 2-13. Learning curves for ridge regression and linear regression on the Boston\\nHousing dataset\\nAs one would expect, the training score is higher than the test score for all dataset\\nsizes, for both ridge and linear regression. Because ridge is regularized, the training\\nscore of ridge is lower than the training score for linear regression across the board.\\nHowever, the test score for ridge is better, particularly for small subsets of the data.\\nFor less than 400 data points, linear regression is not able to learn anything. As more\\nand more data becomes available to the model, both models improve, and linear\\nregression catches up with ridge in the end. The lesson here is that with enough train‐\\ning data, regularization becomes less important, and given enough data, ridge and\\n52 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 66}, page_content='8 The lasso penalizes the L1 norm of the coefficient vector—or in other words, the sum of the absolute values of\\nthe coefficients.\\nlinear regression will have the same performance (the fact that this happens here\\nwhen using the full dataset is just by chance). Another interesting aspect of\\nFigure 2-13 is the decrease in training performance for linear regression. If more data\\nis added, it becomes harder for a model to overfit, or memorize the data.\\nLasso\\nAn alternative to Ridge for regularizing linear regression is Lasso. As with ridge\\nregression, using the lasso also restricts coefficients to be close to zero, but in a\\nslightly different way, called L1 regularization.8 The consequence of L1 regularization\\nis that when using the lasso, some coefficients are exactly zero. This means some fea‐\\ntures are entirely ignored by the model. This can be seen as a form of automatic fea‐\\nture selection. Having some coefficients be exactly zero often makes a model easier to\\ninterpret, and can reveal the most important features of your model.\\nLet’s apply the lasso to the extended Boston Housing dataset:\\nIn[36]:\\nfrom sklearn.linear_model import Lasso\\nlasso = Lasso().fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\\nprint(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))\\nOut[36]:\\nTraining set score: 0.29\\nTest set score: 0.21\\nNumber of features used: 4\\nAs you can see, Lasso does quite badly, both on the training and the test set. This\\nindicates that we are underfitting, and we find that it used only 4 of the 105 features.\\nSimilarly to Ridge, the Lasso also has a regularization parameter, alpha, that controls\\nhow strongly coefficients are pushed toward zero. In the previous example, we used\\nthe default of alpha=1.0. To reduce underfitting, let’s try decreasing alpha. When we\\ndo this, we also need to increase the default setting of max_iter (the maximum num‐\\nber of iterations to run):\\nSupervised Machine Learning Algorithms \\n| \\n53'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 67}, page_content='In[37]:\\n# we increase the default setting of \"max_iter\",\\n# otherwise the model would warn us that we should increase max_iter.\\nlasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\\nprint(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))\\nOut[37]:\\nTraining set score: 0.90\\nTest set score: 0.77\\nNumber of features used: 33\\nA lower alpha allowed us to fit a more complex model, which worked better on the\\ntraining and test data. The performance is slightly better than using Ridge, and we are\\nusing only 33 of the 105 features. This makes this model potentially easier to under‐\\nstand.\\nIf we set alpha too low, however, we again remove the effect of regularization and end\\nup overfitting, with a result similar to LinearRegression:\\nIn[38]:\\nlasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)\\nprint(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\\nprint(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\\nprint(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))\\nOut[38]:\\nTraining set score: 0.95\\nTest set score: 0.64\\nNumber of features used: 94\\nAgain, we can plot the coefficients of the different models, similarly to Figure 2-12.\\nThe result is shown in Figure 2-14:\\nIn[39]:\\nplt.plot(lasso.coef_, \\'s\\', label=\"Lasso alpha=1\")\\nplt.plot(lasso001.coef_, \\'^\\', label=\"Lasso alpha=0.01\")\\nplt.plot(lasso00001.coef_, \\'v\\', label=\"Lasso alpha=0.0001\")\\nplt.plot(ridge01.coef_, \\'o\\', label=\"Ridge alpha=0.1\")\\nplt.legend(ncol=2, loc=(0, 1.05))\\nplt.ylim(-25, 25)\\nplt.xlabel(\"Coefficient index\")\\nplt.ylabel(\"Coefficient magnitude\")\\n54 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 68}, page_content='Figure 2-14. Comparing coefficient magnitudes for lasso regression with different values\\nof alpha and ridge regression\\nFor alpha=1, we not only see that most of the coefficients are zero (which we already\\nknew), but that the remaining coefficients are also small in magnitude. Decreasing\\nalpha to 0.01, we obtain the solution shown as the green dots, which causes most\\nfeatures to be exactly zero. Using alpha=0.00001, we get a model that is quite unregu‐\\nlarized, with most coefficients nonzero and of large magnitude. For comparison, the\\nbest Ridge solution is shown in teal. The Ridge model with alpha=0.1 has similar\\npredictive performance as the lasso model with alpha=0.01, but using Ridge, all coef‐\\nficients are nonzero.\\nIn practice, ridge regression is usually the first choice between these two models.\\nHowever, if you have a large amount of features and expect only a few of them to be\\nimportant, Lasso might be a better choice. Similarly, if you would like to have a\\nmodel that is easy to interpret, Lasso will provide a model that is easier to under‐\\nstand, as it will select only a subset of the input features. scikit-learn also provides\\nthe ElasticNet class, which combines the penalties of Lasso and Ridge. In practice,\\nthis combination works best, though at the price of having two parameters to adjust:\\none for the L1 regularization, and one for the L2 regularization.\\nSupervised Machine Learning Algorithms \\n| \\n55'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 69}, page_content='Linear models for classification\\nLinear models are also extensively used for classification. Let’s look at binary classifi‐\\ncation first. In this case, a prediction is made using the following formula:\\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\\nThe formula looks very similar to the one for linear regression, but instead of just\\nreturning the weighted sum of the features, we threshold the predicted value at zero.\\nIf the function is smaller than zero, we predict the class –1; if it is larger than zero, we\\npredict the class +1. This prediction rule is common to all linear models for classifica‐\\ntion. Again, there are many different ways to find the coefficients (w) and the inter‐\\ncept (b).\\nFor linear models for regression, the output, ŷ, is a linear function of the features: a\\nline, plane, or hyperplane (in higher dimensions). For linear models for classification,\\nthe decision boundary is a linear function of the input. In other words, a (binary) lin‐\\near classifier is a classifier that separates two classes using a line, a plane, or a hyper‐\\nplane. We will see examples of that in this section.\\nThere are many algorithms for learning linear models. These algorithms all differ in\\nthe following two ways:\\n• The way in which they measure how well a particular combination of coefficients\\nand intercept fits the training data\\n• If and what kind of regularization they use\\nDifferent algorithms choose different ways to measure what “fitting the training set\\nwell” means. For technical mathematical reasons, it is not possible to adjust w and b\\nto minimize the number of misclassifications the algorithms produce, as one might\\nhope. For our purposes, and many applications, the different choices for item 1 in the\\npreceding list (called loss functions) are of little significance.\\nThe two most common linear classification algorithms are logistic regression, imple‐\\nmented in linear_model.LogisticRegression, and linear support vector machines\\n(linear SVMs), implemented in svm.LinearSVC (SVC stands for support vector classi‐\\nfier). Despite its name, LogisticRegression is a classification algorithm and not a\\nregression algorithm, and it should not be confused with LinearRegression.\\nWe can apply the LogisticRegression and LinearSVC models to the forge dataset,\\nand visualize the decision boundary as found by the linear models (Figure 2-15):\\n56 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 70}, page_content='In[40]:\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import LinearSVC\\nX, y = mglearn.datasets.make_forge()\\nfig, axes = plt.subplots(1, 2, figsize=(10, 3))\\nfor model, ax in zip([LinearSVC(), LogisticRegression()], axes):\\n    clf = model.fit(X, y)\\n    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\\n                                    ax=ax, alpha=.7)\\n    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\\n    ax.set_title(\"{}\".format(clf.__class__.__name__))\\n    ax.set_xlabel(\"Feature 0\")\\n    ax.set_ylabel(\"Feature 1\")\\naxes[0].legend()\\nFigure 2-15. Decision boundaries of a linear SVM and logistic regression on the forge\\ndataset with the default parameters\\nIn this figure, we have the first feature of the forge dataset on the x-axis and the sec‐\\nond feature on the y-axis, as before. We display the decision boundaries found by\\nLinearSVC and LogisticRegression respectively as straight lines, separating the area\\nclassified as class 1 on the top from the area classified as class 0 on the bottom. In\\nother words, any new data point that lies above the black line will be classified as class\\n1 by the respective classifier, while any point that lies below the black line will be clas‐\\nsified as class 0.\\nThe two models come up with similar decision boundaries. Note that both misclas‐\\nsify two of the points. By default, both models apply an L2 regularization, in the same\\nway that Ridge does for regression.\\nFor LogisticRegression and LinearSVC the trade-off parameter that determines the\\nstrength of the regularization is called C, and higher values of C correspond to less\\nSupervised Machine Learning Algorithms \\n| \\n57'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 71}, page_content='regularization. In other words, when you use a high value for the parameter C, Logis\\nticRegression and LinearSVC try to fit the training set as best as possible, while with\\nlow values of the parameter C, the models put more emphasis on finding a coefficient\\nvector (w) that is close to zero.\\nThere is another interesting aspect of how the parameter C acts. Using low values of C\\nwill cause the algorithms to try to adjust to the “majority” of data points, while using\\na higher value of C stresses the importance that each individual data point be classi‐\\nfied correctly. Here is an illustration using LinearSVC (Figure 2-16):\\nIn[41]:\\nmglearn.plots.plot_linear_svc_regularization()\\nFigure 2-16. Decision boundaries of a linear SVM on the forge dataset for different\\nvalues of C\\nOn the lefthand side, we have a very small C corresponding to a lot of regularization.\\nMost of the points in class 0 are at the top, and most of the points in class 1 are at the\\nbottom. The strongly regularized model chooses a relatively horizontal line, misclas‐\\nsifying two points. In the center plot, C is slightly higher, and the model focuses more\\non the two misclassified samples, tilting the decision boundary. Finally, on the right‐\\nhand side, the very high value of C in the model tilts the decision boundary a lot, now\\ncorrectly classifying all points in class 0. One of the points in class 1 is still misclassi‐\\nfied, as it is not possible to correctly classify all points in this dataset using a straight\\nline. The model illustrated on the righthand side tries hard to correctly classify all\\npoints, but might not capture the overall layout of the classes well. In other words,\\nthis model is likely overfitting.\\nSimilarly to the case of regression, linear models for classification might seem very\\nrestrictive in low-dimensional spaces, only allowing for decision boundaries that are\\nstraight lines or planes. Again, in high dimensions, linear models for classification\\n58 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 72}, page_content='become very powerful, and guarding against overfitting becomes increasingly impor‐\\ntant when considering more features.\\nLet’s analyze LinearLogistic in more detail on the Breast Cancer dataset:\\nIn[42]:\\nfrom sklearn.datasets import load_breast_cancer\\ncancer = load_breast_cancer()\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\\nlogreg = LogisticRegression().fit(X_train, y_train)\\nprint(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\\nprint(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\\nOut[42]:\\nTraining set score: 0.953\\nTest set score: 0.958\\nThe default value of C=1 provides quite good performance, with 95% accuracy on\\nboth the training and the test set. But as training and test set performance are very\\nclose, it is likely that we are underfitting. Let’s try to increase C to fit a more flexible\\nmodel:\\nIn[43]:\\nlogreg100 = LogisticRegression(C=100).fit(X_train, y_train)\\nprint(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train)))\\nprint(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))\\nOut[43]:\\nTraining set score: 0.972\\nTest set score: 0.965\\nUsing C=100 results in higher training set accuracy, and also a slightly increased test\\nset accuracy, confirming our intuition that a more complex model should perform\\nbetter.\\nWe can also investigate what happens if we use an even more regularized model than\\nthe default of C=1, by setting C=0.01:\\nIn[44]:\\nlogreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)\\nprint(\"Training set score: {:.3f}\".format(logreg001.score(X_train, y_train)))\\nprint(\"Test set score: {:.3f}\".format(logreg001.score(X_test, y_test)))\\nOut[44]:\\nTraining set score: 0.934\\nTest set score: 0.930\\nSupervised Machine Learning Algorithms \\n| \\n59'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 73}, page_content='As expected, when moving more to the left along the scale shown in Figure 2-1 from\\nan already underfit model, both training and test set accuracy decrease relative to the\\ndefault parameters.\\nFinally, let’s look at the coefficients learned by the models with the three different set‐\\ntings of the regularization parameter C (Figure 2-17):\\nIn[45]:\\nplt.plot(logreg.coef_.T, \\'o\\', label=\"C=1\")\\nplt.plot(logreg100.coef_.T, \\'^\\', label=\"C=100\")\\nplt.plot(logreg001.coef_.T, \\'v\\', label=\"C=0.001\")\\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\\nplt.hlines(0, 0, cancer.data.shape[1])\\nplt.ylim(-5, 5)\\nplt.xlabel(\"Coefficient index\")\\nplt.ylabel(\"Coefficient magnitude\")\\nplt.legend()\\nAs LogisticRegression applies an L2 regularization by default,\\nthe result looks similar to that produced by Ridge in Figure 2-12.\\nStronger regularization pushes coefficients more and more toward\\nzero, though coefficients never become exactly zero. Inspecting the\\nplot more closely, we can also see an interesting effect in the third\\ncoefficient, for “mean perimeter.” For C=100 and C=1, the coefficient\\nis negative, while for C=0.001, the coefficient is positive, with a\\nmagnitude that is even larger than for C=1. Interpreting a model\\nlike this, one might think the coefficient tells us which class a fea‐\\nture is associated with. For example, one might think that a high\\n“texture error” feature is related to a sample being “malignant.”\\nHowever, the change of sign in the coefficient for “mean perimeter”\\nmeans that depending on which model we look at, a high “mean\\nperimeter” could be taken as being either indicative of “benign” or\\nindicative of “malignant.” This illustrates that interpretations of\\ncoefficients of linear models should always be taken with a grain of\\nsalt.\\n60 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 74}, page_content='Figure 2-17. Coefficients learned by logistic regression on the Breast Cancer dataset for\\ndifferent values of C\\nSupervised Machine Learning Algorithms \\n| \\n61'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 75}, page_content='If we desire a more interpretable model, using L1 regularization might help, as it lim‐\\nits the model to using only a few features. Here is the coefficient plot and classifica‐\\ntion accuracies for L1 regularization (Figure 2-18):\\nIn[46]:\\nfor C, marker in zip([0.001, 1, 100], [\\'o\\', \\'^\\', \\'v\\']):\\n    lr_l1 = LogisticRegression(C=C, penalty=\"l1\").fit(X_train, y_train)\\n    print(\"Training accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\\n          C, lr_l1.score(X_train, y_train)))\\n    print(\"Test accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\\n          C, lr_l1.score(X_test, y_test)))\\n    plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\\nplt.hlines(0, 0, cancer.data.shape[1])\\nplt.xlabel(\"Coefficient index\")\\nplt.ylabel(\"Coefficient magnitude\")\\nplt.ylim(-5, 5)\\nplt.legend(loc=3)\\nOut[46]:\\nTraining accuracy of l1 logreg with C=0.001: 0.91\\nTest accuracy of l1 logreg with C=0.001: 0.92\\nTraining accuracy of l1 logreg with C=1.000: 0.96\\nTest accuracy of l1 logreg with C=1.000: 0.96\\nTraining accuracy of l1 logreg with C=100.000: 0.99\\nTest accuracy of l1 logreg with C=100.000: 0.98\\nAs you can see, there are many parallels between linear models for binary classifica‐\\ntion and linear models for regression. As in regression, the main difference between\\nthe models is the penalty parameter, which influences the regularization and\\nwhether the model will use all available features or select only a subset.\\n62 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 76}, page_content='Figure 2-18. Coefficients learned by logistic regression with L1 penalty on the Breast\\nCancer dataset for different values of C\\nLinear models for multiclass classification\\nMany linear classification models are for binary classification only, and don’t extend\\nnaturally to the multiclass case (with the exception of logistic regression). A common\\ntechnique to extend a binary classification algorithm to a multiclass classification\\nalgorithm is the one-vs.-rest approach. In the one-vs.-rest approach, a binary model is\\nlearned for each class that tries to separate that class from all of the other classes,\\nresulting in as many binary models as there are classes. To make a prediction, all\\nbinary classifiers are run on a test point. The classifier that has the highest score on its\\nsingle class “wins,” and this class label is returned as the prediction.\\nSupervised Machine Learning Algorithms \\n| \\n63'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 77}, page_content='Having one binary classifier per class results in having one vector of coefficients (w)\\nand one intercept (b) for each class. The class for which the result of the classification\\nconfidence formula given here is highest is the assigned class label:\\nw[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\\nThe mathematics behind multiclass logistic regression differ somewhat from the one-\\nvs.-rest approach, but they also result in one coefficient vector and intercept per class,\\nand the same method of making a prediction is applied.\\nLet’s apply the one-vs.-rest method to a simple three-class classification dataset. We\\nuse a two-dimensional dataset, where each class is given by data sampled from a\\nGaussian distribution (see Figure 2-19):\\nIn[47]:\\nfrom sklearn.datasets import make_blobs\\nX, y = make_blobs(random_state=42)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nplt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])\\nFigure 2-19. Two-dimensional toy dataset containing three classes\\n64 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 78}, page_content='Now, we train a LinearSVC classifier on the dataset:\\nIn[48]:\\nlinear_svm = LinearSVC().fit(X, y)\\nprint(\"Coefficient shape: \", linear_svm.coef_.shape)\\nprint(\"Intercept shape: \", linear_svm.intercept_.shape)\\nOut[48]:\\nCoefficient shape:  (3, 2)\\nIntercept shape:  (3,)\\nWe see that the shape of the coef_ is (3, 2), meaning that each row of coef_ con‐\\ntains the coefficient vector for one of the three classes and each column holds the\\ncoefficient value for a specific feature (there are two in this dataset). The intercept_\\nis now a one-dimensional array, storing the intercepts for each class.\\nLet’s visualize the lines given by the three binary classifiers (Figure 2-20):\\nIn[49]:\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nline = np.linspace(-15, 15)\\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\\n                                  [\\'b\\', \\'r\\', \\'g\\']):\\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\\nplt.ylim(-10, 15)\\nplt.xlim(-10, 8)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nplt.legend([\\'Class 0\\', \\'Class 1\\', \\'Class 2\\', \\'Line class 0\\', \\'Line class 1\\',\\n            \\'Line class 2\\'], loc=(1.01, 0.3))\\nYou can see that all the points belonging to class 0 in the training data are above the\\nline corresponding to class 0, which means they are on the “class 0” side of this binary\\nclassifier. The points in class 0 are above the line corresponding to class 2, which\\nmeans they are classified as “rest” by the binary classifier for class 2. The points\\nbelonging to class 0 are to the left of the line corresponding to class 1, which means\\nthe binary classifier for class 1 also classifies them as “rest.” Therefore, any point in\\nthis area will be classified as class 0 by the final classifier (the result of the classifica‐\\ntion confidence formula for classifier 0 is greater than zero, while it is smaller than\\nzero for the other two classes).\\nBut what about the triangle in the middle of the plot? All three binary classifiers clas‐\\nsify points there as “rest.” Which class would a point there be assigned to? The answer\\nis the one with the highest value for the classification formula: the class of the closest\\nline.\\nSupervised Machine Learning Algorithms \\n| \\n65'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 79}, page_content='Figure 2-20. Decision boundaries learned by the three one-vs.-rest classifiers\\nThe following example (Figure 2-21) shows the predictions for all regions of the 2D\\nspace:\\nIn[50]:\\nmglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nline = np.linspace(-15, 15)\\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\\n                                  [\\'b\\', \\'r\\', \\'g\\']):\\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\\nplt.legend([\\'Class 0\\', \\'Class 1\\', \\'Class 2\\', \\'Line class 0\\', \\'Line class 1\\',\\n            \\'Line class 2\\'], loc=(1.01, 0.3))\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n66 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 80}, page_content=\"Figure 2-21. Multiclass decision boundaries derived from the three one-vs.-rest classifiers\\nStrengths, weaknesses, and parameters\\nThe main parameter of linear models is the regularization parameter, called alpha in\\nthe regression models and C in LinearSVC and LogisticRegression. Large values for\\nalpha or small values for C mean simple models. In particular for the regression mod‐\\nels, tuning these parameters is quite important. Usually C and alpha are searched for\\non a logarithmic scale. The other decision you have to make is whether you want to\\nuse L1 regularization or L2 regularization. If you assume that only a few of your fea‐\\ntures are actually important, you should use L1. Otherwise, you should default to L2.\\nL1 can also be useful if interpretability of the model is important. As L1 will use only\\na few features, it is easier to explain which features are important to the model, and\\nwhat the effects of these features are.\\nLinear models are very fast to train, and also fast to predict. They scale to very large\\ndatasets and work well with sparse data. If your data consists of hundreds of thou‐\\nsands or millions of samples, you might want to investigate using the solver='sag'\\noption in LogisticRegression and Ridge, which can be faster than the default on\\nlarge datasets. Other options are the SGDClassifier class and the SGDRegressor\\nclass, which implement even more scalable versions of the linear models described\\nhere.\\nAnother strength of linear models is that they make it relatively easy to understand\\nhow a prediction is made, using the formulas we saw earlier for regression and classi‐\\nfication. Unfortunately, it is often not entirely clear why coefficients are the way they\\nare. This is particularly true if your dataset has highly correlated features; in these\\ncases, the coefficients might be hard to interpret.\\nSupervised Machine Learning Algorithms \\n| \\n67\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 81}, page_content='Linear models often perform well when the number of features is large compared to\\nthe number of samples. They are also often used on very large datasets, simply\\nbecause it’s not feasible to train other models. However, in lower-dimensional spaces,\\nother models might yield better generalization performance. We will look at some\\nexamples in which linear models fail in “Kernelized Support Vector Machines” on\\npage 92.\\nMethod Chaining\\nThe fit method of all scikit-learn models returns self. This allows you to write\\ncode like the following, which we’ve already used extensively in this chapter:\\nIn[51]:\\n# instantiate model and fit it in one line\\nlogreg = LogisticRegression().fit(X_train, y_train)\\nHere, we used the return value of fit (which is self) to assign the trained model to\\nthe variable logreg. This concatenation of method calls (here __init__ and then fit)\\nis known as method chaining. Another common application of method chaining in\\nscikit-learn is to fit and predict in one line:\\nIn[52]:\\nlogreg = LogisticRegression()\\ny_pred = logreg.fit(X_train, y_train).predict(X_test)\\nFinally, you can even do model instantiation, fitting, and predicting in one line:\\nIn[53]:\\ny_pred = LogisticRegression().fit(X_train, y_train).predict(X_test)\\nThis very short variant is not ideal, though. A lot is happening in a single line, which\\nmight make the code hard to read. Additionally, the fitted logistic regression model\\nisn’t stored in any variable, so we can’t inspect it or use it to predict on any other data.\\nNaive Bayes Classifiers\\nNaive Bayes classifiers are a family of classifiers that are quite similar to the linear\\nmodels discussed in the previous section. However, they tend to be even faster in\\ntraining. The price paid for this efficiency is that naive Bayes models often provide\\ngeneralization performance that is slightly worse than that of linear classifiers like\\nLogisticRegression and LinearSVC.\\nThe reason that naive Bayes models are so efficient is that they learn parameters by\\nlooking at each feature individually and collect simple per-class statistics from each\\nfeature. There are three kinds of naive Bayes classifiers implemented in scikit-\\n68 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 82}, page_content='learn: GaussianNB, BernoulliNB, and MultinomialNB. GaussianNB can be applied to\\nany continuous data, while BernoulliNB assumes binary data and MultinomialNB\\nassumes count data (that is, that each feature represents an integer count of some‐\\nthing, like how often a word appears in a sentence). BernoulliNB and MultinomialNB\\nare mostly used in text data classification.\\nThe BernoulliNB classifier counts how often every feature of each class is not zero.\\nThis is most easily understood with an example:\\nIn[54]:\\nX = np.array([[0, 1, 0, 1],\\n              [1, 0, 1, 1],\\n              [0, 0, 0, 1],\\n              [1, 0, 1, 0]])\\ny = np.array([0, 1, 0, 1])\\nHere, we have four data points, with four binary features each. There are two classes,\\n0 and 1. For class 0 (the first and third data points), the first feature is zero two times\\nand nonzero zero times, the second feature is zero one time and nonzero one time,\\nand so on. These same counts are then calculated for the data points in the second\\nclass. Counting the nonzero entries per class in essence looks like this:\\nIn[55]:\\ncounts = {}\\nfor label in np.unique(y):\\n    # iterate over each class\\n    # count (sum) entries of 1 per feature\\n    counts[label] = X[y == label].sum(axis=0)\\nprint(\"Feature counts:\\\\n{}\".format(counts))\\nOut[55]:\\nFeature counts:\\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\\nferent in what kinds of statistics they compute. MultinomialNB takes into account the\\naverage value of each feature for each class, while GaussianNB stores the average value\\nas well as the standard deviation of each feature for each class.\\nTo make a prediction, a data point is compared to the statistics for each of the classes,\\nand the best matching class is predicted. Interestingly, for both MultinomialNB and\\nBernoulliNB, this leads to a prediction formula that is of the same form as in the lin‐\\near models (see “Linear models for classification” on page 56). Unfortunately, coef_\\nfor the naive Bayes models has a somewhat different meaning than in the linear mod‐\\nels, in that coef_ is not the same as w.\\nSupervised Machine Learning Algorithms \\n| \\n69'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 83}, page_content='Strengths, weaknesses, and parameters\\nMultinomialNB and BernoulliNB have a single parameter, alpha, which controls\\nmodel complexity. The way alpha works is that the algorithm adds to the data alpha\\nmany virtual data points that have positive values for all the features. This results in a\\n“smoothing” of the statistics. A large alpha means more smoothing, resulting in less\\ncomplex models. The algorithm’s performance is relatively robust to the setting of\\nalpha, meaning that setting alpha is not critical for good performance. However,\\ntuning it usually improves accuracy somewhat.\\nGaussianNB is mostly used on very high-dimensional data, while the other two var‐\\niants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\\nusually performs better than BinaryNB, particularly on datasets with a relatively large\\nnumber of nonzero features (i.e., large documents).\\nThe naive Bayes models share many of the strengths and weaknesses of the linear\\nmodels. They are very fast to train and to predict, and the training procedure is easy\\nto understand. The models work very well with high-dimensional sparse data and are\\nrelatively robust to the parameters. Naive Bayes models are great baseline models and\\nare often used on very large datasets, where training even a linear model might take\\ntoo long.\\nDecision Trees\\nDecision trees are widely used models for classification and regression tasks. Essen‐\\ntially, they learn a hierarchy of if/else questions, leading to a decision.\\nThese questions are similar to the questions you might ask in a game of 20 Questions.\\nImagine you want to distinguish between the following four animals: bears, hawks,\\npenguins, and dolphins. Your goal is to get to the right answer by asking as few if/else\\nquestions as possible. You might start off by asking whether the animal has feathers, a\\nquestion that narrows down your possible animals to just two. If the answer is “yes,”\\nyou can ask another question that could help you distinguish between hawks and\\npenguins. For example, you could ask whether the animal can fly. If the animal\\ndoesn’t have feathers, your possible animal choices are dolphins and bears, and you\\nwill need to ask a question to distinguish between these two animals—for example,\\nasking whether the animal has fins.\\nThis series of questions can be expressed as a decision tree, as shown in Figure 2-22.\\nIn[56]:\\nmglearn.plots.plot_animal_tree()\\n70 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 84}, page_content='Figure 2-22. A decision tree to distinguish among several animals\\nIn this illustration, each node in the tree either represents a question or a terminal\\nnode (also called a leaf) that contains the answer. The edges connect the answers to a\\nquestion with the next question you would ask.\\nIn machine learning parlance, we built a model to distinguish between four classes of\\nanimals (hawks, penguins, dolphins, and bears) using the three features “has feath‐\\ners,” “can fly,” and “has fins.” Instead of building these models by hand, we can learn\\nthem from data using supervised learning.\\nBuilding decision trees\\nLet’s go through the process of building a decision tree for the 2D classification data‐\\nset shown in Figure 2-23. The dataset consists of two half-moon shapes, with each\\nclass consisting of 75 data points. We will refer to this dataset as two_moons.\\nLearning a decision tree means learning the sequence of if/else questions that gets us\\nto the true answer most quickly. In the machine learning setting, these questions are\\ncalled tests (not to be confused with the test set, which is the data we use to test to see\\nhow generalizable our model is). Usually data does not come in the form of binary\\nyes/no features as in the animal example, but is instead represented as continuous\\nfeatures such as in the 2D dataset shown in Figure 2-23. The tests that are used on\\ncontinuous data are of the form “Is feature i larger than value a?”\\nSupervised Machine Learning Algorithms \\n| \\n71'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 85}, page_content='Figure 2-23. Two-moons dataset on which the decision tree will be built\\nTo build a tree, the algorithm searches over all possible tests and finds the one that is\\nmost informative about the target variable. Figure 2-24 shows the first test that is\\npicked. Splitting the dataset vertically at x[1]=0.0596 yields the most information; it\\nbest separates the points in class 1 from the points in class 2. The top node, also called\\nthe root, represents the whole dataset, consisting of 75 points belonging to class 0 and\\n75 points belonging to class 1. The split is done by testing whether x[1] <= 0.0596,\\nindicated by a black line. If the test is true, a point is assigned to the left node, which\\ncontains 2 points belonging to class 0 and 32 points belonging to class 1. Otherwise\\nthe point is assigned to the right node, which contains 48 points belonging to class 0\\nand 18 points belonging to class 1. These two nodes correspond to the top and bot‐\\ntom regions shown in Figure 2-24. Even though the first split did a good job of sepa‐\\nrating the two classes, the bottom region still contains points belonging to class 0, and\\nthe top region still contains points belonging to class 1. We can build a more accurate\\nmodel by repeating the process of looking for the best test in both regions.\\nFigure 2-25 shows that the most informative next split for the left and the right region\\nis based on x[0].\\n72 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 86}, page_content='Figure 2-24. Decision boundary of tree with depth 1 (left) and corresponding tree (right)\\nFigure 2-25. Decision boundary of tree with depth 2 (left) and corresponding decision\\ntree (right)\\nThis recursive process yields a binary tree of decisions, with each node containing a\\ntest. Alternatively, you can think of each test as splitting the part of the data that is\\ncurrently being considered along one axis. This yields a view of the algorithm as\\nbuilding a hierarchical partition. As each test concerns only a single feature, the\\nregions in the resulting partition always have axis-parallel boundaries.\\nThe recursive partitioning of the data is repeated until each region in the partition\\n(each leaf in the decision tree) only contains a single target value (a single class or a\\nsingle regression value). A leaf of the tree that contains data points that all share the\\nsame target value is called pure. The final partitioning for this dataset is shown in\\nFigure 2-26.\\nSupervised Machine Learning Algorithms \\n| \\n73'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 87}, page_content='Figure 2-26. Decision boundary of tree with depth 9 (left) and part of the corresponding\\ntree (right); the full tree is quite large and hard to visualize\\nA prediction on a new data point is made by checking which region of the partition\\nof the feature space the point lies in, and then predicting the majority target (or the\\nsingle target in the case of pure leaves) in that region. The region can be found by\\ntraversing the tree from the root and going left or right, depending on whether the\\ntest is fulfilled or not.\\nIt is also possible to use trees for regression tasks, using exactly the same technique.\\nTo make a prediction, we traverse the tree based on the tests in each node and find\\nthe leaf the new data point falls into. The output for this data point is the mean target\\nof the training points in this leaf.\\nControlling complexity of decision trees\\nTypically, building a tree as described here and continuing until all leaves are pure\\nleads to models that are very complex and highly overfit to the training data. The\\npresence of pure leaves mean that a tree is 100% accurate on the training set; each\\ndata point in the training set is in a leaf that has the correct majority class. The over‐\\nfitting can be seen on the left of Figure 2-26. You can see the regions determined to\\nbelong to class 1 in the middle of all the points belonging to class 0. On the other\\nhand, there is a small strip predicted as class 0 around the point belonging to class 0\\nto the very right. This is not how one would imagine the decision boundary to look,\\nand the decision boundary focuses a lot on single outlier points that are far away\\nfrom the other points in that class.\\nThere are two common strategies to prevent overfitting: stopping the creation of the\\ntree early (also called pre-pruning), or building the tree but then removing or collaps‐\\ning nodes that contain little information (also called post-pruning or just pruning).\\nPossible criteria for pre-pruning include limiting the maximum depth of the tree,\\nlimiting the maximum number of leaves, or requiring a minimum number of points\\nin a node to keep splitting it.\\n74 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 88}, page_content='Decision trees in scikit-learn are implemented in the DecisionTreeRegressor and\\nDecisionTreeClassifier classes. scikit-learn only implements pre-pruning, not\\npost-pruning.\\nLet’s look at the effect of pre-pruning in more detail on the Breast Cancer dataset. As\\nalways, we import the dataset and split it into a training and a test part. Then we build\\na model using the default setting of fully developing the tree (growing the tree until\\nall leaves are pure). We fix the random_state in the tree, which is used for tie-\\nbreaking internally:\\nIn[58]:\\nfrom sklearn.tree import DecisionTreeClassifier\\ncancer = load_breast_cancer()\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\\ntree = DecisionTreeClassifier(random_state=0)\\ntree.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\\nOut[58]:\\nAccuracy on training set: 1.000\\nAccuracy on test set: 0.937\\nAs expected, the accuracy on the training set is 100%—because the leaves are pure,\\nthe tree was grown deep enough that it could perfectly memorize all the labels on the\\ntraining data. The test set accuracy is slightly worse than for the linear models we\\nlooked at previously, which had around 95% accuracy.\\nIf we don’t restrict the depth of a decision tree, the tree can become arbitrarily deep\\nand complex. Unpruned trees are therefore prone to overfitting and not generalizing\\nwell to new data. Now let’s apply pre-pruning to the tree, which will stop developing\\nthe tree before we perfectly fit to the training data. One option is to stop building the\\ntree after a certain depth has been reached. Here we set max_depth=4, meaning only\\nfour consecutive questions can be asked (cf. Figures 2-24 and 2-26). Limiting the\\ndepth of the tree decreases overfitting. This leads to a lower accuracy on the training\\nset, but an improvement on the test set:\\nIn[59]:\\ntree = DecisionTreeClassifier(max_depth=4, random_state=0)\\ntree.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\\nSupervised Machine Learning Algorithms \\n| \\n75'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 89}, page_content='Out[59]:\\nAccuracy on training set: 0.988\\nAccuracy on test set: 0.951\\nAnalyzing decision trees\\nWe can visualize the tree using the export_graphviz function from the tree module.\\nThis writes a file in the .dot file format, which is a text file format for storing graphs.\\nWe set an option to color the nodes to reflect the majority class in each node and pass\\nthe class and features names so the tree can be properly labeled:\\nIn[61]:\\nfrom sklearn.tree import export_graphviz\\nexport_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"],\\n                feature_names=cancer.feature_names, impurity=False, filled=True)\\nWe can read this file and visualize it, as seen in Figure 2-27, using the graphviz mod‐\\nule (or you can use any program that can read .dot files):\\nIn[61]:\\nimport graphviz\\nwith open(\"tree.dot\") as f:\\n    dot_graph = f.read()\\ngraphviz.Source(dot_graph)\\nFigure 2-27. Visualization of the decision tree built on the Breast Cancer dataset\\n76 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 90}, page_content='The visualization of the tree provides a great in-depth view of how the algorithm\\nmakes predictions, and is a good example of a machine learning algorithm that is\\neasily explained to nonexperts. However, even with a tree of depth four, as seen here,\\nthe tree can become a bit overwhelming. Deeper trees (a depth of 10 is not uncom‐\\nmon) are even harder to grasp. One method of inspecting the tree that may be helpful\\nis to find out which path most of the data actually takes. The n_samples shown in\\neach node in Figure 2-27 gives the number of samples in that node, while value pro‐\\nvides the number of samples per class. Following the branches to the right, we see\\nthat worst radius <= 16.795 creates a node that contains only 8 benign but 134\\nmalignant samples. The rest of this side of the tree then uses some finer distinctions\\nto split off these 8 remaining benign samples. Of the 142 samples that went to the\\nright in the initial split, nearly all of them (132) end up in the leaf to the very right.\\nTaking a left at the root, for worst radius > 16.795 we end up with 25 malignant\\nand 259 benign samples. Nearly all of the benign samples end up in the second leaf\\nfrom the right, with most of the other leaves containing very few samples.\\nFeature importance in trees\\nInstead of looking at the whole tree, which can be taxing, there are some useful prop‐\\nerties that we can derive to summarize the workings of the tree. The most commonly\\nused summary is feature importance, which rates how important each feature is for\\nthe decision a tree makes. It is a number between 0 and 1 for each feature, where 0\\nmeans “not used at all” and 1 means “perfectly predicts the target.” The feature\\nimportances always sum to 1:\\nIn[62]:\\nprint(\"Feature importances:\\\\n{}\".format(tree.feature_importances_))\\nOut[62]:\\nFeature importances:\\n[ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.01\\n  0.048  0.     0.     0.002  0.     0.     0.     0.     0.     0.727  0.046\\n  0.     0.     0.014  0.     0.018  0.122  0.012  0.   ]\\nWe can visualize the feature importances in a way that is similar to the way we visual‐\\nize the coefficients in the linear model (Figure 2-28):\\nIn[63]:\\ndef plot_feature_importances_cancer(model):\\n    n_features = cancer.data.shape[1]\\n    plt.barh(range(n_features), model.feature_importances_, align=\\'center\\')\\n    plt.yticks(np.arange(n_features), cancer.feature_names)\\n    plt.xlabel(\"Feature importance\")\\n    plt.ylabel(\"Feature\")\\nplot_feature_importances_cancer(tree)\\nSupervised Machine Learning Algorithms \\n| \\n77'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 91}, page_content='Figure 2-28. Feature importances computed from a decision tree learned on the Breast\\nCancer dataset\\nHere we see that the feature used in the top split (“worst radius”) is by far the most\\nimportant feature. This confirms our observation in analyzing the tree that the first\\nlevel already separates the two classes fairly well.\\nHowever, if a feature has a low feature_importance, it doesn’t mean that this feature\\nis uninformative. It only means that the feature was not picked by the tree, likely\\nbecause another feature encodes the same information.\\nIn contrast to the coefficients in linear models, feature importances are always posi‐\\ntive, and don’t encode which class a feature is indicative of. The feature importances\\ntell us that “worst radius” is important, but not whether a high radius is indicative of a\\nsample being benign or malignant. In fact, there might not be such a simple relation‐\\nship between features and class, as you can see in the following example (Figures 2-29\\nand 2-30):\\nIn[64]:\\ntree = mglearn.plots.plot_tree_not_monotone()\\ndisplay(tree)\\nOut[64]:\\nFeature importances: [ 0.  1.]\\n78 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 92}, page_content='Figure 2-29. A two-dimensional dataset in which the feature on the y-axis has a nonmo‐\\nnotonous relationship with the class label, and the decision boundaries found by a deci‐\\nsion tree\\nFigure 2-30. Decision tree learned on the data shown in Figure 2-29\\nThe plot shows a dataset with two features and two classes. Here, all the information\\nis contained in X[1], and X[0] is not used at all. But the relation between X[1] and\\nSupervised Machine Learning Algorithms \\n| \\n79'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 93}, page_content='the output class is not monotonous, meaning we cannot say “a high value of X[0]\\nmeans class 0, and a low value means class 1” (or vice versa).\\nWhile we focused our discussion here on decision trees for classification, all that was\\nsaid is similarly true for decision trees for regression, as implemented in Decision\\nTreeRegressor. The usage and analysis of regression trees is very similar to that of\\nclassification trees. There is one particular property of using tree-based models for\\nregression that we want to point out, though. The DecisionTreeRegressor (and all\\nother tree-based regression models) is not able to extrapolate, or make predictions\\noutside of the range of the training data.\\nLet’s look into this in more detail, using a dataset of historical computer memory\\n(RAM) prices. Figure 2-31 shows the dataset, with the date on the x-axis and the price\\nof one megabyte of RAM in that year on the y-axis:\\nIn[65]:\\nimport pandas as pd\\nram_prices = pd.read_csv(\"data/ram_price.csv\")\\nplt.semilogy(ram_prices.date, ram_prices.price)\\nplt.xlabel(\"Year\")\\nplt.ylabel(\"Price in $/Mbyte\")\\nFigure 2-31. Historical development of the price of RAM, plotted on a log scale\\n80 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 94}, page_content='Note the logarithmic scale of the y-axis. When plotting logarithmically, the relation\\nseems to be quite linear and so should be relatively easy to predict, apart from some\\nbumps.\\nWe will make a forecast for the years after 2000 using the historical data up to that\\npoint, with the date as our only feature. We will compare two simple models: a\\nDecisionTreeRegressor and LinearRegression. We rescale the prices using a loga‐\\nrithm, so that the relationship is relatively linear. This doesn’t make a difference for\\nthe DecisionTreeRegressor, but it makes a big difference for LinearRegression (we\\nwill discuss this in more depth in Chapter 4). After training the models and making\\npredictions, we apply the exponential map to undo the logarithm transform. We\\nmake predictions on the whole dataset for visualization purposes here, but for a\\nquantitative evaluation we would only consider the test dataset:\\nIn[66]:\\nfrom sklearn.tree import DecisionTreeRegressor\\n# use historical data to forecast prices after the year 2000\\ndata_train = ram_prices[ram_prices.date < 2000]\\ndata_test = ram_prices[ram_prices.date >= 2000]\\n# predict prices based on date\\nX_train = data_train.date[:, np.newaxis]\\n# we use a log-transform to get a simpler relationship of data to target\\ny_train = np.log(data_train.price)\\ntree = DecisionTreeRegressor().fit(X_train, y_train)\\nlinear_reg = LinearRegression().fit(X_train, y_train)\\n# predict on all data\\nX_all = ram_prices.date[:, np.newaxis]\\npred_tree = tree.predict(X_all)\\npred_lr = linear_reg.predict(X_all)\\n# undo log-transform\\nprice_tree = np.exp(pred_tree)\\nprice_lr = np.exp(pred_lr)\\nFigure 2-32, created here, compares the predictions of the decision tree and the linear\\nregression model with the ground truth:\\nIn[67]:\\nplt.semilogy(data_train.date, data_train.price, label=\"Training data\")\\nplt.semilogy(data_test.date, data_test.price, label=\"Test data\")\\nplt.semilogy(ram_prices.date, price_tree, label=\"Tree prediction\")\\nplt.semilogy(ram_prices.date, price_lr, label=\"Linear prediction\")\\nplt.legend()\\nSupervised Machine Learning Algorithms \\n| \\n81'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 95}, page_content='9 It is actually possible to make very good forecasts with tree-based models (for example, when trying to predict\\nwhether a price will go up or down). The point of this example was not to show that trees are a bad model for\\ntime series, but to illustrate a particular property of how trees make predictions.\\nFigure 2-32. Comparison of predictions made by a linear model and predictions made\\nby a regression tree on the RAM price data\\nThe difference between the models is quite striking. The linear model approximates\\nthe data with a line, as we knew it would. This line provides quite a good forecast for\\nthe test data (the years after 2000), while glossing over some of the finer variations in\\nboth the training and the test data. The tree model, on the other hand, makes perfect\\npredictions on the training data; we did not restrict the complexity of the tree, so it\\nlearned the whole dataset by heart. However, once we leave the data range for which\\nthe model has data, the model simply keeps predicting the last known point. The tree\\nhas no ability to generate “new” responses, outside of what was seen in the training\\ndata. This shortcoming applies to all models based on trees.9\\nStrengths, weaknesses, and parameters\\nAs discussed earlier, the parameters that control model complexity in decision trees\\nare the pre-pruning parameters that stop the building of the tree before it is fully\\ndeveloped. Usually, picking one of the pre-pruning strategies—setting either\\n82 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 96}, page_content='max_depth, max_leaf_nodes, or min_samples_leaf—is sufficient to prevent overfit‐\\nting.\\nDecision trees have two advantages over many of the algorithms we’ve discussed so\\nfar: the resulting model can easily be visualized and understood by nonexperts (at\\nleast for smaller trees), and the algorithms are completely invariant to scaling of the\\ndata. As each feature is processed separately, and the possible splits of the data don’t\\ndepend on scaling, no preprocessing like normalization or standardization of features\\nis needed for decision tree algorithms. In particular, decision trees work well when\\nyou have features that are on completely different scales, or a mix of binary and con‐\\ntinuous features.\\nThe main downside of decision trees is that even with the use of pre-pruning, they\\ntend to overfit and provide poor generalization performance. Therefore, in most\\napplications, the ensemble methods we discuss next are usually used in place of a sin‐\\ngle decision tree.\\nEnsembles of Decision Trees\\nEnsembles are methods that combine multiple machine learning models to create\\nmore powerful models. There are many models in the machine learning literature\\nthat belong to this category, but there are two ensemble models that have proven to\\nbe effective on a wide range of datasets for classification and regression, both of\\nwhich use decision trees as their building blocks: random forests and gradient boos‐\\nted decision trees.\\nRandom forests\\nAs we just observed, a main drawback of decision trees is that they tend to overfit the\\ntraining data. Random forests are one way to address this problem. A random forest\\nis essentially a collection of decision trees, where each tree is slightly different from\\nthe others. The idea behind random forests is that each tree might do a relatively\\ngood job of predicting, but will likely overfit on part of the data. If we build many\\ntrees, all of which work well and overfit in different ways, we can reduce the amount\\nof overfitting by averaging their results. This reduction in overfitting, while retaining\\nthe predictive power of the trees, can be shown using rigorous mathematics.\\nTo implement this strategy, we need to build many decision trees. Each tree should do\\nan acceptable job of predicting the target, and should also be different from the other\\ntrees. Random forests get their name from injecting randomness into the tree build‐\\ning to ensure each tree is different. There are two ways in which the trees in a random\\nforest are randomized: by selecting the data points used to build a tree and by select‐\\ning the features in each split test. Let’s go into this process in more detail.\\nSupervised Machine Learning Algorithms \\n| \\n83'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 97}, page_content=\"Building random forests.    To build a random forest model, you need to decide on the\\nnumber of trees to build (the n_estimators parameter of RandomForestRegressor or\\nRandomForestClassifier). Let’s say we want to build 10 trees. These trees will be\\nbuilt completely independently from each other, and the algorithm will make differ‐\\nent random choices for each tree to make sure the trees are distinct. To build a tree,\\nwe first take what is called a bootstrap sample of our data. That is, from our n_samples\\ndata points, we repeatedly draw an example randomly with replacement (meaning the\\nsame sample can be picked multiple times), n_samples times. This will create a data‐\\nset that is as big as the original dataset, but some data points will be missing from it\\n(approximately one third), and some will be repeated.\\nTo illustrate, let’s say we want to create a bootstrap sample of the list ['a', 'b',\\n'c', 'd']. A possible bootstrap sample would be ['b', 'd', 'd', 'c']. Another\\npossible sample would be ['d', 'a', 'd', 'a'].\\nNext, a decision tree is built based on this newly created dataset. However, the algo‐\\nrithm we described for the decision tree is slightly modified. Instead of looking for\\nthe best test for each node, in each node the algorithm randomly selects a subset of\\nthe features, and it looks for the best possible test involving one of these features. The\\nnumber of features that are selected is controlled by the max_features parameter.\\nThis selection of a subset of features is repeated separately in each node, so that each\\nnode in a tree can make a decision using a different subset of the features.\\nThe bootstrap sampling leads to each decision tree in the random forest being built\\non a slightly different dataset. Because of the selection of features in each node, each\\nsplit in each tree operates on a different subset of features. Together, these two mech‐\\nanisms ensure that all the trees in the random forest are different.\\nA critical parameter in this process is max_features. If we set max_features to n_fea\\ntures, that means that each split can look at all features in the dataset, and no ran‐\\ndomness will be injected in the feature selection (the randomness due to the\\nbootstrapping remains, though). If we set max_features to 1, that means that the\\nsplits have no choice at all on which feature to test, and can only search over different\\nthresholds for the feature that was selected randomly. Therefore, a high max_fea\\ntures means that the trees in the random forest will be quite similar, and they will be\\nable to fit the data easily, using the most distinctive features. A low max_features\\nmeans that the trees in the random forest will be quite different, and that each tree\\nmight need to be very deep in order to fit the data well.\\nTo make a prediction using the random forest, the algorithm first makes a prediction\\nfor every tree in the forest. For regression, we can average these results to get our final\\nprediction. For classification, a “soft voting” strategy is used. This means each algo‐\\nrithm makes a “soft” prediction, providing a probability for each possible output\\n84 \\n| \\nChapter 2: Supervised Learning\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 98}, page_content='label. The probabilities predicted by all the trees are averaged, and the class with the\\nhighest probability is predicted.\\nAnalyzing random forests.    Let’s apply a random forest consisting of five trees to the\\ntwo_moons dataset we studied earlier:\\nIn[68]:\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.datasets import make_moons\\nX, y = make_moons(n_samples=100, noise=0.25, random_state=3)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\\n                                                    random_state=42)\\nforest = RandomForestClassifier(n_estimators=5, random_state=2)\\nforest.fit(X_train, y_train)\\nThe trees that are built as part of the random forest are stored in the estimator_\\nattribute. Let’s visualize the decision boundaries learned by each tree, together with\\ntheir aggregate prediction as made by the forest (Figure 2-33):\\nIn[69]:\\nfig, axes = plt.subplots(2, 3, figsize=(20, 10))\\nfor i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\\n    ax.set_title(\"Tree {}\".format(i))\\n    mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)\\nmglearn.plots.plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1],\\n                                alpha=.4)\\naxes[-1, -1].set_title(\"Random Forest\")\\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\\nYou can clearly see that the decision boundaries learned by the five trees are quite dif‐\\nferent. Each of them makes some mistakes, as some of the training points that are\\nplotted here were not actually included in the training sets of the trees, due to the\\nbootstrap sampling.\\nThe random forest overfits less than any of the trees individually, and provides a\\nmuch more intuitive decision boundary. In any real application, we would use many\\nmore trees (often hundreds or thousands), leading to even smoother boundaries.\\nSupervised Machine Learning Algorithms \\n| \\n85'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 99}, page_content='Figure 2-33. Decision boundaries found by five randomized decision trees and the deci‐\\nsion boundary obtained by averaging their predicted probabilities\\nAs another example, let’s apply a random forest consisting of 100 trees on the Breast\\nCancer dataset:\\nIn[70]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\nforest = RandomForestClassifier(n_estimators=100, random_state=0)\\nforest.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))\\nOut[70]:\\nAccuracy on training set: 1.000\\nAccuracy on test set: 0.972\\nThe random forest gives us an accuracy of 97%, better than the linear models or a\\nsingle decision tree, without tuning any parameters. We could adjust the max_fea\\ntures setting, or apply pre-pruning as we did for the single decision tree. However,\\noften the default parameters of the random forest already work quite well.\\nSimilarly to the decision tree, the random forest provides feature importances, which\\nare computed by aggregating the feature importances over the trees in the forest. Typ‐\\nically, the feature importances provided by the random forest are more reliable than\\nthe ones provided by a single tree. Take a look at Figure 2-34.\\n86 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 100}, page_content='In[71]:\\nplot_feature_importances_cancer(forest)\\nFigure 2-34. Feature importances computed from a random forest that was fit to the\\nBreast Cancer dataset\\nAs you can see, the random forest gives nonzero importance to many more features\\nthan the single tree. Similarly to the single decision tree, the random forest also gives\\na lot of importance to the “worst radius” feature, but it actually chooses “worst perim‐\\neter” to be the most informative feature overall. The randomness in building the ran‐\\ndom forest forces the algorithm to consider many possible explanations, the result\\nbeing that the random forest captures a much broader picture of the data than a sin‐\\ngle tree.\\nStrengths, weaknesses, and parameters.    Random forests for regression and classifica‐\\ntion are currently among the most widely used machine learning methods. They are\\nvery powerful, often work well without heavy tuning of the parameters, and don’t\\nrequire scaling of the data.\\nEssentially, random forests share all of the benefits of decision trees, while making up\\nfor some of their deficiencies. One reason to still use decision trees is if you need a\\ncompact representation of the decision-making process. It is basically impossible to\\ninterpret tens or hundreds of trees in detail, and trees in random forests tend to be\\ndeeper than decision trees (because of the use of feature subsets). Therefore, if you\\nneed to summarize the prediction making in a visual way to nonexperts, a single\\ndecision tree might be a better choice. While building random forests on large data‐\\nsets might be somewhat time consuming, it can be parallelized across multiple CPU\\nSupervised Machine Learning Algorithms \\n| \\n87'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 101}, page_content='cores within a computer easily. If you are using a multi-core processor (as nearly all\\nmodern computers do), you can use the n_jobs parameter to adjust the number of\\ncores to use. Using more CPU cores will result in linear speed-ups (using two cores,\\nthe training of the random forest will be twice as fast), but specifying n_jobs larger\\nthan the number of cores will not help. You can set n_jobs=-1 to use all the cores in\\nyour computer.\\nYou should keep in mind that random forests, by their nature, are random, and set‐\\nting different random states (or not setting the random_state at all) can drastically\\nchange the model that is built. The more trees there are in the forest, the more robust\\nit will be against the choice of random state. If you want to have reproducible results,\\nit is important to fix the random_state.\\nRandom forests don’t tend to perform well on very high dimensional, sparse data,\\nsuch as text data. For this kind of data, linear models might be more appropriate.\\nRandom forests usually work well even on very large datasets, and training can easily\\nbe parallelized over many CPU cores within a powerful computer. However, random\\nforests require more memory and are slower to train and to predict than linear mod‐\\nels. If time and memory are important in an application, it might make sense to use a\\nlinear model instead.\\nThe important parameters to adjust are n_estimators, max_features, and possibly\\npre-pruning options like max_depth. For n_estimators, larger is always better. Aver‐\\naging more trees will yield a more robust ensemble by reducing overfitting. However,\\nthere are diminishing returns, and more trees need more memory and more time to\\ntrain. A common rule of thumb is to build “as many as you have time/memory for.”\\nAs described earlier, max_features determines how random each tree is, and a\\nsmaller max_features reduces overfitting. In general, it’s a good rule of thumb to use\\nthe default values: max_features=sqrt(n_features) for classification and max_fea\\ntures=log2(n_features) for regression. Adding max_features or max_leaf_nodes\\nmight sometimes improve performance. It can also drastically reduce space and time\\nrequirements for training and prediction.\\nGradient boosted regression trees (gradient boosting machines)\\nThe gradient boosted regression tree is another ensemble method that combines mul‐\\ntiple decision trees to create a more powerful model. Despite the “regression” in the\\nname, these models can be used for regression and classification. In contrast to the\\nrandom forest approach, gradient boosting works by building trees in a serial man‐\\nner, where each tree tries to correct the mistakes of the previous one. By default, there\\nis no randomization in gradient boosted regression trees; instead, strong pre-pruning\\nis used. Gradient boosted trees often use very shallow trees, of depth one to five,\\nwhich makes the model smaller in terms of memory and makes predictions faster.\\n88 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 102}, page_content='The main idea behind gradient boosting is to combine many simple models (in this\\ncontext known as weak learners), like shallow trees. Each tree can only provide good\\npredictions on part of the data, and so more and more trees are added to iteratively\\nimprove performance.\\nGradient boosted trees are frequently the winning entries in machine learning com‐\\npetitions, and are widely used in industry. They are generally a bit more sensitive to\\nparameter settings than random forests, but can provide better accuracy if the param‐\\neters are set correctly.\\nApart from the pre-pruning and the number of trees in the ensemble, another impor‐\\ntant parameter of gradient boosting is the learning_rate, which controls how\\nstrongly each tree tries to correct the mistakes of the previous trees. A higher learning\\nrate means each tree can make stronger corrections, allowing for more complex mod‐\\nels. Adding more trees to the ensemble, which can be accomplished by increasing\\nn_estimators, also increases the model complexity, as the model has more chances\\nto correct mistakes on the training set.\\nHere is an example of using GradientBoostingClassifier on the Breast Cancer\\ndataset. By default, 100 trees of maximum depth 3 and a learning rate of 0.1 are used:\\nIn[72]:\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\ngbrt = GradientBoostingClassifier(random_state=0)\\ngbrt.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\\nOut[72]:\\nAccuracy on training set: 1.000\\nAccuracy on test set: 0.958\\nAs the training set accuracy is 100%, we are likely to be overfitting. To reduce overfit‐\\nting, we could either apply stronger pre-pruning by limiting the maximum depth or\\nlower the learning rate:\\nSupervised Machine Learning Algorithms \\n| \\n89'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 103}, page_content='In[73]:\\ngbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\\ngbrt.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\\nOut[73]:\\nAccuracy on training set: 0.991\\nAccuracy on test set: 0.972\\nIn[74]:\\ngbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\\ngbrt.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\\nOut[74]:\\nAccuracy on training set: 0.988\\nAccuracy on test set: 0.965\\nBoth methods of decreasing the model complexity reduced the training set accuracy,\\nas expected. In this case, lowering the maximum depth of the trees provided a signifi‐\\ncant improvement of the model, while lowering the learning rate only increased the\\ngeneralization performance slightly.\\nAs for the other decision tree–based models, we can again visualize the feature\\nimportances to get more insight into our model (Figure 2-35). As we used 100 trees, it\\nis impractical to inspect them all, even if they are all of depth 1:\\nIn[75]:\\ngbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\\ngbrt.fit(X_train, y_train)\\nplot_feature_importances_cancer(gbrt)\\n90 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 104}, page_content='Figure 2-35. Feature importances computed from a gradient boosting classifier that was\\nfit to the Breast Cancer dataset\\nWe can see that the feature importances of the gradient boosted trees are somewhat\\nsimilar to the feature importances of the random forests, though the gradient boost‐\\ning completely ignored some of the features.\\nAs both gradient boosting and random forests perform well on similar kinds of data,\\na common approach is to first try random forests, which work quite robustly. If ran‐\\ndom forests work well but prediction time is at a premium, or it is important to\\nsqueeze out the last percentage of accuracy from the machine learning model, mov‐\\ning to gradient boosting often helps.\\nIf you want to apply gradient boosting to a large-scale problem, it might be worth\\nlooking into the xgboost package and its Python interface, which at the time of writ‐\\ning is faster (and sometimes easier to tune) than the scikit-learn implementation of\\ngradient boosting on many datasets.\\nStrengths, weaknesses, and parameters.    Gradient boosted decision trees are among the\\nmost powerful and widely used models for supervised learning. Their main drawback\\nis that they require careful tuning of the parameters and may take a long time to\\ntrain. Similarly to other tree-based models, the algorithm works well without scaling\\nand on a mixture of binary and continuous features. As with other tree-based models,\\nit also often does not work well on high-dimensional sparse data.\\nThe main parameters of gradient boosted tree models are the number of trees, n_esti\\nmators, and the learning_rate, which controls the degree to which each tree is\\nallowed to correct the mistakes of the previous trees. These two parameters are highly\\nSupervised Machine Learning Algorithms \\n| \\n91'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 105}, page_content='interconnected, as a lower learning_rate means that more trees are needed to build\\na model of similar complexity. In contrast to random forests, where a higher n_esti\\nmators value is always better, increasing n_estimators in gradient boosting leads to a\\nmore complex model, which may lead to overfitting. A common practice is to fit\\nn_estimators depending on the time and memory budget, and then search over dif‐\\nferent learning_rates.\\nAnother important parameter is max_depth (or alternatively max_leaf_nodes), to\\nreduce the complexity of each tree. Usually max_depth is set very low for gradient\\nboosted models, often not deeper than five splits.\\nKernelized Support Vector Machines\\nThe next type of supervised model we will discuss is kernelized support vector\\nmachines. We explored the use of linear support vector machines for classification in\\n“Linear models for classification” on page 56. Kernelized support vector machines\\n(often just referred to as SVMs) are an extension that allows for more complex mod‐\\nels that are not defined simply by hyperplanes in the input space. While there are sup‐\\nport vector machines for classification and regression, we will restrict ourselves to the\\nclassification case, as implemented in SVC. Similar concepts apply to support vector\\nregression, as implemented in SVR.\\nThe math behind kernelized support vector machines is a bit involved, and is beyond\\nthe scope of this book. You can find the details in Chapter 1 of Hastie, Tibshirani, and\\nFriedman’s The Elements of Statistical Learning. However, we will try to give you some\\nsense of the idea behind the method.\\nLinear models and nonlinear features\\nAs you saw in Figure 2-15, linear models can be quite limiting in low-dimensional\\nspaces, as lines and hyperplanes have limited flexibility. One way to make a linear\\nmodel more flexible is by adding more features—for example, by adding interactions\\nor polynomials of the input features.\\nLet’s look at the synthetic dataset we used in “Feature importance in trees” on page 77\\n(see Figure 2-29):\\nIn[76]:\\nX, y = make_blobs(centers=4, random_state=8)\\ny = y % 2\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n92 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 106}, page_content='10 We picked this particular feature to add for illustration purposes. The choice is not particularly important.\\nFigure 2-36. Two-class classification dataset in which classes are not linearly separable\\nA linear model for classification can only separate points using a line, and will not be\\nable to do a very good job on this dataset (see Figure 2-37):\\nIn[77]:\\nfrom sklearn.svm import LinearSVC\\nlinear_svm = LinearSVC().fit(X, y)\\nmglearn.plots.plot_2d_separator(linear_svm, X)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nNow let’s expand the set of input features, say by also adding feature1 ** 2, the\\nsquare of the second feature, as a new feature. Instead of representing each data point\\nas a two-dimensional point, (feature0, feature1), we now represent it as a three-\\ndimensional point, (feature0, feature1, feature1 ** 2).10 This new representa‐\\ntion is illustrated in Figure 2-38 in a three-dimensional scatter plot:\\nSupervised Machine Learning Algorithms \\n| \\n93'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 107}, page_content='Figure 2-37. Decision boundary found by a linear SVM\\nIn[78]:\\n# add the squared first feature\\nX_new = np.hstack([X, X[:, 1:] ** 2])\\nfrom mpl_toolkits.mplot3d import Axes3D, axes3d\\nfigure = plt.figure()\\n# visualize in 3D\\nax = Axes3D(figure, elev=-152, azim=-26)\\n# plot first all the points with y == 0, then all with y == 1\\nmask = y == 0\\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c=\\'b\\',\\n           cmap=mglearn.cm2, s=60)\\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c=\\'r\\', marker=\\'^\\',\\n           cmap=mglearn.cm2, s=60)\\nax.set_xlabel(\"feature0\")\\nax.set_ylabel(\"feature1\")\\nax.set_zlabel(\"feature1 ** 2\")\\n94 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 108}, page_content='Figure 2-38. Expansion of the dataset shown in Figure 2-37, created by adding a third\\nfeature derived from feature1\\nIn the new representation of the data, it is now indeed possible to separate the two\\nclasses using a linear model, a plane in three dimensions. We can confirm this by fit‐\\nting a linear model to the augmented data (see Figure 2-39):\\nIn[79]:\\nlinear_svm_3d = LinearSVC().fit(X_new, y)\\ncoef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\\n# show linear decision boundary\\nfigure = plt.figure()\\nax = Axes3D(figure, elev=-152, azim=-26)\\nxx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\\nyy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\\nXX, YY = np.meshgrid(xx, yy)\\nZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\\nax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c=\\'b\\',\\n           cmap=mglearn.cm2, s=60)\\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c=\\'r\\', marker=\\'^\\',\\n           cmap=mglearn.cm2, s=60)\\nax.set_xlabel(\"feature0\")\\nax.set_ylabel(\"feature1\")\\nax.set_zlabel(\"feature0 ** 2\")\\nSupervised Machine Learning Algorithms \\n| \\n95'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 109}, page_content='Figure 2-39. Decision boundary found by a linear SVM on the expanded three-\\ndimensional dataset\\nAs a function of the original features, the linear SVM model is not actually linear any‐\\nmore. It is not a line, but more of an ellipse, as you can see from the plot created here\\n(Figure 2-40):\\nIn[80]:\\nZZ = YY ** 2\\ndec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\\nplt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\\n             cmap=mglearn.cm2, alpha=0.5)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n96 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 110}, page_content='Figure 2-40. The decision boundary from Figure 2-39 as a function of the original two\\nfeatures\\nThe kernel trick\\nThe lesson here is that adding nonlinear features to the representation of our data can\\nmake linear models much more powerful. However, often we don’t know which fea‐\\ntures to add, and adding many features (like all possible interactions in a 100-\\ndimensional feature space) might make computation very expensive. Luckily, there is\\na clever mathematical trick that allows us to learn a classifier in a higher-dimensional\\nspace without actually computing the new, possibly very large representation. This is\\nknown as the kernel trick, and it works by directly computing the distance (more pre‐\\ncisely, the scalar products) of the data points for the expanded feature representation,\\nwithout ever actually computing the expansion.\\nThere are two ways to map your data into a higher-dimensional space that are com‐\\nmonly used with support vector machines: the polynomial kernel, which computes all\\npossible polynomials up to a certain degree of the original features (like feature1 **\\n2 * feature2 ** 5); and the radial basis function (RBF) kernel, also known as the\\nGaussian kernel. The Gaussian kernel is a bit harder to explain, as it corresponds to\\nan infinite-dimensional feature space. One way to explain the Gaussian kernel is that\\nSupervised Machine Learning Algorithms \\n| \\n97'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 111}, page_content='11 This follows from the Taylor expansion of the exponential map.\\nit considers all possible polynomials of all degrees, but the importance of the features\\ndecreases for higher degrees.11\\nIn practice, the mathematical details behind the kernel SVM are not that important,\\nthough, and how an SVM with an RBF kernel makes a decision can be summarized\\nquite easily—we’ll do so in the next section.\\nUnderstanding SVMs\\nDuring training, the SVM learns how important each of the training data points is to\\nrepresent the decision boundary between the two classes. Typically only a subset of\\nthe training points matter for defining the decision boundary: the ones that lie on the\\nborder between the classes. These are called support vectors and give the support vec‐\\ntor machine its name.\\nTo make a prediction for a new point, the distance to each of the support vectors is\\nmeasured. A classification decision is made based on the distances to the support vec‐\\ntor, and the importance of the support vectors that was learned during training\\n(stored in the dual_coef_ attribute of SVC).\\nThe distance between data points is measured by the Gaussian kernel:\\nkrbf(x1, x2) = exp (ɣǁx1 - x2ǁ2)\\nHere, x1 and x2 are data points, ǁ x1 - x2 ǁ denotes Euclidean distance, and ɣ (gamma)\\nis a parameter that controls the width of the Gaussian kernel.\\nFigure 2-41 shows the result of training a support vector machine on a two-\\ndimensional two-class dataset. The decision boundary is shown in black, and the sup‐\\nport vectors are larger points with the wide outline. The following code creates this\\nplot by training an SVM on the forge dataset:\\nIn[81]:\\nfrom sklearn.svm import SVC\\nX, y = mglearn.tools.make_handcrafted_dataset()\\nsvm = SVC(kernel=\\'rbf\\', C=10, gamma=0.1).fit(X, y)\\nmglearn.plots.plot_2d_separator(svm, X, eps=.5)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\\n# plot support vectors\\nsv = svm.support_vectors_\\n# class labels of support vectors are given by the sign of the dual coefficients\\nsv_labels = svm.dual_coef_.ravel() > 0\\nmglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n98 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 112}, page_content='Figure 2-41. Decision boundary and support vectors found by an SVM with RBF kernel\\nIn this case, the SVM yields a very smooth and nonlinear (not a straight line) bound‐\\nary. We adjusted two parameters here: the C parameter and the gamma parameter,\\nwhich we will now discuss in detail.\\nTuning SVM parameters\\nThe gamma parameter is the one shown in the formula given in the previous section,\\nwhich controls the width of the Gaussian kernel. It determines the scale of what it\\nmeans for points to be close together. The C parameter is a regularization parameter,\\nsimilar to that used in the linear models. It limits the importance of each point (or\\nmore precisely, their dual_coef_).\\nLet’s have a look at what happens when we vary these parameters (Figure 2-42):\\nIn[82]:\\nfig, axes = plt.subplots(3, 3, figsize=(15, 10))\\nfor ax, C in zip(axes, [-1, 0, 3]):\\n    for a, gamma in zip(ax, range(-1, 2)):\\n        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\\naxes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\\n                  ncol=4, loc=(.9, 1.2))\\nSupervised Machine Learning Algorithms \\n| \\n99'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 113}, page_content='Figure 2-42. Decision boundaries and support vectors for different settings of the param‐\\neters C and gamma\\nGoing from left to right, we increase the value of the parameter gamma from 0.1 to 10.\\nA small gamma means a large radius for the Gaussian kernel, which means that many\\npoints are considered close by. This is reflected in very smooth decision boundaries\\non the left, and boundaries that focus more on single points further to the right. A\\nlow value of gamma means that the decision boundary will vary slowly, which yields a\\nmodel of low complexity, while a high value of gamma yields a more complex model.\\nGoing from top to bottom, we increase the C parameter from 0.1 to 1000. As with the\\nlinear models, a small C means a very restricted model, where each data point can\\nonly have very limited influence. You can see that at the top left the decision bound‐\\nary looks nearly linear, with the misclassified points barely having any influence on\\nthe line. Increasing C, as shown on the bottom right, allows these points to have a\\nstronger influence on the model and makes the decision boundary bend to correctly\\nclassify them.\\n100 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 114}, page_content='Let’s apply the RBF kernel SVM to the Breast Cancer dataset. By default, C=1 and\\ngamma=1/n_features:\\nIn[83]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\nsvc = SVC()\\nsvc.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))\\nOut[83]:\\nAccuracy on training set: 1.00\\nAccuracy on test set: 0.63\\nThe model overfits quite substantially, with a perfect score on the training set and\\nonly 63% accuracy on the test set. While SVMs often perform quite well, they are\\nvery sensitive to the settings of the parameters and to the scaling of the data. In par‐\\nticular, they require all the features to vary on a similar scale. Let’s look at the mini‐\\nmum and maximum values for each feature, plotted in log-space (Figure 2-43):\\nIn[84]:\\nplt.plot(X_train.min(axis=0), \\'o\\', label=\"min\")\\nplt.plot(X_train.max(axis=0), \\'^\\', label=\"max\")\\nplt.legend(loc=4)\\nplt.xlabel(\"Feature index\")\\nplt.ylabel(\"Feature magnitude\")\\nplt.yscale(\"log\")\\nFrom this plot we can determine that features in the Breast Cancer dataset are of\\ncompletely different orders of magnitude. This can be somewhat of a problem for\\nother models (like linear models), but it has devastating effects for the kernel SVM.\\nLet’s examine some ways to deal with this issue.\\nSupervised Machine Learning Algorithms \\n| \\n101'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 115}, page_content='Figure 2-43. Feature ranges for the Breast Cancer dataset (note that the y axis has a log‐\\narithmic scale)\\nPreprocessing data for SVMs\\nOne way to resolve this problem is by rescaling each feature so that they are all\\napproximately on the same scale. A common rescaling method for kernel SVMs is to\\nscale the data such that all features are between 0 and 1. We will see how to do this\\nusing the MinMaxScaler preprocessing method in Chapter 3, where we’ll give more\\ndetails. For now, let’s do this “by hand”:\\nIn[85]:\\n# compute the minimum value per feature on the training set\\nmin_on_training = X_train.min(axis=0)\\n# compute the range of each feature (max - min) on the training set\\nrange_on_training = (X_train - min_on_training).max(axis=0)\\n# subtract the min, and divide by range\\n# afterward, min=0 and max=1 for each feature\\nX_train_scaled = (X_train - min_on_training) / range_on_training\\nprint(\"Minimum for each feature\\\\n{}\".format(X_train_scaled.min(axis=0)))\\nprint(\"Maximum for each feature\\\\n {}\".format(X_train_scaled.max(axis=0)))\\n102 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 116}, page_content='Out[85]:\\nMinimum for each feature\\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\\nMaximum for each feature\\n [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\\n   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\\nIn[86]:\\n# use THE SAME transformation on the test set,\\n# using min and range of the training set (see Chapter 3 for details)\\nX_test_scaled = (X_test - min_on_training) / range_on_training\\nIn[87]:\\nsvc = SVC()\\nsvc.fit(X_train_scaled, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(\\n    svc.score(X_train_scaled, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\\nOut[87]:\\nAccuracy on training set: 0.948\\nAccuracy on test set: 0.951\\nScaling the data made a huge difference! Now we are actually in an underfitting\\nregime, where training and test set performance are quite similar but less close to\\n100% accuracy. From here, we can try increasing either C or gamma to fit a more com‐\\nplex model. For example:\\nIn[88]:\\nsvc = SVC(C=1000)\\nsvc.fit(X_train_scaled, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(\\n    svc.score(X_train_scaled, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\\nOut[88]:\\nAccuracy on training set: 0.988\\nAccuracy on test set: 0.972\\nHere, increasing C allows us to improve the model significantly, resulting in 97.2%\\naccuracy.\\nSupervised Machine Learning Algorithms \\n| \\n103'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 117}, page_content='Strengths, weaknesses, and parameters\\nKernelized support vector machines are powerful models and perform well on a vari‐\\nety of datasets. SVMs allow for complex decision boundaries, even if the data has only\\na few features. They work well on low-dimensional and high-dimensional data (i.e.,\\nfew and many features), but don’t scale very well with the number of samples. Run‐\\nning an SVM on data with up to 10,000 samples might work well, but working with\\ndatasets of size 100,000 or more can become challenging in terms of runtime and\\nmemory usage.\\nAnother downside of SVMs is that they require careful preprocessing of the data and\\ntuning of the parameters. This is why, these days, most people instead use tree-based\\nmodels such as random forests or gradient boosting (which require little or no pre‐\\nprocessing) in many applications. Furthermore, SVM models are hard to inspect; it\\ncan be difficult to understand why a particular prediction was made, and it might be\\ntricky to explain the model to a nonexpert.\\nStill, it might be worth trying SVMs, particularly if all of your features represent\\nmeasurements in similar units (e.g., all are pixel intensities) and they are on similar\\nscales.\\nThe important parameters in kernel SVMs are the regularization parameter C, the\\nchoice of the kernel, and the kernel-specific parameters. Although we primarily\\nfocused on the RBF kernel, other choices are available in scikit-learn. The RBF\\nkernel has only one parameter, gamma, which is the inverse of the width of the Gaus‐\\nsian kernel. gamma and C both control the complexity of the model, with large values\\nin either resulting in a more complex model. Therefore, good settings for the two\\nparameters are usually strongly correlated, and C and gamma should be adjusted\\ntogether.\\nNeural Networks (Deep Learning)\\nA family of algorithms known as neural networks has recently seen a revival under\\nthe name “deep learning.” While deep learning shows great promise in many machine\\nlearning applications, deep learning algorithms are often tailored very carefully to a\\nspecific use case. Here, we will only discuss some relatively simple methods, namely\\nmultilayer perceptrons for classification and regression, that can serve as a starting\\npoint for more involved deep learning methods. Multilayer perceptrons (MLPs) are\\nalso known as (vanilla) feed-forward neural networks, or sometimes just neural\\nnetworks.\\nThe neural network model\\nMLPs can be viewed as generalizations of linear models that perform multiple stages\\nof processing to come to a decision.\\n104 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 118}, page_content='Remember that the prediction by a linear regressor is given as:\\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\\nIn plain English, ŷ is a weighted sum of the input features x[0] to x[p], weighted by\\nthe learned coefficients w[0] to w[p]. We could visualize this graphically as shown in\\nFigure 2-44:\\nIn[89]:\\ndisplay(mglearn.plots.plot_logistic_regression_graph())\\nFigure 2-44. Visualization of logistic regression, where input features and predictions are\\nshown as nodes, and the coefficients are connections between the nodes\\nHere, each node on the left represents an input feature, the connecting lines represent\\nthe learned coefficients, and the node on the right represents the output, which is a\\nweighted sum of the inputs.\\nIn an MLP this process of computing weighted sums is repeated multiple times, first\\ncomputing hidden units that represent an intermediate processing step, which are\\nagain combined using weighted sums to yield the final result (Figure 2-45):\\nIn[90]:\\ndisplay(mglearn.plots.plot_single_hidden_layer_graph())\\nSupervised Machine Learning Algorithms \\n| \\n105'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 119}, page_content='Figure 2-45. Illustration of a multilayer perceptron with a single hidden layer\\nThis model has a lot more coefficients (also called weights) to learn: there is one\\nbetween every input and every hidden unit (which make up the hidden layer), and\\none between every unit in the hidden layer and the output.\\nComputing a series of weighted sums is mathematically the same as computing just\\none weighted sum, so to make this model truly more powerful than a linear model,\\nwe need one extra trick. After computing a weighted sum for each hidden unit, a\\nnonlinear function is applied to the result—usually the rectifying nonlinearity (also\\nknown as rectified linear unit or relu) or the tangens hyperbolicus (tanh). The result of\\nthis function is then used in the weighted sum that computes the output, ŷ. The two\\nfunctions are visualized in Figure 2-46. The relu cuts off values below zero, while tanh\\nsaturates to –1 for low input values and +1 for high input values. Either nonlinear\\nfunction allows the neural network to learn much more complicated functions than a\\nlinear model could:\\nIn[91]:\\nline = np.linspace(-3, 3, 100)\\nplt.plot(line, np.tanh(line), label=\"tanh\")\\nplt.plot(line, np.maximum(line, 0), label=\"relu\")\\nplt.legend(loc=\"best\")\\nplt.xlabel(\"x\")\\nplt.ylabel(\"relu(x), tanh(x)\")\\n106 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 120}, page_content='Figure 2-46. The hyperbolic tangent activation function and the rectified linear activa‐\\ntion function\\nFor the small neural network pictured in Figure 2-45, the full formula for computing\\nŷ in the case of regression would be (when using a tanh nonlinearity):\\nh[0] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\\nh[1] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\\nh[2] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\\nŷ = v[0] * h[0] + v[1] * h[1] + v[2] * h[2]\\nHere, w are the weights between the input x and the hidden layer h, and v are the\\nweights between the hidden layer h and the output ŷ. The weights v and w are learned\\nfrom data, x are the input features, ŷ is the computed output, and h are intermediate\\ncomputations. An important parameter that needs to be set by the user is the number\\nof nodes in the hidden layer. This can be as small as 10 for very small or simple data‐\\nsets and as big as 10,000 for very complex data. It is also possible to add additional\\nhidden layers, as shown in Figure 2-47:\\nSupervised Machine Learning Algorithms \\n| \\n107'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 121}, page_content='In[92]:\\nmglearn.plots.plot_two_hidden_layer_graph()\\nFigure 2-47. A multilayer perceptron with two hidden layers\\nHaving large neural networks made up of many of these layers of computation is\\nwhat inspired the term “deep learning.”\\nTuning neural networks\\nLet’s look into the workings of the MLP by applying the MLPClassifier to the\\ntwo_moons dataset we used earlier in this chapter. The results are shown in\\nFigure 2-48:\\nIn[93]:\\nfrom sklearn.neural_network import MLPClassifier\\nfrom sklearn.datasets import make_moons\\nX, y = make_moons(n_samples=100, noise=0.25, random_state=3)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\\n                                                    random_state=42)\\nmlp = MLPClassifier(algorithm=\\'l-bfgs\\', random_state=0).fit(X_train, y_train)\\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n108 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 122}, page_content='Figure 2-48. Decision boundary learned by a neural network with 100 hidden units on\\nthe two_moons dataset\\nAs you can see, the neural network learned a very nonlinear but relatively smooth\\ndecision boundary. We used algorithm=\\'l-bfgs\\', which we will discuss later.\\nBy default, the MLP uses 100 hidden nodes, which is quite a lot for this small dataset.\\nWe can reduce the number (which reduces the complexity of the model) and still get\\na good result (Figure 2-49):\\nIn[94]:\\nmlp = MLPClassifier(algorithm=\\'l-bfgs\\', random_state=0, hidden_layer_sizes=[10])\\nmlp.fit(X_train, y_train)\\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nSupervised Machine Learning Algorithms \\n| \\n109'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 123}, page_content='Figure 2-49. Decision boundary learned by a neural network with 10 hidden units on\\nthe two_moons dataset\\nWith only 10 hidden units, the decision boundary looks somewhat more ragged. The\\ndefault nonlinearity is relu, shown in Figure 2-46. With a single hidden layer, this\\nmeans the decision function will be made up of 10 straight line segments. If we want\\na smoother decision boundary, we could add more hidden units (as in Figure 2-49),\\nadd a second hidden layer (Figure 2-50), or use the tanh nonlinearity (Figure 2-51):\\nIn[95]:\\n# using two hidden layers, with 10 units each\\nmlp = MLPClassifier(algorithm=\\'l-bfgs\\', random_state=0,\\n                    hidden_layer_sizes=[10, 10])\\nmlp.fit(X_train, y_train)\\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\n110 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 124}, page_content='In[96]:\\n# using two hidden layers, with 10 units each, now with tanh nonlinearity\\nmlp = MLPClassifier(algorithm=\\'l-bfgs\\', activation=\\'tanh\\',\\n                    random_state=0, hidden_layer_sizes=[10, 10])\\nmlp.fit(X_train, y_train)\\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nFigure 2-50. Decision boundary learned using 2 hidden layers with 10 hidden units\\neach, with rect activation function\\nSupervised Machine Learning Algorithms \\n| \\n111'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 125}, page_content='Figure 2-51. Decision boundary learned using 2 hidden layers with 10 hidden units\\neach, with tanh activation function\\nFinally, we can also control the complexity of a neural network by using an l2 penalty\\nto shrink the weights toward zero, as we did in ridge regression and the linear classifi‐\\ners. The parameter for this in the MLPClassifier is alpha (as in the linear regression\\nmodels), and it’s set to a very low value (little regularization) by default. Figure 2-52\\nshows the effect of different values of alpha on the two_moons dataset, using two hid‐\\nden layers of 10 or 100 units each:\\nIn[97]:\\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\\nfor axx, n_hidden_nodes in zip(axes, [10, 100]):\\n    for ax, alpha in zip(axx, [0.0001, 0.01, 0.1, 1]):\\n        mlp = MLPClassifier(algorithm=\\'l-bfgs\\', random_state=0,\\n                            hidden_layer_sizes=[n_hidden_nodes, n_hidden_nodes],\\n                            alpha=alpha)\\n        mlp.fit(X_train, y_train)\\n        mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\\n        mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\\n        ax.set_title(\"n_hidden=[{}, {}]\\\\nalpha={:.4f}\".format(\\n                      n_hidden_nodes, n_hidden_nodes, alpha))\\n112 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 126}, page_content=\"Figure 2-52. Decision functions for different numbers of hidden units and different set‐\\ntings of the alpha parameter\\nAs you probably have realized by now, there are many ways to control the complexity\\nof a neural network: the number of hidden layers, the number of units in each hidden\\nlayer, and the regularization (alpha). There are actually even more, which we won’t\\ngo into here.\\nAn important property of neural networks is that their weights are set randomly\\nbefore learning is started, and this random initialization affects the model that is\\nlearned. That means that even when using exactly the same parameters, we can\\nobtain very different models when using different random seeds. If the networks are\\nlarge, and their complexity is chosen properly, this should not affect accuracy too\\nmuch, but it is worth keeping in mind (particularly for smaller networks).\\nFigure 2-53 shows plots of several models, all learned with the same settings of the\\nparameters:\\nIn[98]:\\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\\nfor i, ax in enumerate(axes.ravel()):\\n    mlp = MLPClassifier(algorithm='l-bfgs', random_state=i,\\n                        hidden_layer_sizes=[100, 100])\\n    mlp.fit(X_train, y_train)\\n    mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\\nSupervised Machine Learning Algorithms \\n| \\n113\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 127}, page_content='Figure 2-53. Decision functions learned with the same parameters but different random\\ninitializations\\nTo get a better understanding of neural networks on real-world data, let’s apply the\\nMLPClassifier to the Breast Cancer dataset. We start with the default parameters:\\nIn[99]:\\nprint(\"Cancer data per-feature maxima:\\\\n{}\".format(cancer.data.max(axis=0)))\\nOut[99]:\\nCancer data per-feature maxima:\\n[   28.110    39.280   188.500  2501.000     0.163     0.345     0.427\\n     0.201     0.304     0.097     2.873     4.885    21.980   542.200\\n     0.031     0.135     0.396     0.053     0.079     0.030    36.040\\n    49.540   251.200  4254.000     0.223     1.058     1.252     0.291\\n     0.664     0.207]\\nIn[100]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\nmlp = MLPClassifier(random_state=42)\\nmlp.fit(X_train, y_train)\\nprint(\"Accuracy on training set: {:.2f}\".format(mlp.score(X_train, y_train)))\\nprint(\"Accuracy on test set: {:.2f}\".format(mlp.score(X_test, y_test)))\\nOut[100]:\\nAccuracy on training set: 0.92\\nAccuracy on test set: 0.90\\nThe accuracy of the MLP is quite good, but not as good as the other models. As in the\\nearlier SVC example, this is likely due to scaling of the data. Neural networks also\\nexpect all input features to vary in a similar way, and ideally to have a mean of 0, and\\n114 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 128}, page_content='a variance of 1. We must rescale our data so that it fulfills these requirements. Again,\\nwe will do this by hand here, but we’ll introduce the StandardScaler to do this auto‐\\nmatically in Chapter 3:\\nIn[101]:\\n# compute the mean value per feature on the training set\\nmean_on_train = X_train.mean(axis=0)\\n# compute the standard deviation of each feature on the training set\\nstd_on_train = X_train.std(axis=0)\\n# subtract the mean, and scale by inverse standard deviation\\n# afterward, mean=0 and std=1\\nX_train_scaled = (X_train - mean_on_train) / std_on_train\\n# use THE SAME transformation (using training mean and std) on the test set\\nX_test_scaled = (X_test - mean_on_train) / std_on_train\\nmlp = MLPClassifier(random_state=0)\\nmlp.fit(X_train_scaled, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(\\n    mlp.score(X_train_scaled, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\\nOut[101]:\\nAccuracy on training set: 0.991\\nAccuracy on test set: 0.965\\nConvergenceWarning:\\n    Stochastic Optimizer: Maximum iterations reached and the optimization\\n    hasn\\'t converged yet.\\nThe results are much better after scaling, and already quite competitive. We got a\\nwarning from the model, though, that tells us that the maximum number of iterations\\nhas been reached. This is part of the adam algorithm for learning the model, and tells\\nus that we should increase the number of iterations:\\nIn[102]:\\nmlp = MLPClassifier(max_iter=1000, random_state=0)\\nmlp.fit(X_train_scaled, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(\\n    mlp.score(X_train_scaled, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\\nOut[102]:\\nAccuracy on training set: 0.995\\nAccuracy on test set: 0.965\\nSupervised Machine Learning Algorithms \\n| \\n115'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 129}, page_content='12 You might have noticed at this point that many of the well-performing models achieved exactly the same\\naccuracy of 0.972. This means that all of the models make exactly the same number of mistakes, which is four.\\nIf you compare the actual predictions, you can even see that they make exactly the same mistakes! This might\\nbe a consequence of the dataset being very small, or it may be because these points are really different from\\nthe rest.\\nIncreasing the number of iterations only increased the training set performance, not\\nthe generalization performance. Still, the model is performing quite well. As there is\\nsome gap between the training and the test performance, we might try to decrease the\\nmodel’s complexity to get better generalization performance. Here, we choose to\\nincrease the alpha parameter (quite aggressively, from 0.0001 to 1) to add stronger\\nregularization of the weights:\\nIn[103]:\\nmlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0)\\nmlp.fit(X_train_scaled, y_train)\\nprint(\"Accuracy on training set: {:.3f}\".format(\\n    mlp.score(X_train_scaled, y_train)))\\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\\nOut[103]:\\nAccuracy on training set: 0.988\\nAccuracy on test set: 0.972\\nThis leads to a performance on par with the best models so far.12\\nWhile it is possible to analyze what a neural network has learned, this is usually much\\ntrickier than analyzing a linear model or a tree-based model. One way to introspect\\nwhat was learned is to look at the weights in the model. You can see an example of\\nthis in the scikit-learn example gallery. For the Breast Cancer dataset, this might\\nbe a bit hard to understand. The following plot (Figure 2-54) shows the weights that\\nwere learned connecting the input to the first hidden layer. The rows in this plot cor‐\\nrespond to the 30 input features, while the columns correspond to the 100 hidden\\nunits. Light colors represent large positive values, while dark colors represent nega‐\\ntive values:\\nIn[104]:\\nplt.figure(figsize=(20, 5))\\nplt.imshow(mlp.coefs_[0], interpolation=\\'none\\', cmap=\\'viridis\\')\\nplt.yticks(range(30), cancer.feature_names)\\nplt.xlabel(\"Columns in weight matrix\")\\nplt.ylabel(\"Input feature\")\\nplt.colorbar()\\n116 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 130}, page_content='Figure 2-54. Heat map of the first layer weights in a neural network learned on the\\nBreast Cancer dataset\\nOne possible inference we can make is that features that have very small weights for\\nall of the hidden units are “less important” to the model. We can see that “mean\\nsmoothness” and “mean compactness,” in addition to the features found between\\n“smoothness error” and “fractal dimension error,” have relatively low weights com‐\\npared to other features. This could mean that these are less important features or pos‐\\nsibly that we didn’t represent them in a way that the neural network could use.\\nWe could also visualize the weights connecting the hidden layer to the output layer,\\nbut those are even harder to interpret.\\nWhile the MLPClassifier and MLPRegressor provide easy-to-use interfaces for the\\nmost common neural network architectures, they only capture a small subset of what\\nis possible with neural networks. If you are interested in working with more flexible\\nor larger models, we encourage you to look beyond scikit-learn into the fantastic\\ndeep learning libraries that are out there. For Python users, the most well-established\\nare keras, lasagna, and tensor-flow. lasagna builds on the theano library, while\\nkeras can use either tensor-flow or theano. These libraries provide a much more\\nflexible interface to build neural networks and track the rapid progress in deep learn‐\\ning research. All of the popular deep learning libraries also allow the use of high-\\nperformance graphics processing units (GPUs), which scikit-learn does not\\nsupport. Using GPUs allows us to accelerate computations by factors of 10x to 100x,\\nand they are essential for applying deep learning methods to large-scale datasets.\\nStrengths, weaknesses, and parameters\\nNeural networks have reemerged as state-of-the-art models in many applications of\\nmachine learning. One of their main advantages is that they are able to capture infor‐\\nmation contained in large amounts of data and build incredibly complex models.\\nGiven enough computation time, data, and careful tuning of the parameters, neural\\nnetworks often beat other machine learning algorithms (for classification and regres‐\\nsion tasks).\\nSupervised Machine Learning Algorithms \\n| \\n117'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 131}, page_content=\"This brings us to the downsides. Neural networks—particularly the large and power‐\\nful ones—often take a long time to train. They also require careful preprocessing of\\nthe data, as we saw here. Similarly to SVMs, they work best with “homogeneous”\\ndata, where all the features have similar meanings. For data that has very different\\nkinds of features, tree-based models might work better. Tuning neural network\\nparameters is also an art unto itself. In our experiments, we barely scratched the sur‐\\nface of possible ways to adjust neural network models and how to train them.\\nEstimating complexity in neural networks.    The most important parameters are the num‐\\nber of layers and the number of hidden units per layer. You should start with one or\\ntwo hidden layers, and possibly expand from there. The number of nodes per hidden\\nlayer is often similar to the number of input features, but rarely higher than in the low\\nto mid-thousands.\\nA helpful measure when thinking about the model complexity of a neural network is\\nthe number of weights or coefficients that are learned. If you have a binary classifica‐\\ntion dataset with 100 features, and you have 100 hidden units, then there are 100 *\\n100 = 10,000 weights between the input and the first hidden layer. There are also\\n100 * 1 = 100 weights between the hidden layer and the output layer, for a total of\\naround 10,100 weights. If you add a second hidden layer with 100 hidden units, there\\nwill be another 100 * 100 = 10,000 weights from the first hidden layer to the second\\nhidden layer, resulting in a total of 20,100 weights. If instead you use one layer with\\n1,000 hidden units, you are learning 100 * 1,000 = 100,000 weights from the input to\\nthe hidden layer and 1,000 x 1 weights from the hidden layer to the output layer, for a\\ntotal of 101,000. If you add a second hidden layer you add 1,000 * 1,000 = 1,000,000\\nweights, for a whopping total of 1,101,000—50 times larger than the model with two\\nhidden layers of size 100.\\nA common way to adjust parameters in a neural network is to first create a network\\nthat is large enough to overfit, making sure that the task can actually be learned by\\nthe network. Then, once you know the training data can be learned, either shrink the\\nnetwork or increase alpha to add regularization, which will improve generalization\\nperformance.\\nIn our experiments, we focused mostly on the definition of the model: the number of\\nlayers and nodes per layer, the regularization, and the nonlinearity. These define the\\nmodel we want to learn. There is also the question of how to learn the model, or the\\nalgorithm that is used for learning the parameters, which is set using the algorithm\\nparameter. There are two easy-to-use choices for algorithm. The default is 'adam',\\nwhich works well in most situations but is quite sensitive to the scaling of the data (so\\nit is important to always scale your data to 0 mean and unit variance). The other one\\nis 'l-bfgs', which is quite robust but might take a long time on larger models or\\nlarger datasets. There is also the more advanced 'sgd' option, which is what many\\ndeep learning researchers use. The 'sgd' option comes with many additional param‐\\n118 \\n| \\nChapter 2: Supervised Learning\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 132}, page_content='eters that need to be tuned for best results. You can find all of these parameters and\\ntheir definitions in the user guide. When starting to work with MLPs, we recommend\\nsticking to \\'adam\\' and \\'l-bfgs\\'.\\nfit Resets a Model\\nAn important property of scikit-learn models is that calling fit\\nwill always reset everything a model previously learned. So if you\\nbuild a model on one dataset, and then call fit again on a different\\ndataset, the model will “forget” everything it learned from the first\\ndataset. You can call fit as often as you like on a model, and the\\noutcome will be the same as calling fit on a “new” model.\\nUncertainty Estimates from Classifiers\\nAnother useful part of the scikit-learn interface that we haven’t talked about yet is\\nthe ability of classifiers to provide uncertainty estimates of predictions. Often, you are\\nnot only interested in which class a classifier predicts for a certain test point, but also\\nhow certain it is that this is the right class. In practice, different kinds of mistakes lead\\nto very different outcomes in real-world applications. Imagine a medical application\\ntesting for cancer. Making a false positive prediction might lead to a patient undergo‐\\ning additional tests, while a false negative prediction might lead to a serious disease\\nnot being treated. We will go into this topic in more detail in Chapter 6.\\nThere are two different functions in scikit-learn that can be used to obtain uncer‐\\ntainty estimates from classifiers: decision_function and predict_proba. Most (but\\nnot all) classifiers have at least one of them, and many classifiers have both. Let’s look\\nat what these two functions do on a synthetic two-dimensional dataset, when build‐\\ning a GradientBoostingClassifier classifier, which has both a decision_function\\nand a predict_proba method:\\nIn[105]:\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.datasets import make_blobs, make_circles\\nX, y = make_circles(noise=0.25, factor=0.5, random_state=1)\\n# we rename the classes \"blue\" and \"red\" for illustration purposes\\ny_named = np.array([\"blue\", \"red\"])[y]\\n# we can call train_test_split with arbitrarily many arrays;\\n# all will be split in a consistent manner\\nX_train, X_test, y_train_named, y_test_named, y_train, y_test = \\\\\\n    train_test_split(X, y_named, y, random_state=0)\\n# build the gradient boosting model\\ngbrt = GradientBoostingClassifier(random_state=0)\\ngbrt.fit(X_train, y_train_named)\\nUncertainty Estimates from Classifiers \\n| \\n119'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 133}, page_content='The Decision Function\\nIn the binary classification case, the return value of decision_function is of shape\\n(n_samples,), and it returns one floating-point number for each sample:\\nIn[106]:\\nprint(\"X_test.shape: {}\".format(X_test.shape))\\nprint(\"Decision function shape: {}\".format(\\n    gbrt.decision_function(X_test).shape))\\nOut[106]:\\nX_test.shape: (25, 2)\\nDecision function shape: (25,)\\nThis value encodes how strongly the model believes a data point to belong to the\\n“positive” class, in this case class 1. Positive values indicate a preference for the posi‐\\ntive class, and negative values indicate a preference for the “negative” (other) class:\\nIn[107]:\\n# show the first few entries of decision_function\\nprint(\"Decision function:\\\\n{}\".format(gbrt.decision_function(X_test)[:6]))\\nOut[107]:\\nDecision function:\\n[ 4.136 -1.683 -3.951 -3.626  4.29   3.662]\\nWe can recover the prediction by looking only at the sign of the decision function:\\nIn[108]:\\nprint(\"Thresholded decision function:\\\\n{}\".format(\\n    gbrt.decision_function(X_test) > 0))\\nprint(\"Predictions:\\\\n{}\".format(gbrt.predict(X_test)))\\nOut[108]:\\nThresholded decision function:\\n[ True False False False  True  True False  True  True  True False  True\\n  True False  True False False False  True  True  True  True  True False\\n  False]\\nPredictions:\\n[\\'red\\' \\'blue\\' \\'blue\\' \\'blue\\' \\'red\\' \\'red\\' \\'blue\\' \\'red\\' \\'red\\' \\'red\\' \\'blue\\'\\n \\'red\\' \\'red\\' \\'blue\\' \\'red\\' \\'blue\\' \\'blue\\' \\'blue\\' \\'red\\' \\'red\\' \\'red\\' \\'red\\'\\n \\'red\\' \\'blue\\' \\'blue\\']\\nFor binary classification, the “negative” class is always the first entry of the classes_\\nattribute, and the “positive” class is the second entry of classes_. So if you want to\\nfully recover the output of predict, you need to make use of the classes_ attribute:\\n120 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 134}, page_content='In[109]:\\n# make the boolean True/False into 0 and 1\\ngreater_zero = (gbrt.decision_function(X_test) > 0).astype(int)\\n# use 0 and 1 as indices into classes_\\npred = gbrt.classes_[greater_zero]\\n# pred is the same as the output of gbrt.predict\\nprint(\"pred is equal to predictions: {}\".format(\\n    np.all(pred == gbrt.predict(X_test))))\\nOut[109]:\\npred is equal to predictions: True\\nThe range of decision_function can be arbitrary, and depends on the data and the\\nmodel parameters:\\nIn[110]:\\ndecision_function = gbrt.decision_function(X_test)\\nprint(\"Decision function minimum: {:.2f} maximum: {:.2f}\".format(\\n    np.min(decision_function), np.max(decision_function)))\\nOut[110]:\\nDecision function minimum: -7.69 maximum: 4.29\\nThis arbitrary scaling makes the output of decision_function often hard to\\ninterpret.\\nIn the following example we plot the decision_function for all points in the 2D\\nplane using a color coding, next to a visualization of the decision boundary, as we saw\\nearlier. We show training points as circles and test data as triangles (Figure 2-55):\\nIn[111]:\\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\\nmglearn.tools.plot_2d_separator(gbrt, X, ax=axes[0], alpha=.4,\\n                                fill=True, cm=mglearn.cm2)\\nscores_image = mglearn.tools.plot_2d_scores(gbrt, X, ax=axes[1],\\n                                            alpha=.4, cm=mglearn.ReBl)\\nfor ax in axes:\\n    # plot training and test points\\n    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\\n                             markers=\\'^\\', ax=ax)\\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\\n                             markers=\\'o\\', ax=ax)\\n    ax.set_xlabel(\"Feature 0\")\\n    ax.set_ylabel(\"Feature 1\")\\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1))\\nUncertainty Estimates from Classifiers \\n| \\n121'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 135}, page_content='Figure 2-55. Decision boundary (left) and decision function (right) for a gradient boost‐\\ning model on a two-dimensional toy dataset\\nEncoding not only the predicted outcome but also how certain the classifier is pro‐\\nvides additional information. However, in this visualization, it is hard to make out the\\nboundary between the two classes.\\nPredicting Probabilities\\nThe output of predict_proba is a probability for each class, and is often more easily\\nunderstood than the output of decision_function. It is always of shape (n_samples,\\n2) for binary classification:\\nIn[112]:\\nprint(\"Shape of probabilities: {}\".format(gbrt.predict_proba(X_test).shape))\\nOut[112]:\\nShape of probabilities: (25, 2)\\nThe first entry in each row is the estimated probability of the first class, and the sec‐\\nond entry is the estimated probability of the second class. Because it is a probability,\\nthe output of predict_proba is always between 0 and 1, and the sum of the entries\\nfor both classes is always 1:\\nIn[113]:\\n# show the first few entries of predict_proba\\nprint(\"Predicted probabilities:\\\\n{}\".format(\\n    gbrt.predict_proba(X_test[:6])))\\n122 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 136}, page_content='13 Because the probabilities are floating-point numbers, it is unlikely that they will both be exactly 0.500. How‐\\never, if that happens, the prediction is made at random.\\nOut[113]:\\nPredicted probabilities:\\n[[ 0.016  0.984]\\n [ 0.843  0.157]\\n [ 0.981  0.019]\\n [ 0.974  0.026]\\n [ 0.014  0.986]\\n [ 0.025  0.975]]\\nBecause the probabilities for the two classes sum to 1, exactly one of the classes will\\nbe above 50% certainty. That class is the one that is predicted.13\\nYou can see in the previous output that the classifier is relatively certain for most\\npoints. How well the uncertainty actually reflects uncertainty in the data depends on\\nthe model and the parameters. A model that is more overfitted tends to make more\\ncertain predictions, even if they might be wrong. A model with less complexity usu‐\\nally has more uncertainty in its predictions. A model is called calibrated if the\\nreported uncertainty actually matches how correct it is—in a calibrated model, a pre‐\\ndiction made with 70% certainty would be correct 70% of the time.\\nIn the following example (Figure 2-56) we again show the decision boundary on the\\ndataset, next to the class probabilities for the class 1:\\nIn[114]:\\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\\nmglearn.tools.plot_2d_separator(\\n    gbrt, X, ax=axes[0], alpha=.4, fill=True, cm=mglearn.cm2)\\nscores_image = mglearn.tools.plot_2d_scores(\\n    gbrt, X, ax=axes[1], alpha=.5, cm=mglearn.ReBl, function=\\'predict_proba\\')\\nfor ax in axes:\\n    # plot training and test points\\n    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\\n                             markers=\\'^\\', ax=ax)\\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\\n                             markers=\\'o\\', ax=ax)\\n    ax.set_xlabel(\"Feature 0\")\\n    ax.set_ylabel(\"Feature 1\")\\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1))\\nUncertainty Estimates from Classifiers \\n| \\n123'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 137}, page_content='Figure 2-56. Decision boundary (left) and predicted probabilities for the gradient boost‐\\ning model shown in Figure 2-55\\nThe boundaries in this plot are much more well-defined, and the small areas of\\nuncertainty are clearly visible.\\nThe scikit-learn website has a great comparison of many models and what their\\nuncertainty estimates look like. We’ve reproduced this in Figure 2-57, and we encour‐\\nage you to go though the example there.\\nFigure 2-57. Comparison of several classifiers in scikit-learn on synthetic datasets (image\\ncourtesy http://scikit-learn.org)\\nUncertainty in Multiclass Classification\\nSo far, we’ve only talked about uncertainty estimates in binary classification. But the\\ndecision_function and predict_proba methods also work in the multiclass setting.\\nLet’s apply them on the Iris dataset, which is a three-class classification dataset:\\n124 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 138}, page_content='In[115]:\\nfrom sklearn.datasets import load_iris\\niris = load_iris()\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris.data, iris.target, random_state=42)\\ngbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)\\ngbrt.fit(X_train, y_train)\\nIn[116]:\\nprint(\"Decision function shape: {}\".format(gbrt.decision_function(X_test).shape))\\n# plot the first few entries of the decision function\\nprint(\"Decision function:\\\\n{}\".format(gbrt.decision_function(X_test)[:6, :]))\\nOut[116]:\\nDecision function shape: (38, 3)\\nDecision function:\\n[[-0.529  1.466 -0.504]\\n [ 1.512 -0.496 -0.503]\\n [-0.524 -0.468  1.52 ]\\n [-0.529  1.466 -0.504]\\n [-0.531  1.282  0.215]\\n [ 1.512 -0.496 -0.503]]\\nIn the multiclass case, the decision_function has the shape (n_samples,\\nn_classes) and each column provides a “certainty score” for each class, where a large\\nscore means that a class is more likely and a small score means the class is less likely.\\nYou can recover the predictions from these scores by finding the maximum entry for\\neach data point:\\nIn[117]:\\nprint(\"Argmax of decision function:\\\\n{}\".format(\\n      np.argmax(gbrt.decision_function(X_test), axis=1)))\\nprint(\"Predictions:\\\\n{}\".format(gbrt.predict(X_test)))\\nOut[117]:\\nArgmax of decision function:\\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\\nPredictions:\\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\\nThe output of predict_proba has the same shape, (n_samples, n_classes). Again,\\nthe probabilities for the possible classes for each data point sum to 1:\\nUncertainty Estimates from Classifiers \\n| \\n125'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 139}, page_content='In[118]:\\n# show the first few entries of predict_proba\\nprint(\"Predicted probabilities:\\\\n{}\".format(gbrt.predict_proba(X_test)[:6]))\\n# show that sums across rows are one\\nprint(\"Sums: {}\".format(gbrt.predict_proba(X_test)[:6].sum(axis=1)))\\nOut[118]:\\nPredicted probabilities:\\n[[ 0.107  0.784  0.109]\\n [ 0.789  0.106  0.105]\\n [ 0.102  0.108  0.789]\\n [ 0.107  0.784  0.109]\\n [ 0.108  0.663  0.228]\\n [ 0.789  0.106  0.105]]\\nSums: [ 1.  1.  1.  1.  1.  1.]\\nWe can again recover the predictions by computing the argmax of predict_proba:\\nIn[119]:\\nprint(\"Argmax of predicted probabilities:\\\\n{}\".format(\\n    np.argmax(gbrt.predict_proba(X_test), axis=1)))\\nprint(\"Predictions:\\\\n{}\".format(gbrt.predict(X_test)))\\nOut[119]:\\nArgmax of predicted probabilities:\\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\\nPredictions:\\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\\nTo summarize, predict_proba and decision_function always have shape (n_sam\\nples, n_classes)—apart from decision_function in the special binary case. In the\\nbinary case, decision_function only has one column, corresponding to the “posi‐\\ntive” class classes_[1]. This is mostly for historical reasons.\\nYou can recover the prediction when there are n_classes many columns by comput‐\\ning the argmax across columns. Be careful, though, if your classes are strings, or you\\nuse integers but they are not consecutive and starting from 0. If you want to compare\\nresults obtained with predict to results obtained via decision_function or pre\\ndict_proba, make sure to use the classes_ attribute of the classifier to get the actual\\nclass names:\\n126 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 140}, page_content='In[120]:\\nlogreg = LogisticRegression()\\n# represent each target by its class name in the iris dataset\\nnamed_target = iris.target_names[y_train]\\nlogreg.fit(X_train, named_target)\\nprint(\"unique classes in training data: {}\".format(logreg.classes_))\\nprint(\"predictions: {}\".format(logreg.predict(X_test)[:10]))\\nargmax_dec_func = np.argmax(logreg.decision_function(X_test), axis=1)\\nprint(\"argmax of decision function: {}\".format(argmax_dec_func[:10]))\\nprint(\"argmax combined with classes_: {}\".format(\\n        logreg.classes_[argmax_dec_func][:10]))\\nOut[120]:\\nunique classes in training data: [\\'setosa\\' \\'versicolor\\' \\'virginica\\']\\npredictions: [\\'versicolor\\' \\'setosa\\' \\'virginica\\' \\'versicolor\\' \\'versicolor\\'\\n \\'setosa\\' \\'versicolor\\' \\'virginica\\' \\'versicolor\\' \\'versicolor\\']\\nargmax of decision function: [1 0 2 1 1 0 1 2 1 1]\\nargmax combined with classes_: [\\'versicolor\\' \\'setosa\\' \\'virginica\\' \\'versicolor\\'\\n \\'versicolor\\' \\'setosa\\' \\'versicolor\\' \\'virginica\\' \\'versicolor\\' \\'versicolor\\']\\nSummary and Outlook\\nWe started this chapter with a discussion of model complexity, then discussed gener‐\\nalization, or learning a model that is able to perform well on new, previously unseen\\ndata. This led us to the concepts of underfitting, which describes a model that cannot\\ncapture the variations present in the training data, and overfitting, which describes a\\nmodel that focuses too much on the training data and is not able to generalize to new\\ndata very well.\\nWe then discussed a wide array of machine learning models for classification and\\nregression, what their advantages and disadvantages are, and how to control model\\ncomplexity for each of them. We saw that for many of the algorithms, setting the right\\nparameters is important for good performance. Some of the algorithms are also sensi‐\\ntive to how we represent the input data, and in particular to how the features are\\nscaled. Therefore, blindly applying an algorithm to a dataset without understanding\\nthe assumptions the model makes and the meanings of the parameter settings will\\nrarely lead to an accurate model.\\nThis chapter contains a lot of information about the algorithms, and it is not neces‐\\nsary for you to remember all of these details for the following chapters. However,\\nsome knowledge of the models described here—and which to use in a specific situa‐\\ntion—is important for successfully applying machine learning in practice. Here is a\\nquick summary of when to use each model:\\nSummary and Outlook \\n| \\n127'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 141}, page_content='Nearest neighbors\\nFor small datasets, good as a baseline, easy to explain.\\nLinear models\\nGo-to as a first algorithm to try, good for very large datasets, good for very high-\\ndimensional data.\\nNaive Bayes\\nOnly for classification. Even faster than linear models, good for very large data‐\\nsets and high-dimensional data. Often less accurate than linear models.\\nDecision trees\\nVery fast, don’t need scaling of the data, can be visualized and easily explained.\\nRandom forests\\nNearly always perform better than a single decision tree, very robust and power‐\\nful. Don’t need scaling of data. Not good for very high-dimensional sparse data.\\nGradient boosted decision trees\\nOften slightly more accurate than random forests. Slower to train but faster to\\npredict than random forests, and smaller in memory. Need more parameter tun‐\\ning than random forests.\\nSupport vector machines\\nPowerful for medium-sized datasets of features with similar meaning. Require\\nscaling of data, sensitive to parameters.\\nNeural networks\\nCan build very complex models, particularly for large datasets. Sensitive to scal‐\\ning of the data and to the choice of parameters. Large models need a long time to\\ntrain.\\nWhen working with a new dataset, it is in general a good idea to start with a simple\\nmodel, such as a linear model or a naive Bayes or nearest neighbors classifier, and see\\nhow far you can get. After understanding more about the data, you can consider\\nmoving to an algorithm that can build more complex models, such as random forests,\\ngradient boosted decision trees, SVMs, or neural networks.\\nYou should now be in a position where you have some idea of how to apply, tune, and\\nanalyze the models we discussed here. In this chapter, we focused on the binary clas‐\\nsification case, as this is usually easiest to understand. Most of the algorithms presen‐\\nted have classification and regression variants, however, and all of the classification\\nalgorithms support both binary and multiclass classification. Try applying any of\\nthese algorithms to the built-in datasets in scikit-learn, like the boston_housing or\\ndiabetes datasets for regression, or the digits dataset for multiclass classification.\\nPlaying around with the algorithms on different datasets will give you a better feel for\\n128 \\n| \\nChapter 2: Supervised Learning'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 142}, page_content='how long they need to train, how easy it is to analyze the models, and how sensitive\\nthey are to the representation of the data.\\nWhile we analyzed the consequences of different parameter settings for the algo‐\\nrithms we investigated, building a model that actually generalizes well to new data in\\nproduction is a bit trickier than that. We will see how to properly adjust parameters\\nand how to find good parameters automatically in Chapter 6.\\nFirst, though, we will dive in more detail into unsupervised learning and preprocess‐\\ning in the next chapter.\\nSummary and Outlook \\n| \\n129'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 143}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 144}, page_content='CHAPTER 3\\nUnsupervised Learning and Preprocessing\\nThe second family of machine learning algorithms that we will discuss is unsuper‐\\nvised learning algorithms. Unsupervised learning subsumes all kinds of machine\\nlearning where there is no known output, no teacher to instruct the learning algo‐\\nrithm. In unsupervised learning, the learning algorithm is just shown the input data\\nand asked to extract knowledge from this data.\\nTypes of Unsupervised Learning\\nWe will look into two kinds of unsupervised learning in this chapter: transformations\\nof the dataset and clustering.\\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\\ntion of the data which might be easier for humans or other machine learning algo‐\\nrithms to understand compared to the original representation of the data. A common\\napplication of unsupervised transformations is dimensionality reduction, which takes\\na high-dimensional representation of the data, consisting of many features, and finds\\na new way to represent this data that summarizes the essential characteristics with\\nfewer features. A common application for dimensionality reduction is reduction to\\ntwo dimensions for visualization purposes.\\nAnother application for unsupervised transformations is finding the parts or compo‐\\nnents that “make up” the data. An example of this is topic extraction on collections of\\ntext documents. Here, the task is to find the unknown topics that are talked about in\\neach document, and to learn what topics appear in each document. This can be useful\\nfor tracking the discussion of themes like elections, gun control, or pop stars on social\\nmedia.\\nClustering algorithms, on the other hand, partition data into distinct groups of similar\\nitems. Consider the example of uploading photos to a social media site. To allow you\\n131'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 145}, page_content='to organize your pictures, the site might want to group together pictures that show\\nthe same person. However, the site doesn’t know which pictures show whom, and it\\ndoesn’t know how many different people appear in your photo collection. A sensible\\napproach would be to extract all the faces and divide them into groups of faces that\\nlook similar. Hopefully, these correspond to the same person, and the images can be\\ngrouped together for you.\\nChallenges in Unsupervised Learning\\nA major challenge in unsupervised learning is evaluating whether the algorithm\\nlearned something useful. Unsupervised learning algorithms are usually applied to\\ndata that does not contain any label information, so we don’t know what the right\\noutput should be. Therefore, it is very hard to say whether a model “did well.” For\\nexample, our hypothetical clustering algorithm could have grouped together all the\\npictures that show faces in profile and all the full-face pictures. This would certainly\\nbe a possible way to divide a collection of pictures of people’s faces, but it’s not the one\\nwe were looking for. However, there is no way for us to “tell” the algorithm what we\\nare looking for, and often the only way to evaluate the result of an unsupervised algo‐\\nrithm is to inspect it manually.\\nAs a consequence, unsupervised algorithms are used often in an exploratory setting,\\nwhen a data scientist wants to understand the data better, rather than as part of a\\nlarger automatic system. Another common application for unsupervised algorithms\\nis as a preprocessing step for supervised algorithms. Learning a new representation of\\nthe data can sometimes improve the accuracy of supervised algorithms, or can lead to\\nreduced memory and time consumption.\\nBefore we start with “real” unsupervised algorithms, we will briefly discuss some sim‐\\nple preprocessing methods that often come in handy. Even though preprocessing and\\nscaling are often used in tandem with supervised learning algorithms, scaling meth‐\\nods don’t make use of the supervised information, making them unsupervised.\\nPreprocessing and Scaling\\nIn the previous chapter we saw that some algorithms, like neural networks and SVMs,\\nare very sensitive to the scaling of the data. Therefore, a common practice is to adjust\\nthe features so that the data representation is more suitable for these algorithms.\\nOften, this is a simple per-feature rescaling and shift of the data. The following code\\n(Figure 3-1) shows a simple example:\\nIn[2]:\\nmglearn.plots.plot_scaling()\\n132 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 146}, page_content='1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of\\nthe numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\\nsmaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x.\\nFigure 3-1. Different ways to rescale and preprocess a dataset\\nDifferent Kinds of Preprocessing\\nThe first plot in Figure 3-1 shows a synthetic two-class classification dataset with two\\nfeatures. The first feature (the x-axis value) is between 10 and 15. The second feature\\n(the y-axis value) is between around 1 and 9.\\nThe following four plots show four different ways to transform the data that yield\\nmore standard ranges. The StandardScaler in scikit-learn ensures that for each\\nfeature the mean is 0 and the variance is 1, bringing all features to the same magni‐\\ntude. However, this scaling does not ensure any particular minimum and maximum\\nvalues for the features. The RobustScaler works similarly to the StandardScaler in\\nthat it ensures statistical properties for each feature that guarantee that they are on the\\nsame scale. However, the RobustScaler uses the median and quartiles,1 instead of\\nmean and variance. This makes the RobustScaler ignore data points that are very\\ndifferent from the rest (like measurement errors). These odd data points are also\\ncalled outliers, and can lead to trouble for other scaling techniques.\\nThe MinMaxScaler, on the other hand, shifts the data such that all features are exactly\\nbetween 0 and 1. For the two-dimensional dataset this means all of the data is con‐\\nPreprocessing and Scaling \\n| \\n133'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 147}, page_content='tained within the rectangle created by the x-axis between 0 and 1 and the y-axis\\nbetween 0 and 1.\\nFinally, the Normalizer does a very different kind of rescaling. It scales each data\\npoint such that the feature vector has a Euclidean length of 1. In other words, it\\nprojects a data point on the circle (or sphere, in the case of higher dimensions) with a\\nradius of 1. This means every data point is scaled by a different number (by the\\ninverse of its length). This normalization is often used when only the direction (or\\nangle) of the data matters, not the length of the feature vector.\\nApplying Data Transformations\\nNow that we’ve seen what the different kinds of transformations do, let’s apply them\\nusing scikit-learn. We will use the cancer dataset that we saw in Chapter 2. Pre‐\\nprocessing methods like the scalers are usually applied before applying a supervised\\nmachine learning algorithm. As an example, say we want to apply the kernel SVM\\n(SVC) to the cancer dataset, and use MinMaxScaler for preprocessing the data. We\\nstart by loading our dataset and splitting it into a training set and a test set (we need\\nseparate training and test sets to evaluate the supervised model we will build after the\\npreprocessing):\\nIn[3]:\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\ncancer = load_breast_cancer()\\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\\n                                                    random_state=1)\\nprint(X_train.shape)\\nprint(X_test.shape)\\nOut[3]:\\n(426, 30)\\n(143, 30)\\nAs a reminder, the dataset contains 569 data points, each represented by 30 measure‐\\nments. We split the dataset into 426 samples for the training set and 143 samples for\\nthe test set.\\nAs with the supervised models we built earlier, we first import the class that imple‐\\nments the preprocessing, and then instantiate it:\\nIn[4]:\\nfrom sklearn.preprocessing import MinMaxScaler\\nscaler = MinMaxScaler()\\n134 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 148}, page_content='We then fit the scaler using the fit method, applied to the training data. For the Min\\nMaxScaler, the fit method computes the minimum and maximum value of each fea‐\\nture on the training set. In contrast to the classifiers and regressors of Chapter 2, the\\nscaler is only provided with the data (X_train) when fit is called, and y_train is not\\nused:\\nIn[5]:\\nscaler.fit(X_train)\\nOut[5]:\\nMinMaxScaler(copy=True, feature_range=(0, 1))\\nTo apply the transformation that we just learned—that is, to actually scale the training\\ndata—we use the transform method of the scaler. The transform method is used in\\nscikit-learn whenever a model returns a new representation of the data:\\nIn[6]:\\n# transform data\\nX_train_scaled = scaler.transform(X_train)\\n# print dataset properties before and after scaling\\nprint(\"transformed shape: {}\".format(X_train_scaled.shape))\\nprint(\"per-feature minimum before scaling:\\\\n {}\".format(X_train.min(axis=0)))\\nprint(\"per-feature maximum before scaling:\\\\n {}\".format(X_train.max(axis=0)))\\nprint(\"per-feature minimum after scaling:\\\\n {}\".format(\\n    X_train_scaled.min(axis=0)))\\nprint(\"per-feature maximum after scaling:\\\\n {}\".format(\\n    X_train_scaled.max(axis=0)))\\nOut[6]:\\ntransformed shape: (426, 30)\\nper-feature minimum before scaling:\\n [   6.98    9.71   43.79  143.50    0.05    0.02    0.      0.      0.11\\n     0.05    0.12    0.36    0.76    6.80    0.      0.      0.      0.\\n     0.01    0.      7.93   12.02   50.41  185.20    0.07    0.03    0.\\n     0.      0.16    0.06]\\nper-feature maximum before scaling:\\n [   28.11    39.28   188.5   2501.0     0.16     0.29     0.43     0.2\\n     0.300    0.100    2.87     4.88    21.98   542.20     0.03     0.14\\n     0.400    0.050    0.06     0.03    36.04    49.54   251.20  4254.00\\n     0.220    0.940    1.17     0.29     0.58     0.15]\\nper-feature minimum after scaling:\\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\\nper-feature maximum after scaling:\\n [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\\n   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\\nPreprocessing and Scaling \\n| \\n135'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 149}, page_content='The transformed data has the same shape as the original data—the features are simply\\nshifted and scaled. You can see that all of the features are now between 0 and 1, as\\ndesired.\\nTo apply the SVM to the scaled data, we also need to transform the test set. This is\\nagain done by calling the transform method, this time on X_test:\\nIn[7]:\\n# transform test data\\nX_test_scaled = scaler.transform(X_test)\\n# print test data properties after scaling\\nprint(\"per-feature minimum after scaling:\\\\n{}\".format(X_test_scaled.min(axis=0)))\\nprint(\"per-feature maximum after scaling:\\\\n{}\".format(X_test_scaled.max(axis=0)))\\nOut[7]:\\nper-feature minimum after scaling:\\n[ 0.034  0.023  0.031  0.011  0.141  0.044  0.     0.     0.154 -0.006\\n -0.001  0.006  0.004  0.001  0.039  0.011  0.     0.    -0.032  0.007\\n  0.027  0.058  0.02   0.009  0.109  0.026  0.     0.    -0.    -0.002]\\nper-feature maximum after scaling:\\n[ 0.958  0.815  0.956  0.894  0.811  1.22   0.88   0.933  0.932  1.037\\n  0.427  0.498  0.441  0.284  0.487  0.739  0.767  0.629  1.337  0.391\\n  0.896  0.793  0.849  0.745  0.915  1.132  1.07   0.924  1.205  1.631]\\nMaybe somewhat surprisingly, you can see that for the test set, after scaling, the mini‐\\nmum and maximum are not 0 and 1. Some of the features are even outside the 0–1\\nrange! The explanation is that the MinMaxScaler (and all the other scalers) always\\napplies exactly the same transformation to the training and the test set. This means\\nthe transform method always subtracts the training set minimum and divides by the\\ntraining set range, which might be different from the minimum and range for the test\\nset.\\nScaling Training and Test Data the Same Way\\nIt is important to apply exactly the same transformation to the training set and the\\ntest set for the supervised model to work on the test set. The following example\\n(Figure 3-2) illustrates what would happen if we were to use the minimum and range\\nof the test set instead:\\nIn[8]:\\nfrom sklearn.datasets import make_blobs\\n# make synthetic data\\nX, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\\n# split it into training and test sets\\nX_train, X_test = train_test_split(X, random_state=5, test_size=.1)\\n# plot the training and test sets\\nfig, axes = plt.subplots(1, 3, figsize=(13, 4))\\n136 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 150}, page_content='axes[0].scatter(X_train[:, 0], X_train[:, 1],\\n                c=mglearn.cm2(0), label=\"Training set\", s=60)\\naxes[0].scatter(X_test[:, 0], X_test[:, 1], marker=\\'^\\',\\n                c=mglearn.cm2(1), label=\"Test set\", s=60)\\naxes[0].legend(loc=\\'upper left\\')\\naxes[0].set_title(\"Original Data\")\\n# scale the data using MinMaxScaler\\nscaler = MinMaxScaler()\\nscaler.fit(X_train)\\nX_train_scaled = scaler.transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n# visualize the properly scaled data\\naxes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\\n                c=mglearn.cm2(0), label=\"Training set\", s=60)\\naxes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker=\\'^\\',\\n                c=mglearn.cm2(1), label=\"Test set\", s=60)\\naxes[1].set_title(\"Scaled Data\")\\n# rescale the test set separately\\n# so test set min is 0 and test set max is 1\\n# DO NOT DO THIS! For illustration purposes only.\\ntest_scaler = MinMaxScaler()\\ntest_scaler.fit(X_test)\\nX_test_scaled_badly = test_scaler.transform(X_test)\\n# visualize wrongly scaled data\\naxes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\\n                c=mglearn.cm2(0), label=\"training set\", s=60)\\naxes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],\\n                marker=\\'^\\', c=mglearn.cm2(1), label=\"test set\", s=60)\\naxes[2].set_title(\"Improperly Scaled Data\")\\nfor ax in axes:\\n    ax.set_xlabel(\"Feature 0\")\\n    ax.set_ylabel(\"Feature 1\")\\nFigure 3-2. Effect of scaling training and test data shown on the left together (center) and\\nseparately (right)\\nPreprocessing and Scaling \\n| \\n137'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 151}, page_content='The first panel is an unscaled two-dimensional dataset, with the training set shown as\\ncircles and the test set shown as triangles. The second panel is the same data, but\\nscaled using the MinMaxScaler. Here, we called fit on the training set, and then\\ncalled transform on the training and test sets. You can see that the dataset in the sec‐\\nond panel looks identical to the first; only the ticks on the axes have changed. Now all\\nthe features are between 0 and 1. You can also see that the minimum and maximum\\nfeature values for the test data (the triangles) are not 0 and 1.\\nThe third panel shows what would happen if we scaled the training set and test set\\nseparately. In this case, the minimum and maximum feature values for both the train‐\\ning and the test set are 0 and 1. But now the dataset looks different. The test points\\nmoved incongruously to the training set, as they were scaled differently. We changed\\nthe arrangement of the data in an arbitrary way. Clearly this is not what we want to\\ndo.\\nAs another way to think about this, imagine your test set is a single point. There is no\\nway to scale a single point correctly, to fulfill the minimum and maximum require‐\\nments of the MinMaxScaler. But the size of your test set should not change your\\nprocessing.\\nShortcuts and Efficient Alternatives\\nOften, you want to fit a model on some dataset, and then transform it. This is a very\\ncommon task, which can often be computed more efficiently than by simply calling\\nfit and then transform. For this use case, all models that have a transform method\\nalso have a fit_transform method. Here is an example using StandardScaler:\\nIn[9]:\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\n# calling fit and transform in sequence (using method chaining)\\nX_scaled = scaler.fit(X).transform(X)\\n# same result, but more efficient computation\\nX_scaled_d = scaler.fit_transform(X)\\nWhile fit_transform is not necessarily more efficient for all models, it is still good\\npractice to use this method when trying to transform the training set.\\nThe Effect of Preprocessing on Supervised Learning\\nNow let’s go back to the cancer dataset and see the effect of using the MinMaxScaler\\non learning the SVC (this is a different way of doing the same scaling we did in Chap‐\\nter 2). First, let’s fit the SVC on the original data again for comparison:\\n138 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 152}, page_content='In[10]:\\nfrom sklearn.svm import SVC\\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\\n                                                    random_state=0)\\nsvm = SVC(C=100)\\nsvm.fit(X_train, y_train)\\nprint(\"Test set accuracy: {:.2f}\".format(svm.score(X_test, y_test)))\\nOut[10]:\\nTest set accuracy: 0.63\\nNow, let’s scale the data using MinMaxScaler before fitting the SVC:\\nIn[11]:\\n# preprocessing using 0-1 scaling\\nscaler = MinMaxScaler()\\nscaler.fit(X_train)\\nX_train_scaled = scaler.transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n# learning an SVM on the scaled training data\\nsvm.fit(X_train_scaled, y_train)\\n# scoring on the scaled test set\\nprint(\"Scaled test set accuracy: {:.2f}\".format(\\n    svm.score(X_test_scaled, y_test)))\\nOut[11]:\\nScaled test set accuracy: 0.97\\nAs we saw before, the effect of scaling the data is quite significant. Even though scal‐\\ning the data doesn’t involve any complicated math, it is good practice to use the scal‐\\ning mechanisms provided by scikit-learn instead of reimplementing them yourself,\\nas it’s easy to make mistakes even in these simple computations.\\nYou can also easily replace one preprocessing algorithm with another by changing the\\nclass you use, as all of the preprocessing classes have the same interface, consisting of\\nthe fit and transform methods:\\nIn[12]:\\n# preprocessing using zero mean and unit variance scaling\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nscaler.fit(X_train)\\nX_train_scaled = scaler.transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\nPreprocessing and Scaling \\n| \\n139'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 153}, page_content='# learning an SVM on the scaled training data\\nsvm.fit(X_train_scaled, y_train)\\n# scoring on the scaled test set\\nprint(\"SVM test accuracy: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\\nOut[12]:\\nSVM test accuracy: 0.96\\nNow that we’ve seen how simple data transformations for preprocessing work, let’s\\nmove on to more interesting transformations using unsupervised learning.\\nDimensionality Reduction, Feature Extraction, and\\nManifold Learning\\nAs we discussed earlier, transforming data using unsupervised learning can have\\nmany motivations. The most common motivations are visualization, compressing the\\ndata, and finding a representation that is more informative for further processing.\\nOne of the simplest and most widely used algorithms for all of these is principal com‐\\nponent analysis. We’ll also look at two other algorithms: non-negative matrix factori‐\\nzation (NMF), which is commonly used for feature extraction, and t-SNE, which is\\ncommonly used for visualization using two-dimensional scatter plots.\\nPrincipal Component Analysis (PCA)\\nPrincipal component analysis is a method that rotates the dataset in a way such that\\nthe rotated features are statistically uncorrelated. This rotation is often followed by\\nselecting only a subset of the new features, according to how important they are for\\nexplaining the data. The following example (Figure 3-3) illustrates the effect of PCA\\non a synthetic two-dimensional dataset:\\nIn[13]:\\nmglearn.plots.plot_pca_illustration()\\nThe first plot (top left) shows the original data points, colored to distinguish among\\nthem. The algorithm proceeds by first finding the direction of maximum variance,\\nlabeled “Component 1.” This is the direction (or vector) in the data that contains most\\nof the information, or in other words, the direction along which the features are most\\ncorrelated with each other. Then, the algorithm finds the direction that contains the\\nmost information while being orthogonal (at a right angle) to the first direction. In\\ntwo dimensions, there is only one possible orientation that is at a right angle, but in\\nhigher-dimensional spaces there would be (infinitely) many orthogonal directions.\\nAlthough the two components are drawn as arrows, it doesn’t really matter where the\\nhead and the tail are; we could have drawn the first component from the center up to\\n140 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 154}, page_content='the top left instead of down to the bottom right. The directions found using this pro‐\\ncess are called principal components, as they are the main directions of variance in the\\ndata. In general, there are as many principal components as original features.\\nFigure 3-3. Transformation of data with PCA\\nThe second plot (top right) shows the same data, but now rotated so that the first\\nprincipal component aligns with the x-axis and the second principal component\\naligns with the y-axis. Before the rotation, the mean was subtracted from the data, so\\nthat the transformed data is centered around zero. In the rotated representation\\nfound by PCA, the two axes are uncorrelated, meaning that the correlation matrix of\\nthe data in this representation is zero except for the diagonal.\\nWe can use PCA for dimensionality reduction by retaining only some of the principal\\ncomponents. In this example, we might keep only the first principal component, as\\nDimensionality Reduction, Feature Extraction, and Manifold Learning \\n| \\n141'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 155}, page_content='shown in the third panel in Figure 3-3 (bottom left). This reduces the data from a\\ntwo-dimensional dataset to a one-dimensional dataset. Note, however, that instead of\\nkeeping only one of the original features, we found the most interesting direction\\n(top left to bottom right in the first panel) and kept this direction, the first principal\\ncomponent.\\nFinally, we can undo the rotation and add the mean back to the data. This will result\\nin the data shown in the last panel in Figure 3-3. These points are in the original fea‐\\nture space, but we kept only the information contained in the first principal compo‐\\nnent. This transformation is sometimes used to remove noise effects from the data or\\nvisualize what part of the information is retained using the principal components.\\nApplying PCA to the cancer dataset for visualization\\nOne of the most common applications of PCA is visualizing high-dimensional data‐\\nsets. As we saw in Chapter 1, it is hard to create scatter plots of data that has more\\nthan two features. For the Iris dataset, we were able to create a pair plot (Figure 1-3 in\\nChapter 1) that gave us a partial picture of the data by showing us all the possible\\ncombinations of two features. But if we want to look at the Breast Cancer dataset,\\neven using a pair plot is tricky. This dataset has 30 features, which would result in\\n30 * 14 = 420 scatter plots! We’d never be able to look at all these plots in detail, let\\nalone try to understand them.\\nThere is an even simpler visualization we can use, though—computing histograms of\\neach of the features for the two classes, benign and malignant cancer (Figure 3-4):\\nIn[14]:\\nfig, axes = plt.subplots(15, 2, figsize=(10, 20))\\nmalignant = cancer.data[cancer.target == 0]\\nbenign = cancer.data[cancer.target == 1]\\nax = axes.ravel()\\nfor i in range(30):\\n    _, bins = np.histogram(cancer.data[:, i], bins=50)\\n    ax[i].hist(malignant[:, i], bins=bins, color=mglearn.cm3(0), alpha=.5)\\n    ax[i].hist(benign[:, i], bins=bins, color=mglearn.cm3(2), alpha=.5)\\n    ax[i].set_title(cancer.feature_names[i])\\n    ax[i].set_yticks(())\\nax[0].set_xlabel(\"Feature magnitude\")\\nax[0].set_ylabel(\"Frequency\")\\nax[0].legend([\"malignant\", \"benign\"], loc=\"best\")\\nfig.tight_layout()\\n142 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 156}, page_content='Figure 3-4. Per-class feature histograms on the Breast Cancer dataset\\nDimensionality Reduction, Feature Extraction, and Manifold Learning \\n| \\n143'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 157}, page_content='Here we create a histogram for each of the features, counting how often a data point\\nappears with a feature in a certain range (called a bin). Each plot overlays two histo‐\\ngrams, one for all of the points in the benign class (blue) and one for all the points in\\nthe malignant class (red). This gives us some idea of how each feature is distributed\\nacross the two classes, and allows us to venture a guess as to which features are better\\nat distinguishing malignant and benign samples. For example, the feature “smooth‐\\nness error” seems quite uninformative, because the two histograms mostly overlap,\\nwhile the feature “worst concave points” seems quite informative, because the histo‐\\ngrams are quite disjoint.\\nHowever, this plot doesn’t show us anything about the interactions between variables\\nand how these relate to the classes. Using PCA, we can capture the main interactions\\nand get a slightly more complete picture. We can find the first two principal compo‐\\nnents, and visualize the data in this new two-dimensional space with a single scatter\\nplot.\\nBefore we apply PCA, we scale our data so that each feature has unit variance using\\nStandardScaler:\\nIn[15]:\\nfrom sklearn.datasets import load_breast_cancer\\ncancer = load_breast_cancer()\\nscaler = StandardScaler()\\nscaler.fit(cancer.data)\\nX_scaled = scaler.transform(cancer.data)\\nLearning the PCA transformation and applying it is as simple as applying a prepro‐\\ncessing transformation. We instantiate the PCA object, find the principal components\\nby calling the fit method, and then apply the rotation and dimensionality reduction\\nby calling transform. By default, PCA only rotates (and shifts) the data, but keeps all\\nprincipal components. To reduce the dimensionality of the data, we need to specify\\nhow many components we want to keep when creating the PCA object:\\nIn[16]:\\nfrom sklearn.decomposition import PCA\\n# keep the first two principal components of the data\\npca = PCA(n_components=2)\\n# fit PCA model to breast cancer data\\npca.fit(X_scaled)\\n# transform data onto the first two principal components\\nX_pca = pca.transform(X_scaled)\\nprint(\"Original shape: {}\".format(str(X_scaled.shape)))\\nprint(\"Reduced shape: {}\".format(str(X_pca.shape)))\\n144 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 158}, page_content='Out[16]:\\nOriginal shape: (569, 30)\\nReduced shape: (569, 2)\\nWe can now plot the first two principal components (Figure 3-5):\\nIn[17]:\\n# plot first vs. second principal component, colored by class\\nplt.figure(figsize=(8, 8))\\nmglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)\\nplt.legend(cancer.target_names, loc=\"best\")\\nplt.gca().set_aspect(\"equal\")\\nplt.xlabel(\"First principal component\")\\nplt.ylabel(\"Second principal component\")\\nFigure 3-5. Two-dimensional scatter plot of the Breast Cancer dataset using the first two\\nprincipal components\\nIt is important to note that PCA is an unsupervised method, and does not use any class\\ninformation when finding the rotation. It simply looks at the correlations in the data.\\nFor the scatter plot shown here, we plotted the first principal component against the\\nDimensionality Reduction, Feature Extraction, and Manifold Learning \\n| \\n145'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 159}, page_content='second principal component, and then used the class information to color the points.\\nYou can see that the two classes separate quite well in this two-dimensional space.\\nThis leads us to believe that even a linear classifier (that would learn a line in this\\nspace) could do a reasonably good job at distinguishing the two classes. We can also\\nsee that the malignant (red) points are more spread out than the benign (blue) points\\n—something that we could already see a bit from the histograms in Figure 3-4.\\nA downside of PCA is that the two axes in the plot are often not very easy to interpret.\\nThe principal components correspond to directions in the original data, so they are\\ncombinations of the original features. However, these combinations are usually very\\ncomplex, as we’ll see shortly. The principal components themselves are stored in the\\ncomponents_ attribute of the PCA object during fitting:\\nIn[18]:\\nprint(\"PCA component shape: {}\".format(pca.components_.shape))\\nOut[18]:\\nPCA component shape: (2, 30)\\nEach row in components_ corresponds to one principal component, and they are sor‐\\nted by their importance (the first principal component comes first, etc.). The columns\\ncorrespond to the original features attribute of the PCA in this example, “mean\\nradius,” “mean texture,” and so on. Let’s have a look at the content of components_:\\nIn[19]:\\nprint(\"PCA components:\\\\n{}\".format(pca.components_))\\nOut[19]:\\nPCA components:\\n[[ 0.219  0.104  0.228  0.221  0.143  0.239  0.258  0.261  0.138  0.064\\n   0.206  0.017  0.211  0.203  0.015  0.17   0.154  0.183  0.042  0.103\\n   0.228  0.104  0.237  0.225  0.128  0.21   0.229  0.251  0.123  0.132]\\n [-0.234 -0.06  -0.215 -0.231  0.186  0.152  0.06  -0.035  0.19   0.367\\n  -0.106  0.09  -0.089 -0.152  0.204  0.233  0.197  0.13   0.184  0.28\\n  -0.22  -0.045 -0.2   -0.219  0.172  0.144  0.098 -0.008  0.142  0.275]]\\nWe can also visualize the coefficients using a heat map (Figure 3-6), which might be\\neasier to understand:\\nIn[20]:\\nplt.matshow(pca.components_, cmap=\\'viridis\\')\\nplt.yticks([0, 1], [\"First component\", \"Second component\"])\\nplt.colorbar()\\nplt.xticks(range(len(cancer.feature_names)),\\n           cancer.feature_names, rotation=60, ha=\\'left\\')\\nplt.xlabel(\"Feature\")\\nplt.ylabel(\"Principal components\")\\n146 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 160}, page_content=\"Figure 3-6. Heat map of the first two principal components on the Breast Cancer dataset\\nYou can see that in the first component, all features have the same sign (it’s negative,\\nbut as we mentioned earlier, it doesn’t matter which direction the arrow points in).\\nThat means that there is a general correlation between all features. As one measure‐\\nment is high, the others are likely to be high as well. The second component has\\nmixed signs, and both of the components involve all of the 30 features. This mixing of\\nall features is what makes explaining the axes in Figure 3-6 so tricky.\\nEigenfaces for feature extraction\\nAnother application of PCA that we mentioned earlier is feature extraction. The idea\\nbehind feature extraction is that it is possible to find a representation of your data\\nthat is better suited to analysis than the raw representation you were given. A great\\nexample of an application where feature extraction is helpful is with images. Images\\nare made up of pixels, usually stored as red, green, and blue (RGB) intensities.\\nObjects in images are usually made up of thousands of pixels, and only together are\\nthey meaningful.\\nWe will give a very simple application of feature extraction on images using PCA, by\\nworking with face images from the Labeled Faces in the Wild dataset. This dataset\\ncontains face images of celebrities downloaded from the Internet, and it includes\\nfaces of politicians, singers, actors, and athletes from the early 2000s. We use gray‐\\nscale versions of these images, and scale them down for faster processing. You can see\\nsome of the images in Figure 3-7:\\nIn[21]:\\nfrom sklearn.datasets import fetch_lfw_people\\npeople = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\\nimage_shape = people.images[0].shape\\nfix, axes = plt.subplots(2, 5, figsize=(15, 8),\\n                         subplot_kw={'xticks': (), 'yticks': ()})\\nfor target, image, ax in zip(people.target, people.images, axes.ravel()):\\n    ax.imshow(image)\\n    ax.set_title(people.target_names[target])\\nDimensionality Reduction, Feature Extraction, and Manifold Learning \\n| \\n147\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 161}, page_content='Figure 3-7. Some images from the Labeled Faces in the Wild dataset\\nThere are 3,023 images, each 87×65 pixels large, belonging to 62 different people:\\nIn[22]:\\nprint(\"people.images.shape: {}\".format(people.images.shape))\\nprint(\"Number of classes: {}\".format(len(people.target_names)))\\nOut[22]:\\npeople.images.shape: (3023, 87, 65)\\nNumber of classes: 62\\nThe dataset is a bit skewed, however, containing a lot of images of George W. Bush\\nand Colin Powell, as you can see here:\\nIn[23]:\\n# count how often each target appears\\ncounts = np.bincount(people.target)\\n# print counts next to target names\\nfor i, (count, name) in enumerate(zip(counts, people.target_names)):\\n    print(\"{0:25} {1:3}\".format(name, count), end=\\'   \\')\\n    if (i + 1) % 3 == 0:\\n        print()\\n148 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 162}, page_content='Out[23]:\\nAlejandro Toledo           39   Alvaro Uribe               35\\nAmelie Mauresmo            21   Andre Agassi               36\\nAngelina Jolie             20   Arnold Schwarzenegger      42\\nAtal Bihari Vajpayee       24   Bill Clinton               29\\nCarlos Menem               21   Colin Powell              236\\nDavid Beckham              31   Donald Rumsfeld           121\\nGeorge W Bush             530   George Robertson           22\\nGerhard Schroeder         109   Gloria Macapagal Arroyo    44\\nGray Davis                 26   Guillermo Coria            30\\nHamid Karzai               22   Hans Blix                  39\\nHugo Chavez                71   Igor Ivanov                20\\n[...]                           [...]\\nLaura Bush                 41   Lindsay Davenport          22\\nLleyton Hewitt             41   Luiz Inacio Lula da Silva  48\\nMahmoud Abbas              29   Megawati Sukarnoputri      33\\nMichael Bloomberg          20   Naomi Watts                22\\nNestor Kirchner            37   Paul Bremer                20\\nPete Sampras               22   Recep Tayyip Erdogan       30\\nRicardo Lagos              27   Roh Moo-hyun               32\\nRudolph Giuliani           26   Saddam Hussein             23\\nSerena Williams            52   Silvio Berlusconi          33\\nTiger Woods                23   Tom Daschle                25\\nTom Ridge                  33   Tony Blair                144\\nVicente Fox                32   Vladimir Putin             49\\nWinona Ryder               24\\nTo make the data less skewed, we will only take up to 50 images of each person\\n(otherwise, the feature extraction would be overwhelmed by the likelihood of George\\nW. Bush):\\nIn[24]:\\nmask = np.zeros(people.target.shape, dtype=np.bool)\\nfor target in np.unique(people.target):\\n    mask[np.where(people.target == target)[0][:50]] = 1\\nX_people = people.data[mask]\\ny_people = people.target[mask]\\n# scale the grayscale values to be between 0 and 1\\n# instead of 0 and 255 for better numeric stability\\nX_people = X_people / 255.\\nA common task in face recognition is to ask if a previously unseen face belongs to a\\nknown person from a database. This has applications in photo collection, social\\nmedia, and security applications. One way to solve this problem would be to build a\\nclassifier where each person is a separate class. However, there are usually many dif‐\\nferent people in face databases, and very few images of the same person (i.e., very few\\ntraining examples per class). That makes it hard to train most classifiers. Additionally,\\nDimensionality Reduction, Feature Extraction, and Manifold Learning \\n| \\n149'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 163}, page_content='you often want to be able to add new people easily, without needing to retrain a large\\nmodel.\\nA simple solution is to use a one-nearest-neighbor classifier that looks for the most\\nsimilar face image to the face you are classifying. This classifier could in principle\\nwork with only a single training example per class. Let’s take a look at how well\\nKNeighborsClassifier does here:\\nIn[25]:\\nfrom sklearn.neighbors import KNeighborsClassifier\\n# split the data into training and test sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_people, y_people, stratify=y_people, random_state=0)\\n# build a KNeighborsClassifier using one neighbor\\nknn = KNeighborsClassifier(n_neighbors=1)\\nknn.fit(X_train, y_train)\\nprint(\"Test set score of 1-nn: {:.2f}\".format(knn.score(X_test, y_test)))\\nOut[25]:\\nTest set score of 1-nn: 0.27\\nWe obtain an accuracy of 26.6%, which is not actually that bad for a 62-class classifi‐\\ncation problem (random guessing would give you around 1/62 = 1.5% accuracy), but\\nis also not great. We only correctly identify a person every fourth time.\\nThis is where PCA comes in. Computing distances in the original pixel space is quite\\na bad way to measure similarity between faces. When using a pixel representation to\\ncompare two images, we compare the grayscale value of each individual pixel to the\\nvalue of the pixel in the corresponding position in the other image. This representa‐\\ntion is quite different from how humans would interpret the image of a face, and it is\\nhard to capture the facial features using this raw representation. For example, using\\npixel distances means that shifting a face by one pixel to the right corresponds to a\\ndrastic change, with a completely different representation. We hope that using distan‐\\nces along principal components can improve our accuracy. Here, we enable the\\nwhitening option of PCA, which rescales the principal components to have the same\\nscale. This is the same as using StandardScaler after the transformation. Reusing the\\ndata from Figure 3-3 again, whitening corresponds to not only rotating the data, but\\nalso rescaling it so that the center panel is a circle instead of an ellipse (see\\nFigure 3-8):\\nIn[26]:\\nmglearn.plots.plot_pca_whitening()\\n150 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 164}, page_content='Figure 3-8. Transformation of data with PCA using whitening\\nWe fit the PCA object to the training data and extract the first 100 principal compo‐\\nnents. Then we transform the training and test data:\\nIn[27]:\\npca = PCA(n_components=100, whiten=True, random_state=0).fit(X_train)\\nX_train_pca = pca.transform(X_train)\\nX_test_pca = pca.transform(X_test)\\nprint(\"X_train_pca.shape: {}\".format(X_train_pca.shape))\\nOut[27]:\\nX_train_pca.shape: (1537, 100)\\nThe new data has 100 features, the first 100 principal components. Now, we can use\\nthe new representation to classify our images using a one-nearest-neighbors classifier:\\nIn[28]:\\nknn = KNeighborsClassifier(n_neighbors=1)\\nknn.fit(X_train_pca, y_train)\\nprint(\"Test set accuracy: {:.2f}\".format(knn.score(X_test_pca, y_test)))\\nOut[28]:\\nTest set accuracy: 0.36\\nOur accuracy improved quite significantly, from 26.6% to 35.7%, confirming our\\nintuition that the principal components might provide a better representation of the\\ndata.\\nDimensionality Reduction, Feature Extraction, and Manifold Learning \\n| \\n151'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 165}, page_content='For image data, we can also easily visualize the principal components that are found.\\nRemember that components correspond to directions in the input space. The input\\nspace here is 50×37-pixel grayscale images, so directions within this space are also\\n50×37-pixel grayscale images.\\nLet’s look at the first couple of principal components (Figure 3-9):\\nIn[29]:\\nprint(\"pca.components_.shape: {}\".format(pca.components_.shape))\\nOut[29]:\\npca.components_.shape: (100, 5655)\\nIn[30]:\\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\nfor i, (component, ax) in enumerate(zip(pca.components_, axes.ravel())):\\n    ax.imshow(component.reshape(image_shape),\\n              cmap=\\'viridis\\')\\n    ax.set_title(\"{}. component\".format((i + 1)))\\nWhile we certainly cannot understand all aspects of these components, we can guess\\nwhich aspects of the face images some of the components are capturing. The first\\ncomponent seems to mostly encode the contrast between the face and the back‐\\nground, the second component encodes differences in lighting between the right and\\nthe left half of the face, and so on. While this representation is slightly more semantic\\nthan the raw pixel values, it is still quite far from how a human might perceive a face.\\nAs the PCA model is based on pixels, the alignment of the face (the position of eyes,\\nchin, and nose) and the lighting both have a strong influence on how similar two\\nimages are in their pixel representation. But alignment and lighting are probably not\\nwhat a human would perceive first. When asking people to rate similarity of faces,\\nthey are more likely to use attributes like age, gender, facial expression, and hair style,\\nwhich are attributes that are hard to infer from the pixel intensities. It’s important to\\nkeep in mind that algorithms often interpret data (particularly visual data, such as\\nimages, which humans are very familiar with) quite differently from how a human\\nwould.\\n152 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 166}, page_content='Figure 3-9. Component vectors of the first 15 principal components of the faces dataset\\nLet’s come back to the specific case of PCA, though. We introduced the PCA transfor‐\\nmation as rotating the data and then dropping the components with low variance.\\nAnother useful interpretation is to try to find some numbers (the new feature values\\nafter the PCA rotation) so that we can express the test points as a weighted sum of the\\nprincipal components (see Figure 3-10).\\nFigure 3-10. Schematic view of PCA as decomposing an image into a weighted sum of\\ncomponents\\nHere, x0, x1, and so on are the coefficients of the principal components for this data\\npoint; in other words, they are the representation of the image in the rotated space.\\nDimensionality Reduction, Feature Extraction, and Manifold Learning \\n| \\n153'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 167}, page_content='Another way we can try to understand what a PCA model is doing is by looking at\\nthe reconstructions of the original data using only some components. In Figure 3-3,\\nafter dropping the second component and arriving at the third panel, we undid the\\nrotation and added the mean back to obtain new points in the original space with the\\nsecond component removed, as shown in the last panel. We can do a similar transfor‐\\nmation for the faces by reducing the data to only some principal components and\\nthen rotating back into the original space. This return to the original feature space\\ncan be done using the inverse_transform method. Here, we visualize the recon‐\\nstruction of some faces using 10, 50, 100, 500, or 2,000 components (Figure 3-11):\\nIn[32]:\\nmglearn.plots.plot_pca_faces(X_train, X_test, image_shape)\\nFigure 3-11. Reconstructing three face images using increasing numbers of principal\\ncomponents\\nYou can see that when we use only the first 10 principal components, only the essence\\nof the picture, like the face orientation and lighting, is captured. By using more and\\nmore principal components, more and more details in the image are preserved. This\\n154 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 168}, page_content='corresponds to extending the sum in Figure 3-10 to include more and more terms.\\nUsing as many components as there are pixels would mean that we would not discard\\nany information after the rotation, and we would reconstruct the image perfectly.\\nWe can also try to use PCA to visualize all the faces in the dataset in a scatter plot\\nusing the first two principal components (Figure 3-12), with classes given by who is\\nshown in the image, similarly to what we did for the cancer dataset:\\nIn[33]:\\nmglearn.discrete_scatter(X_train_pca[:, 0], X_train_pca[:, 1], y_train)\\nplt.xlabel(\"First principal component\")\\nplt.ylabel(\"Second principal component\")\\nFigure 3-12. Scatter plot of the faces dataset using the first two principal components (see\\nFigure 3-5 for the corresponding image for the cancer dataset)\\nAs you can see, when we use only the first two principal components the whole data\\nis just a big blob, with no separation of classes visible. This is not very surprising,\\ngiven that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐\\ntures very rough characteristics of the faces.\\nDimensionality Reduction, Feature Extraction, and Manifold Learning \\n| \\n155'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 169}, page_content='Non-Negative Matrix Factorization (NMF)\\nNon-negative matrix factorization is another unsupervised learning algorithm that\\naims to extract useful features. It works similarly to PCA and can also be used for\\ndimensionality reduction. As in PCA, we are trying to write each data point as a\\nweighted sum of some components, as illustrated in Figure 3-10. But whereas in PCA\\nwe wanted components that were orthogonal and that explained as much variance of\\nthe data as possible, in NMF, we want the components and the coefficients to be non-\\nnegative; that is, we want both the components and the coefficients to be greater than\\nor equal to zero. Consequently, this method can only be applied to data where each\\nfeature is non-negative, as a non-negative sum of non-negative components cannot\\nbecome negative.\\nThe process of decomposing data into a non-negative weighted sum is particularly\\nhelpful for data that is created as the addition (or overlay) of several independent\\nsources, such as an audio track of multiple people speaking, or music with many\\ninstruments. In these situations, NMF can identify the original components that\\nmake up the combined data. Overall, NMF leads to more interpretable components\\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and\\nnegative parts, and as we mentioned in the description of PCA, the sign is actually\\narbitrary. Before we apply NMF to the face dataset, let’s briefly revisit the synthetic\\ndata.\\nApplying NMF to synthetic data\\nIn contrast to when using PCA, we need to ensure that our data is positive for NMF\\nto be able to operate on the data. This means where the data lies relative to the origin\\n(0, 0) actually matters for NMF. Therefore, you can think of the non-negative compo‐\\nnents that are extracted as directions from (0, 0) toward the data.\\nThe following example (Figure 3-13) shows the results of NMF on the two-\\ndimensional toy data:\\nIn[34]:\\nmglearn.plots.plot_nmf_illustration()\\n156 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 170}, page_content='Figure 3-13. Components found by non-negative matrix factorization with two compo‐\\nnents (left) and one component (right)\\nFor NMF with two components, as shown on the left, it is clear that all points in the\\ndata can be written as a positive combination of the two components. If there are\\nenough components to perfectly reconstruct the data (as many components as there\\nare features), the algorithm will choose directions that point toward the extremes of\\nthe data.\\nIf we only use a single component, NMF creates a component that points toward the\\nmean, as pointing there best explains the data. You can see that in contrast with PCA,\\nreducing the number of components not only removes some directions, but creates\\nan entirely different set of components! Components in NMF are also not ordered in\\nany specific way, so there is no “first non-negative component”: all components play\\nan equal part.\\nNMF uses a random initialization, which might lead to different results depending on\\nthe random seed. In relatively simple cases such as the synthetic data with two com‐\\nponents, where all the data can be explained perfectly, the randomness has little effect\\n(though it might change the order or scale of the components). In more complex sit‐\\nuations, there might be more drastic changes.\\nApplying NMF to face images\\nNow, let’s apply NMF to the Labeled Faces in the Wild dataset we used earlier. The\\nmain parameter of NMF is how many components we want to extract. Usually this is\\nlower than the number of input features (otherwise, the data could be explained by\\nmaking each pixel a separate component).\\nFirst, let’s inspect how the number of components impacts how well the data can be\\nreconstructed using NMF (Figure 3-14):\\nDimensionality Reduction, Feature Extraction, and Manifold Learning \\n| \\n157'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 171}, page_content='In[35]:\\nmglearn.plots.plot_nmf_faces(X_train, X_test, image_shape)\\nFigure 3-14. Reconstructing three face images using increasing numbers of components\\nfound by NMF\\nThe quality of the back-transformed data is similar to when using PCA, but slightly\\nworse. This is expected, as PCA finds the optimum directions in terms of reconstruc‐\\ntion. NMF is usually not used for its ability to reconstruct or encode data, but rather\\nfor finding interesting patterns within the data.\\nAs a first look into the data, let’s try extracting only a few components (say, 15).\\nFigure 3-15 shows the result:\\n158 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 172}, page_content='In[36]:\\nfrom sklearn.decomposition import NMF\\nnmf = NMF(n_components=15, random_state=0)\\nnmf.fit(X_train)\\nX_train_nmf = nmf.transform(X_train)\\nX_test_nmf = nmf.transform(X_test)\\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\nfor i, (component, ax) in enumerate(zip(nmf.components_, axes.ravel())):\\n    ax.imshow(component.reshape(image_shape))\\n    ax.set_title(\"{}. component\".format(i))\\nFigure 3-15. The components found by NMF on the faces dataset when using 15 compo‐\\nnents\\nThese components are all positive, and so resemble prototypes of faces much more so\\nthan the components shown for PCA in Figure 3-9. For example, one can clearly see\\nthat component 3 shows a face rotated somewhat to the right, while component 7\\nshows a face somewhat rotated to the left. Let’s look at the images for which these\\ncomponents are particularly strong, shown in Figures 3-16 and 3-17:\\nDimensionality Reduction, Feature Extraction, and Manifold Learning \\n| \\n159'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 173}, page_content=\"In[37]:\\ncompn = 3\\n# sort by 3rd component, plot first 10 images\\ninds = np.argsort(X_train_nmf[:, compn])[::-1]\\nfig, axes = plt.subplots(2, 5, figsize=(15, 8),\\n                         subplot_kw={'xticks': (), 'yticks': ()})\\nfor i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\\n    ax.imshow(X_train[ind].reshape(image_shape))\\ncompn = 7\\n# sort by 7th component, plot first 10 images\\ninds = np.argsort(X_train_nmf[:, compn])[::-1]\\nfig, axes = plt.subplots(2, 5, figsize=(15, 8),\\n                         subplot_kw={'xticks': (), 'yticks': ()})\\nfor i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\\n    ax.imshow(X_train[ind].reshape(image_shape))\\nFigure 3-16. Faces that have a large coefficient for component 3\\n160 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 174}, page_content='Figure 3-17. Faces that have a large coefficient for component 7\\nAs expected, faces that have a high coefficient for component 3 are faces looking to\\nthe right (Figure 3-16), while faces with a high coefficient for component 7 are look‐\\ning to the left (Figure 3-17). As mentioned earlier, extracting patterns like these works\\nbest for data with additive structure, including audio, gene expression, and text data.\\nLet’s walk through one example on synthetic data to see what this might look like.\\nLet’s say we are interested in a signal that is a combination of three different sources\\n(Figure 3-18):\\nIn[38]:\\nS = mglearn.datasets.make_signals()\\nplt.figure(figsize=(6, 1))\\nplt.plot(S, \\'-\\')\\nplt.xlabel(\"Time\")\\nplt.ylabel(\"Signal\")\\nFigure 3-18. Original signal sources\\nDimensionality Reduction, Feature Extraction, and Manifold Learning \\n| \\n161'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 175}, page_content='Unfortunately we cannot observe the original signals, but only an additive mixture of\\nall three of them. We want to recover the decomposition of the mixed signal into the\\noriginal components. We assume that we have many different ways to observe the\\nmixture (say 100 measurement devices), each of which provides us with a series of\\nmeasurements:\\nIn[39]:\\n# mix data into a 100-dimensional state\\nA = np.random.RandomState(0).uniform(size=(100, 3))\\nX = np.dot(S, A.T)\\nprint(\"Shape of measurements: {}\".format(X.shape))\\nOut[39]:\\nShape of measurements: (2000, 100)\\nWe can use NMF to recover the three signals:\\nIn[40]:\\nnmf = NMF(n_components=3, random_state=42)\\nS_ = nmf.fit_transform(X)\\nprint(\"Recovered signal shape: {}\".format(S_.shape))\\nOut[40]:\\nRecovered signal shape: (2000, 3)\\nFor comparison, we also apply PCA:\\nIn[41]:\\npca = PCA(n_components=3)\\nH = pca.fit_transform(X)\\nFigure 3-19 shows the signal activity that was discovered by NMF and PCA:\\nIn[42]:\\nmodels = [X, S, S_, H]\\nnames = [\\'Observations (first three measurements)\\',\\n         \\'True sources\\',\\n         \\'NMF recovered signals\\',\\n         \\'PCA recovered signals\\']\\nfig, axes = plt.subplots(4, figsize=(8, 4), gridspec_kw={\\'hspace\\': .5},\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\nfor model, name, ax in zip(models, names, axes):\\n    ax.set_title(name)\\n    ax.plot(model[:, :3], \\'-\\')\\n162 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 176}, page_content='Figure 3-19. Recovering mixed sources using NMF and PCA\\nThe figure includes 3 of the 100 measurements from X for reference. As you can see,\\nNMF did a reasonable job of discovering the original sources, while PCA failed and\\nused the first component to explain the majority of the variation in the data. Keep in\\nmind that the components produced by NMF have no natural ordering. In this exam‐\\nple, the ordering of the NMF components is the same as in the original signal (see the\\nshading of the three curves), but this is purely accidental.\\nThere are many other algorithms that can be used to decompose each data point into\\na weighted sum of a fixed set of components, as PCA and NMF do. Discussing all of\\nthem is beyond the scope of this book, and describing the constraints made on the\\ncomponents and coefficients often involves probability theory. If you are interested in\\nthis kind of pattern extraction, we recommend that you study the sections of the sci\\nkit_learn user guide on independent component analysis (ICA), factor analysis\\n(FA), and sparse coding (dictionary learning), all of which you can find on the page\\nabout decomposition methods.\\nManifold Learning with t-SNE\\nWhile PCA is often a good first approach for transforming your data so that you\\nmight be able to visualize it using a scatter plot, the nature of the method (applying a\\nrotation and then dropping directions) limits its usefulness, as we saw with the scatter\\nplot of the Labeled Faces in the Wild dataset. There is a class of algorithms for visuali‐\\nzation called manifold learning algorithms that allow for much more complex map‐\\npings, and often provide better visualizations. A particularly useful one is the t-SNE\\nalgorithm.\\nDimensionality Reduction, Feature Extraction, and Manifold Learning \\n| \\n163'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 177}, page_content=\"2 Not to be confused with the much larger MNIST dataset.\\nManifold learning algorithms are mainly aimed at visualization, and so are rarely\\nused to generate more than two new features. Some of them, including t-SNE, com‐\\npute a new representation of the training data, but don’t allow transformations of new\\ndata. This means these algorithms cannot be applied to a test set: rather, they can only\\ntransform the data they were trained for. Manifold learning can be useful for explora‐\\ntory data analysis, but is rarely used if the final goal is supervised learning. The idea\\nbehind t-SNE is to find a two-dimensional representation of the data that preserves\\nthe distances between points as best as possible. t-SNE starts with a random two-\\ndimensional representation for each data point, and then tries to make points that are\\nclose in the original feature space closer, and points that are far apart in the original\\nfeature space farther apart. t-SNE puts more emphasis on points that are close by,\\nrather than preserving distances between far-apart points. In other words, it tries to\\npreserve the information indicating which points are neighbors to each other.\\nWe will apply the t-SNE manifold learning algorithm on a dataset of handwritten dig‐\\nits that is included in scikit-learn.2 Each data point in this dataset is an 8×8 gray‐\\nscale image of a handwritten digit between 0 and 1. Figure 3-20 shows an example\\nimage for each class:\\nIn[43]:\\nfrom sklearn.datasets import load_digits\\ndigits = load_digits()\\nfig, axes = plt.subplots(2, 5, figsize=(10, 5),\\n                         subplot_kw={'xticks':(), 'yticks': ()})\\nfor ax, img in zip(axes.ravel(), digits.images):\\n    ax.imshow(img)\\n164 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 178}, page_content='Figure 3-20. Example images from the digits dataset\\nLet’s use PCA to visualize the data reduced to two dimensions. We plot the first two\\nprincipal components, and color each dot by its class (see Figure 3-21):\\nIn[44]:\\n# build a PCA model\\npca = PCA(n_components=2)\\npca.fit(digits.data)\\n# transform the digits data onto the first two principal components\\ndigits_pca = pca.transform(digits.data)\\ncolors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\\n          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\", \"#535D8E\"]\\nplt.figure(figsize=(10, 10))\\nplt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\\nplt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\\nfor i in range(len(digits.data)):\\n    # actually plot the digits as text instead of using scatter\\n    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\\n             color = colors[digits.target[i]],\\n             fontdict={\\'weight\\': \\'bold\\', \\'size\\': 9})\\nplt.xlabel(\"First principal component\")\\nplt.ylabel(\"Second principal component\")\\nHere, we actually used the true digit classes as glyphs, to show which class is where.\\nThe digits zero, six, and four are relatively well separated using the first two principal\\ncomponents, though they still overlap. Most of the other digits overlap significantly.\\nDimensionality Reduction, Feature Extraction, and Manifold Learning \\n| \\n165'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 179}, page_content='Figure 3-21. Scatter plot of the digits dataset using the first two principal components\\nLet’s apply t-SNE to the same dataset, and compare the results. As t-SNE does not\\nsupport transforming new data, the TSNE class has no transform method. Instead, we\\ncan call the fit_transform method, which will build the model and immediately\\nreturn the transformed data (see Figure 3-22):\\nIn[45]:\\nfrom sklearn.manifold import TSNE\\ntsne = TSNE(random_state=42)\\n# use fit_transform instead of fit, as TSNE has no transform method\\ndigits_tsne = tsne.fit_transform(digits.data)\\n166 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 180}, page_content='In[46]:\\nplt.figure(figsize=(10, 10))\\nplt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\\nplt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\\nfor i in range(len(digits.data)):\\n    # actually plot the digits as text instead of using scatter\\n    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\\n             color = colors[digits.target[i]],\\n             fontdict={\\'weight\\': \\'bold\\', \\'size\\': 9})\\nplt.xlabel(\"t-SNE feature 0\")\\nplt.xlabel(\"t-SNE feature 1\")\\nFigure 3-22. Scatter plot of the digits dataset using two components found by t-SNE\\nDimensionality Reduction, Feature Extraction, and Manifold Learning \\n| \\n167'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 181}, page_content='The result of t-SNE is quite remarkable. All the classes are quite clearly separated.\\nThe ones and nines are somewhat split up, but most of the classes form a single dense\\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\\npletely unsupervised. Still, it can find a representation of the data in two dimensions\\nthat clearly separates the classes, based solely on how close points are in the original\\nspace.\\nThe t-SNE algorithm has some tuning parameters, though it often works well with\\nthe default settings. You can try playing with perplexity and early_exaggeration,\\nbut the effects are usually minor.\\nClustering\\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\\ncalled clusters. The goal is to split up the data in such a way that points within a single\\ncluster are very similar and points in different clusters are different. Similarly to clas‐\\nsification algorithms, clustering algorithms assign (or predict) a number to each data\\npoint, indicating which cluster a particular point belongs to.\\nk-Means Clustering\\nk-means clustering is one of the simplest and most commonly used clustering algo‐\\nrithms. It tries to find cluster centers that are representative of certain regions of the\\ndata. The algorithm alternates between two steps: assigning each data point to the\\nclosest cluster center, and then setting each cluster center as the mean of the data\\npoints that are assigned to it. The algorithm is finished when the assignment of\\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\\ntrates the algorithm on a synthetic dataset:\\nIn[47]:\\nmglearn.plots.plot_kmeans_algorithm()\\n168 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 182}, page_content='Figure 3-23. Input data and three steps of the k-means algorithm\\nCluster centers are shown as triangles, while data points are shown as circles. Colors\\nindicate cluster membership. We specified that we are looking for three clusters, so\\nthe algorithm was initialized by declaring three data points randomly as cluster cen‐\\nters (see “Initialization”). Then the iterative algorithm starts. First, each data point is\\nassigned to the cluster center it is closest to (see “Assign Points (1)”). Next, the cluster\\ncenters are updated to be the mean of the assigned points (see “Recompute Centers\\n(1)”). Then the process is repeated two more times. After the third iteration, the\\nassignment of points to cluster centers remained unchanged, so the algorithm stops.\\nGiven new data points, k-means will assign each to the closest cluster center. The next\\nexample (Figure 3-24) shows the boundaries of the cluster centers that were learned\\nin Figure 3-23:\\nIn[48]:\\nmglearn.plots.plot_kmeans_boundaries()\\nClustering \\n| \\n169'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 183}, page_content='3 If you don’t provide n_clusters, it is set to 8 by default. There is no particular reason why you should use this\\nvalue.\\nFigure 3-24. Cluster centers and cluster boundaries found by the k-means algorithm\\nApplying k-means with scikit-learn is quite straightforward. Here, we apply it to\\nthe synthetic data that we used for the preceding plots. We instantiate the KMeans\\nclass, and set the number of clusters we are looking for.3 Then we call the fit method\\nwith the data:\\nIn[49]:\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.cluster import KMeans\\n# generate synthetic two-dimensional data\\nX, y = make_blobs(random_state=1)\\n# build the clustering model\\nkmeans = KMeans(n_clusters=3)\\nkmeans.fit(X)\\nDuring the algorithm, each training data point in X is assigned a cluster label. You can\\nfind these labels in the kmeans.labels_ attribute:\\n170 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 184}, page_content='In[50]:\\nprint(\"Cluster memberships:\\\\n{}\".format(kmeans.labels_))\\nOut[50]:\\nCluster memberships:\\n[1 2 2 2 0 0 0 2 1 1 2 2 0 1 0 0 0 1 2 2 0 2 0 1 2 0 0 1 1 0 1 1 0 1 2 0 2\\n 2 2 0 0 2 1 2 2 0 1 1 1 1 2 0 0 0 1 0 2 2 1 1 2 0 0 2 2 0 1 0 1 2 2 2 0 1\\n 1 2 0 0 1 2 1 2 2 0 1 1 1 1 2 1 0 1 1 2 2 0 0 1 0 1]\\nAs we asked for three clusters, the clusters are numbered 0 to 2.\\nYou can also assign cluster labels to new points, using the predict method. Each new\\npoint is assigned to the closest cluster center when predicting, but the existing model\\nis not changed. Running predict on the training set returns the same result as\\nlabels_:\\nIn[51]:\\nprint(kmeans.predict(X))\\nOut[51]:\\n[1 2 2 2 0 0 0 2 1 1 2 2 0 1 0 0 0 1 2 2 0 2 0 1 2 0 0 1 1 0 1 1 0 1 2 0 2\\n 2 2 0 0 2 1 2 2 0 1 1 1 1 2 0 0 0 1 0 2 2 1 1 2 0 0 2 2 0 1 0 1 2 2 2 0 1\\n 1 2 0 0 1 2 1 2 2 0 1 1 1 1 2 1 0 1 1 2 2 0 0 1 0 1]\\nYou can see that clustering is somewhat similar to classification, in that each item gets\\na label. However, there is no ground truth, and consequently the labels themselves\\nhave no a priori meaning. Let’s go back to the example of clustering face images that\\nwe discussed before. It might be that the cluster 3 found by the algorithm contains\\nonly faces of your friend Bela. You can only know that after you look at the pictures,\\nthough, and the number 3 is arbitrary. The only information the algorithm gives you\\nis that all faces labeled as 3 are similar.\\nFor the clustering we just computed on the two-dimensional toy dataset, that means\\nthat we should not assign any significance to the fact that one group was labeled 0\\nand another one was labeled 1. Running the algorithm again might result in a differ‐\\nent numbering of clusters because of the random nature of the initialization.\\nHere is a plot of this data again (Figure 3-25). The cluster centers are stored in the\\ncluster_centers_ attribute, and we plot them as triangles:\\nIn[52]:\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers=\\'o\\')\\nmglearn.discrete_scatter(\\n    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2],\\n    markers=\\'^\\', markeredgewidth=2)\\nClustering \\n| \\n171'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 185}, page_content='Figure 3-25. Cluster assignments and cluster centers found by k-means with three\\nclusters\\nWe can also use more or fewer cluster centers (Figure 3-26):\\nIn[53]:\\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\\n# using two cluster centers:\\nkmeans = KMeans(n_clusters=2)\\nkmeans.fit(X)\\nassignments = kmeans.labels_\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[0])\\n# using five cluster centers:\\nkmeans = KMeans(n_clusters=5)\\nkmeans.fit(X)\\nassignments = kmeans.labels_\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[1])\\n172 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 186}, page_content='Figure 3-26. Cluster assignments found by k-means using two clusters (left) and five\\nclusters (right)\\nFailure cases of k-means\\nEven if you know the “right” number of clusters for a given dataset, k-means might\\nnot always be able to recover them. Each cluster is defined solely by its center, which\\nmeans that each cluster is a convex shape. As a result of this, k-means can only cap‐\\nture relatively simple shapes. k-means also assumes that all clusters have the same\\n“diameter” in some sense; it always draws the boundary between clusters to be exactly\\nin the middle between the cluster centers. That can sometimes lead to surprising\\nresults, as shown in Figure 3-27:\\nIn[54]:\\nX_varied, y_varied = make_blobs(n_samples=200,\\n                                cluster_std=[1.0, 2.5, 0.5],\\n                                random_state=170)\\ny_pred = KMeans(n_clusters=3, random_state=0).fit_predict(X_varied)\\nmglearn.discrete_scatter(X_varied[:, 0], X_varied[:, 1], y_pred)\\nplt.legend([\"cluster 0\", \"cluster 1\", \"cluster 2\"], loc=\\'best\\')\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nClustering \\n| \\n173'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 187}, page_content='Figure 3-27. Cluster assignments found by k-means when clusters have different\\ndensities\\nOne might have expected the dense region in the lower left to be the first cluster, the\\ndense region in the upper right to be the second, and the less dense region in the cen‐\\nter to be the third. Instead, both cluster 0 and cluster 1 have some points that are far\\naway from all the other points in these clusters that “reach” toward the center.\\nk-means also assumes that all directions are equally important for each cluster. The\\nfollowing plot (Figure 3-28) shows a two-dimensional dataset where there are three\\nclearly separated parts in the data. However, these groups are stretched toward the\\ndiagonal. As k-means only considers the distance to the nearest cluster center, it can’t\\nhandle this kind of data:\\nIn[55]:\\n# generate some random cluster data\\nX, y = make_blobs(random_state=170, n_samples=600)\\nrng = np.random.RandomState(74)\\n# transform the data to be stretched\\ntransformation = rng.normal(size=(2, 2))\\nX = np.dot(X, transformation)\\n174 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 188}, page_content='# cluster the data into three clusters\\nkmeans = KMeans(n_clusters=3)\\nkmeans.fit(X)\\ny_pred = kmeans.predict(X)\\n# plot the cluster assignments and cluster centers\\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm3)\\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\\n            marker=\\'^\\', c=[0, 1, 2], s=100, linewidth=2, cmap=mglearn.cm3)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nFigure 3-28. k-means fails to identify nonspherical clusters\\nk-means also performs poorly if the clusters have more complex shapes, like the\\ntwo_moons data we encountered in Chapter 2 (see Figure 3-29):\\nIn[56]:\\n# generate synthetic two_moons data (with less noise this time)\\nfrom sklearn.datasets import make_moons\\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\\n# cluster the data into two clusters\\nkmeans = KMeans(n_clusters=2)\\nkmeans.fit(X)\\ny_pred = kmeans.predict(X)\\nClustering \\n| \\n175'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 189}, page_content='# plot the cluster assignments and cluster centers\\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)\\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\\n            marker=\\'^\\', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nFigure 3-29. k-means fails to identify clusters with complex shapes\\nHere, we would hope that the clustering algorithm can discover the two half-moon\\nshapes. However, this is not possible using the k-means algorithm.\\nVector quantization, or seeing k-means as decomposition\\nEven though k-means is a clustering algorithm, there are interesting parallels between\\nk-means and the decomposition methods like PCA and NMF that we discussed ear‐\\nlier. You might remember that PCA tries to find directions of maximum variance in\\nthe data, while NMF tries to find additive components, which often correspond to\\n“extremes” or “parts” of the data (see Figure 3-13). Both methods tried to express the\\ndata points as a sum over some components. k-means, on the other hand, tries to rep‐\\nresent each data point using a cluster center. You can think of that as each point being\\nrepresented using only a single component, which is given by the cluster center. This\\nview of k-means as a decomposition method, where each point is represented using a\\nsingle component, is called vector quantization.\\n176 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 190}, page_content='Let’s do a side-by-side comparison of PCA, NMF, and k-means, showing the compo‐\\nnents extracted (Figure 3-30), as well as reconstructions of faces from the test set\\nusing 100 components (Figure 3-31). For k-means, the reconstruction is the closest\\ncluster center found on the training set:\\nIn[57]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_people, y_people, stratify=y_people, random_state=0)\\nnmf = NMF(n_components=100, random_state=0)\\nnmf.fit(X_train)\\npca = PCA(n_components=100, random_state=0)\\npca.fit(X_train)\\nkmeans = KMeans(n_clusters=100, random_state=0)\\nkmeans.fit(X_train)\\nX_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))\\nX_reconstructed_kmeans = kmeans.cluster_centers_[kmeans.predict(X_test)]\\nX_reconstructed_nmf = np.dot(nmf.transform(X_test), nmf.components_)\\nIn[58]:\\nfig, axes = plt.subplots(3, 5, figsize=(8, 8),\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\nfig.suptitle(\"Extracted Components\")\\nfor ax, comp_kmeans, comp_pca, comp_nmf in zip(\\n        axes.T, kmeans.cluster_centers_, pca.components_, nmf.components_):\\n    ax[0].imshow(comp_kmeans.reshape(image_shape))\\n    ax[1].imshow(comp_pca.reshape(image_shape), cmap=\\'viridis\\')\\n    ax[2].imshow(comp_nmf.reshape(image_shape))\\naxes[0, 0].set_ylabel(\"kmeans\")\\naxes[1, 0].set_ylabel(\"pca\")\\naxes[2, 0].set_ylabel(\"nmf\")\\nfig, axes = plt.subplots(4, 5, subplot_kw={\\'xticks\\': (), \\'yticks\\': ()},\\n                         figsize=(8, 8))\\nfig.suptitle(\"Reconstructions\")\\nfor ax, orig, rec_kmeans, rec_pca, rec_nmf in zip(\\n        axes.T, X_test, X_reconstructed_kmeans, X_reconstructed_pca,\\n        X_reconstructed_nmf):\\n    ax[0].imshow(orig.reshape(image_shape))\\n    ax[1].imshow(rec_kmeans.reshape(image_shape))\\n    ax[2].imshow(rec_pca.reshape(image_shape))\\n    ax[3].imshow(rec_nmf.reshape(image_shape))\\naxes[0, 0].set_ylabel(\"original\")\\naxes[1, 0].set_ylabel(\"kmeans\")\\naxes[2, 0].set_ylabel(\"pca\")\\naxes[3, 0].set_ylabel(\"nmf\")\\nClustering \\n| \\n177'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 191}, page_content='Figure 3-30. Comparing k-means cluster centers to components found by PCA and NMF\\n178 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 192}, page_content='Figure 3-31. Comparing image reconstructions using k-means, PCA, and NMF with 100\\ncomponents (or cluster centers)—k-means uses only a single cluster center per image\\nAn interesting aspect of vector quantization using k-means is that we can use many\\nmore clusters than input dimensions to encode our data. Let’s go back to the\\ntwo_moons data. Using PCA or NMF, there is nothing much we can do to this data, as\\nit lives in only two dimensions. Reducing it to one dimension with PCA or NMF\\nwould completely destroy the structure of the data. But we can find a more expressive\\nrepresentation with k-means, by using more cluster centers (see Figure 3-32):\\nClustering \\n| \\n179'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 193}, page_content='In[59]:\\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\\nkmeans = KMeans(n_clusters=10, random_state=0)\\nkmeans.fit(X)\\ny_pred = kmeans.predict(X)\\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, s=60, cmap=\\'Paired\\')\\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=60,\\n            marker=\\'^\\', c=range(kmeans.n_clusters), linewidth=2, cmap=\\'Paired\\')\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nprint(\"Cluster memberships:\\\\n{}\".format(y_pred))\\nOut[59]:\\nCluster memberships:\\n[9 2 5 4 2 7 9 6 9 6 1 0 2 6 1 9 3 0 3 1 7 6 8 6 8 5 2 7 5 8 9 8 6 5 3 7 0\\n 9 4 5 0 1 3 5 2 8 9 1 5 6 1 0 7 4 6 3 3 6 3 8 0 4 2 9 6 4 8 2 8 4 0 4 0 5\\n 6 4 5 9 3 0 7 8 0 7 5 8 9 8 0 7 3 9 7 1 7 2 2 0 4 5 6 7 8 9 4 5 4 1 2 3 1\\n 8 8 4 9 2 3 7 0 9 9 1 5 8 5 1 9 5 6 7 9 1 4 0 6 2 6 4 7 9 5 5 3 8 1 9 5 6\\n 3 5 0 2 9 3 0 8 6 0 3 3 5 6 3 2 0 2 3 0 2 6 3 4 4 1 5 6 7 1 1 3 2 4 7 2 7\\n 3 8 6 4 1 4 3 9 9 5 1 7 5 8 2]\\nFigure 3-32. Using many k-means clusters to cover the variation in a complex dataset\\n180 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 194}, page_content='4 In this case, “best” means that the sum of variances of the clusters is small.\\nWe used 10 cluster centers, which means each point is now assigned a number\\nbetween 0 and 9. We can see this as the data being represented using 10 components\\n(that is, we have 10 new features), with all features being 0, apart from the one that\\nrepresents the cluster center the point is assigned to. Using this 10-dimensional repre‐\\nsentation, it would now be possible to separate the two half-moon shapes using a lin‐\\near model, which would not have been possible using the original two features. It is\\nalso possible to get an even more expressive representation of the data by using the\\ndistances to each of the cluster centers as features. This can be accomplished using\\nthe transform method of kmeans:\\nIn[60]:\\ndistance_features = kmeans.transform(X)\\nprint(\"Distance feature shape: {}\".format(distance_features.shape))\\nprint(\"Distance features:\\\\n{}\".format(distance_features))\\nOut[60]:\\nDistance feature shape: (200, 10)\\nDistance features:\\n[[ 0.922  1.466  1.14  ...,  1.166  1.039  0.233]\\n [ 1.142  2.517  0.12  ...,  0.707  2.204  0.983]\\n [ 0.788  0.774  1.749 ...,  1.971  0.716  0.944]\\n ...,\\n [ 0.446  1.106  1.49  ...,  1.791  1.032  0.812]\\n [ 1.39   0.798  1.981 ...,  1.978  0.239  1.058]\\n [ 1.149  2.454  0.045 ...,  0.572  2.113  0.882]]\\nk-means is a very popular algorithm for clustering, not only because it is relatively\\neasy to understand and implement, but also because it runs relatively quickly. k-\\nmeans scales easily to large datasets, and scikit-learn even includes a more scalable\\nvariant in the MiniBatchKMeans class, which can handle very large datasets.\\nOne of the drawbacks of k-means is that it relies on a random initialization, which\\nmeans the outcome of the algorithm depends on a random seed. By default, scikit-\\nlearn runs the algorithm 10 times with 10 different random initializations, and\\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\\nassumptions made on the shape of clusters, and the requirement to specify the num‐\\nber of clusters you are looking for (which might not be known in a real-world\\napplication).\\nNext, we will look at two more clustering algorithms that improve upon these proper‐\\nties in some ways.\\nClustering \\n| \\n181'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 195}, page_content='Agglomerative Clustering\\nAgglomerative clustering refers to a collection of clustering algorithms that all build\\nupon the same principles: the algorithm starts by declaring each point its own cluster,\\nand then merges the two most similar clusters until some stopping criterion is satis‐\\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\\nso similar clusters are merged until only the specified number of clusters are left.\\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\\nmeasured. This measure is always defined between two existing clusters.\\nThe following three choices are implemented in scikit-learn:\\nward\\nThe default choice, ward picks the two clusters to merge such that the variance\\nwithin all clusters increases the least. This often leads to clusters that are rela‐\\ntively equally sized.\\naverage\\naverage linkage merges the two clusters that have the smallest average distance\\nbetween all their points.\\ncomplete\\ncomplete linkage (also known as maximum linkage) merges the two clusters that\\nhave the smallest maximum distance between their points.\\nward works on most datasets, and we will use it in our examples. If the clusters have\\nvery dissimilar numbers of members (if one is much bigger than all the others, for\\nexample), average or complete might work better.\\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\\ning on a two-dimensional dataset, looking for three clusters:\\nIn[61]:\\nmglearn.plots.plot_agglomerative_algorithm()\\n182 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 196}, page_content='5 We could also use the labels_ attribute, as we did for k-means.\\nFigure 3-33. Agglomerative clustering iteratively joins the two closest clusters\\nInitially, each point is its own cluster. Then, in each step, the two clusters that are\\nclosest are merged. In the first four steps, two single-point clusters are picked and\\nthese are joined into two-point clusters. In step 5, one of the two-point clusters is\\nextended to a third point, and so on. In step 9, there are only three clusters remain‐\\ning. As we specified that we are looking for three clusters, the algorithm then stops.\\nLet’s have a look at how agglomerative clustering performs on the simple three-\\ncluster data we used here. Because of the way the algorithm works, agglomerative\\nclustering cannot make predictions for new data points. Therefore, Agglomerative\\nClustering has no predict method. To build the model and get the cluster member‐\\nships on the training set, use the fit_predict method instead.5 The result is shown\\nin Figure 3-34:\\nIn[62]:\\nfrom sklearn.cluster import AgglomerativeClustering\\nX, y = make_blobs(random_state=1)\\nagg = AgglomerativeClustering(n_clusters=3)\\nassignment = agg.fit_predict(X)\\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nClustering \\n| \\n183'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 197}, page_content='Figure 3-34. Cluster assignment using agglomerative clustering with three clusters\\nAs expected, the algorithm recovers the clustering perfectly. While the scikit-learn\\nimplementation of agglomerative clustering requires you to specify the number of\\nclusters you want the algorithm to find, agglomerative clustering methods provide\\nsome help with choosing the right number, which we will discuss next.\\nHierarchical clustering and dendrograms\\nAgglomerative clustering produces what is known as a hierarchical clustering. The\\nclustering proceeds iteratively, and every point makes a journey from being a single\\npoint cluster to belonging to some final cluster. Each intermediate step provides a\\nclustering of the data (with a different number of clusters). It is sometimes helpful to\\nlook at all possible clusterings jointly. The next example (Figure 3-35) shows an over‐\\nlay of all the possible clusterings shown in Figure 3-33, providing some insight into\\nhow each cluster breaks up into smaller clusters:\\nIn[63]:\\nmglearn.plots.plot_agglomerative()\\n184 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 198}, page_content='Figure 3-35. Hierarchical cluster assignment (shown as lines) generated with agglomera‐\\ntive clustering, with numbered data points (cf. Figure 3-36)\\nWhile this visualization provides a very detailed view of the hierarchical clustering, it\\nrelies on the two-dimensional nature of the data and therefore cannot be used on\\ndatasets that have more than two features. There is, however, another tool to visualize\\nhierarchical clustering, called a dendrogram, that can handle multidimensional\\ndatasets.\\nUnfortunately, scikit-learn currently does not have the functionality to draw den‐\\ndrograms. However, you can generate them easily using SciPy. The SciPy clustering\\nalgorithms have a slightly different interface to the scikit-learn clustering algo‐\\nrithms. SciPy provides a function that takes a data array X and computes a linkage\\narray, which encodes hierarchical cluster similarities. We can then feed this linkage\\narray into the scipy dendrogram function to plot the dendrogram (Figure 3-36):\\nIn[64]:\\n# Import the dendrogram function and the ward clustering function from SciPy\\nfrom scipy.cluster.hierarchy import dendrogram, ward\\nX, y = make_blobs(random_state=0, n_samples=12)\\n# Apply the ward clustering to the data array X\\n# The SciPy ward function returns an array that specifies the distances\\n# bridged when performing agglomerative clustering\\nlinkage_array = ward(X)\\nClustering \\n| \\n185'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 199}, page_content='# Now we plot the dendrogram for the linkage_array containing the distances\\n# between clusters\\ndendrogram(linkage_array)\\n# Mark the cuts in the tree that signify two or three clusters\\nax = plt.gca()\\nbounds = ax.get_xbound()\\nax.plot(bounds, [7.25, 7.25], \\'--\\', c=\\'k\\')\\nax.plot(bounds, [4, 4], \\'--\\', c=\\'k\\')\\nax.text(bounds[1], 7.25, \\' two clusters\\', va=\\'center\\', fontdict={\\'size\\': 15})\\nax.text(bounds[1], 4, \\' three clusters\\', va=\\'center\\', fontdict={\\'size\\': 15})\\nplt.xlabel(\"Sample index\")\\nplt.ylabel(\"Cluster distance\")\\nFigure 3-36. Dendrogram of the clustering shown in Figure 3-35 with lines indicating\\nsplits into two and three clusters\\nThe dendrogram shows data points as points on the bottom (numbered from 0 to\\n11). Then, a tree is plotted with these points (representing single-point clusters) as the\\nleaves, and a new node parent is added for each two clusters that are joined.\\nReading from bottom to top, the data points 1 and 4 are joined first (as you could see\\nin Figure 3-33). Next, points 6 and 9 are joined into a cluster, and so on. At the top\\nlevel, there are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the\\nother consisting of points 1, 4, 3, 2, and 8. These correspond to the two largest clus‐\\nters in the lefthand side of the plot.\\n186 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 200}, page_content='The y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐\\nrithm two clusters get merged. The length of each branch also shows how far apart\\nthe merged clusters are. The longest branches in this dendrogram are the three lines\\nthat are marked by the dashed line labeled “three clusters.” That these are the longest\\nbranches indicates that going from three to two clusters meant merging some very\\nfar-apart points. We see this again at the top of the chart, where merging the two\\nremaining clusters into a single cluster again bridges a relatively large distance.\\nUnfortunately, agglomerative clustering still fails at separating complex shapes like\\nthe two_moons dataset. But the same is not true for the next algorithm we will look at,\\nDBSCAN.\\nDBSCAN\\nAnother very useful clustering algorithm is DBSCAN (which stands for “density-\\nbased spatial clustering of applications with noise”). The main benefits of DBSCAN\\nare that it does not require the user to set the number of clusters a priori, it can cap‐\\nture clusters of complex shapes, and it can identify points that are not part of any\\ncluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\\nstill scales to relatively large datasets.\\nDBSCAN works by identifying points that are in “crowded” regions of the feature\\nspace, where many data points are close together. These regions are referred to as\\ndense regions in feature space. The idea behind DBSCAN is that clusters form dense\\nregions of data, separated by regions that are relatively empty.\\nPoints that are within a dense region are called core samples (or core points), and they\\nare defined as follows. There are two parameters in DBSCAN: min_samples and eps.\\nIf there are at least min_samples many data points within a distance of eps to a given\\ndata point, that data point is classified as a core sample. Core samples that are closer\\nto each other than the distance eps are put into the same cluster by DBSCAN.\\nThe algorithm works by picking an arbitrary point to start with. It then finds all\\npoints with distance eps or less from that point. If there are less than min_samples\\npoints within distance eps of the starting point, this point is labeled as noise, meaning\\nthat it doesn’t belong to any cluster. If there are more than min_samples points within\\na distance of eps, the point is labeled a core sample and assigned a new cluster label.\\nThen, all neighbors (within eps) of the point are visited. If they have not been\\nassigned a cluster yet, they are assigned the new cluster label that was just created. If\\nthey are core samples, their neighbors are visited in turn, and so on. The cluster\\ngrows until there are no more core samples within distance eps of the cluster. Then\\nanother point that hasn’t yet been visited is picked, and the same procedure is\\nrepeated.\\nClustering \\n| \\n187'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 201}, page_content='In the end, there are three kinds of points: core points, points that are within distance\\neps of core points (called boundary points), and noise. When the DBSCAN algorithm\\nis run on a particular dataset multiple times, the clustering of the core points is always\\nthe same, and the same points will always be labeled as noise. However, a boundary\\npoint might be neighbor to core samples of more than one cluster. Therefore, the\\ncluster membership of boundary points depends on the order in which points are vis‐\\nited. Usually there are only few boundary points, and this slight dependence on the\\norder of points is not important.\\nLet’s apply DBSCAN on the synthetic dataset we used to demonstrate agglomerative\\nclustering. Like agglomerative clustering, DBSCAN does not allow predictions on\\nnew test data, so we will use the fit_predict method to perform clustering and\\nreturn the cluster labels in one step:\\nIn[65]:\\nfrom sklearn.cluster import DBSCAN\\nX, y = make_blobs(random_state=0, n_samples=12)\\ndbscan = DBSCAN()\\nclusters = dbscan.fit_predict(X)\\nprint(\"Cluster memberships:\\\\n{}\".format(clusters))\\nOut[65]:\\nCluster memberships:\\n[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\nAs you can see, all data points were assigned the label -1, which stands for noise. This\\nis a consequence of the default parameter settings for eps and min_samples, which\\nare not tuned for small toy datasets. The cluster assignments for different values of\\nmin_samples and eps are shown below, and visualized in Figure 3-37:\\nIn[66]:\\nmglearn.plots.plot_dbscan()\\nOut[66]:\\nmin_samples: 2 eps: 1.000000  cluster: [-1  0  0 -1  0 -1  1  1  0  1 -1 -1]\\nmin_samples: 2 eps: 1.500000  cluster: [0 1 1 1 1 0 2 2 1 2 2 0]\\nmin_samples: 2 eps: 2.000000  cluster: [0 1 1 1 1 0 0 0 1 0 0 0]\\nmin_samples: 2 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\\nmin_samples: 3 eps: 1.000000  cluster: [-1  0  0 -1  0 -1  1  1  0  1 -1 -1]\\nmin_samples: 3 eps: 1.500000  cluster: [0 1 1 1 1 0 2 2 1 2 2 0]\\nmin_samples: 3 eps: 2.000000  cluster: [0 1 1 1 1 0 0 0 1 0 0 0]\\nmin_samples: 3 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\\nmin_samples: 5 eps: 1.000000  cluster: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\nmin_samples: 5 eps: 1.500000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\\nmin_samples: 5 eps: 2.000000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\\nmin_samples: 5 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\\n188 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 202}, page_content='Figure 3-37. Cluster assignments found by DBSCAN with varying settings for the\\nmin_samples and eps parameters\\nIn this plot, points that belong to clusters are solid, while the noise points are shown\\nin white. Core samples are shown as large markers, while boundary points are dis‐\\nplayed as smaller markers. Increasing eps (going from left to right in the figure)\\nmeans that more points will be included in a cluster. This makes clusters grow, but\\nmight also lead to multiple clusters joining into one. Increasing min_samples (going\\nfrom top to bottom in the figure) means that fewer points will be core points, and\\nmore points will be labeled as noise.\\nThe parameter eps is somewhat more important, as it determines what it means for\\npoints to be “close.” Setting eps to be very small will mean that no points are core\\nsamples, and may lead to all points being labeled as noise. Setting eps to be very large\\nwill result in all points forming a single cluster.\\nThe min_samples setting mostly determines whether points in less dense regions will\\nbe labeled as outliers or as their own clusters. If you decrease min_samples, anything\\nthat would have been a cluster with less than min_samples many samples will now be\\nlabeled as noise. min_samples therefore determines the minimum cluster size. You\\ncan see this very clearly in Figure 3-37, when going from min_samples=3 to min_sam\\nples=5 with eps=1.5. With min_samples=3, there are three clusters: one of four\\nClustering \\n| \\n189'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 203}, page_content='points, one of five points, and one of three points. Using min_samples=5, the two\\nsmaller clusters (with three and four points) are now labeled as noise, and only the\\ncluster with five samples remains.\\nWhile DBSCAN doesn’t require setting the number of clusters explicitly, setting eps\\nimplicitly controls how many clusters will be found. Finding a good setting for eps is\\nsometimes easier after scaling the data using StandardScaler or MinMaxScaler, as\\nusing these scaling techniques will ensure that all features have similar ranges.\\nFigure 3-38 shows the result of running DBSCAN on the two_moons dataset. The\\nalgorithm actually finds the two half-circles and separates them using the default\\nsettings:\\nIn[67]:\\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\\n# rescale the data to zero mean and unit variance\\nscaler = StandardScaler()\\nscaler.fit(X)\\nX_scaled = scaler.transform(X)\\ndbscan = DBSCAN()\\nclusters = dbscan.fit_predict(X_scaled)\\n# plot the cluster assignments\\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm2, s=60)\\nplt.xlabel(\"Feature 0\")\\nplt.ylabel(\"Feature 1\")\\nAs the algorithm produced the desired number of clusters (two), the parameter set‐\\ntings seem to work well. If we decrease eps to 0.2 (from the default of 0.5), we will\\nget eight clusters, which is clearly too many. Increasing eps to 0.7 results in a single\\ncluster.\\nWhen using DBSCAN, you need to be careful about handling the returned cluster\\nassignments. The use of -1 to indicate noise might result in unexpected effects when\\nusing the cluster labels to index another array.\\n190 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 204}, page_content='Figure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\\nComparing and Evaluating Clustering Algorithms\\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\\nhow well an algorithm worked, and to compare outcomes between different algo‐\\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\\nand DBSCAN, we will now compare them on some real-world datasets.\\nEvaluating clustering with ground truth\\nThere are metrics that can be used to assess the outcome of a clustering algorithm\\nrelative to a ground truth clustering, the most important ones being the adjusted rand\\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\\ntative measure between 0 and 1.\\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\\nusing ARI. We also include what it looks like when we randomly assign points to two\\nclusters for comparison (see Figure 3-39):\\nClustering \\n| \\n191'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 205}, page_content='In[68]:\\nfrom sklearn.metrics.cluster import adjusted_rand_score\\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\\n# rescale the data to zero mean and unit variance\\nscaler = StandardScaler()\\nscaler.fit(X)\\nX_scaled = scaler.transform(X)\\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\n# make a list of algorithms to use\\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\\n              DBSCAN()]\\n# create a random cluster assignment for reference\\nrandom_state = np.random.RandomState(seed=0)\\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\\n# plot random assignment\\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\\n                cmap=mglearn.cm3, s=60)\\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\\n        adjusted_rand_score(y, random_clusters)))\\nfor ax, algorithm in zip(axes[1:], algorithms):\\n    # plot the cluster assignments and cluster centers\\n    clusters = algorithm.fit_predict(X_scaled)\\n    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters,\\n               cmap=mglearn.cm3, s=60)\\n    ax.set_title(\"{} - ARI: {:.2f}\".format(algorithm.__class__.__name__,\\n                                           adjusted_rand_score(y, clusters)))\\nFigure 3-39. Comparing random assignment, k-means, agglomerative clustering, and\\nDBSCAN on the two_moons dataset using the supervised ARI score\\nThe adjusted rand index provides intuitive results, with a random cluster assignment\\nhaving a score of 0 and DBSCAN (which recovers the desired clustering perfectly)\\nhaving a score of 1.\\n192 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 206}, page_content='A common mistake when evaluating clustering in this way is to use accuracy_score\\ninstead of adjusted_rand_score, normalized_mutual_info_score, or some other\\nclustering metric. The problem in using accuracy is that it requires the assigned clus‐\\nter labels to exactly match the ground truth. However, the cluster labels themselves\\nare meaningless—the only thing that matters is which points are in the same cluster:\\nIn[69]:\\nfrom sklearn.metrics import accuracy_score\\n# these two labelings of points correspond to the same clustering\\nclusters1 = [0, 0, 1, 1, 0]\\nclusters2 = [1, 1, 0, 0, 1]\\n# accuracy is zero, as none of the labels are the same\\nprint(\"Accuracy: {:.2f}\".format(accuracy_score(clusters1, clusters2)))\\n# adjusted rand score is 1, as the clustering is exactly the same\\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(clusters1, clusters2)))\\nOut[69]:\\nAccuracy: 0.00\\nARI: 1.00\\nEvaluating clustering without ground truth\\nAlthough we have just shown one way to evaluate clustering algorithms, in practice,\\nthere is a big problem with using measures like ARI. When applying clustering algo‐\\nrithms, there is usually no ground truth to which to compare the results. If we knew\\nthe right clustering of the data, we could use this information to build a supervised\\nmodel like a classifier. Therefore, using metrics like ARI and NMI usually only helps\\nin developing algorithms, not in assessing success in an application.\\nThere are scoring metrics for clustering that don’t require ground truth, like the sil‐\\nhouette coefficient. However, these often don’t work well in practice. The silhouette\\nscore computes the compactness of a cluster, where higher is better, with a perfect\\nscore of 1. While compact clusters are good, compactness doesn’t allow for complex\\nshapes.\\nHere is an example comparing the outcome of k-means, agglomerative clustering,\\nand DBSCAN on the two-moons dataset using the silhouette score (Figure 3-40):\\nIn[70]:\\nfrom sklearn.metrics.cluster import silhouette_score\\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\\n# rescale the data to zero mean and unit variance\\nscaler = StandardScaler()\\nscaler.fit(X)\\nX_scaled = scaler.transform(X)\\nClustering \\n| \\n193'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 207}, page_content='fig, axes = plt.subplots(1, 4, figsize=(15, 3),\\n                         subplot_kw={\\'xticks\\': (), \\'yticks\\': ()})\\n# create a random cluster assignment for reference\\nrandom_state = np.random.RandomState(seed=0)\\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\\n# plot random assignment\\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\\n    cmap=mglearn.cm3, s=60)\\naxes[0].set_title(\"Random assignment: {:.2f}\".format(\\n    silhouette_score(X_scaled, random_clusters)))\\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\\n              DBSCAN()]\\nfor ax, algorithm in zip(axes[1:], algorithms):\\n    clusters = algorithm.fit_predict(X_scaled)\\n    # plot the cluster assignments and cluster centers\\n    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm3,\\n               s=60)\\n    ax.set_title(\"{} : {:.2f}\".format(algorithm.__class__.__name__,\\n                                      silhouette_score(X_scaled, clusters)))\\nFigure 3-40. Comparing random assignment, k-means, agglomerative clustering, and\\nDBSCAN on the two_moons dataset using the unsupervised silhouette score—the more\\nintuitive result of DBSCAN has a lower silhouette score than the assignments found by\\nk-means\\nAs you can see, k-means gets the highest silhouette score, even though we might pre‐\\nfer the result produced by DBSCAN. A slightly better strategy for evaluating clusters\\nis using robustness-based clustering metrics. These run an algorithm after adding\\nsome noise to the data, or using different parameter settings, and compare the out‐\\ncomes. The idea is that if many algorithm parameters and many perturbations of the\\ndata return the same result, it is likely to be trustworthy. Unfortunately, this strategy is\\nnot implemented in scikit-learn at the time of writing.\\nEven if we get a very robust clustering, or a very high silhouette score, we still don’t\\nknow if there is any semantic meaning in the clustering, or whether the clustering\\n194 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 208}, page_content='reflects an aspect of the data that we are interested in. Let’s go back to the example of\\nface images. We hope to find groups of similar faces—say, men and women, or old\\npeople and young people, or people with beards and without. Let’s say we cluster the\\ndata into two clusters, and all algorithms agree about which points should be clus‐\\ntered together. We still don’t know if the clusters that are found correspond in any\\nway to the concepts we are interested in. It could be that they found side views versus\\nfront views, or pictures taken at night versus pictures taken during the day, or pic‐\\ntures taken with iPhones versus pictures taken with Android phones. The only way to\\nknow whether the clustering corresponds to anything we are interested in is to ana‐\\nlyze the clusters manually.\\nComparing algorithms on the faces dataset\\nLet’s apply the k-means, DBSCAN, and agglomerative clustering algorithms to the\\nLabeled Faces in the Wild dataset, and see if any of them find interesting structure.\\nWe will use the eigenface representation of the data, as produced by\\nPCA(whiten=True), with 100 components:\\nIn[71]:\\n# extract eigenfaces from lfw data and transform data\\nfrom sklearn.decomposition import PCA\\npca = PCA(n_components=100, whiten=True, random_state=0)\\npca.fit_transform(X_people)\\nX_pca = pca.transform(X_people)\\nWe saw earlier that this is a more semantic representation of the face images than the\\nraw pixels. It will also make computation faster. A good exercise would be for you to\\nrun the following experiments on the original data, without PCA, and see if you find\\nsimilar clusters.\\nAnalyzing the faces dataset with DBSCAN.    We will start by applying DBSCAN, which we\\njust discussed:\\nIn[72]:\\n# apply DBSCAN with default parameters\\ndbscan = DBSCAN()\\nlabels = dbscan.fit_predict(X_pca)\\nprint(\"Unique labels: {}\".format(np.unique(labels)))\\nOut[72]:\\nUnique labels: [-1]\\nWe see that all the returned labels are –1, so all of the data was labeled as “noise” by\\nDBSCAN. There are two things we can change to help this: we can make eps higher,\\nto expand the neighborhood of each point, and set min_samples lower, to consider\\nsmaller groups of points as clusters. Let’s try changing min_samples first:\\nClustering \\n| \\n195'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 209}, page_content='In[73]:\\ndbscan = DBSCAN(min_samples=3)\\nlabels = dbscan.fit_predict(X_pca)\\nprint(\"Unique labels: {}\".format(np.unique(labels)))\\nOut[73]:\\nUnique labels: [-1]\\nEven when considering groups of three points, everything is labeled as noise. So, we\\nneed to increase eps:\\nIn[74]:\\ndbscan = DBSCAN(min_samples=3, eps=15)\\nlabels = dbscan.fit_predict(X_pca)\\nprint(\"Unique labels: {}\".format(np.unique(labels)))\\nOut[74]:\\nUnique labels: [-1  0]\\nUsing a much larger eps of 15, we get only a single cluster and noise points. We can\\nuse this result to find out what the “noise” looks like compared to the rest of the data.\\nTo understand better what’s happening, let’s look at how many points are noise, and\\nhow many points are inside the cluster:\\nIn[75]:\\n# Count number of points in all clusters and noise.\\n# bincount doesn\\'t allow negative numbers, so we need to add 1.\\n# The first number in the result corresponds to noise points.\\nprint(\"Number of points per cluster: {}\".format(np.bincount(labels + 1)))\\nOut[75]:\\nNumber of points per cluster: [  27 2036]\\nThere are very few noise points—only 27—so we can look at all of them (see\\nFigure 3-41):\\nIn[76]:\\nnoise = X_people[labels==-1]\\nfig, axes = plt.subplots(3, 9, subplot_kw={\\'xticks\\': (), \\'yticks\\': ()},\\n                         figsize=(12, 4))\\nfor image, ax in zip(noise, axes.ravel()):\\n    ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\\n196 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 210}, page_content='Figure 3-41. Samples from the faces dataset labeled as noise by DBSCAN\\nComparing these images to the random sample of face images from Figure 3-7, we\\ncan guess why they were labeled as noise: the fifth image in the first row shows a per‐\\nson drinking from a glass, there are images of people wearing hats, and in the last\\nimage there’s a hand in front of the person’s face. The other images contain odd angles\\nor crops that are too close or too wide.\\nThis kind of analysis—trying to find “the odd one out”—is called outlier detection. If\\nthis was a real application, we might try to do a better job of cropping images, to get\\nmore homogeneous data. There is little we can do about people in photos sometimes\\nwearing hats, drinking, or holding something in front of their faces, but it’s good to\\nknow that these are issues in the data that any algorithm we might apply needs to\\nhandle.\\nIf we want to find more interesting clusters than just one large one, we need to set eps\\nsmaller, somewhere between 15 and 0.5 (the default). Let’s have a look at what differ‐\\nent values of eps result in:\\nIn[77]:\\nfor eps in [1, 3, 5, 7, 9, 11, 13]:\\n    print(\"\\\\neps={}\".format(eps))\\n    dbscan = DBSCAN(eps=eps, min_samples=3)\\n    labels = dbscan.fit_predict(X_pca)\\n    print(\"Clusters present: {}\".format(np.unique(labels)))\\n    print(\"Cluster sizes: {}\".format(np.bincount(labels + 1)))\\nOut[78]:\\neps=1\\nClusters present: [-1]\\nCluster sizes: [2063]\\neps=3\\nClusters present: [-1]\\nCluster sizes: [2063]\\nClustering \\n| \\n197'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 211}, page_content=\"eps=5\\nClusters present: [-1]\\nCluster sizes: [2063]\\neps=7\\nClusters present: [-1  0  1  2  3  4  5  6  7  8  9 10 11 12]\\nCluster sizes: [2006  4  6  6  6  9  3  3  4  3  3  3  3  4]\\neps=9\\nClusters present: [-1  0  1  2]\\nCluster sizes: [1269  788    3    3]\\neps=11\\nClusters present: [-1  0]\\nCluster sizes: [ 430 1633]\\neps=13\\nClusters present: [-1  0]\\nCluster sizes: [ 112 1951]\\nFor low settings of eps, all points are labeled as noise. For eps=7, we get many noise\\npoints and many smaller clusters. For eps=9 we still get many noise points, but we get\\none big cluster and some smaller clusters. Starting from eps=11, we get only one large\\ncluster and noise.\\nWhat is interesting to note is that there is never more than one large cluster. At most,\\nthere is one large cluster containing most of the points, and some smaller clusters.\\nThis indicates that there are not two or three different kinds of face images in the data\\nthat are very distinct, but rather that all images are more or less equally similar to (or\\ndissimilar from) the rest.\\nThe results for eps=7 look most interesting, with many small clusters. We can investi‐\\ngate this clustering in more detail by visualizing all of the points in each of the 13\\nsmall clusters (Figure 3-42):\\nIn[78]:\\ndbscan = DBSCAN(min_samples=3, eps=7)\\nlabels = dbscan.fit_predict(X_pca)\\nfor cluster in range(max(labels) + 1):\\n    mask = labels == cluster\\n    n_images =  np.sum(mask)\\n    fig, axes = plt.subplots(1, n_images, figsize=(n_images * 1.5, 4),\\n                             subplot_kw={'xticks': (), 'yticks': ()})\\n    for image, label, ax in zip(X_people[mask], y_people[mask], axes):\\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\\n        ax.set_title(people.target_names[label].split()[-1])\\n198 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 212}, page_content='Figure 3-42. Clusters found by DBSCAN with eps=7\\nSome of the clusters correspond to people with very distinct faces (within this data‐\\nset), such as Sharon or Koizumi. Within each cluster, the orientation of the face is also\\nClustering \\n| \\n199'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 213}, page_content='quite fixed, as well as the facial expression. Some of the clusters contain faces of mul‐\\ntiple people, but they share a similar orientation and expression.\\nThis concludes our analysis of the DBSCAN algorithm applied to the faces dataset. As\\nyou can see, we are doing a manual analysis here, different from the much more auto‐\\nmatic search approach we could use for supervised learning based on R2 score or\\naccuracy.\\nLet’s move on to applying k-means and agglomerative clustering.\\nAnalyzing the faces dataset with k-means.    We saw that it was not possible to create\\nmore than one big cluster using DBSCAN. Agglomerative clustering and k-means are\\nmuch more likely to create clusters of even size, but we do need to set a target num‐\\nber of clusters. We could set the number of clusters to the known number of people in\\nthe dataset, though it is very unlikely that an unsupervised clustering algorithm will\\nrecover them. Instead, we can start with a low number of clusters, like 10, which\\nmight allow us to analyze each of the clusters:\\nIn[79]:\\n# extract clusters with k-means\\nkm = KMeans(n_clusters=10, random_state=0)\\nlabels_km = km.fit_predict(X_pca)\\nprint(\"Cluster sizes k-means: {}\".format(np.bincount(labels_km)))\\nOut[79]:\\nCluster sizes k-means: [269 128 170 186 386 222 237  64 253 148]\\nAs you can see, k-means clustering partitioned the data into relatively similarly sized\\nclusters from 64 to 386. This is quite different from the result of DBSCAN.\\nWe can further analyze the outcome of k-means by visualizing the cluster centers\\n(Figure 3-43). As we clustered in the representation produced by PCA, we need to\\nrotate the cluster centers back into the original space to visualize them, using\\npca.inverse_transform:\\nIn[80]:\\nfig, axes = plt.subplots(2, 5, subplot_kw={\\'xticks\\': (), \\'yticks\\': ()},\\n                         figsize=(12, 4))\\nfor center, ax in zip(km.cluster_centers_, axes.ravel()):\\n    ax.imshow(pca.inverse_transform(center).reshape(image_shape),\\n              vmin=0, vmax=1)\\n200 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 214}, page_content='Figure 3-43. Cluster centers found by k-means when setting the number of clusters to 10\\nThe cluster centers found by k-means are very smooth versions of faces. This is not\\nvery surprising, given that each center is an average of 64 to 386 face images. Working\\nwith a reduced PCA representation adds to the smoothness of the images (compared\\nto the faces reconstructed using 100 PCA dimensions in Figure 3-11). The clustering\\nseems to pick up on different orientations of the face, different expressions (the third\\ncluster center seems to show a smiling face), and the presence of shirt collars (see the\\nsecond-to-last cluster center).\\nFor a more detailed view, in Figure 3-44 we show for each cluster center the five most\\ntypical images in the cluster (the images assigned to the cluster that are closest to the\\ncluster center) and the five most atypical images in the cluster (the images assigned to\\nthe cluster that are furthest from the cluster center):\\nIn[81]:\\nmglearn.plots.plot_kmeans_faces(km, pca, X_pca, X_people,\\n                                y_people, people.target_names)\\nClustering \\n| \\n201'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 215}, page_content='Figure 3-44. Sample images for each cluster found by k-means—the cluster centers are\\non the left, followed by the five closest points to each center and the five points that are\\nassigned to the cluster but are furthest away from the center\\n202 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 216}, page_content='Figure 3-44 confirms our intuition about smiling faces for the third cluster, and also\\nthe importance of orientation for the other clusters. The “atypical” points are not very\\nsimilar to the cluster centers, though, and their assignment seems somewhat arbi‐\\ntrary. This can be attributed to the fact that k-means partitions all the data points and\\ndoesn’t have a concept of “noise” points, as DBSCAN does. Using a larger number of\\nclusters, the algorithm could find finer distinctions. However, adding more clusters\\nmakes manual inspection even harder.\\nAnalyzing the faces dataset with agglomerative clustering.    Now, let’s look at the results of\\nagglomerative clustering:\\nIn[82]:\\n# extract clusters with ward agglomerative clustering\\nagglomerative = AgglomerativeClustering(n_clusters=10)\\nlabels_agg = agglomerative.fit_predict(X_pca)\\nprint(\"Cluster sizes agglomerative clustering: {}\".format(\\n    np.bincount(labels_agg)))\\nOut[82]:\\nCluster sizes agglomerative clustering: [255 623  86 102 122 199 265  26 230 155]\\nAgglomerative clustering also produces relatively equally sized clusters, with cluster\\nsizes between 26 and 623. These are more uneven than those produced by k-means,\\nbut much more even than the ones produced by DBSCAN.\\nWe can compute the ARI to measure whether the two partitions of the data given by\\nagglomerative clustering and k-means are similar:\\nIn[83]:\\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(labels_agg, labels_km)))\\nOut[83]:\\nARI: 0.13\\nAn ARI of only 0.13 means that the two clusterings labels_agg and labels_km have\\nlittle in common. This is not very surprising, given the fact that points further away\\nfrom the cluster centers seem to have little in common for k-means.\\nNext, we might want to plot the dendrogram (Figure 3-45). We’ll limit the depth of\\nthe tree in the plot, as branching down to the individual 2,063 data points would\\nresult in an unreadably dense plot:\\nClustering \\n| \\n203'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 217}, page_content='In[84]:\\nlinkage_array = ward(X_pca)\\n# now we plot the dendrogram for the linkage_array\\n# containing the distances between clusters\\nplt.figure(figsize=(20, 5))\\ndendrogram(linkage_array, p=7, truncate_mode=\\'level\\', no_labels=True)\\nplt.xlabel(\"Sample index\")\\nplt.ylabel(\"Cluster distance\")\\nFigure 3-45. Dendrogram of agglomerative clustering on the faces dataset\\nCreating 10 clusters, we cut across the tree at the very top, where there are 10 vertical\\nlines. In the dendrogram for the toy data shown in Figure 3-36, you could see by the\\nlength of the branches that two or three clusters might capture the data appropriately.\\nFor the faces data, there doesn’t seem to be a very natural cutoff point. There are\\nsome branches that represent more distinct groups, but there doesn’t appear to be a\\nparticular number of clusters that is a good fit. This is not surprising, given the results\\nof DBSCAN, which tried to cluster all points together.\\nLet’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that\\nthere is no notion of cluster center in agglomerative clustering (though we could\\ncompute the mean), and we simply show the first couple of points in each cluster. We\\nshow the number of points in each cluster to the left of the first image:\\nIn[85]:\\nn_clusters = 10\\nfor cluster in range(n_clusters):\\n    mask = labels_agg == cluster\\n    fig, axes = plt.subplots(1, 10, subplot_kw={\\'xticks\\': (), \\'yticks\\': ()},\\n                             figsize=(15, 8))\\n    axes[0].set_ylabel(np.sum(mask))\\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\\n                                      labels_agg[mask], axes):\\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\\n        ax.set_title(people.target_names[label].split()[-1],\\n                     fontdict={\\'fontsize\\': 9})\\n204 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 218}, page_content='Figure 3-46. Random images from the clusters generated by In[82]—each row corre‐\\nsponds to one cluster; the number to the left lists the number of images in each cluster\\nClustering \\n| \\n205'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 219}, page_content='While some of the clusters seem to have a semantic theme, many of them are too\\nlarge to be actually homogeneous. To get more homogeneous clusters, we can run the\\nalgorithm again, this time with 40 clusters, and pick out some of the clusters that are\\nparticularly interesting (Figure 3-47):\\nIn[86]:\\n# extract clusters with ward agglomerative clustering\\nagglomerative = AgglomerativeClustering(n_clusters=40)\\nlabels_agg = agglomerative.fit_predict(X_pca)\\nprint(\"cluster sizes agglomerative clustering: {}\".format(np.bincount(labels_agg)))\\nn_clusters = 40\\nfor cluster in [10, 13, 19, 22, 36]: # hand-picked \"interesting\" clusters\\n    mask = labels_agg == cluster\\n    fig, axes = plt.subplots(1, 15, subplot_kw={\\'xticks\\': (), \\'yticks\\': ()},\\n                             figsize=(15, 8))\\n    cluster_size = np.sum(mask)\\n    axes[0].set_ylabel(\"#{}: {}\".format(cluster, cluster_size))\\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\\n                                      labels_agg[mask], axes):\\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\\n        ax.set_title(people.target_names[label].split()[-1],\\n                     fontdict={\\'fontsize\\': 9})\\n    for i in range(cluster_size, 15):\\n        axes[i].set_visible(False)\\nOut[86]:\\ncluster sizes agglomerative clustering:\\n [ 58  80  79  40 222  50  55  78 172  28  26  34  14  11  60  66 152  27\\n  47  31  54   5   8  56   3   5   8  18  22  82  37  89  28  24  41  40\\n  21  10 113  69]\\n206 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 220}, page_content='Figure 3-47. Images from selected clusters found by agglomerative clustering when set‐\\nting the number of clusters to 40—the text to the left shows the index of the cluster and\\nthe total number of points in the cluster\\nHere, the clustering seems to have picked up on “dark skinned and smiling,” “collared\\nshirt,” “smiling woman,” “Hussein,” and “high forehead.” We could also find these\\nhighly similar clusters using the dendrogram, if we did more a detailed analysis.\\nSummary of Clustering Methods\\nThis section has shown that applying and evaluating clustering is a highly qualitative\\nprocedure, and often most helpful in the exploratory phase of data analysis. We\\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\\ning. All three have a way of controlling the granularity of clustering. k-means and\\nagglomerative clustering allow you to specify the number of desired clusters, while\\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\\natively easy to understand, and allow for clustering into many clusters.\\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\\nposition method, where each data point is represented by its cluster center. DBSCAN\\nallows for the detection of “noise points” that are not assigned any cluster, and it can\\nhelp automatically determine the number of clusters. In contrast to the other two\\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\\npartitions of the data, which can be easily inspected via dendrograms.\\nClustering \\n| \\n207'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 221}, page_content='Summary and Outlook\\nThis chapter introduced a range of unsupervised learning algorithms that can be\\napplied for exploratory data analysis and preprocessing. Having the right representa‐\\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\\nand preprocessing and decomposition methods play an important part in data prepa‐\\nration.\\nDecomposition, manifold learning, and clustering are essential tools to further your\\nunderstanding of your data, and can be the only ways to make sense of your data in\\nthe absence of supervision information. Even in a supervised setting, exploratory\\ntools are important for a better understanding of the properties of the data. Often it is\\nhard to quantify the usefulness of an unsupervised algorithm, though this shouldn’t\\ndeter you from using them to gather insights from your data. With these methods\\nunder your belt, you are now equipped with all the essential learning algorithms that\\nmachine learning practitioners use every day.\\nWe encourage you to try clustering and decomposition methods both on two-\\ndimensional toy data and on real-world datasets included in scikit-learn, like the\\ndigits, iris, and cancer datasets.\\n208 \\n| \\nChapter 3: Unsupervised Learning and Preprocessing'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 222}, page_content='Summary of the Estimator Interface\\nLet’s briefly review the API that we introduced in Chapters 2 and 3. All algorithms in\\nscikit-learn, whether preprocessing, supervised learning, or unsupervised learning\\nalgorithms, are implemented as classes. These classes are called estimators in scikit-\\nlearn. To apply an algorithm, you first have to instantiate an object of the particular\\nclass:\\nIn[87]:\\nfrom sklearn.linear_model import LogisticRegression\\nlogreg = LogisticRegression()\\nThe estimator class contains the algorithm, and also stores the model that is learned\\nfrom data using the algorithm.\\nYou should set any parameters of the model when constructing the model object.\\nThese parameters include regularization, complexity control, number of clusters to\\nfind, etc. All estimators have a fit method, which is used to build the model. The fit\\nmethod always requires as its first argument the data X, represented as a NumPy array\\nor a SciPy sparse matrix, where each row represents a single data point. The data X is\\nalways assumed to be a NumPy array or SciPy sparse matrix that has continuous\\n(floating-point) entries. Supervised algorithms also require a y argument, which is a\\none-dimensional NumPy array containing target values for regression or classifica‐\\ntion (i.e., the known output labels or responses).\\nThere are two main ways to apply a learned model in scikit-learn. To create a pre‐\\ndiction in the form of a new output like y, you use the predict method. To create a\\nnew representation of the input data X, you use the transform method. Table 3-1\\nsummarizes the use cases of the predict and transform methods.\\nTable 3-1. scikit-learn API summary\\nestimator.fit(x_train, [y_train])\\nestimator.predict(X_text)\\nestimator.transform(X_test)\\nClassification\\nPreprocessing\\nRegression\\nDimensionality reduction\\nClustering\\nFeature extraction\\n \\nFeature selection\\nAdditionally, all supervised models have a score(X_test, y_test) method that\\nallows an evaluation of the model. In Table 3-1, X_train and y_train refer to the\\ntraining data and training labels, while X_test and y_test refer to the test data and\\ntest labels (if applicable).\\nSummary and Outlook \\n| \\n209'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 223}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 224}, page_content='CHAPTER 4\\nRepresenting Data and\\nEngineering Features\\nSo far, we’ve assumed that our data comes in as a two-dimensional array of floating-\\npoint numbers, where each column is a continuous feature that describes the data\\npoints. For many applications, this is not how the data is collected. A particularly\\ncommon type of feature is the categorical features. Also known as discrete features,\\nthese are usually not numeric. The distinction between categorical features and con‐\\ntinuous features is analogous to the distinction between classification and regression,\\nonly on the input side rather than the output side. Examples of continuous features\\nthat we have seen are pixel brightnesses and size measurements of plant flowers.\\nExamples of categorical features are the brand of a product, the color of a product, or\\nthe department (books, clothing, hardware) it is sold in. These are all properties that\\ncan describe a product, but they don’t vary in a continuous way. A product belongs\\neither in the clothing department or in the books department. There is no middle\\nground between books and clothing, and no natural order for the different categories\\n(books is not greater or less than clothing, hardware is not between books and cloth‐\\ning, etc.).\\nRegardless of the types of features your data consists of, how you represent them can\\nhave an enormous effect on the performance of machine learning models. We saw in\\nChapters 2 and 3 that scaling of the data is important. In other words, if you don’t\\nrescale your data (say, to unit variance), then it makes a difference whether you repre‐\\nsent a measurement in centimeters or inches. We also saw in Chapter 2 that it can be\\nhelpful to augment your data with additional features, like adding interactions (prod‐\\nucts) of features or more general polynomials.\\nThe question of how to represent your data best for a particular application is known\\nas feature engineering, and it is one of the main tasks of data scientists and machine\\n211'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 225}, page_content='learning practitioners trying to solve real-world problems. Representing your data in\\nthe right way can have a bigger influence on the performance of a supervised model\\nthan the exact parameters you choose.\\nIn this chapter, we will first go over the important and very common case of categori‐\\ncal features, and then give some examples of helpful transformations for specific\\ncombinations of features and models.\\nCategorical Variables\\nAs an example, we will use the dataset of adult incomes in the United States, derived\\nfrom the 1994 census database. The task of the adult dataset is to predict whether a\\nworker has an income of over $50,000 or under $50,000. The features in this dataset\\ninclude the workers’ ages, how they are employed (self employed, private industry\\nemployee, government employee, etc.), their education, their gender, their working\\nhours per week, occupation, and more. Table 4-1 shows the first few entries in the\\ndataset.\\nTable 4-1. The first few entries in the adult dataset\\nage\\nworkclass\\neducation\\ngender hours-per-week occupation\\nincome\\n0\\n39\\nState-gov\\nBachelors\\nMale\\n40\\nAdm-clerical\\n<=50K\\n1\\n50\\nSelf-emp-not-inc\\nBachelors\\nMale\\n13\\nExec-managerial\\n<=50K\\n2\\n38\\nPrivate\\nHS-grad\\nMale\\n40\\nHandlers-cleaners <=50K\\n3\\n53\\nPrivate\\n11th\\nMale\\n40\\nHandlers-cleaners <=50K\\n4\\n28\\nPrivate\\nBachelors\\nFemale\\n40\\nProf-specialty\\n<=50K\\n5\\n37\\nPrivate\\nMasters\\nFemale\\n40\\nExec-managerial\\n<=50K\\n6\\n49\\nPrivate\\n9th\\nFemale\\n16\\nOther-service\\n<=50K\\n7\\n52\\nSelf-emp-not-inc\\nHS-grad\\nMale\\n45\\nExec-managerial\\n>50K\\n8\\n31\\nPrivate\\nMasters\\nFemale\\n50\\nProf-specialty\\n>50K\\n9\\n42\\nPrivate\\nBachelors\\nMale\\n40\\nExec-managerial\\n>50K\\n10 37\\nPrivate\\nSome-college Male\\n80\\nExec-managerial\\n>50K\\nThe task is phrased as a classification task with the two classes being income <=50k\\nand >50k. It would also be possible to predict the exact income, and make this a\\nregression task. However, that would be much more difficult, and the 50K division is\\ninteresting to understand on its own.\\nIn this dataset, age and hours-per-week are continuous features, which we know\\nhow to treat. The workclass, education, sex, and occupation features are categori‐\\ncal, however. All of them come from a fixed list of possible values, as opposed to a\\nrange, and denote a qualitative property, as opposed to a quantity.\\n212 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 226}, page_content='As a starting point, let’s say we want to learn a logistic regression classifier on this\\ndata. We know from Chapter 2 that a logistic regression makes predictions, ŷ, using\\nthe following formula:\\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\\nwhere w[i] and b are coefficients learned from the training set and x[i] are the input\\nfeatures. This formula makes sense when x[i] are numbers, but not when x[2] is\\n\"Masters\" or \"Bachelors\". Clearly we need to represent our data in some different\\nway when applying logistic regression. The next section will explain how we can\\novercome this problem.\\nOne-Hot-Encoding (Dummy Variables)\\nBy far the most common way to represent categorical variables is using the one-hot-\\nencoding or one-out-of-N encoding, also known as dummy variables. The idea behind\\ndummy variables is to replace a categorical variable with one or more new features\\nthat can have the values 0 and 1. The values 0 and 1 make sense in the formula for\\nlinear binary classification (and for all other models in scikit-learn), and we can\\nrepresent any number of categories by introducing one new feature per category, as\\ndescribed here.\\nLet’s say for the workclass feature we have possible values of \"Government\\nEmployee\", \"Private Employee\", \"Self Employed\", and \"Self Employed Incorpo\\nrated\". To encode these four possible values, we create four new features, called \"Gov\\nernment Employee\", \"Private Employee\", \"Self Employed\", and \"Self Employed\\nIncorporated\". A feature is 1 if workclass for this person has the corresponding\\nvalue and 0 otherwise, so exactly one of the four new features will be 1 for each data\\npoint. This is why this is called one-hot or one-out-of-N encoding.\\nThe principle is illustrated in Table 4-2. A single feature is encoded using four new\\nfeatures. When using this data in a machine learning algorithm, we would drop the\\noriginal workclass feature and only keep the 0–1 features.\\nTable 4-2. Encoding the workclass feature using one-hot encoding\\nworkclass\\nGovernment Employee\\nPrivate Employee\\nSelf Employed\\nSelf Employed Incorporated\\nGovernment Employee\\n1\\n0\\n0\\n0\\nPrivate Employee\\n0\\n1\\n0\\n0\\nSelf Employed\\n0\\n0\\n1\\n0\\nSelf Employed Incorporated 0\\n0\\n0\\n1\\nCategorical Variables \\n| \\n213'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 227}, page_content='The one-hot encoding we use is quite similar, but not identical, to\\nthe dummy encoding used in statistics. For simplicity, we encode\\neach category with a different binary feature. In statistics, it is com‐\\nmon to encode a categorical feature with k different possible values\\ninto k–1 features (the last one is represented as all zeros). This is\\ndone to simplify the analysis (more technically, this will avoid mak‐\\ning the data matrix rank-deficient).\\nThere are two ways to convert your data to a one-hot encoding of categorical vari‐\\nables, using either pandas or scikit-learn. At the time of writing, using pandas is\\nslightly easier, so let’s go this route. First we load the data using pandas from a\\ncomma-separated values (CSV) file:\\nIn[2]:\\nimport pandas as pd\\n# The file has no headers naming the columns, so we pass header=None\\n# and provide the column names explicitly in \"names\"\\ndata = pd.read_csv(\\n    \"/home/andy/datasets/adult.data\", header=None, index_col=False,\\n    names=[\\'age\\', \\'workclass\\', \\'fnlwgt\\', \\'education\\',  \\'education-num\\',\\n           \\'marital-status\\', \\'occupation\\', \\'relationship\\', \\'race\\', \\'gender\\',\\n           \\'capital-gain\\', \\'capital-loss\\', \\'hours-per-week\\', \\'native-country\\',\\n           \\'income\\'])\\n# For illustration purposes, we only select some of the columns\\ndata = data[[\\'age\\', \\'workclass\\', \\'education\\', \\'gender\\', \\'hours-per-week\\',\\n             \\'occupation\\', \\'income\\']]\\n# IPython.display allows nice output formatting within the Jupyter notebook\\ndisplay(data.head())\\nTable 4-3 shows the result.\\nTable 4-3. The first five rows of the adult dataset\\nage\\nworkclass\\neducation gender hours-per-week occupation\\nincome\\n0\\n39\\nState-gov\\nBachelors\\nMale\\n40\\nAdm-clerical\\n<=50K\\n1\\n50\\nSelf-emp-not-inc\\nBachelors\\nMale\\n13\\nExec-managerial\\n<=50K\\n2\\n38\\nPrivate\\nHS-grad\\nMale\\n40\\nHandlers-cleaners <=50K\\n3\\n53\\nPrivate\\n11th\\nMale\\n40\\nHandlers-cleaners <=50K\\n4\\n28\\nPrivate\\nBachelors\\nFemale\\n40\\nProf-specialty\\n<=50K\\nChecking string-encoded categorical data\\nAfter reading a dataset like this, it is often good to first check if a column actually\\ncontains meaningful categorical data. When working with data that was input by\\nhumans (say, users on a website), there might not be a fixed set of categories, and dif‐\\nferences in spelling and capitalization might require preprocessing. For example, it\\nmight be that some people specified gender as “male” and some as “man,” and we\\n214 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 228}, page_content='might want to represent these two inputs using the same category. A good way to\\ncheck the contents of a column is using the value_counts function of a pandas\\nSeries (the type of a single column in a DataFrame), to show us what the unique val‐\\nues are and how often they appear:\\nIn[3]:\\nprint(data.gender.value_counts())\\nOut[3]:\\n Male      21790\\n Female    10771\\nName: gender, dtype: int64\\nWe can see that there are exactly two values for gender in this dataset, Male and\\nFemale, meaning the data is already in a good format to be represented using one-\\nhot-encoding. In a real application, you should look at all columns and check their\\nvalues. We will skip this here for brevity’s sake.\\nThere is a very simple way to encode the data in pandas, using the get_dummies func‐\\ntion. The get_dummies function automatically transforms all columns that have\\nobject type (like strings) or are categorical (which is a special pandas concept that we\\nhaven’t talked about yet):\\nIn[4]:\\nprint(\"Original features:\\\\n\", list(data.columns), \"\\\\n\")\\ndata_dummies = pd.get_dummies(data)\\nprint(\"Features after get_dummies:\\\\n\", list(data_dummies.columns))\\nOut[4]:\\nOriginal features:\\n [\\'age\\', \\'workclass\\', \\'education\\', \\'gender\\', \\'hours-per-week\\', \\'occupation\\',\\n  \\'income\\']\\nFeatures after get_dummies:\\n [\\'age\\', \\'hours-per-week\\', \\'workclass_ ?\\', \\'workclass_ Federal-gov\\',\\n  \\'workclass_ Local-gov\\', \\'workclass_ Never-worked\\', \\'workclass_ Private\\',\\n  \\'workclass_ Self-emp-inc\\', \\'workclass_ Self-emp-not-inc\\',\\n  \\'workclass_ State-gov\\', \\'workclass_ Without-pay\\', \\'education_ 10th\\',\\n  \\'education_ 11th\\', \\'education_ 12th\\', \\'education_ 1st-4th\\',\\n   ...\\n  \\'education_ Preschool\\', \\'education_ Prof-school\\', \\'education_ Some-college\\',\\n  \\'gender_ Female\\', \\'gender_ Male\\', \\'occupation_ ?\\',\\n  \\'occupation_ Adm-clerical\\', \\'occupation_ Armed-Forces\\',\\n  \\'occupation_ Craft-repair\\', \\'occupation_ Exec-managerial\\',\\n  \\'occupation_ Farming-fishing\\', \\'occupation_ Handlers-cleaners\\',\\n  ...\\n  \\'occupation_ Tech-support\\', \\'occupation_ Transport-moving\\',\\n  \\'income_ <=50K\\', \\'income_ >50K\\']\\nCategorical Variables \\n| \\n215'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 229}, page_content='You can see that the continuous features age and hours-per-week were not touched,\\nwhile the categorical features were expanded into one new feature for each possible\\nvalue:\\nIn[5]:\\ndata_dummies.head()\\nOut[5]:\\nage\\nhours-\\nper-\\nweek\\nworkclass_ ? workclass_\\nFederal-\\ngov\\nworkclass_\\nLocal-gov\\n…\\noccupation_\\nTech-\\nsupport\\noccupation_\\nTransport-\\nmoving\\nincome_\\n<=50K\\nincome_\\n>50K\\n0\\n39\\n40\\n0.0\\n0.0\\n0.0\\n…\\n0.0\\n0.0\\n1.0\\n0.0\\n1\\n50\\n13\\n0.0\\n0.0\\n0.0\\n…\\n0.0\\n0.0\\n1.0\\n0.0\\n2\\n38\\n40\\n0.0\\n0.0\\n0.0\\n…\\n0.0\\n0.0\\n1.0\\n0.0\\n3\\n53\\n40\\n0.0\\n0.0\\n0.0\\n…\\n0.0\\n0.0\\n1.0\\n0.0\\n4\\n28\\n40\\n0.0\\n0.0\\n0.0\\n…\\n0.0\\n0.0\\n1.0\\n0.0\\n5 rows × 46 columns\\nWe can now use the values attribute to convert the data_dummies DataFrame into a\\nNumPy array, and then train a machine learning model on it. Be careful to separate\\nthe target variable (which is now encoded in two income columns) from the data\\nbefore training a model. Including the output variable, or some derived property of\\nthe output variable, into the feature representation is a very common mistake in\\nbuilding supervised machine learning models.\\nBe careful: column indexing in pandas includes the end of the\\nrange, so \\'age\\':\\'occupation_ Transport-moving\\' is inclusive of\\noccupation_ Transport-moving. This is different from slicing a\\nNumPy array, where the end of a range is not included: for exam‐\\nple, np.arange(11)[0:10] doesn’t include the entry with index 10.\\nIn this case, we extract only the columns containing features—that is, all columns\\nfrom age to occupation_ Transport-moving. This range contains all the features but\\nnot the target:\\nIn[6]:\\nfeatures = data_dummies.ix[:, \\'age\\':\\'occupation_ Transport-moving\\']\\n# Extract NumPy arrays\\nX = features.values\\ny = data_dummies[\\'income_ >50K\\'].values\\nprint(\"X.shape: {}  y.shape: {}\".format(X.shape, y.shape))\\n216 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 230}, page_content='Out[6]:\\nX.shape: (32561, 44)  y.shape: (32561,)\\nNow the data is represented in a way that scikit-learn can work with, and we can\\nproceed as usual:\\nIn[7]:\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nlogreg = LogisticRegression()\\nlogreg.fit(X_train, y_train)\\nprint(\"Test score: {:.2f}\".format(logreg.score(X_test, y_test)))\\nOut[7]:\\nTest score: 0.81\\nIn this example, we called get_dummies on a DataFrame containing\\nboth the training and the test data. This is important to ensure cat‐\\negorical values are represented in the same way in the training set\\nand the test set.\\nImagine we have the training and test sets in two different Data\\nFrames. If the \"Private Employee\" value for the workclass feature\\ndoes not appear in the test set, pandas will assume there are only\\nthree possible values for this feature and will create only three new\\ndummy features. Now our training and test sets have different\\nnumbers of features, and we can’t apply the model we learned on\\nthe training set to the test set anymore. Even worse, imagine the\\nworkclass feature has the values \"Government Employee\" and\\n\"Private Employee\" in the training set, and \"Self Employed\" and\\n\"Self Employed Incorporated\" in the test set. In both cases,\\npandas will create two new dummy features, so the encoded Data\\nFrames will have the same number of features. However, the two\\ndummy features have entirely different meanings in the training\\nand test sets. The column that means \"Government Employee\" for\\nthe training set would encode \"Self Employed\" for the test set.\\nIf we built a machine learning model on this data it would work\\nvery badly, because it would assume the columns mean the same\\nthings (because they are in the same position) when in fact they\\nmean very different things. To fix this, either call get_dummies on a\\nDataFrame that contains both the training and the test data points,\\nor make sure that the column names are the same for the training\\nand test sets after calling get_dummies, to ensure they have the\\nsame semantics.\\nCategorical Variables \\n| \\n217'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 231}, page_content='Numbers Can Encode Categoricals\\nIn the example of the adult dataset, the categorical variables were encoded as strings.\\nOn the one hand, that opens up the possibility of spelling errors, but on the other\\nhand, it clearly marks a variable as categorical. Often, whether for ease of storage or\\nbecause of the way the data is collected, categorical variables are encoded as integers.\\nFor example, imagine the census data in the adult dataset was collected using a ques‐\\ntionnaire, and the answers for workclass were recorded as 0 (first box ticked), 1 (sec‐\\nond box ticked), 2 (third box ticked), and so on. Now the column will contain\\nnumbers from 0 to 8, instead of strings like \"Private\", and it won’t be immediately\\nobvious to someone looking at the table representing the dataset whether they should\\ntreat this variable as continuous or categorical. Knowing that the numbers indicate\\nemployment status, however, it is clear that these are very distinct states and should\\nnot be modeled by a single continuous variable.\\nCategorical features are often encoded using integers. That they are\\nnumbers doesn’t mean that they should necessarily be treated as\\ncontinuous features. It is not always clear whether an integer fea‐\\nture should be treated as continuous or discrete (and one-hot-\\nencoded). If there is no ordering between the semantics that are\\nencoded (like in the workclass example), the feature must be\\ntreated as discrete. For other cases, like five-star ratings, the better\\nencoding depends on the particular task and data and which\\nmachine learning algorithm is used.\\nThe get_dummies function in pandas treats all numbers as continuous and will not\\ncreate dummy variables for them. To get around this, you can either use scikit-\\nlearn’s OneHotEncoder, for which you can specify which variables are continuous\\nand which are discrete, or convert numeric columns in the DataFrame to strings. To\\nillustrate, let’s create a DataFrame object with two columns, one containing strings\\nand one containing integers:\\nIn[8]:\\n# create a DataFrame with an integer feature and a categorical string feature\\ndemo_df = pd.DataFrame({\\'Integer Feature\\': [0, 1, 2, 1],\\n                        \\'Categorical Feature\\': [\\'socks\\', \\'fox\\', \\'socks\\', \\'box\\']})\\ndisplay(demo_df)\\nTable 4-4 shows the result.\\n218 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 232}, page_content=\"Table 4-4. DataFrame containing categorical string features and integer features\\nCategorical Feature Integer Feature\\n0\\nsocks\\n0\\n1\\nfox\\n1\\n2\\nsocks\\n2\\n3\\nbox\\n1\\nUsing get_dummies will only encode the string feature and will not change the integer\\nfeature, as you can see in Table 4-5:\\nIn[9]:\\npd.get_dummies(demo_df)\\nTable 4-5. One-hot-encoded version of the data from Table 4-4, leaving the integer feature\\nunchanged\\nInteger Feature\\nCategorical Feature_box\\nCategorical Feature_fox\\nCategorical Feature_socks\\n0\\n0\\n0.0\\n0.0\\n1.0\\n1\\n1\\n0.0\\n1.0\\n0.0\\n2\\n2\\n0.0\\n0.0\\n1.0\\n3\\n1\\n1.0\\n0.0\\n0.0\\nIf you want dummy variables to be created for the “Integer Feature” column, you can\\nexplicitly list the columns you want to encode using the columns parameter. Then,\\nboth features will be treated as categorical (see Table 4-6):\\nIn[10]:\\ndemo_df['Integer Feature'] = demo_df['Integer Feature'].astype(str)\\npd.get_dummies(demo_df, columns=['Integer Feature', 'Categorical Feature'])\\nTable 4-6. One-hot encoding of the data shown in Table 4-4, encoding the integer and string\\nfeatures\\nInteger\\nFeature_0\\nInteger\\nFeature_1\\nInteger\\nFeature_2\\nCategorical\\nFeature_box\\nCategorical\\nFeature_fox\\nCategorical\\nFeature_socks\\n0\\n1.0\\n0.0\\n0.0\\n0.0\\n0.0\\n1.0\\n1\\n0.0\\n1.0\\n0.0\\n0.0\\n1.0\\n0.0\\n2\\n0.0\\n0.0\\n1.0\\n0.0\\n0.0\\n1.0\\n3\\n0.0\\n1.0\\n0.0\\n1.0\\n0.0\\n0.0\\nCategorical Variables \\n| \\n219\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 233}, page_content='Binning, Discretization, Linear Models, and Trees\\nThe best way to represent data depends not only on the semantics of the data, but also\\non the kind of model you are using. Linear models and tree-based models (such as\\ndecision trees, gradient boosted trees, and random forests), two large and very com‐\\nmonly used families, have very different properties when it comes to how they work\\nwith different feature representations. Let’s go back to the wave regression dataset that\\nwe used in Chapter 2. It has only a single input feature. Here is a comparison of a\\nlinear regression model and a decision tree regressor on this dataset (see Figure 4-1):\\nIn[11]:\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.tree import DecisionTreeRegressor\\nX, y = mglearn.datasets.make_wave(n_samples=100)\\nline = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)\\nreg = DecisionTreeRegressor(min_samples_split=3).fit(X, y)\\nplt.plot(line, reg.predict(line), label=\"decision tree\")\\nreg = LinearRegression().fit(X, y)\\nplt.plot(line, reg.predict(line), label=\"linear regression\")\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nplt.legend(loc=\"best\")\\nAs you know, linear models can only model linear relationships, which are lines in\\nthe case of a single feature. The decision tree can build a much more complex model\\nof the data. However, this is strongly dependent on the representation of the data.\\nOne way to make linear models more powerful on continuous data is to use binning\\n(also known as discretization) of the feature to split it up into multiple features, as\\ndescribed here.\\n220 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 234}, page_content='Figure 4-1. Comparing linear regression and a decision tree on the wave dataset\\nWe imagine a partition of the input range for the feature (in this case, the numbers\\nfrom –3 to 3) into a fixed number of bins—say, 10. A data point will then be repre‐\\nsented by which bin it falls into. To determine this, we first have to define the bins. In\\nthis case, we’ll define 10 bins equally spaced between –3 and 3. We use the\\nnp.linspace function for this, creating 11 entries, which will create 10 bins—they are\\nthe spaces in between two consecutive boundaries:\\nIn[12]:\\nbins = np.linspace(-3, 3, 11)\\nprint(\"bins: {}\".format(bins))\\nOut[12]:\\nbins: [-3.  -2.4 -1.8 -1.2 -0.6  0.   0.6  1.2  1.8  2.4  3. ]\\nHere, the first bin contains all data points with feature values –3 to –2.68, the second\\nbin contains all points with feature values from –2.68 to –2.37, and so on.\\nNext, we record for each data point which bin it falls into. This can be easily compu‐\\nted using the np.digitize function:\\nBinning, Discretization, Linear Models, and Trees \\n| \\n221'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 235}, page_content='In[13]:\\nwhich_bin = np.digitize(X, bins=bins)\\nprint(\"\\\\nData points:\\\\n\", X[:5])\\nprint(\"\\\\nBin membership for data points:\\\\n\", which_bin[:5])\\nOut[13]:\\nData points:\\n [[-0.753]\\n  [ 2.704]\\n  [ 1.392]\\n  [ 0.592]\\n [-2.064]]\\nBin membership for data points:\\n [[ 4]\\n  [10]\\n  [ 8]\\n  [ 6]\\n [ 2]]\\nWhat we did here is transform the single continuous input feature in the wave dataset\\ninto a categorical feature that encodes which bin a data point is in. To use a scikit-\\nlearn model on this data, we transform this discrete feature to a one-hot encoding\\nusing the OneHotEncoder from the preprocessing module. The OneHotEncoder does\\nthe same encoding as pandas.get_dummies, though it currently only works on cate‐\\ngorical variables that are integers:\\nIn[14]:\\nfrom sklearn.preprocessing import OneHotEncoder\\n# transform using the OneHotEncoder\\nencoder = OneHotEncoder(sparse=False)\\n# encoder.fit finds the unique values that appear in which_bin\\nencoder.fit(which_bin)\\n# transform creates the one-hot encoding\\nX_binned = encoder.transform(which_bin)\\nprint(X_binned[:5])\\nOut[14]:\\n[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\\n [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\\n [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\\n [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\\nBecause we specified 10 bins, the transformed dataset X_binned now is made up of 10\\nfeatures:\\n222 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 236}, page_content='In[15]:\\nprint(\"X_binned.shape: {}\".format(X_binned.shape))\\nOut[15]:\\nX_binned.shape: (100, 10)\\nNow we build a new linear regression model and a new decision tree model on the\\none-hot-encoded data. The result is visualized in Figure 4-2, together with the bin\\nboundaries, shown as dotted black lines:\\nIn[16]:\\nline_binned = encoder.transform(np.digitize(line, bins=bins))\\nreg = LinearRegression().fit(X_binned, y)\\nplt.plot(line, reg.predict(line_binned), label=\\'linear regression binned\\')\\nreg = DecisionTreeRegressor(min_samples_split=3).fit(X_binned, y)\\nplt.plot(line, reg.predict(line_binned), label=\\'decision tree binned\\')\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nplt.vlines(bins, -3, 3, linewidth=1, alpha=.2)\\nplt.legend(loc=\"best\")\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nFigure 4-2. Comparing linear regression and decision tree regression on binned features\\nBinning, Discretization, Linear Models, and Trees \\n| \\n223'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 237}, page_content=\"The dashed line and solid line are exactly on top of each other, meaning the linear\\nregression model and the decision tree make exactly the same predictions. For each\\nbin, they predict a constant value. As features are constant within each bin, any\\nmodel must predict the same value for all points within a bin. Comparing what the\\nmodels learned before binning the features and after, we see that the linear model\\nbecame much more flexible, because it now has a different value for each bin, while\\nthe decision tree model got much less flexible. Binning features generally has no ben‐\\neficial effect for tree-based models, as these models can learn to split up the data any‐\\nwhere. In a sense, that means decision trees can learn whatever binning is most useful\\nfor predicting on this data. Additionally, decision trees look at multiple features at\\nonce, while binning is usually done on a per-feature basis. However, the linear model\\nbenefited greatly in expressiveness from the transformation of the data.\\nIf there are good reasons to use a linear model for a particular dataset—say, because it\\nis very large and high-dimensional, but some features have nonlinear relations with\\nthe output—binning can be a great way to increase modeling power.\\nInteractions and Polynomials\\nAnother way to enrich a feature representation, particularly for linear models, is\\nadding interaction features and polynomial features of the original data. This kind of\\nfeature engineering is often used in statistical modeling, but it’s also common in many\\npractical machine learning applications.\\nAs a first example, look again at Figure 4-2. The linear model learned a constant value\\nfor each bin in the wave dataset. We know, however, that linear models can learn not\\nonly offsets, but also slopes. One way to add a slope to the linear model on the binned\\ndata is to add the original feature (the x-axis in the plot) back in. This leads to an 11-\\ndimensional dataset, as seen in Figure 4-3:\\nIn[17]:\\nX_combined = np.hstack([X, X_binned])\\nprint(X_combined.shape)\\nOut[17]:\\n(100, 11)\\nIn[18]:\\nreg = LinearRegression().fit(X_combined, y)\\nline_combined = np.hstack([line, line_binned])\\nplt.plot(line, reg.predict(line_combined), label='linear regression combined')\\nfor bin in bins:\\n    plt.plot([bin, bin], [-3, 3], ':', c='k')\\n224 \\n| \\nChapter 4: Representing Data and Engineering Features\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 238}, page_content='plt.legend(loc=\"best\")\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nFigure 4-3. Linear regression using binned features and a single global slope\\nIn this example, the model learned an offset for each bin, together with a slope. The\\nlearned slope is downward, and shared across all the bins—there is a single x-axis fea‐\\nture, which has a single slope. Because the slope is shared across all bins, it doesn’t\\nseem to be very helpful. We would rather have a separate slope for each bin! We can\\nachieve this by adding an interaction or product feature that indicates which bin a\\ndata point is in and where it lies on the x-axis. This feature is a product of the bin\\nindicator and the original feature. Let’s create this dataset:\\nIn[19]:\\nX_product = np.hstack([X_binned, X * X_binned])\\nprint(X_product.shape)\\nOut[19]:\\n(100, 20)\\nThe dataset now has 20 features: the indicators for which bin a data point is in, and a\\nproduct of the original feature and the bin indicator. You can think of the product\\nInteractions and Polynomials \\n| \\n225'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 239}, page_content='feature as a separate copy of the x-axis feature for each bin. It is the original feature\\nwithin the bin, and zero everywhere else. Figure 4-4 shows the result of the linear\\nmodel on this new representation:\\nIn[20]:\\nreg = LinearRegression().fit(X_product, y)\\nline_product = np.hstack([line_binned, line * line_binned])\\nplt.plot(line, reg.predict(line_product), label=\\'linear regression product\\')\\nfor bin in bins:\\n    plt.plot([bin, bin], [-3, 3], \\':\\', c=\\'k\\')\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nplt.legend(loc=\"best\")\\nFigure 4-4. Linear regression with a separate slope per bin\\nAs you can see, now each bin has its own offset and slope in this model.\\n226 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 240}, page_content='Using binning is one way to expand a continuous feature. Another one is to use poly‐\\nnomials of the original features. For a given feature x, we might want to consider\\nx ** 2, x ** 3, x ** 4, and so on. This is implemented in PolynomialFeatures in\\nthe preprocessing module:\\nIn[21]:\\nfrom sklearn.preprocessing import PolynomialFeatures\\n# include polynomials up to x ** 10:\\n# the default \"include_bias=True\" adds a feature that\\'s constantly 1\\npoly = PolynomialFeatures(degree=10, include_bias=False)\\npoly.fit(X)\\nX_poly = poly.transform(X)\\nUsing a degree of 10 yields 10 features:\\nIn[22]:\\nprint(\"X_poly.shape: {}\".format(X_poly.shape))\\nOut[22]:\\nX_poly.shape: (100, 10)\\nLet’s compare the entries of X_poly to those of X:\\nIn[23]:\\nprint(\"Entries of X:\\\\n{}\".format(X[:5]))\\nprint(\"Entries of X_poly:\\\\n{}\".format(X_poly[:5]))\\nOut[23]:\\nEntries of X:\\n[[-0.753]\\n [ 2.704]\\n [ 1.392]\\n [ 0.592]\\n [-2.064]]\\nEntries of X_poly:\\n[[    -0.753      0.567     -0.427      0.321     -0.242      0.182\\n      -0.137      0.103     -0.078      0.058]\\n [     2.704      7.313     19.777     53.482    144.632    391.125\\n    1057.714   2860.360   7735.232  20918.278]\\n [     1.392      1.938      2.697      3.754      5.226      7.274\\n      10.125     14.094     19.618     27.307]\\n [     0.592      0.350      0.207      0.123      0.073      0.043\\n       0.025      0.015      0.009      0.005]\\n [    -2.064      4.260     -8.791     18.144    -37.448     77.289\\n    -159.516    329.222   -679.478   1402.367]]\\nYou can obtain the semantics of the features by calling the get_feature_names\\nmethod, which provides the exponent for each feature:\\nInteractions and Polynomials \\n| \\n227'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 241}, page_content='In[24]:\\nprint(\"Polynomial feature names:\\\\n{}\".format(poly.get_feature_names()))\\nOut[24]:\\nPolynomial feature names:\\n[\\'x0\\', \\'x0^2\\', \\'x0^3\\', \\'x0^4\\', \\'x0^5\\', \\'x0^6\\', \\'x0^7\\', \\'x0^8\\', \\'x0^9\\', \\'x0^10\\']\\nYou can see that the first column of X_poly corresponds exactly to X, while the other\\ncolumns are the powers of the first entry. It’s interesting to see how large some of the\\nvalues can get. The second column has entries above 20,000, orders of magnitude dif‐\\nferent from the rest.\\nUsing polynomial features together with a linear regression model yields the classical\\nmodel of polynomial regression (see Figure 4-5):\\nIn[26]:\\nreg = LinearRegression().fit(X_poly, y)\\nline_poly = poly.transform(line)\\nplt.plot(line, reg.predict(line_poly), label=\\'polynomial linear regression\\')\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nplt.legend(loc=\"best\")\\nFigure 4-5. Linear regression with tenth-degree polynomial features\\n228 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 242}, page_content='As you can see, polynomial features yield a very smooth fit on this one-dimensional\\ndata. However, polynomials of high degree tend to behave in extreme ways on the\\nboundaries or in regions with little data.\\nAs a comparison, here is a kernel SVM model learned on the original data, without\\nany transformation (see Figure 4-6):\\nIn[26]:\\nfrom sklearn.svm import SVR\\nfor gamma in [1, 10]:\\n    svr = SVR(gamma=gamma).fit(X, y)\\n    plt.plot(line, svr.predict(line), label=\\'SVR gamma={}\\'.format(gamma))\\nplt.plot(X[:, 0], y, \\'o\\', c=\\'k\\')\\nplt.ylabel(\"Regression output\")\\nplt.xlabel(\"Input feature\")\\nplt.legend(loc=\"best\")\\nFigure 4-6. Comparison of different gamma parameters for an SVM with RBF kernel\\nUsing a more complex model, a kernel SVM, we are able to learn a similarly complex\\nprediction to the polynomial regression without an explicit transformation of the\\nfeatures.\\nInteractions and Polynomials \\n| \\n229'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 243}, page_content='As a more realistic application of interactions and polynomials, let’s look again at the\\nBoston Housing dataset. We already used polynomial features on this dataset in\\nChapter 2. Now let’s have a look at how these features were constructed, and at how\\nmuch the polynomial features help. First we load the data, and rescale it to be\\nbetween 0 and 1 using MinMaxScaler:\\nIn[27]:\\nfrom sklearn.datasets import load_boston\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import MinMaxScaler\\nboston = load_boston()\\nX_train, X_test, y_train, y_test = train_test_split\\n    (boston.data, boston.target, random_state=0)\\n# rescale data\\nscaler = MinMaxScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\nNow, we extract polynomial features and interactions up to a degree of 2:\\nIn[28]:\\npoly = PolynomialFeatures(degree=2).fit(X_train_scaled)\\nX_train_poly = poly.transform(X_train_scaled)\\nX_test_poly = poly.transform(X_test_scaled)\\nprint(\"X_train.shape: {}\".format(X_train.shape))\\nprint(\"X_train_poly.shape: {}\".format(X_train_poly.shape))\\nOut[28]:\\nX_train.shape: (379, 13)\\nX_train_poly.shape: (379, 105)\\nThe data originally had 13 features, which were expanded into 105 interaction fea‐\\ntures. These new features represent all possible interactions between two different\\noriginal features, as well as the square of each original feature. degree=2 here means\\nthat we look at all features that are the product of up to two original features. The\\nexact correspondence between input and output features can be found using the\\nget_feature_names method:\\nIn[29]:\\nprint(\"Polynomial feature names:\\\\n{}\".format(poly.get_feature_names()))\\nOut[29]:\\nPolynomial feature names:\\n[\\'1\\', \\'x0\\', \\'x1\\', \\'x2\\', \\'x3\\', \\'x4\\', \\'x5\\', \\'x6\\', \\'x7\\', \\'x8\\', \\'x9\\', \\'x10\\',\\n\\'x11\\', \\'x12\\', \\'x0^2\\', \\'x0 x1\\', \\'x0 x2\\', \\'x0 x3\\', \\'x0 x4\\', \\'x0 x5\\', \\'x0 x6\\',\\n\\'x0 x7\\', \\'x0 x8\\', \\'x0 x9\\', \\'x0 x10\\', \\'x0 x11\\', \\'x0 x12\\', \\'x1^2\\', \\'x1 x2\\',\\n230 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 244}, page_content='\\'x1 x3\\', \\'x1 x4\\', \\'x1 x5\\', \\'x1 x6\\', \\'x1 x7\\', \\'x1 x8\\', \\'x1 x9\\', \\'x1 x10\\',\\n\\'x1 x11\\', \\'x1 x12\\', \\'x2^2\\', \\'x2 x3\\', \\'x2 x4\\', \\'x2 x5\\', \\'x2 x6\\', \\'x2 x7\\',\\n\\'x2 x8\\', \\'x2 x9\\', \\'x2 x10\\', \\'x2 x11\\', \\'x2 x12\\', \\'x3^2\\', \\'x3 x4\\', \\'x3 x5\\',\\n\\'x3 x6\\', \\'x3 x7\\', \\'x3 x8\\', \\'x3 x9\\', \\'x3 x10\\', \\'x3 x11\\', \\'x3 x12\\', \\'x4^2\\',\\n\\'x4 x5\\', \\'x4 x6\\', \\'x4 x7\\', \\'x4 x8\\', \\'x4 x9\\', \\'x4 x10\\', \\'x4 x11\\', \\'x4 x12\\',\\n\\'x5^2\\', \\'x5 x6\\', \\'x5 x7\\', \\'x5 x8\\', \\'x5 x9\\', \\'x5 x10\\', \\'x5 x11\\', \\'x5 x12\\',\\n\\'x6^2\\', \\'x6 x7\\', \\'x6 x8\\', \\'x6 x9\\', \\'x6 x10\\', \\'x6 x11\\', \\'x6 x12\\', \\'x7^2\\',\\n\\'x7 x8\\', \\'x7 x9\\', \\'x7 x10\\', \\'x7 x11\\', \\'x7 x12\\', \\'x8^2\\', \\'x8 x9\\', \\'x8 x10\\',\\n\\'x8 x11\\', \\'x8 x12\\', \\'x9^2\\', \\'x9 x10\\', \\'x9 x11\\', \\'x9 x12\\', \\'x10^2\\', \\'x10 x11\\',\\n\\'x10 x12\\', \\'x11^2\\', \\'x11 x12\\', \\'x12^2\\']\\nThe first new feature is a constant feature, called \"1\" here. The next 13 features are\\nthe original features (called \"x0\" to \"x12\"). Then follows the first feature squared\\n(\"x0^2\") and combinations of the first and the other features.\\nLet’s compare the performance using Ridge on the data with and without interac‐\\ntions:\\nIn[30]:\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge().fit(X_train_scaled, y_train)\\nprint(\"Score without interactions: {:.3f}\".format(\\n    ridge.score(X_test_scaled, y_test)))\\nridge = Ridge().fit(X_train_poly, y_train)\\nprint(\"Score with interactions: {:.3f}\".format(\\n    ridge.score(X_test_poly, y_test)))\\nOut[30]:\\nScore without interactions: 0.621\\nScore with interactions: 0.753\\nClearly, the interactions and polynomial features gave us a good boost in perfor‐\\nmance when using Ridge. When using a more complex model like a random forest,\\nthe story is a bit different, though:\\nIn[31]:\\nfrom sklearn.ensemble import RandomForestRegressor\\nrf = RandomForestRegressor(n_estimators=100).fit(X_train_scaled, y_train)\\nprint(\"Score without interactions: {:.3f}\".format(\\n    rf.score(X_test_scaled, y_test)))\\nrf = RandomForestRegressor(n_estimators=100).fit(X_train_poly, y_train)\\nprint(\"Score with interactions: {:.3f}\".format(rf.score(X_test_poly, y_test)))\\nOut[31]:\\nScore without interactions: 0.799\\nScore with interactions: 0.763\\nInteractions and Polynomials \\n| \\n231'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 245}, page_content='You can see that even without additional features, the random forest beats the\\nperformance of Ridge. Adding interactions and polynomials actually decreases per‐\\nformance slightly.\\nUnivariate Nonlinear Transformations\\nWe just saw that adding squared or cubed features can help linear models for regres‐\\nsion. There are other transformations that often prove useful for transforming certain\\nfeatures: in particular, applying mathematical functions like log, exp, or sin. While\\ntree-based models only care about the ordering of the features, linear models and\\nneural networks are very tied to the scale and distribution of each feature, and if there\\nis a nonlinear relation between the feature and the target, that becomes hard to model\\n—particularly in regression. The functions log and exp can help by adjusting the rel‐\\native scales in the data so that they can be captured better by a linear model or neural\\nnetwork. We saw an application of that in Chapter 2 with the memory price data. The\\nsin and cos functions can come in handy when dealing with data that encodes peri‐\\nodic patterns.\\nMost models work best when each feature (and in regression also the target) is loosely\\nGaussian distributed—that is, a histogram of each feature should have something\\nresembling the familiar “bell curve” shape. Using transformations like log and exp is\\na hacky but simple and efficient way to achieve this. A particularly common case\\nwhen such a transformation can be helpful is when dealing with integer count data.\\nBy count data, we mean features like “how often did user A log in?” Counts are never\\nnegative, and often follow particular statistical patterns. We are using a synthetic\\ndataset of counts here that has properties similar to those you can find in the wild.\\nThe features are all integer-valued, while the response is continuous:\\nIn[32]:\\nrnd = np.random.RandomState(0)\\nX_org = rnd.normal(size=(1000, 3))\\nw = rnd.normal(size=3)\\nX = rnd.poisson(10 * np.exp(X_org))\\ny = np.dot(X_org, w)\\nLet’s look at the first 10 entries of the first feature. All are integer values and positive,\\nbut apart from that it’s hard to make out a particular pattern.\\nIf we count the appearance of each value, the distribution of values becomes clearer:\\n232 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 246}, page_content='In[33]:\\nprint(\"Number of feature appearances:\\\\n{}\".format(np.bincount(X[:, 0])))\\nOut[33]:\\nNumber of feature appearances:\\n[28 38 68 48 61 59 45 56 37 40 35 34 36 26 23 26 27 21 23 23 18 21 10  9 17\\n  9  7 14 12  7  3  8  4  5  5  3  4  2  4  1  1  3  2  5  3  8  2  5  2  1\\n  2  3  3  2  2  3  3  0  1  2  1  0  0  3  1  0  0  0  1  3  0  1  0  2  0\\n  1  1  0  0  0  0  1  0  0  2  2  0  1  1  0  0  0  0  1  1  0  0  0  0  0\\n  0  0  1  0  0  0  0  0  1  1  0  0  1  0  0  0  0  0  0  0  1  0  0  0  0\\n  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]\\nThe value 2 seems to be the most common, with 62 appearances (bincount always\\nstarts at 0), and the counts for higher values fall quickly. However, there are some\\nvery high values, like 134 appearing twice. We visualize the counts in Figure 4-7:\\nIn[34]:\\nbins = np.bincount(X[:, 0])\\nplt.bar(range(len(bins)), bins, color=\\'w\\')\\nplt.ylabel(\"Number of appearances\")\\nplt.xlabel(\"Value\")\\nFigure 4-7. Histogram of feature values for X[0]\\nUnivariate Nonlinear Transformations \\n| \\n233'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 247}, page_content='1 This is a Poisson distribution, which is quite fundamental to count data.\\nFeatures X[:, 1] and X[:, 2] have similar properties. This kind of distribution of\\nvalues (many small ones and a few very large ones) is very common in practice.1\\nHowever, it is something most linear models can’t handle very well. Let’s try to fit a\\nridge regression to this model:\\nIn[35]:\\nfrom sklearn.linear_model import Ridge\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nscore = Ridge().fit(X_train, y_train).score(X_test, y_test)\\nprint(\"Test score: {:.3f}\".format(score))\\nOut[35]:\\nTest score: 0.622\\nAs you can see from the relatively low R2 score, Ridge was not able to really capture\\nthe relationship between X and y. Applying a logarithmic transformation can help,\\nthough. Because the value 0 appears in the data (and the logarithm is not defined at\\n0), we can’t actually just apply log, but we have to compute log(X + 1):\\nIn[36]:\\nX_train_log = np.log(X_train + 1)\\nX_test_log = np.log(X_test + 1)\\nAfter the transformation, the distribution of the data is less asymmetrical and doesn’t\\nhave very large outliers anymore (see Figure 4-8):\\nIn[37]:\\nplt.hist(np.log(X_train_log[:, 0] + 1), bins=25, color=\\'gray\\')\\nplt.ylabel(\"Number of appearances\")\\nplt.xlabel(\"Value\")\\n234 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 248}, page_content='2 This is a very crude approximation of using Poisson regression, which would be the proper solution from a\\nprobabilistic standpoint.\\nFigure 4-8. Histogram of feature values for X[0] after logarithmic transformation\\nBuilding a ridge model on the new data provides a much better fit:\\nIn[38]:\\nscore = Ridge().fit(X_train_log, y_train).score(X_test_log, y_test)\\nprint(\"Test score: {:.3f}\".format(score))\\nOut[38]:\\nTest score: 0.875\\nFinding the transformation that works best for each combination of dataset and\\nmodel is somewhat of an art. In this example, all the features had the same properties.\\nThis is rarely the case in practice, and usually only a subset of the features should be\\ntransformed, or sometimes each feature needs to be transformed in a different way.\\nAs we mentioned earlier, these kinds of transformations are irrelevant for tree-based\\nmodels but might be essential for linear models. Sometimes it is also a good idea to\\ntransform the target variable y in regression. Trying to predict counts (say, number of\\norders) is a fairly common task, and using the log(y + 1) transformation often\\nhelps.2\\nUnivariate Nonlinear Transformations \\n| \\n235'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 249}, page_content='As you saw in the previous examples, binning, polynomials, and interactions can\\nhave a huge influence on how models perform on a given dataset. This is particularly\\ntrue for less complex models like linear models and naive Bayes models. Tree-based\\nmodels, on the other hand, are often able to discover important interactions them‐\\nselves, and don’t require transforming the data explicitly most of the time. Other\\nmodels, like SVMs, nearest neighbors, and neural networks, might sometimes benefit\\nfrom using binning, interactions, or polynomials, but the implications there are usu‐\\nally much less clear than in the case of linear models.\\nAutomatic Feature Selection\\nWith so many ways to create new features, you might get tempted to increase the\\ndimensionality of the data way beyond the number of original features. However,\\nadding more features makes all models more complex, and so increases the chance of\\noverfitting. When adding new features, or with high-dimensional datasets in general,\\nit can be a good idea to reduce the number of features to only the most useful ones,\\nand discard the rest. This can lead to simpler models that generalize better. But how\\ncan you know how good each feature is? There are three basic strategies: univariate\\nstatistics, model-based selection, and iterative selection. We will discuss all three of\\nthem in detail. All of these methods are supervised methods, meaning they need the\\ntarget for fitting the model. This means we need to split the data into training and test\\nsets, and fit the feature selection only on the training part of the data.\\nUnivariate Statistics\\nIn univariate statistics, we compute whether there is a statistically significant relation‐\\nship between each feature and the target. Then the features that are related with the\\nhighest confidence are selected. In the case of classification, this is also known as\\nanalysis of variance (ANOVA). A key property of these tests is that they are univari‐\\nate, meaning that they only consider each feature individually. Consequently, a fea‐\\nture will be discarded if it is only informative when combined with another feature.\\nUnivariate tests are often very fast to compute, and don’t require building a model.\\nOn the other hand, they are completely independent of the model that you might\\nwant to apply after the feature selection.\\nTo use univariate feature selection in scikit-learn, you need to choose a test, usu‐\\nally either f_classif (the default) for classification or f_regression for regression,\\nand a method to discard features based on the p-values determined in the test. All\\nmethods for discarding parameters use a threshold to discard all features with too\\nhigh a p-value (which means they are unlikely to be related to the target). The meth‐\\nods differ in how they compute this threshold, with the simplest ones being SelectKB\\nest, which selects a fixed number k of features, and SelectPercentile, which selects\\na fixed percentage of features. Let’s apply the feature selection for classification on the\\n236 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 250}, page_content='cancer dataset. To make the task a bit harder, we’ll add some noninformative noise\\nfeatures to the data. We expect the feature selection to be able to identify the features\\nthat are noninformative and remove them:\\nIn[39]:\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.feature_selection import SelectPercentile\\nfrom sklearn.model_selection import train_test_split\\ncancer = load_breast_cancer()\\n# get deterministic random numbers\\nrng = np.random.RandomState(42)\\nnoise = rng.normal(size=(len(cancer.data), 50))\\n# add noise features to the data\\n# the first 30 features are from the dataset, the next 50 are noise\\nX_w_noise = np.hstack([cancer.data, noise])\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_w_noise, cancer.target, random_state=0, test_size=.5)\\n# use f_classif (the default) and SelectPercentile to select 50% of features\\nselect = SelectPercentile(percentile=50)\\nselect.fit(X_train, y_train)\\n# transform training set\\nX_train_selected = select.transform(X_train)\\nprint(\"X_train.shape: {}\".format(X_train.shape))\\nprint(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\\nOut[39]:\\nX_train.shape: (284, 80)\\nX_train_selected.shape: (284, 40)\\nAs you can see, the number of features was reduced from 80 to 40 (50 percent of the\\noriginal number of features). We can find out which features have been selected using\\nthe get_support method, which returns a Boolean mask of the selected features\\n(visualized in Figure 4-9):\\nIn[40]:\\nmask = select.get_support()\\nprint(mask)\\n# visualize the mask -- black is True, white is False\\nplt.matshow(mask.reshape(1, -1), cmap=\\'gray_r\\')\\nplt.xlabel(\"Sample index\")\\nOut[40]:\\n[ True  True  True  True  True  True  True  True  True False  True False\\n  True  True  True  True  True  True False False  True  True  True  True\\n  True  True  True  True  True  True False False False  True False  True\\nAutomatic Feature Selection \\n| \\n237'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 251}, page_content='False False  True False False False False  True False False  True False\\n False  True False  True False False False False False False  True False\\n  True False False False False  True False  True False False False False\\n  True  True False  True False False False False]\\nFigure 4-9. Features selected by SelectPercentile\\nAs you can see from the visualization of the mask, most of the selected features are\\nthe original features, and most of the noise features were removed. However, the\\nrecovery of the original features is not perfect. Let’s compare the performance of\\nlogistic regression on all features against the performance using only the selected\\nfeatures:\\nIn[41]:\\nfrom sklearn.linear_model import LogisticRegression\\n# transform test data\\nX_test_selected = select.transform(X_test)\\nlr = LogisticRegression()\\nlr.fit(X_train, y_train)\\nprint(\"Score with all features: {:.3f}\".format(lr.score(X_test, y_test)))\\nlr.fit(X_train_selected, y_train)\\nprint(\"Score with only selected features: {:.3f}\".format(\\n    lr.score(X_test_selected, y_test)))\\nOut[41]:\\nScore with all features: 0.930\\nScore with only selected features: 0.940\\nIn this case, removing the noise features improved performance, even though some\\nof the original features were lost. This was a very simple synthetic example, and out‐\\ncomes on real data are usually mixed. Univariate feature selection can still be very\\nhelpful, though, if there is such a large number of features that building a model on\\nthem is infeasible, or if you suspect that many features are completely uninformative.\\nModel-Based Feature Selection\\nModel-based feature selection uses a supervised machine learning model to judge the\\nimportance of each feature, and keeps only the most important ones. The supervised\\nmodel that is used for feature selection doesn’t need to be the same model that is used\\nfor the final supervised modeling. The feature selection model needs to provide some\\nmeasure of importance for each feature, so that they can be ranked by this measure.\\nDecision trees and decision tree–based models provide a feature_importances_\\n238 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 252}, page_content='attribute, which directly encodes the importance of each feature. Linear models have\\ncoefficients, which can also be used to capture feature importances by considering the\\nabsolute values. As we saw in Chapter 2, linear models with L1 penalty learn sparse\\ncoefficients, which only use a small subset of features. This can be viewed as a form of\\nfeature selection for the model itself, but can also be used as a preprocessing step to\\nselect features for another model. In contrast to univariate selection, model-based\\nselection considers all features at once, and so can capture interactions (if the model\\ncan capture them). To use model-based feature selection, we need to use the\\nSelectFromModel transformer:\\nIn[42]:\\nfrom sklearn.feature_selection import SelectFromModel\\nfrom sklearn.ensemble import RandomForestClassifier\\nselect = SelectFromModel(\\n    RandomForestClassifier(n_estimators=100, random_state=42),\\n    threshold=\"median\")\\nThe SelectFromModel class selects all features that have an importance measure of\\nthe feature (as provided by the supervised model) greater than the provided thresh‐\\nold. To get a comparable result to what we got with univariate feature selection, we\\nused the median as a threshold, so that half of the features will be selected. We use a\\nrandom forest classifier with 100 trees to compute the feature importances. This is a\\nquite complex model and much more powerful than using univariate tests. Now let’s\\nactually fit the model:\\nIn[43]:\\nselect.fit(X_train, y_train)\\nX_train_l1 = select.transform(X_train)\\nprint(\"X_train.shape: {}\".format(X_train.shape))\\nprint(\"X_train_l1.shape: {}\".format(X_train_l1.shape))\\nOut[43]:\\nX_train.shape: (284, 80)\\nX_train_l1.shape: (284, 40)\\nAgain, we can have a look at the features that were selected (Figure 4-10):\\nIn[44]:\\nmask = select.get_support()\\n# visualize the mask -- black is True, white is False\\nplt.matshow(mask.reshape(1, -1), cmap=\\'gray_r\\')\\nplt.xlabel(\"Sample index\")\\nFigure 4-10. Features selected by SelectFromModel using the RandomForestClassifier\\nAutomatic Feature Selection \\n| \\n239'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 253}, page_content='This time, all but two of the original features were selected. Because we specified to\\nselect 40 features, some of the noise features are also selected. Let’s take a look at the\\nperformance:\\nIn[45]:\\nX_test_l1 = select.transform(X_test)\\nscore = LogisticRegression().fit(X_train_l1, y_train).score(X_test_l1, y_test)\\nprint(\"Test score: {:.3f}\".format(score))\\nOut[45]:\\nTest score: 0.951\\nWith the better feature selection, we also gained some improvements here.\\nIterative Feature Selection\\nIn univariate testing we used no model, while in model-based selection we used a sin‐\\ngle model to select features. In iterative feature selection, a series of models are built,\\nwith varying numbers of features. There are two basic methods: starting with no fea‐\\ntures and adding features one by one until some stopping criterion is reached, or\\nstarting with all features and removing features one by one until some stopping crite‐\\nrion is reached. Because a series of models are built, these methods are much more\\ncomputationally expensive than the methods we discussed previously. One particular\\nmethod of this kind is recursive feature elimination (RFE), which starts with all fea‐\\ntures, builds a model, and discards the least important feature according to the\\nmodel. Then a new model is built using all but the discarded feature, and so on until\\nonly a prespecified number of features are left. For this to work, the model used for\\nselection needs to provide some way to determine feature importance, as was the case\\nfor the model-based selection. Here, we use the same random forest model that we\\nused earlier, and get the results shown in Figure 4-11:\\nIn[46]:\\nfrom sklearn.feature_selection import RFE\\nselect = RFE(RandomForestClassifier(n_estimators=100, random_state=42),\\n             n_features_to_select=40)\\nselect.fit(X_train, y_train)\\n# visualize the selected features:\\nmask = select.get_support()\\nplt.matshow(mask.reshape(1, -1), cmap=\\'gray_r\\')\\nplt.xlabel(\"Sample index\")\\n240 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 254}, page_content='Figure 4-11. Features selected by recursive feature elimination with the random forest\\nclassifier model\\nThe feature selection got better compared to the univariate and model-based selec‐\\ntion, but one feature was still missed. Running this code also takes significantly longer\\nthan that for the model-based selection, because a random forest model is trained 40\\ntimes, once for each feature that is dropped. Let’s test the accuracy of the logistic\\nregression model when using RFE for feature selection:\\nIn[47]:\\nX_train_rfe= select.transform(X_train)\\nX_test_rfe= select.transform(X_test)\\nscore = LogisticRegression().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\\nprint(\"Test score: {:.3f}\".format(score))\\nOut[47]:\\nTest score: 0.951\\nWe can also use the model used inside the RFE to make predictions. This uses only\\nthe feature set that was selected:\\nIn[48]:\\nprint(\"Test score: {:.3f}\".format(select.score(X_test, y_test)))\\nOut[48]:\\nTest score: 0.951\\nHere, the performance of the random forest used inside the RFE is the same as that\\nachieved by training a logistic regression model on top of the selected features. In\\nother words, once we’ve selected the right features, the linear model performs as well\\nas the random forest.\\nIf you are unsure when selecting what to use as input to your machine learning algo‐\\nrithms, automatic feature selection can be quite helpful. It is also great for reducing\\nthe amount of features needed—for example, to speed up prediction or to allow for\\nmore interpretable models. In most real-world cases, applying feature selection is\\nunlikely to provide large gains in performance. However, it is still a valuable tool in\\nthe toolbox of the feature engineer.\\nAutomatic Feature Selection \\n| \\n241'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 255}, page_content='Utilizing Expert Knowledge\\nFeature engineering is often an important place to use expert knowledge for a particu‐\\nlar application. While the purpose of machine learning in many cases is to avoid hav‐\\ning to create a set of expert-designed rules, that doesn’t mean that prior knowledge of\\nthe application or domain should be discarded. Often, domain experts can help in\\nidentifying useful features that are much more informative than the initial represen‐\\ntation of the data. Imagine you work for a travel agency and want to predict flight\\nprices. Let’s say you have a record of prices together with dates, airlines, start loca‐\\ntions, and destinations. A machine learning model might be able to build a decent\\nmodel from that. Some important factors in flight prices, however, cannot be learned.\\nFor example, flights are usually more expensive during peak vacation months and\\naround holidays. While the dates of some holidays (like Christmas) are fixed, and\\ntheir effect can therefore be learned from the date, others might depend on the phases\\nof the moon (like Hanukkah and Easter) or be set by authorities (like school holi‐\\ndays). These events cannot be learned from the data if each flight is only recorded\\nusing the (Gregorian) date. However, it is easy to add a feature that encodes whether a\\nflight was on, preceding, or following a public or school holiday. In this way, prior\\nknowledge about the nature of the task can be encoded in the features to aid a\\nmachine learning algorithm. Adding a feature does not force a machine learning\\nalgorithm to use it, and even if the holiday information turns out to be noninforma‐\\ntive for flight prices, augmenting the data with this information doesn’t hurt.\\nWe’ll now look at one particular case of using expert knowledge—though in this case\\nit might be more rightfully called “common sense.” The task is predicting bicycle rent‐\\nals in front of Andreas’s house.\\nIn New York, Citi Bike operates a network of bicycle rental stations with a subscrip‐\\ntion system. The stations are all over the city and provide a convenient way to get\\naround. Bike rental data is made public in an anonymized form and has been ana‐\\nlyzed in various ways. The task we want to solve is to predict for a given time and day\\nhow many people will rent a bike in front of Andreas’s house—so he knows if any\\nbikes will be left for him.\\nWe first load the data for August 2015 for this particular station as a pandas Data\\nFrame. We resample the data into three-hour intervals to obtain the main trends for\\neach day:\\nIn[49]:\\ncitibike = mglearn.datasets.load_citibike()\\n242 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 256}, page_content='In[50]:\\nprint(\"Citi Bike data:\\\\n{}\".format(citibike.head()))\\nOut[50]:\\nCiti Bike data:\\nstarttime\\n2015-08-01 00:00:00     3.0\\n2015-08-01 03:00:00     0.0\\n2015-08-01 06:00:00     9.0\\n2015-08-01 09:00:00    41.0\\n2015-08-01 12:00:00    39.0\\nFreq: 3H, Name: one, dtype: float64\\nThe following example shows a visualization of the rental frequencies for the whole\\nmonth (Figure 4-12):\\nIn[51]:\\nplt.figure(figsize=(10, 3))\\nxticks = pd.date_range(start=citibike.index.min(), end=citibike.index.max(),\\n                       freq=\\'D\\')\\nplt.xticks(xticks, xticks.strftime(\"%a %m-%d\"), rotation=90, ha=\"left\")\\nplt.plot(citibike, linewidth=1)\\nplt.xlabel(\"Date\")\\nplt.ylabel(\"Rentals\")\\nFigure 4-12. Number of bike rentals over time for a selected Citi Bike station\\nLooking at the data, we can clearly distinguish day and night for each 24-hour inter‐\\nval. The patterns for weekdays and weekends also seem to be quite different. When\\nevaluating a prediction task on a time series like this, we usually want to learn from\\nthe past and predict for the future. This means when doing a split into a training and a\\ntest set, we want to use all the data up to a certain date as the training set and all the\\ndata past that date as the test set. This is how we would usually use time series predic‐\\ntion: given everything that we know about rentals in the past, what do we think will\\nUtilizing Expert Knowledge \\n| \\n243'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 257}, page_content='happen tomorrow? We will use the first 184 data points, corresponding to the first 23\\ndays, as our training set, and the remaining 64 data points, corresponding to the\\nremaining 8 days, as our test set.\\nThe only feature that we are using in our prediction task is the date and time when a\\nparticular number of rentals occurred. So, the input feature is the date and time—say,\\n2015-08-01 00:00:00—and the output is the number of rentals in the following\\nthree hours (three in this case, according to our DataFrame).\\nA (surprisingly) common way that dates are stored on computers is using POSIX\\ntime, which is the number of seconds since January 1970 00:00:00 (aka the beginning\\nof Unix time). As a first try, we can use this single integer feature as our data repre‐\\nsentation:\\nIn[52]:\\n# extract the target values (number of rentals)\\ny = citibike.values\\n# convert the time to POSIX time using \"%s\"\\nX = citibike.index.strftime(\"%s\").astype(\"int\").reshape(-1, 1)\\nWe first define a function to split the data into training and test sets, build the model,\\nand visualize the result:\\nIn[54]:\\n# use the first 184 data points for training, and the rest for testing\\nn_train = 184\\n# function to evaluate and plot a regressor on a given feature set\\ndef eval_on_features(features, target, regressor):\\n    # split the given features into a training and a test set\\n    X_train, X_test = features[:n_train], features[n_train:]\\n    # also split the target array\\n    y_train, y_test = target[:n_train], target[n_train:]\\n    regressor.fit(X_train, y_train)\\n    print(\"Test-set R^2: {:.2f}\".format(regressor.score(X_test, y_test)))\\n    y_pred = regressor.predict(X_test)\\n    y_pred_train = regressor.predict(X_train)\\n    plt.figure(figsize=(10, 3))\\n    plt.xticks(range(0, len(X), 8), xticks.strftime(\"%a %m-%d\"), rotation=90,\\n               ha=\"left\")\\n    plt.plot(range(n_train), y_train, label=\"train\")\\n    plt.plot(range(n_train, len(y_test) + n_train), y_test, \\'-\\', label=\"test\")\\n    plt.plot(range(n_train), y_pred_train, \\'--\\', label=\"prediction train\")\\n    plt.plot(range(n_train, len(y_test) + n_train), y_pred, \\'--\\',\\n             label=\"prediction test\")\\n    plt.legend(loc=(1.01, 0))\\n    plt.xlabel(\"Date\")\\n    plt.ylabel(\"Rentals\")\\n244 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 258}, page_content='We saw earlier that random forests require very little preprocessing of the data, which\\nmakes this seem like a good model to start with. We use the POSIX time feature X and\\npass a random forest regressor to our eval_on_features function. Figure 4-13 shows\\nthe result:\\nIn[55]:\\nfrom sklearn.ensemble import RandomForestRegressor\\nregressor = RandomForestRegressor(n_estimators=100, random_state=0)\\nplt.figure()\\neval_on_features(X, y, regressor)\\nOut[55]:\\nTest-set R^2: -0.04\\nFigure 4-13. Predictions made by a random forest using only the POSIX time\\nThe predictions on the training set are quite good, as is usual for random forests.\\nHowever, for the test set, a constant line is predicted. The R2 is –0.03, which means\\nthat we learned nothing. What happened?\\nThe problem lies in the combination of our feature and the random forest. The value\\nof the POSIX time feature for the test set is outside of the range of the feature values\\nin the training set: the points in the test set have timestamps that are later than all the\\npoints in the training set. Trees, and therefore random forests, cannot extrapolate to\\nfeature ranges outside the training set. The result is that the model simply predicts the\\ntarget value of the closest point in the training set—which is the last time it observed\\nany data.\\nClearly we can do better than this. This is where our “expert knowledge” comes in.\\nFrom looking at the rental figures in the training data, two factors seem to be very\\nimportant: the time of day and the day of the week. So, let’s add these two features.\\nWe can’t really learn anything from the POSIX time, so we drop that feature. First,\\nlet’s use only the hour of the day. As Figure 4-14 shows, now the predictions have the\\nsame pattern for each day of the week:\\nUtilizing Expert Knowledge \\n| \\n245'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 259}, page_content='In[56]:\\nX_hour = citibike.index.hour.reshape(-1, 1)\\neval_on_features(X_hour, y, regressor)\\nOut[56]:\\nTest-set R^2: 0.60\\nFigure 4-14. Predictions made by a random forest using only the hour of the day\\nThe R2 is already much better, but the predictions clearly miss the weekly pattern.\\nNow let’s also add the day of the week (see Figure 4-15):\\nIn[57]:\\nX_hour_week = np.hstack([citibike.index.dayofweek.reshape(-1, 1),\\n                         citibike.index.hour.reshape(-1, 1)])\\neval_on_features(X_hour_week, y, regressor)\\nOut[57]:\\nTest-set R^2: 0.84\\nFigure 4-15. Predictions with a random forest using day of week and hour of day\\nfeatures\\n246 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 260}, page_content='Now we have a model that captures the periodic behavior by considering the day of\\nweek and time of day. It has an R2 of 0.84, and shows pretty good predictive perfor‐\\nmance. What this model likely is learning is the mean number of rentals for each\\ncombination of weekday and time of day from the first 23 days of August. This\\nactually does not require a complex model like a random forest, so let’s try with a\\nsimpler model, LinearRegression (see Figure 4-16):\\nIn[58]:\\nfrom sklearn.linear_model import LinearRegression\\neval_on_features(X_hour_week, y, LinearRegression())\\nOut[58]:\\nTest-set R^2: 0.13\\nFigure 4-16. Predictions made by linear regression using day of week and hour of day as\\nfeatures\\nLinearRegression works much worse, and the periodic pattern looks odd. The rea‐\\nson for this is that we encoded day of week and time of day using integers, which are\\ninterpreted as categorical variables. Therefore, the linear model can only learn a lin‐\\near function of the time of day—and it learned that later in the day, there are more\\nrentals. However, the patterns are much more complex than that. We can capture this\\nby interpreting the integers as categorical variables, by transforming them using One\\nHotEncoder (see Figure 4-17):\\nIn[59]:\\nenc = OneHotEncoder()\\nX_hour_week_onehot = enc.fit_transform(X_hour_week).toarray()\\nUtilizing Expert Knowledge \\n| \\n247'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 261}, page_content='In[60]:\\neval_on_features(X_hour_week_onehot, y, Ridge())\\nOut[60]:\\nTest-set R^2: 0.62\\nFigure 4-17. Predictions made by linear regression using a one-hot encoding of hour of\\nday and day of week\\nThis gives us a much better match than the continuous feature encoding. Now the\\nlinear model learns one coefficient for each day of the week, and one coefficient for\\neach time of the day. That means that the “time of day” pattern is shared over all days\\nof the week, though.\\nUsing interaction features, we can allow the model to learn one coefficient for each\\ncombination of day and time of day (see Figure 4-18):\\nIn[61]:\\npoly_transformer = PolynomialFeatures(degree=2, interaction_only=True,\\n                                      include_bias=False)\\nX_hour_week_onehot_poly = poly_transformer.fit_transform(X_hour_week_onehot)\\nlr = Ridge()\\neval_on_features(X_hour_week_onehot_poly, y, lr)\\nOut[61]:\\nTest-set R^2: 0.85\\n248 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 262}, page_content='Figure 4-18. Predictions made by linear regression using a product of the day of week\\nand hour of day features\\nThis transformation finally yields a model that performs similarly well to the random\\nforest. A big benefit of this model is that it is very clear what is learned: one coeffi‐\\ncient for each day and time. We can simply plot the coefficients learned by the model,\\nsomething that would not be possible for the random forest.\\nFirst, we create feature names for the hour and day features:\\nIn[62]:\\nhour = [\"%02d:00\" % i for i in range(0, 24, 3)]\\nday = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\\nfeatures =  day + hour\\nThen we name all the interaction features extracted by PolynomialFeatures, using\\nthe get_feature_names method, and keep only the features with nonzero coeffi‐\\ncients:\\nIn[63]:\\nfeatures_poly = poly_transformer.get_feature_names(features)\\nfeatures_nonzero = np.array(features_poly)[lr.coef_ != 0]\\ncoef_nonzero = lr.coef_[lr.coef_ != 0]\\nNow we can visualize the coefficients learned by the linear model, as seen in\\nFigure 4-19:\\nIn[64]:\\nplt.figure(figsize=(15, 2))\\nplt.plot(coef_nonzero, \\'o\\')\\nplt.xticks(np.arange(len(coef_nonzero)), features_nonzero, rotation=90)\\nplt.xlabel(\"Feature magnitude\")\\nplt.ylabel(\"Feature\")\\nUtilizing Expert Knowledge \\n| \\n249'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 263}, page_content='Figure 4-19. Coefficients of the linear regression model using a product of hour and day\\nSummary and Outlook\\nIn this chapter, we discussed how to deal with different data types (in particular, with\\ncategorical variables). We emphasized the importance of representing data in a way\\nthat is suitable for the machine learning algorithm—for example, by one-hot-\\nencoding categorical variables. We also discussed the importance of engineering new\\nfeatures, and the possibility of utilizing expert knowledge in creating derived features\\nfrom your data. In particular, linear models might benefit greatly from generating\\nnew features via binning and adding polynomials and interactions, while more com‐\\nplex, nonlinear models like random forests and SVMs might be able to learn more\\ncomplex tasks without explicitly expanding the feature space. In practice, the features\\nthat are used (and the match between features and method) is often the most impor‐\\ntant piece in making a machine learning approach work well.\\nNow that you have a good idea of how to represent your data in an appropriate way\\nand which algorithm to use for which task, the next chapter will focus on evaluating\\nthe performance of machine learning models and selecting the right parameter\\nsettings.\\n250 \\n| \\nChapter 4: Representing Data and Engineering Features'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 264}, page_content='CHAPTER 5\\nModel Evaluation and Improvement\\nHaving discussed the fundamentals of supervised and unsupervised learning, and\\nhaving explored a variety of machine learning algorithms, we will now dive more\\ndeeply into evaluating models and selecting parameters.\\nWe will focus on the supervised methods, regression and classification, as evaluating\\nand selecting models in unsupervised learning is often a very qualitative process (as\\nwe saw in Chapter 3).\\nTo evaluate our supervised models, so far we have split our dataset into a training set\\nand a test set using the train_test_split function, built a model on the training set\\nby calling the fit method, and evaluated it on the test set using the score method,\\nwhich for classification computes the fraction of correctly classified samples. Here’s\\nan example of that process:\\nIn[2]:\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\n# create a synthetic dataset\\nX, y = make_blobs(random_state=0)\\n# split data and labels into a training and a test set\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n# instantiate a model and fit it to the training set\\nlogreg = LogisticRegression().fit(X_train, y_train)\\n# evaluate the model on the test set\\nprint(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\\nOut[2]:\\nTest set score: 0.88\\n251'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 265}, page_content='Remember, the reason we split our data into training and test sets is that we are inter‐\\nested in measuring how well our model generalizes to new, previously unseen data.\\nWe are not interested in how well our model fit the training set, but rather in how\\nwell it can make predictions for data that was not observed during training.\\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\\nduce cross-validation, a more robust way to assess generalization performance, and\\ndiscuss methods to evaluate classification and regression performance that go beyond\\nthe default measures of accuracy and R2 provided by the score method.\\nWe will also discuss grid search, an effective method for adjusting the parameters in\\nsupervised models for the best generalization performance.\\nCross-Validation\\nCross-validation is a statistical method of evaluating generalization performance that\\nis more stable and thorough than using a split into a training and a test set. In cross-\\nvalidation, the data is instead split repeatedly and multiple models are trained. The\\nmost commonly used version of cross-validation is k-fold cross-validation, where k is\\na user-specified number, usually 5 or 10. When performing five-fold cross-validation,\\nthe data is first partitioned into five parts of (approximately) equal size, called folds.\\nNext, a sequence of models is trained. The first model is trained using the first fold as\\nthe test set, and the remaining folds (2–5) are used as the training set. The model is\\nbuilt using the data in folds 2–5, and then the accuracy is evaluated on fold 1. Then\\nanother model is built, this time using fold 2 as the test set and the data in folds 1, 3,\\n4, and 5 as the training set. This process is repeated using folds 3, 4, and 5 as test sets.\\nFor each of these five splits of the data into training and test sets, we compute the\\naccuracy. In the end, we have collected five accuracy values. The process is illustrated\\nin Figure 5-1:\\nIn[3]:\\nmglearn.plots.plot_cross_validation()\\nFigure 5-1. Data splitting in five-fold cross-validation\\nUsually, the first fifth of the data is the first fold, the second fifth of the data is the\\nsecond fold, and so on.\\n252 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 266}, page_content='Cross-Validation in scikit-learn\\nCross-validation is implemented in scikit-learn using the cross_val_score func‐\\ntion from the model_selection module. The parameters of the cross_val_score\\nfunction are the model we want to evaluate, the training data, and the ground-truth\\nlabels. Let’s evaluate LogisticRegression on the iris dataset:\\nIn[4]:\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.linear_model import LogisticRegression\\niris = load_iris()\\nlogreg = LogisticRegression()\\nscores = cross_val_score(logreg, iris.data, iris.target)\\nprint(\"Cross-validation scores: {}\".format(scores))\\nOut[4]:\\nCross-validation scores: [ 0.961  0.922  0.958]\\nBy default, cross_val_score performs three-fold cross-validation, returning three\\naccuracy values. We can change the number of folds used by changing the cv parame‐\\nter:\\nIn[5]:\\nscores = cross_val_score(logreg, iris.data, iris.target, cv=5)\\nprint(\"Cross-validation scores: {}\".format(scores))\\nOut[5]:\\nCross-validation scores: [ 1.     0.967  0.933  0.9    1.   ]\\nA common way to summarize the cross-validation accuracy is to compute the mean:\\nIn[6]:\\nprint(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\\nOut[6]:\\nAverage cross-validation score: 0.96\\nUsing the mean cross-validation we can conclude that we expect the model to be\\naround 96% accurate on average. Looking at all five scores produced by the five-fold\\ncross-validation, we can also conclude that there is a relatively high variance in the\\naccuracy between folds, ranging from 100% accuracy to 90% accuracy. This could\\nimply that the model is very dependent on the particular folds used for training, but it\\ncould also just be a consequence of the small size of the dataset.\\nCross-Validation \\n| \\n253'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 267}, page_content='Benefits of Cross-Validation\\nThere are several benefits to using cross-validation instead of a single split into a\\ntraining and a test set. First, remember that train_test_split performs a random\\nsplit of the data. Imagine that we are “lucky” when randomly splitting the data, and\\nall examples that are hard to classify end up in the training set. In that case, the test\\nset will only contain “easy” examples, and our test set accuracy will be unrealistically\\nhigh. Conversely, if we are “unlucky,” we might have randomly put all the hard-to-\\nclassify examples in the test set and consequently obtain an unrealistically low score.\\nHowever, when using cross-validation, each example will be in the training set exactly\\nonce: each example is in one of the folds, and each fold is the test set once. Therefore,\\nthe model needs to generalize well to all of the samples in the dataset for all of the\\ncross-validation scores (and their mean) to be high.\\nHaving multiple splits of the data also provides some information about how sensi‐\\ntive our model is to the selection of the training dataset. For the iris dataset, we saw\\naccuracies between 90% and 100%. This is quite a range, and it provides us with an\\nidea about how the model might perform in the worst case and best case scenarios\\nwhen applied to new data.\\nAnother benefit of cross-validation as compared to using a single split of the data is\\nthat we use our data more effectively. When using train_test_split, we usually use\\n75% of the data for training and 25% of the data for evaluation. When using five-fold\\ncross-validation, in each iteration we can use four-fifths of the data (80%) to fit the\\nmodel. When using 10-fold cross-validation, we can use nine-tenths of the data\\n(90%) to fit the model. More data will usually result in more accurate models.\\nThe main disadvantage of cross-validation is increased computational cost. As we are\\nnow training k models instead of a single model, cross-validation will be roughly k\\ntimes slower than doing a single split of the data.\\nIt is important to keep in mind that cross-validation is not a way to\\nbuild a model that can be applied to new data. Cross-validation\\ndoes not return a model. When calling cross_val_score, multiple\\nmodels are built internally, but the purpose of cross-validation is\\nonly to evaluate how well a given algorithm will generalize when\\ntrained on a specific dataset.\\nStratified k-Fold Cross-Validation and Other Strategies\\nSplitting the dataset into k folds by starting with the first one-k-th part of the data, as\\ndescribed in the previous section, might not always be a good idea. For example, let’s\\nhave a look at the iris dataset:\\n254 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 268}, page_content='In[7]:\\nfrom sklearn.datasets import load_iris\\niris = load_iris()\\nprint(\"Iris labels:\\\\n{}\".format(iris.target))\\nOut[7]:\\nIris labels:\\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\\n 2 2]\\nAs you can see, the first third of the data is the class 0, the second third is the class 1,\\nand the last third is the class 2. Imagine doing three-fold cross-validation on this\\ndataset. The first fold would be only class 0, so in the first split of the data, the test set\\nwould be only class 0, and the training set would be only classes 1 and 2. As the\\nclasses in training and test sets would be different for all three splits, the three-fold\\ncross-validation accuracy would be zero on this dataset. That is not very helpful, as\\nwe can do much better than 0% accuracy on iris.\\nAs the simple k-fold strategy fails here, scikit-learn does not use it for classifica‐\\ntion, but rather uses stratified k-fold cross-validation. In stratified cross-validation, we\\nsplit the data such that the proportions between classes are the same in each fold as\\nthey are in the whole dataset, as illustrated in Figure 5-2:\\nIn[8]:\\nmglearn.plots.plot_stratified_cross_validation()\\nFigure 5-2. Comparison of standard cross-validation and stratified cross-validation\\nwhen the data is ordered by class label\\nCross-Validation \\n| \\n255'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 269}, page_content='For example, if 90% of your samples belong to class A and 10% of your samples\\nbelong to class B, then stratified cross-validation ensures that in each fold, 90% of\\nsamples belong to class A and 10% of samples belong to class B.\\nIt is usually a good idea to use stratified k-fold cross-validation instead of k-fold\\ncross-validation to evaluate a classifier, because it results in more reliable estimates of\\ngeneralization performance. In the case of only 10% of samples belonging to class B,\\nusing standard k-fold cross-validation it might easily happen that one fold only con‐\\ntains samples of class A. Using this fold as a test set would not be very informative\\nabout the overall performance of the classifier.\\nFor regression, scikit-learn uses the standard k-fold cross-validation by default. It\\nwould be possible to also try to make each fold representative of the different values\\nthe regression target has, but this is not a commonly used strategy and would be sur‐\\nprising to most users.\\nMore control over cross-validation\\nWe saw earlier that we can adjust the number of folds that are used in\\ncross_val_score using the cv parameter. However, scikit-learn allows for much\\nfiner control over what happens during the splitting of the data by providing a cross-\\nvalidation splitter as the cv parameter. For most use cases, the defaults of k-fold cross-\\nvalidation for regression and stratified k-fold for classification work well, but there\\nare some cases where you might want to use a different strategy. Say, for example, we\\nwant to use the standard k-fold cross-validation on a classification dataset to repro‐\\nduce someone else’s results. To do this, we first have to import the KFold splitter class\\nfrom the model_selection module and instantiate it with the number of folds we\\nwant to use:\\nIn[9]:\\nfrom sklearn.model_selection import KFold\\nkfold = KFold(n_splits=5)\\nThen, we can pass the kfold splitter object as the cv parameter to cross_val_score:\\nIn[10]:\\nprint(\"Cross-validation scores:\\\\n{}\".format(\\n      cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\\nOut[10]:\\nCross-validation scores:\\n[ 1.     0.933  0.433  0.967  0.433]\\nThis way, we can verify that it is indeed a really bad idea to use three-fold (nonstrati‐\\nfied) cross-validation on the iris dataset:\\n256 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 270}, page_content='In[11]:\\nkfold = KFold(n_splits=3)\\nprint(\"Cross-validation scores:\\\\n{}\".format(\\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\\nOut[11]:\\nCross-validation scores:\\n[ 0.  0.  0.]\\nRemember: each fold corresponds to one of the classes in the iris dataset, and so\\nnothing can be learned. Another way to resolve this problem is to shuffle the data\\ninstead of stratifying the folds, to remove the ordering of the samples by label. We can\\ndo that by setting the shuffle parameter of KFold to True. If we shuffle the data, we\\nalso need to fix the random_state to get a reproducible shuffling. Otherwise, each run\\nof cross_val_score would yield a different result, as each time a different split would\\nbe used (this might not be a problem, but can be surprising). Shuffling the data before\\nsplitting it yields a much better result:\\nIn[12]:\\nkfold = KFold(n_splits=3, shuffle=True, random_state=0)\\nprint(\"Cross-validation scores:\\\\n{}\".format(\\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\\nOut[12]:\\nCross-validation scores:\\n[ 0.9   0.96  0.96]\\nLeave-one-out cross-validation\\nAnother frequently used cross-validation method is leave-one-out. You can think of\\nleave-one-out cross-validation as k-fold cross-validation where each fold is a single\\nsample. For each split, you pick a single data point to be the test set. This can be very\\ntime consuming, particularly for large datasets, but sometimes provides better esti‐\\nmates on small datasets:\\nIn[13]:\\nfrom sklearn.model_selection import LeaveOneOut\\nloo = LeaveOneOut()\\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\\nprint(\"Number of cv iterations: \", len(scores))\\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))\\nOut[13]:\\nNumber of cv iterations:  150\\nMean accuracy: 0.95\\nCross-Validation \\n| \\n257'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 271}, page_content='Shuffle-split cross-validation\\nAnother, very flexible strategy for cross-validation is shuffle-split cross-validation. In\\nshuffle-split cross-validation, each split samples train_size many points for the\\ntraining set and test_size many (disjoint) point for the test set. This splitting is\\nrepeated n_iter times. Figure 5-3 illustrates running four iterations of splitting a\\ndataset consisting of 10 points, with a training set of 5 points and test sets of 2 points\\neach (you can use integers for train_size and test_size to use absolute sizes for\\nthese sets, or floating-point numbers to use fractions of the whole dataset):\\nIn[14]:\\nmglearn.plots.plot_shuffle_split()\\nFigure 5-3. ShuffleSplit with 10 points, train_size=5, test_size=2, and n_iter=4\\nThe following code splits the dataset into 50% training set and 50% test set for 10\\niterations:\\nIn[15]:\\nfrom sklearn.model_selection import ShuffleSplit\\nshuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\\nscores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split)\\nprint(\"Cross-validation scores:\\\\n{}\".format(scores))\\nOut[15]:\\nCross-validation scores:\\n[ 0.96   0.907  0.947  0.96   0.96   0.907  0.893  0.907  0.92   0.973]\\nShuffle-split cross-validation allows for control over the number of iterations inde‐\\npendently of the training and test sizes, which can sometimes be helpful. It also allows\\nfor using only part of the data in each iteration, by providing train_size and\\ntest_size settings that don’t add up to one. Subsampling the data in this way can be\\nuseful for experimenting with large datasets.\\nThere is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS\\nplit, which can provide more reliable results for classification tasks.\\n258 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 272}, page_content='Cross-validation with groups\\nAnother very common setting for cross-validation is when there are groups in the\\ndata that are highly related. Say you want to build a system to recognize emotions\\nfrom pictures of faces, and you collect a dataset of pictures of 100 people where each\\nperson is captured multiple times, showing various emotions. The goal is to build a\\nclassifier that can correctly identify emotions of people not in the dataset. You could\\nuse the default stratified cross-validation to measure the performance of a classifier\\nhere. However, it is likely that pictures of the same person will be in both the training\\nand the test set. It will be much easier for a classifier to detect emotions in a face that\\nis part of the training set, compared to a completely new face. To accurately evaluate\\nthe generalization to new faces, we must therefore ensure that the training and test\\nsets contain images of different people.\\nTo achieve this, we can use GroupKFold, which takes an array of groups as argument\\nthat we can use to indicate which person is in the image. The groups array here indi‐\\ncates groups in the data that should not be split when creating the training and test\\nsets, and should not be confused with the class label.\\nThis example of groups in the data is common in medical applications, where you\\nmight have multiple samples from the same patient, but are interested in generalizing\\nto new patients. Similarly, in speech recognition, you might have multiple recordings\\nof the same speaker in your dataset, but are interested in recognizing speech of new\\nspeakers.\\nThe following is an example of using a synthetic dataset with a grouping given by the\\ngroups array. The dataset consists of 12 data points, and for each of the data points,\\ngroups specifies which group (think patient) the point belongs to. The groups specify\\nthat there are four groups, and the first three samples belong to the first group, the\\nnext four samples belong to the second group, and so on:\\nIn[17]:\\nfrom sklearn.model_selection import GroupKFold\\n# create synthetic dataset\\nX, y = make_blobs(n_samples=12, random_state=0)\\n# assume the first three samples belong to the same group,\\n# then the next four, etc.\\ngroups = [0, 0, 0, 1, 1, 1, 1, 2, 2, 3, 3, 3]\\nscores = cross_val_score(logreg, X, y, groups, cv=GroupKFold(n_splits=3))\\nprint(\"Cross-validation scores:\\\\n{}\".format(scores))\\nOut[17]:\\nCross-validation scores:\\n[ 0.75   0.8    0.667]\\nThe samples don’t need to be ordered by group; we just did this for illustration pur‐\\nposes. The splits that are calculated based on these labels are visualized in Figure 5-4.\\nCross-Validation \\n| \\n259'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 273}, page_content='As you can see, for each split, each group is either entirely in the training set or\\nentirely in the test set:\\nIn[16]:\\nmglearn.plots.plot_label_kfold()\\nFigure 5-4. Label-dependent splitting with GroupKFold\\nThere are more splitting strategies for cross-validation in scikit-learn, which allow\\nfor an even greater variety of use cases (you can find these in the scikit-learn user\\nguide). However, the standard KFold, StratifiedKFold, and GroupKFold are by far\\nthe most commonly used ones.\\nGrid Search\\nNow that we know how to evaluate how well a model generalizes, we can take the\\nnext step and improve the model’s generalization performance by tuning its parame‐\\nters. We discussed the parameter settings of many of the algorithms in scikit-learn\\nin Chapters 2 and 3, and it is important to understand what the parameters mean\\nbefore trying to adjust them. Finding the values of the important parameters of a\\nmodel (the ones that provide the best generalization performance) is a tricky task, but\\nnecessary for almost all models and datasets. Because it is such a common task, there\\nare standard methods in scikit-learn to help you with it. The most commonly used\\nmethod is grid search, which basically means trying all possible combinations of the\\nparameters of interest.\\nConsider the case of a kernel SVM with an RBF (radial basis function) kernel, as\\nimplemented in the SVC class. As we discussed in Chapter 2, there are two important\\nparameters: the kernel bandwidth, gamma, and the regularization parameter, C. Say we\\nwant to try the values 0.001, 0.01, 0.1, 1, 10, and 100 for the parameter C, and the\\nsame for gamma. Because we have six different settings for C and gamma that we want to\\ntry, we have 36 combinations of parameters in total. Looking at all possible combina‐\\ntions creates a table (or grid) of parameter settings for the SVM, as shown here:\\n260 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 274}, page_content='C = 0.001\\nC = 0.01\\n… C = 10\\ngamma=0.001\\nSVC(C=0.001, gamma=0.001)\\nSVC(C=0.01, gamma=0.001)\\n… SVC(C=10, gamma=0.001)\\ngamma=0.01\\nSVC(C=0.001, gamma=0.01)\\nSVC(C=0.01, gamma=0.01)\\n… SVC(C=10, gamma=0.01)\\n…\\n…\\n…\\n… …\\ngamma=100\\nSVC(C=0.001, gamma=100)\\nSVC(C=0.01, gamma=100)\\n… SVC(C=10, gamma=100)\\nSimple Grid Search\\nWe can implement a simple grid search just as for loops over the two parameters,\\ntraining and evaluating a classifier for each combination:\\nIn[18]:\\n# naive grid search implementation\\nfrom sklearn.svm import SVC\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris.data, iris.target, random_state=0)\\nprint(\"Size of training set: {}   size of test set: {}\".format(\\n      X_train.shape[0], X_test.shape[0]))\\nbest_score = 0\\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\\n        # for each combination of parameters, train an SVC\\n        svm = SVC(gamma=gamma, C=C)\\n        svm.fit(X_train, y_train)\\n        # evaluate the SVC on the test set\\n        score = svm.score(X_test, y_test)\\n        # if we got a better score, store the score and parameters\\n        if score > best_score:\\n            best_score = score\\n            best_parameters = {\\'C\\': C, \\'gamma\\': gamma}\\nprint(\"Best score: {:.2f}\".format(best_score))\\nprint(\"Best parameters: {}\".format(best_parameters))\\nOut[18]:\\nSize of training set: 112   size of test set: 38\\nBest score: 0.97\\nBest parameters: {\\'C\\': 100, \\'gamma\\': 0.001}\\nThe Danger of Overfitting the Parameters and the Validation Set\\nGiven this result, we might be tempted to report that we found a model that performs\\nwith 97% accuracy on our dataset. However, this claim could be overly optimistic (or\\njust wrong), for the following reason: we tried many different parameters and\\nGrid Search \\n| \\n261'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 275}, page_content='selected the one with best accuracy on the test set, but this accuracy won’t necessarily\\ncarry over to new data. Because we used the test data to adjust the parameters, we can\\nno longer use it to assess how good the model is. This is the same reason we needed\\nto split the data into training and test sets in the first place; we need an independent\\ndataset to evaluate, one that was not used to create the model.\\nOne way to resolve this problem is to split the data again, so we have three sets: the\\ntraining set to build the model, the validation (or development) set to select the\\nparameters of the model, and the test set to evaluate the performance of the selected\\nparameters. Figure 5-5 shows what this looks like:\\nIn[19]:\\nmglearn.plots.plot_threefold_split()\\nFigure 5-5. A threefold split of data into training set, validation set, and test set\\nAfter selecting the best parameters using the validation set, we can rebuild a model\\nusing the parameter settings we found, but now training on both the training data\\nand the validation data. This way, we can use as much data as possible to build our\\nmodel. This leads to the following implementation:\\nIn[20]:\\nfrom sklearn.svm import SVC\\n# split data into train+validation set and test set\\nX_trainval, X_test, y_trainval, y_test = train_test_split(\\n    iris.data, iris.target, random_state=0)\\n# split train+validation set into training and validation sets\\nX_train, X_valid, y_train, y_valid = train_test_split(\\n    X_trainval, y_trainval, random_state=1)\\nprint(\"Size of training set: {}   size of validation set: {}   size of test set:\"\\n      \" {}\\\\n\".format(X_train.shape[0], X_valid.shape[0], X_test.shape[0]))\\nbest_score = 0\\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\\n        # for each combination of parameters, train an SVC\\n        svm = SVC(gamma=gamma, C=C)\\n        svm.fit(X_train, y_train)\\n        # evaluate the SVC on the test set\\n        score = svm.score(X_valid, y_valid)\\n        # if we got a better score, store the score and parameters\\n        if score > best_score:\\n            best_score = score\\n            best_parameters = {\\'C\\': C, \\'gamma\\': gamma}\\n262 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 276}, page_content='# rebuild a model on the combined training and validation set,\\n# and evaluate it on the test set\\nsvm = SVC(**best_parameters)\\nsvm.fit(X_trainval, y_trainval)\\ntest_score = svm.score(X_test, y_test)\\nprint(\"Best score on validation set: {:.2f}\".format(best_score))\\nprint(\"Best parameters: \", best_parameters)\\nprint(\"Test set score with best parameters: {:.2f}\".format(test_score))\\nOut[20]:\\nSize of training set: 84   size of validation set: 28   size of test set: 38\\nBest score on validation set: 0.96\\nBest parameters:  {\\'C\\': 10, \\'gamma\\': 0.001}\\nTest set score with best parameters: 0.92\\nThe best score on the validation set is 96%: slightly lower than before, probably\\nbecause we used less data to train the model (X_train is smaller now because we split\\nour dataset twice). However, the score on the test set—the score that actually tells us\\nhow well we generalize—is even lower, at 92%. So we can only claim to classify new\\ndata 92% correctly, not 97% correctly as we thought before!\\nThe distinction between the training set, validation set, and test set is fundamentally\\nimportant to applying machine learning methods in practice. Any choices made\\nbased on the test set accuracy “leak” information from the test set into the model.\\nTherefore, it is important to keep a separate test set, which is only used for the final\\nevaluation. It is good practice to do all exploratory analysis and model selection using\\nthe combination of a training and a validation set, and reserve the test set for a final\\nevaluation—this is even true for exploratory visualization. Strictly speaking, evaluat‐\\ning more than one model on the test set and choosing the better of the two will result\\nin an overly optimistic estimate of how accurate the model is.\\nGrid Search with Cross-Validation\\nWhile the method of splitting the data into a training, a validation, and a test set that\\nwe just saw is workable, and relatively commonly used, it is quite sensitive to how\\nexactly the data is split. From the output of the previous code snippet we can see that\\nGridSearchCV selects \\'C\\': 10, \\'gamma\\': 0.001 as the best parameters, while the\\noutput of the code in the previous section selects \\'C\\': 100, \\'gamma\\': 0.001 as the\\nbest parameters. For a better estimate of the generalization performance, instead of\\nusing a single split into a training and a validation set, we can use cross-validation to\\nevaluate the performance of each parameter combination. This method can be coded\\nup as follows:\\nGrid Search \\n| \\n263'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 277}, page_content=\"In[21]:\\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\\n        # for each combination of parameters,\\n        # train an SVC\\n        svm = SVC(gamma=gamma, C=C)\\n        # perform cross-validation\\n        scores = cross_val_score(svm, X_trainval, y_trainval, cv=5)\\n        # compute mean cross-validation accuracy\\n        score = np.mean(scores)\\n        # if we got a better score, store the score and parameters\\n        if score > best_score:\\n            best_score = score\\n            best_parameters = {'C': C, 'gamma': gamma}\\n# rebuild a model on the combined training and validation set\\nsvm = SVC(**best_parameters)\\nsvm.fit(X_trainval, y_trainval)\\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using\\nfive-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine,\\nthe main downside of the use of cross-validation is the time it takes to train all these\\nmodels.\\nThe following visualization (Figure 5-6) illustrates how the best parameter setting is\\nselected in the preceding code:\\nIn[22]:\\nmglearn.plots.plot_cross_val_selection()\\nFigure 5-6. Results of grid search with cross-validation\\nFor each parameter setting (only a subset is shown), five accuracy values are compu‐\\nted, one for each split in the cross-validation. Then the mean validation accuracy is\\ncomputed for each parameter setting. The parameters with the highest mean valida‐\\ntion accuracy are chosen, marked by the circle.\\n264 \\n| \\nChapter 5: Model Evaluation and Improvement\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 278}, page_content='As we said earlier, cross-validation is a way to evaluate a given algo‐\\nrithm on a specific dataset. However, it is often used in conjunction\\nwith parameter search methods like grid search. For this reason,\\nmany people use the term cross-validation colloquially to refer to\\ngrid search with cross-validation.\\nThe overall process of splitting the data, running the grid search, and evaluating the\\nfinal parameters is illustrated in Figure 5-7:\\nIn[23]:\\nmglearn.plots.plot_grid_search_overview()\\nFigure 5-7. Overview of the process of parameter selection and model evaluation with\\nGridSearchCV\\nBecause grid search with cross-validation is such a commonly used method to adjust\\nparameters, scikit-learn provides the GridSearchCV class, which implements it in\\nthe form of an estimator. To use the GridSearchCV class, you first need to specify the\\nparameters you want to search over using a dictionary. GridSearchCV will then per‐\\nform all the necessary model fits. The keys of the dictionary are the names of parame‐\\nters we want to adjust (as given when constructing the model—in this case, C and\\ngamma), and the values are the parameter settings we want to try out. Trying the val‐\\nues 0.001, 0.01, 0.1, 1, 10, and 100 for C and gamma translates to the following\\ndictionary:\\nIn[24]:\\nparam_grid = {\\'C\\': [0.001, 0.01, 0.1, 1, 10, 100],\\n              \\'gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\nprint(\"Parameter grid:\\\\n{}\".format(param_grid))\\nOut[24]:\\nParameter grid:\\n{\\'C\\': [0.001, 0.01, 0.1, 1, 10, 100], \\'gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\nGrid Search \\n| \\n265'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 279}, page_content='1 A scikit-learn estimator that is created using another estimator is called a meta-estimator. GridSearchCV is\\nthe most commonly used meta-estimator, but we will see more later.\\nWe can now instantiate the GridSearchCV class with the model (SVC), the parameter\\ngrid to search (param_grid), and the cross-validation strategy we want to use (say,\\nfive-fold stratified cross-validation):\\nIn[25]:\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.svm import SVC\\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5)\\nGridSearchCV will use cross-validation in place of the split into a training and valida‐\\ntion set that we used before. However, we still need to split the data into a training\\nand a test set, to avoid overfitting the parameters:\\nIn[26]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    iris.data, iris.target, random_state=0)\\nThe grid_search object that we created behaves just like a classifier; we can call the\\nstandard methods fit, predict, and score on it.1 However, when we call fit, it will\\nrun cross-validation for each combination of parameters we specified in param_grid:\\nIn[27]:\\ngrid_search.fit(X_train, y_train)\\nFitting the GridSearchCV object not only searches for the best parameters, but also\\nautomatically fits a new model on the whole training dataset with the parameters that\\nyielded the best cross-validation performance. What happens in fit is therefore\\nequivalent to the result of the In[21] code we saw at the beginning of this section. The\\nGridSearchCV class provides a very convenient interface to access the retrained\\nmodel using the predict and score methods. To evaluate how well the best found\\nparameters generalize, we can call score on the test set:\\nIn[28]:\\nprint(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))\\nOut[28]:\\nTest set score: 0.97\\nChoosing the parameters using cross-validation, we actually found a model that ach‐\\nieves 97% accuracy on the test set. The important thing here is that we did not use the\\ntest set to choose the parameters. The parameters that were found are scored in the\\n266 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 280}, page_content='best_params_ attribute, and the best cross-validation accuracy (the mean accuracy\\nover the different splits for this parameter setting) is stored in best_score_:\\nIn[29]:\\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\\nOut[29]:\\nBest parameters: {\\'C\\': 100, \\'gamma\\': 0.01}\\nBest cross-validation score: 0.97\\nAgain, be careful not to confuse best_score_ with the generaliza‐\\ntion performance of the model as computed by the score method\\non the test set. Using the score method (or evaluating the output of\\nthe predict method) employs a model trained on the whole train‐\\ning set. The best_score_ attribute stores the mean cross-validation\\naccuracy, with cross-validation performed on the training set.\\nSometimes it is helpful to have access to the actual model that was found—for exam‐\\nple, to look at coefficients or feature importances. You can access the model with the\\nbest parameters trained on the whole training set using the best_estimator_\\nattribute:\\nIn[30]:\\nprint(\"Best estimator:\\\\n{}\".format(grid_search.best_estimator_))\\nOut[30]:\\nBest estimator:\\nSVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\\n   decision_function_shape=None, degree=3, gamma=0.01, kernel=\\'rbf\\',\\n   max_iter=-1, probability=False, random_state=None, shrinking=True,\\n   tol=0.001, verbose=False)\\nBecause grid_search itself has predict and score methods, using best_estimator_\\nis not needed to make predictions or evaluate the model.\\nAnalyzing the result of cross-validation\\nIt is often helpful to visualize the results of cross-validation, to understand how the\\nmodel generalization depends on the parameters we are searching. As grid searches\\nare quite computationally expensive to run, often it is a good idea to start with a rela‐\\ntively coarse and small grid. We can then inspect the results of the cross-validated\\ngrid search, and possibly expand our search. The results of a grid search can be found\\nin the cv_results_ attribute, which is a dictionary storing all aspects of the search. It\\nGrid Search \\n| \\n267'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 281}, page_content='contains a lot of details, as you can see in the following output, and is best looked at\\nafter converting it to a pandas DataFrame:\\nIn[31]:\\nimport pandas as pd\\n# convert to DataFrame\\nresults = pd.DataFrame(grid_search.cv_results_)\\n# show the first 5 rows\\ndisplay(results.head())\\nOut[31]:\\n    param_C   param_gamma   params                        mean_test_score\\n0   0.001     0.001         {\\'C\\': 0.001, \\'gamma\\': 0.001}       0.366\\n1   0.001      0.01         {\\'C\\': 0.001, \\'gamma\\': 0.01}        0.366\\n2   0.001       0.1         {\\'C\\': 0.001, \\'gamma\\': 0.1}         0.366\\n3   0.001         1         {\\'C\\': 0.001, \\'gamma\\': 1}           0.366\\n4   0.001        10         {\\'C\\': 0.001, \\'gamma\\': 10}          0.366\\n       rank_test_score  split0_test_score  split1_test_score  split2_test_score\\n0               22              0.375           0.347           0.363\\n1               22              0.375           0.347           0.363\\n2               22              0.375           0.347           0.363\\n3               22              0.375           0.347           0.363\\n4               22              0.375           0.347           0.363\\n       split3_test_score  split4_test_score  std_test_score\\n0           0.363              0.380           0.011\\n1           0.363              0.380           0.011\\n2           0.363              0.380           0.011\\n3           0.363              0.380           0.011\\n4           0.363              0.380           0.011\\nEach row in results corresponds to one particular parameter setting. For each set‐\\nting, the results of all cross-validation splits are recorded, as well as the mean and\\nstandard deviation over all splits. As we were searching a two-dimensional grid of\\nparameters (C and gamma), this is best visualized as a heat map (Figure 5-8). First we\\nextract the mean validation scores, then we reshape the scores so that the axes corre‐\\nspond to C and gamma:\\nIn[32]:\\nscores = np.array(results.mean_test_score).reshape(6, 6)\\n# plot the mean cross-validation scores\\nmglearn.tools.heatmap(scores, xlabel=\\'gamma\\', xticklabels=param_grid[\\'gamma\\'],\\n                      ylabel=\\'C\\', yticklabels=param_grid[\\'C\\'], cmap=\"viridis\")\\n268 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 282}, page_content='Figure 5-8. Heat map of mean cross-validation score as a function of C and gamma\\nEach point in the heat map corresponds to one run of cross-validation, with a partic‐\\nular parameter setting. The color encodes the cross-validation accuracy, with light\\ncolors meaning high accuracy and dark colors meaning low accuracy. You can see\\nthat SVC is very sensitive to the setting of the parameters. For many of the parameter\\nsettings, the accuracy is around 40%, which is quite bad; for other settings the accu‐\\nracy is around 96%. We can take away from this plot several things. First, the parame‐\\nters we adjusted are very important for obtaining good performance. Both parameters\\n(C and gamma) matter a lot, as adjusting them can change the accuracy from 40% to\\n96%. Additionally, the ranges we picked for the parameters are ranges in which we\\nsee significant changes in the outcome. It’s also important to note that the ranges for\\nthe parameters are large enough: the optimum values for each parameter are not on\\nthe edges of the plot.\\nNow let’s look at some plots (shown in Figure 5-9) where the result is less ideal,\\nbecause the search ranges were not chosen properly:\\nFigure 5-9. Heat map visualizations of misspecified search grids\\nGrid Search \\n| \\n269'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 283}, page_content='In[33]:\\nfig, axes = plt.subplots(1, 3, figsize=(13, 5))\\nparam_grid_linear = {\\'C\\': np.linspace(1, 2, 6),\\n                     \\'gamma\\':  np.linspace(1, 2, 6)}\\nparam_grid_one_log = {\\'C\\': np.linspace(1, 2, 6),\\n                      \\'gamma\\':  np.logspace(-3, 2, 6)}\\nparam_grid_range = {\\'C\\': np.logspace(-3, 2, 6),\\n                    \\'gamma\\':  np.logspace(-7, -2, 6)}\\nfor param_grid, ax in zip([param_grid_linear, param_grid_one_log,\\n                           param_grid_range], axes):\\n    grid_search = GridSearchCV(SVC(), param_grid, cv=5)\\n    grid_search.fit(X_train, y_train)\\n    scores = grid_search.cv_results_[\\'mean_test_score\\'].reshape(6, 6)\\n    # plot the mean cross-validation scores\\n    scores_image = mglearn.tools.heatmap(\\n        scores, xlabel=\\'gamma\\', ylabel=\\'C\\', xticklabels=param_grid[\\'gamma\\'],\\n        yticklabels=param_grid[\\'C\\'], cmap=\"viridis\", ax=ax)\\nplt.colorbar(scores_image, ax=axes.tolist())\\nThe first panel shows no changes at all, with a constant color over the whole parame‐\\nter grid. In this case, this is caused by improper scaling and range of the parameters C\\nand gamma. However, if no change in accuracy is visible over the different parameter\\nsettings, it could also be that a parameter is just not important at all. It is usually good\\nto try very extreme values first, to see if there are any changes in the accuracy as a\\nresult of changing a parameter.\\nThe second panel shows a vertical stripe pattern. This indicates that only the setting\\nof the gamma parameter makes any difference. This could mean that the gamma param‐\\neter is searching over interesting values but the C parameter is not—or it could mean\\nthe C parameter is not important.\\nThe third panel shows changes in both C and gamma. However, we can see that in the\\nentire bottom left of the plot, nothing interesting is happening. We can probably\\nexclude the very small values from future grid searches. The optimum parameter set‐\\nting is at the top right. As the optimum is in the border of the plot, we can expect that\\nthere might be even better values beyond this border, and we might want to change\\nour search range to include more parameters in this region.\\nTuning the parameter grid based on the cross-validation scores is perfectly fine, and a\\ngood way to explore the importance of different parameters. However, you should\\nnot test different parameter ranges on the final test set—as we discussed earlier, eval‐\\n270 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 284}, page_content='uation of the test set should happen only once we know exactly what model we want\\nto use.\\nSearch over spaces that are not grids\\nIn some cases, trying all possible combinations of all parameters as GridSearchCV\\nusually does, is not a good idea. For example, SVC has a kernel parameter, and\\ndepending on which kernel is chosen, other parameters will be relevant. If ker\\nnel=\\'linear\\', the model is linear, and only the C parameter is used. If kernel=\\'rbf\\',\\nboth the C and gamma parameters are used (but not other parameters like degree). In\\nthis case, searching over all possible combinations of C, gamma, and kernel wouldn’t\\nmake sense: if kernel=\\'linear\\', gamma is not used, and trying different values for\\ngamma would be a waste of time. To deal with these kinds of “conditional” parameters,\\nGridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the\\nlist is expanded into an independent grid. A possible grid search involving kernel and\\nparameters could look like this:\\nIn[34]:\\nparam_grid = [{\\'kernel\\': [\\'rbf\\'],\\n               \\'C\\': [0.001, 0.01, 0.1, 1, 10, 100],\\n               \\'gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]},\\n              {\\'kernel\\': [\\'linear\\'],\\n               \\'C\\': [0.001, 0.01, 0.1, 1, 10, 100]}]\\nprint(\"List of grids:\\\\n{}\".format(param_grid))\\nOut[34]:\\nList of grids:\\n[{\\'kernel\\': [\\'rbf\\'], \\'C\\': [0.001, 0.01, 0.1, 1, 10, 100],\\n  \\'gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]},\\n {\\'kernel\\': [\\'linear\\'], \\'C\\': [0.001, 0.01, 0.1, 1, 10, 100]}]\\nIn the first grid, the kernel parameter is always set to \\'rbf\\' (not that the entry for\\nkernel is a list of length one), and both the C and gamma parameters are varied. In the\\nsecond grid, the kernel parameter is always set to linear, and only C is varied. Now\\nlet’s apply this more complex parameter search:\\nIn[35]:\\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5)\\ngrid_search.fit(X_train, y_train)\\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\\nOut[35]:\\nBest parameters: {\\'C\\': 100, \\'kernel\\': \\'rbf\\', \\'gamma\\': 0.01}\\nBest cross-validation score: 0.97\\nGrid Search \\n| \\n271'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 285}, page_content=\"Let’s look at the cv_results_ again. As expected, if kernel is 'linear', then only C is\\nvaried:\\nIn[36]:\\nresults = pd.DataFrame(grid_search.cv_results_)\\n# we display the transposed table so that it better fits on the page:\\ndisplay(results.T)\\nOut[36]:\\n0\\n1\\n2\\n3\\n… 38\\n39\\n40\\n41\\nparam_C\\n0.001\\n0.001\\n0.001\\n0.001\\n… 0.1\\n1\\n10\\n100\\nparam_gamma\\n0.001\\n0.01\\n0.1\\n1\\n… NaN\\nNaN\\nNaN\\nNaN\\nparam_kernel\\nrbf\\nrbf\\nrbf\\nrbf\\n… linear\\nlinear\\nlinear\\nlinear\\nparams\\n{C: 0.001,\\nkernel: rbf,\\ngamma:\\n0.001}\\n{C: 0.001,\\nkernel: rbf,\\ngamma:\\n0.01}\\n{C: 0.001,\\nkernel: rbf,\\ngamma:\\n0.1}\\n{C: 0.001,\\nkernel: rbf,\\ngamma: 1}\\n… {C: 0.1,\\nkernel:\\nlinear}\\n{C: 1,\\nkernel:\\nlinear}\\n{C: 10,\\nkernel:\\nlinear}\\n{C: 100,\\nkernel:\\nlinear}\\nmean_test_score\\n0.37\\n0.37\\n0.37\\n0.37\\n… 0.95\\n0.97\\n0.96\\n0.96\\nrank_test_score\\n27\\n27\\n27\\n27\\n… 11\\n1\\n3\\n3\\nsplit0_test_score\\n0.38\\n0.38\\n0.38\\n0.38\\n… 0.96\\n1\\n0.96\\n0.96\\nsplit1_test_score\\n0.35\\n0.35\\n0.35\\n0.35\\n… 0.91\\n0.96\\n1\\n1\\nsplit2_test_score\\n0.36\\n0.36\\n0.36\\n0.36\\n… 1\\n1\\n1\\n1\\nsplit3_test_score\\n0.36\\n0.36\\n0.36\\n0.36\\n… 0.91\\n0.95\\n0.91\\n0.91\\nsplit4_test_score\\n0.38\\n0.38\\n0.38\\n0.38\\n… 0.95\\n0.95\\n0.95\\n0.95\\nstd_test_score\\n0.011\\n0.011\\n0.011\\n0.011\\n… 0.033\\n0.022\\n0.034\\n0.034\\n12 rows × 42 columns\\nUsing different cross-validation strategies with grid search\\nSimilarly to cross_val_score, GridSearchCV uses stratified k-fold cross-validation\\nby default for classification, and k-fold cross-validation for regression. However, you\\ncan also pass any cross-validation splitter, as described in “More control over cross-\\nvalidation” on page 256, as the cv parameter in GridSearchCV. In particular, to get\\nonly a single split into a training and a validation set, you can use ShuffleSplit or\\nStratifiedShuffleSplit with n_iter=1. This might be helpful for very large data‐\\nsets, or very slow models.\\nNested cross-validation\\nIn the preceding examples, we went from using a single split of the data into training,\\nvalidation, and test sets to splitting the data into training and test sets and then per‐\\nforming cross-validation on the training set. But when using GridSearchCV as\\n272 \\n| \\nChapter 5: Model Evaluation and Improvement\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 286}, page_content='described earlier, we still have a single split of the data into training and test sets,\\nwhich might make our results unstable and make us depend too much on this single\\nsplit of the data. We can go a step further, and instead of splitting the original data\\ninto training and test sets once, use multiple splits of cross-validation. This will result\\nin what is called nested cross-validation. In nested cross-validation, there is an outer\\nloop over splits of the data into training and test sets. For each of them, a grid search\\nis run (which might result in different best parameters for each split in the outer\\nloop). Then, for each outer split, the test set score using the best settings is reported.\\nThe result of this procedure is a list of scores—not a model, and not a parameter set‐\\nting. The scores tell us how well a model generalizes, given the best parameters found\\nby the grid. As it doesn’t provide a model that can be used on new data, nested cross-\\nvalidation is rarely used when looking for a predictive model to apply to future data.\\nHowever, it can be useful for evaluating how well a given model works on a particular\\ndataset.\\nImplementing nested cross-validation in scikit-learn is straightforward. We call\\ncross_val_score with an instance of GridSearchCV as the model:\\nIn[34]:\\nscores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5),\\n                         iris.data, iris.target, cv=5)\\nprint(\"Cross-validation scores: \", scores)\\nprint(\"Mean cross-validation score: \", scores.mean())\\nOut[34]:\\nCross-validation scores:  [ 0.967  1.     0.967  0.967  1.   ]\\nMean cross-validation score:  0.98\\nThe result of our nested cross-validation can be summarized as “SVC can achieve 98%\\nmean cross-validation accuracy on the iris dataset”—nothing more and nothing\\nless.\\nHere, we used stratified five-fold cross-validation in both the inner and the outer\\nloop. As our param_grid contains 36 combinations of parameters, this results in a\\nwhopping 36 * 5 * 5 = 900 models being built, making nested cross-validation a very\\nexpensive procedure. Here, we used the same cross-validation splitter in the inner\\nand the outer loop; however, this is not necessary and you can use any combination\\nof cross-validation strategies in the inner and outer loops. It can be a bit tricky to\\nunderstand what is happening in the single line given above, and it can be helpful to\\nvisualize it as for loops, as done in the following simplified implementation:\\nGrid Search \\n| \\n273'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 287}, page_content='In[35]:\\ndef nested_cv(X, y, inner_cv, outer_cv, Classifier, parameter_grid):\\n    outer_scores = []\\n    # for each split of the data in the outer cross-validation\\n    # (split method returns indices)\\n    for training_samples, test_samples in outer_cv.split(X, y):\\n        # find best parameter using inner cross-validation\\n        best_parms = {}\\n        best_score = -np.inf\\n        # iterate over parameters\\n        for parameters in parameter_grid:\\n            # accumulate score over inner splits\\n            cv_scores = []\\n            # iterate over inner cross-validation\\n            for inner_train, inner_test in inner_cv.split(\\n                    X[training_samples], y[training_samples]):\\n                # build classifier given parameters and training data\\n                clf = Classifier(**parameters)\\n                clf.fit(X[inner_train], y[inner_train])\\n                # evaluate on inner test set\\n                score = clf.score(X[inner_test], y[inner_test])\\n                cv_scores.append(score)\\n            # compute mean score over inner folds\\n            mean_score = np.mean(cv_scores)\\n            if mean_score > best_score:\\n                # if better than so far, remember parameters\\n                best_score = mean_score\\n                best_params = parameters\\n        # build classifier on best parameters using outer training set\\n        clf = Classifier(**best_params)\\n        clf.fit(X[training_samples], y[training_samples])\\n        # evaluate\\n        outer_scores.append(clf.score(X[test_samples], y[test_samples]))\\n    return np.array(outer_scores)\\nNow, let’s run this function on the iris dataset:\\nIn[36]:\\nfrom sklearn.model_selection import ParameterGrid, StratifiedKFold\\nscores = nested_cv(iris.data, iris.target, StratifiedKFold(5),\\n          StratifiedKFold(5), SVC, ParameterGrid(param_grid))\\nprint(\"Cross-validation scores: {}\".format(scores))\\nOut[36]:\\nCross-validation scores: [ 0.967  1.     0.967  0.967  1.   ]\\nParallelizing cross-validation and grid search\\nWhile running a grid search over many parameters and on large datasets can be com‐\\nputationally challenging, it is also embarrassingly parallel. This means that building a\\n274 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 288}, page_content='model using a particular parameter setting on a particular cross-validation split can\\nbe done completely independently from the other parameter settings and models.\\nThis makes grid search and cross-validation ideal candidates for parallelization over\\nmultiple CPU cores or over a cluster. You can make use of multiple cores in Grid\\nSearchCV and cross_val_score by setting the n_jobs parameter to the number of\\nCPU cores you want to use. You can set n_jobs=-1 to use all available cores.\\nYou should be aware that scikit-learn does not allow nesting of parallel operations.\\nSo, if you are using the n_jobs option on your model (for example, a random forest),\\nyou cannot use it in GridSearchCV to search over this model. If your dataset and\\nmodel are very large, it might be that using many cores uses up too much memory,\\nand you should monitor your memory usage when building large models in parallel.\\nIt is also possible to parallelize grid search and cross-validation over multiple\\nmachines in a cluster, although at the time of writing this is not supported within\\nscikit-learn. It is, however, possible to use the IPython parallel framework for par‐\\nallel grid searches, if you don’t mind writing the for loop over parameters as we did\\nin “Simple Grid Search” on page 261.\\nFor Spark users, there is also the recently developed spark-sklearn package, which\\nallows running a grid search over an already established Spark cluster.\\nEvaluation Metrics and Scoring\\nSo far, we have evaluated classification performance using accuracy (the fraction of\\ncorrectly classified samples) and regression performance using R2. However, these are\\nonly two of the many possible ways to summarize how well a supervised model per‐\\nforms on a given dataset. In practice, these evaluation metrics might not be appropri‐\\nate for your application, and it is important to choose the right metric when selecting\\nbetween models and adjusting parameters.\\nKeep the End Goal in Mind\\nWhen selecting a metric, you should always have the end goal of the machine learn‐\\ning application in mind. In practice, we are usually interested not just in making\\naccurate predictions, but in using these predictions as part of a larger decision-\\nmaking process. Before picking a machine learning metric, you should think about\\nthe high-level goal of the application, often called the business metric. The conse‐\\nquences of choosing a particular algorithm for a machine learning application are\\nEvaluation Metrics and Scoring \\n| \\n275'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 289}, page_content='2 We ask scientifically minded readers to excuse the commercial language in this section. Not losing track of the\\nend goal is equally important in science, though the authors are not aware of a similar phrase to “business\\nimpact” being used in that realm.\\ncalled the business impact.2 Maybe the high-level goal is avoiding traffic accidents, or\\ndecreasing the number of hospital admissions. It could also be getting more users for\\nyour website, or having users spend more money in your shop. When choosing a\\nmodel or adjusting parameters, you should pick the model or parameter values that\\nhave the most positive influence on the business metric. Often this is hard, as assess‐\\ning the business impact of a particular model might require putting it in production\\nin a real-life system.\\nIn the early stages of development, and for adjusting parameters, it is often infeasible\\nto put models into production just for testing purposes, because of the high business\\nor personal risks that can be involved. Imagine evaluating the pedestrian avoidance\\ncapabilities of a self-driving car by just letting it drive around, without verifying it\\nfirst; if your model is bad, pedestrians will be in trouble! Therefore we often need to\\nfind some surrogate evaluation procedure, using an evaluation metric that is easier to\\ncompute. For example, we could test classifying images of pedestrians against non-\\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\\npays off to find the closest metric to the original business goal that is feasible to evalu‐\\nate. This closest metric should be used whenever possible for model evaluation and\\nselection. The result of this evaluation might not be a single number—the conse‐\\nquence of your algorithm could be that you have 10% more customers, but each cus‐\\ntomer will spend 15% less—but it should capture the expected business impact of\\nchoosing one model over another.\\nIn this section, we will first discuss metrics for the important special case of binary\\nclassification, then turn to multiclass classification and finally regression.\\nMetrics for Binary Classification\\nBinary classification is arguably the most common and conceptually simple applica‐\\ntion of machine learning in practice. However, there are still a number of caveats in\\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\\nlook at the ways in which measuring accuracy might be misleading. Remember that\\nfor binary classification, we often speak of a positive class and a negative class, with\\nthe understanding that the positive class is the one we are looking for.\\nKinds of errors\\nOften, accuracy is not a good measure of predictive performance, as the number of\\nmistakes we make does not contain all the information we are interested in. Imagine\\nan application to screen for the early detection of cancer using an automated test. If\\n276 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 290}, page_content='the test is negative, the patient will be assumed healthy, while if the test is positive, the\\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\\ncation of cancer) the positive class, and a negative test the negative class. We can’t\\nassume that our model will always work perfectly, and it will make mistakes. For any\\napplication, we need to ask ourselves what the consequences of these mistakes might\\nbe in the real world.\\nOne possible mistake is that a healthy patient will be classified as positive, leading to\\nadditional testing. This leads to some costs and an inconvenience for the patient (and\\npossibly some mental distress). An incorrect positive prediction is called a false posi‐\\ntive. The other possible mistake is that a sick patient will be classified as negative, and\\nwill not receive further tests and treatment. The undiagnosed cancer might lead to\\nserious health issues, and could even be fatal. A mistake of this kind—an incorrect\\nnegative prediction—is called a false negative. In statistics, a false positive is also\\nknown as type I error, and a false negative as type II error. We will stick to “false nega‐\\ntive” and “false positive,” as they are more explicit and easier to remember. In the can‐\\ncer diagnosis example, it is clear that we want to avoid false negatives as much as\\npossible, while false positives can be viewed as more of a minor nuisance.\\nWhile this is a particularly drastic example, the consequence of false positives and\\nfalse negatives are rarely the same. In commercial applications, it might be possible to\\nassign dollar values to both kinds of mistakes, which would allow measuring the error\\nof a particular prediction in dollars, instead of accuracy. This might be much more\\nmeaningful for making business decisions on which model to use.\\nImbalanced datasets\\nTypes of errors play an important role when one of two classes is much more frequent\\nthan the other one. This is very common in practice; a good example is click-through\\nprediction, where each data point represents an “impression,” an item that was shown\\nto a user. This item might be an ad, or a related story, or a related person to follow on\\na social media site. The goal is to predict whether, if shown a particular item, a user\\nwill click on it (indicating they are interested). Most things users are shown on the\\nInternet (in particular, ads) will not result in a click. You might need to show a user\\n100 ads or articles before they find something interesting enough to click on. This\\nresults in a dataset where for each 99 “no click” data points, there is 1 “clicked” data\\npoint; in other words, 99% of the samples belong to the “no click” class. Datasets in\\nwhich one class is much more frequent than the other are often called imbalanced\\ndatasets, or datasets with imbalanced classes. In reality, imbalanced data is the norm,\\nand it is rare that the events of interest have equal or even similar frequency in the\\ndata.\\nNow let’s say you build a classifier that is 99% accurate on the click prediction task.\\nWhat does that tell you? 99% accuracy sounds impressive, but this doesn’t take the\\nEvaluation Metrics and Scoring \\n| \\n277'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 291}, page_content='class imbalance into account. You can achieve 99% accuracy without building a\\nmachine learning model, by always predicting “no click.” On the other hand, even\\nwith imbalanced data, a 99% accurate model could in fact be quite good. However,\\naccuracy doesn’t allow us to distinguish the constant “no click” model from a poten‐\\ntially good model.\\nTo illustrate, we’ll create a 9:1 imbalanced dataset from the digits dataset, by classify‐\\ning the digit 9 against the nine other classes:\\nIn[37]:\\nfrom sklearn.datasets import load_digits\\ndigits = load_digits()\\ny = digits.target == 9\\nX_train, X_test, y_train, y_test = train_test_split(\\n    digits.data, y, random_state=0)\\nWe can use the DummyClassifier to always predict the majority class (here\\n“not nine”) to see how uninformative accuracy can be:\\nIn[38]:\\nfrom sklearn.dummy import DummyClassifier\\ndummy_majority = DummyClassifier(strategy=\\'most_frequent\\').fit(X_train, y_train)\\npred_most_frequent = dummy_majority.predict(X_test)\\nprint(\"Unique predicted labels: {}\".format(np.unique(pred_most_frequent)))\\nprint(\"Test score: {:.2f}\".format(dummy_majority.score(X_test, y_test)))\\nOut[38]:\\nUnique predicted labels: [False]\\nTest score: 0.90\\nWe obtained close to 90% accuracy without learning anything. This might seem strik‐\\ning, but think about it for a minute. Imagine someone telling you their model is 90%\\naccurate. You might think they did a very good job. But depending on the problem,\\nthat might be possible by just predicting one class! Let’s compare this against using an\\nactual classifier:\\nIn[39]:\\nfrom sklearn.tree import DecisionTreeClassifier\\ntree = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\\npred_tree = tree.predict(X_test)\\nprint(\"Test score: {:.2f}\".format(tree.score(X_test, y_test)))\\nOut[39]:\\nTest score: 0.92\\n278 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 292}, page_content='According to accuracy, the DecisionTreeClassifier is only slightly better than the\\nconstant predictor. This could indicate either that something is wrong with how we\\nused DecisionTreeClassifier, or that accuracy is in fact not a good measure here.\\nFor comparison purposes, let’s evaluate two more classifiers, LogisticRegression\\nand the default DummyClassifier, which makes random predictions but produces\\nclasses with the same proportions as in the training set:\\nIn[40]:\\nfrom sklearn.linear_model import LogisticRegression\\ndummy = DummyClassifier().fit(X_train, y_train)\\npred_dummy = dummy.predict(X_test)\\nprint(\"dummy score: {:.2f}\".format(dummy.score(X_test, y_test)))\\nlogreg = LogisticRegression(C=0.1).fit(X_train, y_train)\\npred_logreg = logreg.predict(X_test)\\nprint(\"logreg score: {:.2f}\".format(logreg.score(X_test, y_test)))\\nOut[40]:\\ndummy score: 0.80\\nlogreg score: 0.98\\nThe dummy classifier that produces random output is clearly the worst of the lot\\n(according to accuracy), while LogisticRegression produces very good results.\\nHowever, even the random classifier yields over 80% accuracy. This makes it very\\nhard to judge which of these results is actually helpful. The problem here is that accu‐\\nracy is an inadequate measure for quantifying predictive performance in this imbal‐\\nanced setting. For the rest of this chapter, we will explore alternative metrics that\\nprovide better guidance in selecting models. In particular, we would like to have met‐\\nrics that tell us how much better a model is than making “most frequent” predictions\\nor random predictions, as they are computed in pred_most_frequent and\\npred_dummy. If we use a metric to assess our models, it should definitely be able to\\nweed out these nonsense predictions.\\nConfusion matrices\\nOne of the most comprehensive ways to represent the result of evaluating binary clas‐\\nsification is using confusion matrices. Let’s inspect the predictions of LogisticRegres\\nsion from the previous section using the confusion_matrix function. We already\\nstored the predictions on the test set in pred_logreg:\\nEvaluation Metrics and Scoring \\n| \\n279'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 293}, page_content='In[41]:\\nfrom sklearn.metrics import confusion_matrix\\nconfusion = confusion_matrix(y_test, pred_logreg)\\nprint(\"Confusion matrix:\\\\n{}\".format(confusion))\\nOut[41]:\\nConfusion matrix:\\n[[401   2]\\n [  8  39]]\\nThe output of confusion_matrix is a two-by-two array, where the rows correspond\\nto the true classes and the columns correspond to the predicted classes. Each entry\\ncounts how often a sample that belongs to the class corresponding to the row (here,\\n“not nine” and “nine”) was classified as the class corresponding to the column. The\\nfollowing plot (Figure 5-10) illustrates this meaning:\\nIn[42]:\\nmglearn.plots.plot_confusion_matrix_illustration()\\nFigure 5-10. Confusion matrix of the “nine vs. rest” classification task\\n280 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 294}, page_content='3 The main diagonal of a two-dimensional array or matrix A is A[i, i].\\nEntries on the main diagonal3 of the confusion matrix correspond to correct classifi‐\\ncations, while other entries tell us how many samples of one class got mistakenly clas‐\\nsified as another class.\\nIf we declare “a nine” the positive class, we can relate the entries of the confusion\\nmatrix with the terms false positive and false negative that we introduced earlier. To\\ncomplete the picture, we call correctly classified samples belonging to the positive\\nclass true positives and correctly classified samples belonging to the negative class true\\nnegatives. These terms are usually abbreviated FP, FN, TP, and TN and lead to the fol‐\\nlowing interpretation for the confusion matrix (Figure 5-11):\\nIn[43]:\\nmglearn.plots.plot_binary_confusion_matrix()\\nFigure 5-11. Confusion matrix for binary classification\\nNow let’s use the confusion matrix to compare the models we fitted earlier (the two\\ndummy models, the decision tree, and the logistic regression):\\nIn[44]:\\nprint(\"Most frequent class:\")\\nprint(confusion_matrix(y_test, pred_most_frequent))\\nprint(\"\\\\nDummy model:\")\\nprint(confusion_matrix(y_test, pred_dummy))\\nprint(\"\\\\nDecision tree:\")\\nprint(confusion_matrix(y_test, pred_tree))\\nprint(\"\\\\nLogistic Regression\")\\nprint(confusion_matrix(y_test, pred_logreg))\\nEvaluation Metrics and Scoring \\n| \\n281'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 295}, page_content='Out[44]:\\nMost frequent class:\\n[[403   0]\\n [ 47   0]]\\nDummy model:\\n[[361  42]\\n [ 43   4]]\\nDecision tree:\\n[[390  13]\\n [ 24  23]]\\nLogistic Regression\\n[[401   2]\\n [  8  39]]\\nLooking at the confusion matrix, it is quite clear that something is wrong with\\npred_most_frequent, because it always predicts the same class. pred_dummy, on the\\nother hand, has a very small number of true positives (4), particularly compared to\\nthe number of false negatives and false positives—there are many more false positives\\nthan true positives! The predictions made by the decision tree make much more\\nsense than the dummy predictions, even though the accuracy was nearly the same.\\nFinally, we can see that logistic regression does better than pred_tree in all aspects: it\\nhas more true positives and true negatives while having fewer false positives and false\\nnegatives. From this comparison, it is clear that only the decision tree and the logistic\\nregression give reasonable results, and that the logistic regression works better than\\nthe tree on all accounts. However, inspecting the full confusion matrix is a bit cum‐\\nbersome, and while we gained a lot of insight from looking at all aspects of the\\nmatrix, the process was very manual and qualitative. There are several ways to sum‐\\nmarize the information in the confusion matrix, which we will discuss next.\\nRelation to accuracy.    We already saw one way to summarize the result in the confu‐\\nsion matrix—by computing accuracy, which can be expressed as:\\nAccuracy =\\nTP+TN\\nTP+TN + FP + FN\\nIn other words, accuracy is the number of correct predictions (TP and TN) divided\\nby the number of all samples (all entries of the confusion matrix summed up).\\nPrecision, recall, and f-score.    There are several other ways to summarize the confusion\\nmatrix, with the most common ones being precision and recall. Precision measures\\nhow many of the samples predicted as positive are actually positive:\\n282 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 296}, page_content='Precision =\\nTP\\nTP+FP\\nPrecision is used as a performance metric when the goal is to limit the number of\\nfalse positives. As an example, imagine a model for predicting whether a new drug\\nwill be effective in treating a disease in clinical trials. Clinical trials are notoriously\\nexpensive, and a pharmaceutical company will only want to run an experiment if it is\\nvery sure that the drug will actually work. Therefore, it is important that the model\\ndoes not produce many false positives—in other words, that it has a high precision.\\nPrecision is also known as positive predictive value (PPV).\\nRecall, on the other hand, measures how many of the positive samples are captured\\nby the positive predictions:\\nRecall =\\nTP\\nTP+FN\\nRecall is used as performance metric when we need to identify all positive samples;\\nthat is, when it is important to avoid false negatives. The cancer diagnosis example\\nfrom earlier in this chapter is a good example for this: it is important to find all peo‐\\nple that are sick, possibly including healthy patients in the prediction. Other names\\nfor recall are sensitivity, hit rate, or true positive rate (TPR).\\nThere is a trade-off between optimizing recall and optimizing precision. You can triv‐\\nially obtain a perfect recall if you predict all samples to belong to the positive class—\\nthere will be no false negatives, and no true negatives either. However, predicting all\\nsamples as positive will result in many false positives, and therefore the precision will\\nbe very low. On the other hand, if you find a model that predicts only the single data\\npoint it is most sure about as positive and the rest as negative, then precision will be\\nperfect (assuming this data point is in fact positive), but recall will be very bad.\\nPrecision and recall are only two of many classification measures\\nderived from TP, FP, TN, and FN. You can find a great summary of\\nall the measures on Wikipedia. In the machine learning commu‐\\nnity, precision and recall are arguably the most commonly used\\nmeasures for binary classification, but other communities might\\nuse other related metrics.\\nSo, while precision and recall are very important measures, looking at only one of\\nthem will not provide you with the full picture. One way to summarize them is the\\nf-score or f-measure, which is with the harmonic mean of precision and recall:\\nF = 2 · precision·recall\\nprecision+recall\\nEvaluation Metrics and Scoring \\n| \\n283'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 297}, page_content='This particular variant is also known as the f1-score. As it takes precision and recall\\ninto account, it can be a better measure than accuracy on imbalanced binary classifi‐\\ncation datasets. Let’s run it on the predictions for the “nine vs. rest” dataset that we\\ncomputed earlier. Here, we will assume that the “nine” class is the positive class (it is\\nlabeled as True while the rest is labeled as False), so the positive class is the minority\\nclass:\\nIn[45]:\\nfrom sklearn.metrics import f1_score\\nprint(\"f1 score most frequent: {:.2f}\".format(\\n        f1_score(y_test, pred_most_frequent)))\\nprint(\"f1 score dummy: {:.2f}\".format(f1_score(y_test, pred_dummy)))\\nprint(\"f1 score tree: {:.2f}\".format(f1_score(y_test, pred_tree)))\\nprint(\"f1 score logistic regression: {:.2f}\".format(\\n        f1_score(y_test, pred_logreg)))\\nOut[45]:\\nf1 score most frequent: 0.00\\nf1 score dummy: 0.10\\nf1 score tree: 0.55\\nf1 score logistic regression: 0.89\\nWe can note two things here. First, we get an error message for the most_frequent\\nprediction, as there were no predictions of the positive class (which makes the\\ndenominator in the f-score zero). Also, we can see a pretty strong distinction between\\nthe dummy predictions and the tree predictions, which wasn’t clear when looking at\\naccuracy alone. Using the f-score for evaluation, we summarized the predictive per‐\\nformance again in one number. However, the f-score seems to capture our intuition\\nof what makes a good model much better than accuracy did. A disadvantage of the\\nf-score, however, is that it is harder to interpret and explain than accuracy.\\nIf we want a more comprehensive summary of precision, recall, and f1-score, we can\\nuse the classification_report convenience function to compute all three at once,\\nand print them in a nice format:\\nIn[46]:\\nfrom sklearn.metrics import classification_report\\nprint(classification_report(y_test, pred_most_frequent,\\n                            target_names=[\"not nine\", \"nine\"]))\\n284 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 298}, page_content='Out[46]:\\n             precision    recall  f1-score   support\\n   not nine       0.90      1.00      0.94       403\\n       nine       0.00      0.00      0.00        47\\navg / total       0.80      0.90      0.85       450\\nThe classification_report function produces one line per class (here, True and\\nFalse) and reports precision, recall, and f-score with this class as the positive class.\\nBefore, we assumed the minority “nine” class was the positive class. If we change the\\npositive class to “not nine,” we can see from the output of classification_report\\nthat we obtain an f-score of 0.94 with the most_frequent model. Furthermore, for the\\n“not nine” class we have a recall of 1, as we classified all samples as “not nine.” The\\nlast column next to the f-score provides the support of each class, which simply means\\nthe number of samples in this class according to the ground truth.\\nThe last row in the classification report shows a weighted (by the number of samples\\nin the class) average of the numbers for each class. Here are two more reports, one for\\nthe dummy classifier and one for the logistic regression:\\nIn[47]:\\nprint(classification_report(y_test, pred_dummy,\\n                            target_names=[\"not nine\", \"nine\"]))\\nOut[47]:\\n             precision    recall  f1-score   support\\n   not nine       0.90      0.92      0.91       403\\n       nine       0.11      0.09      0.10        47\\navg / total       0.81      0.83      0.82       450\\nIn[48]:\\nprint(classification_report(y_test, pred_logreg,\\n                            target_names=[\"not nine\", \"nine\"]))\\nOut[48]:\\n             precision    recall  f1-score   support\\n   not nine       0.98      1.00      0.99       403\\n       nine       0.95      0.83      0.89        47\\navg / total       0.98      0.98      0.98       450\\nEvaluation Metrics and Scoring \\n| \\n285'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 299}, page_content='As you may notice when looking at the reports, the differences between the dummy\\nmodels and a very good model are not as clear any more. Picking which class is\\ndeclared the positive class has a big impact on the metrics. While the f-score for the\\ndummy classification is 0.13 (vs. 0.89 for the logistic regression) on the “nine” class,\\nfor the “not nine” class it is 0.90 vs. 0.99, which both seem like reasonable results.\\nLooking at all the numbers together paints a pretty accurate picture, though, and we\\ncan clearly see the superiority of the logistic regression model.\\nTaking uncertainty into account\\nThe confusion matrix and the classification report provide a very detailed analysis of\\na particular set of predictions. However, the predictions themselves already threw\\naway a lot of information that is contained in the model. As we discussed in Chap‐\\nter 2, most classifiers provide a decision_function or a predict_proba method to\\nassess degrees of certainty about predictions. Making predictions can be seen as\\nthresholding the output of decision_function or predict_proba at a certain fixed\\npoint—in binary classification we use 0 for the decision function and 0.5 for\\npredict_proba.\\nThe following is an example of an imbalanced binary classification task, with 400\\npoints in the negative class classified against 50 points in the positive class. The train‐\\ning data is shown on the left in Figure 5-12. We train a kernel SVM model on this\\ndata, and the plots to the right of the training data illustrate the values of the decision\\nfunction as a heat map. You can see a black circle in the plot in the top center, which\\ndenotes the threshold of the decision_function being exactly zero. Points inside this\\ncircle will be classified as the positive class, and points outside as the negative class:\\nIn[49]:\\nfrom mglearn.datasets import make_blobs\\nX, y = make_blobs(n_samples=(400, 50), centers=2, cluster_std=[7.0, 2],\\n                  random_state=22)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nsvc = SVC(gamma=.05).fit(X_train, y_train)\\nIn[50]:\\nmglearn.plots.plot_decision_threshold()\\n286 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 300}, page_content='Figure 5-12. Heatmap of the decision function and the impact of changing the decision\\nthreshold\\nWe can use the classification_report function to evaluate precision and recall for\\nboth classes:\\nIn[51]:\\nprint(classification_report(y_test, svc.predict(X_test)))\\nOut[51]:\\n             precision    recall  f1-score   support\\n          0       0.97      0.89      0.93       104\\n          1       0.35      0.67      0.46         9\\navg / total       0.92      0.88      0.89       113\\nFor class 1, we get a fairly small recall, and precision is mixed. Because class 0 is so\\nmuch larger, the classifier focuses on getting class 0 right, and not the smaller class 1.\\nLet’s assume in our application it is more important to have a high recall for class 1, as\\nin the cancer screening example earlier. This means we are willing to risk more false\\npositives (false class 1) in exchange for more true positives (which will increase the\\nrecall). The predictions generated by svc.predict really do not fulfill this require‐\\nment, but we can adjust the predictions to focus on a higher recall of class 1 by\\nchanging the decision threshold away from 0. By default, points with a deci\\nsion_function value greater than 0 will be classified as class 1. We want more points\\nto be classified as class 1, so we need to decrease the threshold:\\nEvaluation Metrics and Scoring \\n| \\n287'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 301}, page_content='In[52]:\\ny_pred_lower_threshold = svc.decision_function(X_test) > -.8\\nLet’s look at the classification report for this prediction:\\nIn[53]:\\nprint(classification_report(y_test, y_pred_lower_threshold))\\nOut[53]:\\n             precision    recall  f1-score   support\\n          0       1.00      0.82      0.90       104\\n          1       0.32      1.00      0.49         9\\navg / total       0.95      0.83      0.87       113\\nAs expected, the recall of class 1 went up, and the precision went down. We are now\\nclassifying a larger region of space as class 1, as illustrated in the top-right panel of\\nFigure 5-12. If you value precision over recall or the other way around, or your data is\\nheavily imbalanced, changing the decision threshold is the easiest way to obtain bet‐\\nter results. As the decision_function can have arbitrary ranges, it is hard to provide\\na rule of thumb regarding how to pick a threshold.\\nIf you do set a threshold, you need to be careful not to do so using\\nthe test set. As with any other parameter, setting a decision thresh‐\\nold on the test set is likely to yield overly optimistic results. Use a\\nvalidation set or cross-validation instead.\\nPicking a threshold for models that implement the predict_proba method can be\\neasier, as the output of predict_proba is on a fixed 0 to 1 scale, and models probabil‐\\nities. By default, the threshold of 0.5 means that if the model is more than 50% “sure”\\nthat a point is of the positive class, it will be classified as such. Increasing the thresh‐\\nold means that the model needs to be more confident to make a positive decision\\n(and less confident to make a negative decision). While working with probabilities\\nmay be more intuitive than working with arbitrary thresholds, not all models provide\\nrealistic models of uncertainty (a DecisionTree that is grown to its full depth is\\nalways 100% sure of its decisions, even though it might often be wrong). This relates\\nto the concept of calibration: a calibrated model is a model that provides an accurate\\nmeasure of its uncertainty. Discussing calibration in detail is beyond the scope of this\\nbook, but you can find more details in the paper “Predicting Good Probabilities with\\nSupervised Learning” by Alexandru Niculescu-Mizil and Rich Caruana.\\n288 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 302}, page_content='Precision-recall curves and ROC curves\\nAs we just discussed, changing the threshold that is used to make a classification deci‐\\nsion in a model is a way to adjust the trade-off of precision and recall for a given clas‐\\nsifier. Maybe you want to miss less than 10% of positive samples, meaning a desired\\nrecall of 90%. This decision depends on the application, and it should be driven by\\nbusiness goals. Once a particular goal is set—say, a particular recall or precision value\\nfor a class—a threshold can be set appropriately. It is always possible to set a thresh‐\\nold to fulfill a particular target, like 90% recall. The hard part is to develop a model\\nthat still has reasonable precision with this threshold—if you classify everything as\\npositive, you will have 100% recall, but your model will be useless.\\nSetting a requirement on a classifier like 90% recall is often called setting the operat‐\\ning point. Fixing an operating point is often helpful in business settings to make per‐\\nformance guarantees to customers or other groups inside your organization.\\nOften, when developing a new model, it is not entirely clear what the operating point\\nwill be. For this reason, and to understand a modeling problem better, it is instructive\\nto look at all possible thresholds, or all possible trade-offs of precision and recalls at\\nonce. This is possible using a tool called the precision-recall curve. You can find the\\nfunction to compute the precision-recall curve in the sklearn.metrics module. It\\nneeds the ground truth labeling and predicted uncertainties, created via either\\ndecision_function or predict_proba:\\nIn[54]:\\nfrom sklearn.metrics import precision_recall_curve\\nprecision, recall, thresholds = precision_recall_curve(\\n    y_test, svc.decision_function(X_test))\\nThe precision_recall_curve function returns a list of precision and recall values\\nfor all possible thresholds (all values that appear in the decision function) in sorted\\norder, so we can plot a curve, as seen in Figure 5-13:\\nIn[55]:\\n# Use more data points for a smoother curve\\nX, y = make_blobs(n_samples=(4000, 500), centers=2, cluster_std=[7.0, 2],\\n                  random_state=22)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nsvc = SVC(gamma=.05).fit(X_train, y_train)\\nprecision, recall, thresholds = precision_recall_curve(\\n    y_test, svc.decision_function(X_test))\\n# find threshold closest to zero\\nclose_zero = np.argmin(np.abs(thresholds))\\nplt.plot(precision[close_zero], recall[close_zero], \\'o\\', markersize=10,\\n         label=\"threshold zero\", fillstyle=\"none\", c=\\'k\\', mew=2)\\nplt.plot(precision, recall, label=\"precision recall curve\")\\nplt.xlabel(\"Precision\")\\nplt.ylabel(\"Recall\")\\nEvaluation Metrics and Scoring \\n| \\n289'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 303}, page_content='Figure 5-13. Precision recall curve for SVC(gamma=0.05)\\nEach point along the curve in Figure 5-13 corresponds to a possible threshold of the\\ndecision_function. We can see, for example, that we can achieve a recall of 0.4 at a\\nprecision of about 0.75. The black circle marks the point that corresponds to a thresh‐\\nold of 0, the default threshold for decision_function. This point is the trade-off that\\nis chosen when calling the predict method.\\nThe closer a curve stays to the upper-right corner, the better the classifier. A point at\\nthe upper right means high precision and high recall for the same threshold. The\\ncurve starts at the top-left corner, corresponding to a very low threshold, classifying\\neverything as the positive class. Raising the threshold moves the curve toward higher\\nprecision, but also lower recall. Raising the threshold more and more, we get to a sit‐\\nuation where most of the points classified as being positive are true positives, leading\\nto a very high precision but lower recall. The more the model keeps recall high as\\nprecision goes up, the better.\\nLooking at this particular curve a bit more, we can see that with this model it is possi‐\\nble to get a precision of up to around 0.5 with very high recall. If we want a much\\nhigher precision, we have to sacrifice a lot of recall. In other words, on the left the\\ncurve is relatively flat, meaning that recall does not go down a lot when we require\\nincreased precision. For precision greater than 0.5, each gain in precision costs us a\\nlot of recall.\\nDifferent classifiers can work well in different parts of the curve—that is, at different\\noperating points. Let’s compare the SVM we trained to a random forest trained on the\\nsame dataset. The RandomForestClassifier doesn’t have a decision_function, only\\npredict_proba. The precision_recall_curve function expects as its second argu‐\\nment a certainty measure for the positive class (class 1), so we pass the probability of\\na sample being class 1—that is, rf.predict_proba(X_test)[:, 1]. The default\\nthreshold for predict_proba in binary classification is 0.5, so this is the point we\\nmarked on the curve (see Figure 5-14):\\n290 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 304}, page_content='In[56]:\\nfrom sklearn.ensemble import RandomForestClassifier\\nrf = RandomForestClassifier(n_estimators=100, random_state=0, max_features=2)\\nrf.fit(X_train, y_train)\\n# RandomForestClassifier has predict_proba, but not decision_function\\nprecision_rf, recall_rf, thresholds_rf = precision_recall_curve(\\n    y_test, rf.predict_proba(X_test)[:, 1])\\nplt.plot(precision, recall, label=\"svc\")\\nplt.plot(precision[close_zero], recall[close_zero], \\'o\\', markersize=10,\\n         label=\"threshold zero svc\", fillstyle=\"none\", c=\\'k\\', mew=2)\\nplt.plot(precision_rf, recall_rf, label=\"rf\")\\nclose_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\\nplt.plot(precision_rf[close_default_rf], recall_rf[close_default_rf], \\'^\\', c=\\'k\\',\\n         markersize=10, label=\"threshold 0.5 rf\", fillstyle=\"none\", mew=2)\\nplt.xlabel(\"Precision\")\\nplt.ylabel(\"Recall\")\\nplt.legend(loc=\"best\")\\nFigure 5-14. Comparing precision recall curves of SVM and random forest\\nFrom the comparison plot we can see that the random forest performs better at the\\nextremes, for very high recall or very high precision requirements. Around the mid‐\\ndle (approximately precision=0.7), the SVM performs better. If we only looked at the\\nf1-score to compare overall performance, we would have missed these subtleties. The\\nf1-score only captures one point on the precision-recall curve, the one given by the\\ndefault threshold:\\nEvaluation Metrics and Scoring \\n| \\n291'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 305}, page_content='4 There are some minor technical differences between the area under the precision-recall curve and average\\nprecision. However, this explanation conveys the general idea.\\nIn[57]:\\nprint(\"f1_score of random forest: {:.3f}\".format(\\n    f1_score(y_test, rf.predict(X_test))))\\nprint(\"f1_score of svc: {:.3f}\".format(f1_score(y_test, svc.predict(X_test))))\\nOut[57]:\\nf1_score of random forest: 0.610\\nf1_score of svc: 0.656\\nComparing two precision-recall curves provides a lot of detailed insight, but is a fairly\\nmanual process. For automatic model comparison, we might want to summarize the\\ninformation contained in the curve, without limiting ourselves to a particular thresh‐\\nold or operating point. One particular way to summarize the precision-recall curve is\\nby computing the integral or area under the curve of the precision-recall curve, also\\nknown as the average precision.4 You can use the average_precision_score function\\nto compute the average precision. Because we need to compute the ROC curve and\\nconsider multiple thresholds, the result of decision_function or predict_proba\\nneeds to be passed to average_precision_score, not the result of predict:\\nIn[58]:\\nfrom sklearn.metrics import average_precision_score\\nap_rf = average_precision_score(y_test, rf.predict_proba(X_test)[:, 1])\\nap_svc = average_precision_score(y_test, svc.decision_function(X_test))\\nprint(\"Average precision of random forest: {:.3f}\".format(ap_rf))\\nprint(\"Average precision of svc: {:.3f}\".format(ap_svc))\\nOut[58]:\\nAverage precision of random forest: 0.666\\nAverage precision of svc: 0.663\\nWhen averaging over all possible thresholds, we see that the random forest and SVC\\nperform similarly well, with the random forest even slightly ahead. This is quite dif‐\\nferent from the result we got from f1_score earlier. Because average precision is the\\narea under a curve that goes from 0 to 1, average precision always returns a value\\nbetween 0 (worst) and 1 (best). The average precision of a classifier that assigns\\ndecision_function at random is the fraction of positive samples in the dataset.\\nReceiver operating characteristics (ROC) and AUC\\nThere is another tool that is commonly used to analyze the behavior of classifiers at\\ndifferent thresholds: the receiver operating characteristics curve, or ROC curve for\\nshort. Similar to the precision-recall curve, the ROC curve considers all possible\\n292 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 306}, page_content='thresholds for a given classifier, but instead of reporting precision and recall, it shows\\nthe false positive rate (FPR) against the true positive rate (TPR). Recall that the true\\npositive rate is simply another name for recall, while the false positive rate is the frac‐\\ntion of false positives out of all negative samples:\\nFPR =\\nFP\\nFP+TN\\nThe ROC curve can be computed using the roc_curve function (see Figure 5-15):\\nIn[59]:\\nfrom sklearn.metrics import roc_curve\\nfpr, tpr, thresholds = roc_curve(y_test, svc.decision_function(X_test))\\nplt.plot(fpr, tpr, label=\"ROC Curve\")\\nplt.xlabel(\"FPR\")\\nplt.ylabel(\"TPR (recall)\")\\n# find threshold closest to zero\\nclose_zero = np.argmin(np.abs(thresholds))\\nplt.plot(fpr[close_zero], tpr[close_zero], \\'o\\', markersize=10,\\n         label=\"threshold zero\", fillstyle=\"none\", c=\\'k\\', mew=2)\\nplt.legend(loc=4)\\nFigure 5-15. ROC curve for SVM\\nFor the ROC curve, the ideal curve is close to the top left: you want a classifier that\\nproduces a high recall while keeping a low false positive rate. Compared to the default\\nthreshold of 0, the curve shows that we can achieve a significantly higher recall\\n(around 0.9) while only increasing the FPR slightly. The point closest to the top left\\nmight be a better operating point than the one chosen by default. Again, be aware that\\nchoosing a threshold should not be done on the test set, but on a separate validation\\nset.\\nEvaluation Metrics and Scoring \\n| \\n293'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 307}, page_content='You can find a comparison of the random forest and the SVM using ROC curves in\\nFigure 5-16:\\nIn[60]:\\nfrom sklearn.metrics import roc_curve\\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf.predict_proba(X_test)[:, 1])\\nplt.plot(fpr, tpr, label=\"ROC Curve SVC\")\\nplt.plot(fpr_rf, tpr_rf, label=\"ROC Curve RF\")\\nplt.xlabel(\"FPR\")\\nplt.ylabel(\"TPR (recall)\")\\nplt.plot(fpr[close_zero], tpr[close_zero], \\'o\\', markersize=10,\\n         label=\"threshold zero SVC\", fillstyle=\"none\", c=\\'k\\', mew=2)\\nclose_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\\nplt.plot(fpr_rf[close_default_rf], tpr[close_default_rf], \\'^\\', markersize=10,\\n         label=\"threshold 0.5 RF\", fillstyle=\"none\", c=\\'k\\', mew=2)\\nplt.legend(loc=4)\\nFigure 5-16. Comparing ROC curves for SVM and random forest\\nAs for the precision-recall curve, we often want to summarize the ROC curve using a\\nsingle number, the area under the curve (this is commonly just referred to as the\\nAUC, and it is understood that the curve in question is the ROC curve). We can com‐\\npute the area under the ROC curve using the roc_auc_score function:\\n294 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 308}, page_content='In[61]:\\nfrom sklearn.metrics import roc_auc_score\\nrf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\\nsvc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\\nprint(\"AUC for Random Forest: {:.3f}\".format(rf_auc))\\nprint(\"AUC for SVC: {:.3f}\".format(svc_auc))\\nOut[61]:\\nAUC for Random Forest: 0.937\\nAUC for SVC: 0.916\\nComparing the random forest and SVM using the AUC score, we find that the ran‐\\ndom forest performs quite a bit better than the SVM. Recall that because average pre‐\\ncision is the area under a curve that goes from 0 to 1, average precision always returns\\na value between 0 (worst) and 1 (best). Predicting randomly always produces an AUC\\nof 0.5, no matter how imbalanced the classes in a dataset are. This makes AUC a\\nmuch better metric for imbalanced classification problems than accuracy. The AUC\\ncan be interpreted as evaluating the ranking of positive samples. It’s equivalent to the\\nprobability that a randomly picked point of the positive class will have a higher score\\naccording to the classifier than a randomly picked point from the negative class. So, a\\nperfect AUC of 1 means that all positive points have a higher score than all negative\\npoints. For classification problems with imbalanced classes, using AUC for model\\nselection is often much more meaningful than using accuracy.\\nLet’s go back to the problem we studied earlier of classifying all nines in the digits\\ndataset versus all other digits. We will classify the dataset with an SVM with three dif‐\\nferent settings of the kernel bandwidth, gamma (see Figure 5-17):\\nIn[62]:\\ny = digits.target == 9\\nX_train, X_test, y_train, y_test = train_test_split(\\n    digits.data, y, random_state=0)\\nplt.figure()\\nfor gamma in [1, 0.05, 0.01]:\\n    svc = SVC(gamma=gamma).fit(X_train, y_train)\\n    accuracy = svc.score(X_test, y_test)\\n    auc = roc_auc_score(y_test, svc.decision_function(X_test))\\n    fpr, tpr, _ = roc_curve(y_test , svc.decision_function(X_test))\\n    print(\"gamma = {:.2f}  accuracy = {:.2f}  AUC = {:.2f}\".format(\\n    gamma, accuracy, auc))\\n    plt.plot(fpr, tpr, label=\"gamma={:.3f}\".format(gamma))\\nplt.xlabel(\"FPR\")\\nplt.ylabel(\"TPR\")\\nplt.xlim(-0.01, 1)\\nplt.ylim(0, 1.02)\\nplt.legend(loc=\"best\")\\nEvaluation Metrics and Scoring \\n| \\n295'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 309}, page_content='5 Looking at the curve for gamma=0.01 in detail, you can see a small kink close to the top left. That means that at\\nleast one point was not ranked correctly. The AUC of 1.0 is a consequence of rounding to the second decimal\\npoint.\\nOut[62]:\\ngamma = 1.00  accuracy = 0.90  AUC = 0.50\\ngamma = 0.05  accuracy = 0.90  AUC = 0.90\\ngamma = 0.01  accuracy = 0.90  AUC = 1.00\\nFigure 5-17. Comparing ROC curves of SVMs with different settings of gamma\\nThe accuracy of all three settings of gamma is the same, 90%. This might be the same\\nas chance performance, or it might not. Looking at the AUC and the corresponding\\ncurve, however, we see a clear distinction between the three models. With gamma=1.0,\\nthe AUC is actually at chance level, meaning that the output of the decision_func\\ntion is as good as random. With gamma=0.05, performance drastically improves to an\\nAUC of 0.5. Finally, with gamma=0.01, we get a perfect AUC of 1.0. That means that\\nall positive points are ranked higher than all negative points according to the decision\\nfunction. In other words, with the right threshold, this model can classify the data\\nperfectly!5 Knowing this, we can adjust the threshold on this model and obtain great\\npredictions. If we had only used accuracy, we would never have discovered this.\\nFor this reason, we highly recommend using AUC when evaluating models on imbal‐\\nanced data. Keep in mind that AUC does not make use of the default threshold,\\nthough, so adjusting the decision threshold might be necessary to obtain useful classi‐\\nfication results from a model with a high AUC.\\nMetrics for Multiclass Classification\\nNow that we have discussed evaluation of binary classification tasks in depth, let’s\\nmove on to metrics to evaluate multiclass classification. Basically, all metrics for\\nmulticlass classification are derived from binary classification metrics, but averaged\\n296 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 310}, page_content='over all classes. Accuracy for multiclass classification is again defined as the fraction\\nof correctly classified examples. And again, when classes are imbalanced, accuracy is\\nnot a great evaluation measure. Imagine a three-class classification problem with 85%\\nof points belonging to class A, 10% belonging to class B, and 5% belonging to class C.\\nWhat does being 85% accurate mean on this dataset? In general, multiclass classifica‐\\ntion results are harder to understand than binary classification results. Apart from\\naccuracy, common tools are the confusion matrix and the classification report we saw\\nin the binary case in the previous section. Let’s apply these two detailed evaluation\\nmethods on the task of classifying the 10 different handwritten digits in the digits\\ndataset:\\nIn[63]:\\nfrom sklearn.metrics import accuracy_score\\nX_train, X_test, y_train, y_test = train_test_split(\\n    digits.data, digits.target, random_state=0)\\nlr = LogisticRegression().fit(X_train, y_train)\\npred = lr.predict(X_test)\\nprint(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, pred)))\\nprint(\"Confusion matrix:\\\\n{}\".format(confusion_matrix(y_test, pred)))\\nOut[63]:\\nAccuracy: 0.953\\nConfusion matrix:\\n[[37  0  0  0  0  0  0  0  0  0]\\n [ 0 39  0  0  0  0  2  0  2  0]\\n [ 0  0 41  3  0  0  0  0  0  0]\\n [ 0  0  1 43  0  0  0  0  0  1]\\n [ 0  0  0  0 38  0  0  0  0  0]\\n [ 0  1  0  0  0 47  0  0  0  0]\\n [ 0  0  0  0  0  0 52  0  0  0]\\n [ 0  1  0  1  1  0  0 45  0  0]\\n [ 0  3  1  0  0  0  0  0 43  1]\\n [ 0  0  0  1  0  1  0  0  1 44]]\\nThe model has an accuracy of 95.3%, which already tells us that we are doing pretty\\nwell. The confusion matrix provides us with some more detail. As for the binary case,\\neach row corresponds to a true label, and each column corresponds to a predicted\\nlabel. You can find a visually more appealing plot in Figure 5-18:\\nIn[64]:\\nscores_image = mglearn.tools.heatmap(\\n    confusion_matrix(y_test, pred), xlabel=\\'Predicted label\\',\\n    ylabel=\\'True label\\', xticklabels=digits.target_names,\\n    yticklabels=digits.target_names, cmap=plt.cm.gray_r, fmt=\"%d\")\\nplt.title(\"Confusion matrix\")\\nplt.gca().invert_yaxis()\\nEvaluation Metrics and Scoring \\n| \\n297'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 311}, page_content='Figure 5-18. Confusion matrix for the 10-digit classification task\\nFor the first class, the digit 0, there are 37 samples in the class, and all of these sam‐\\nples were classified as class 0 (there are no false negatives for class 0). We can see that\\nbecause all other entries in the first row of the confusion matrix are 0. We can also see\\nthat no other digits were mistakenly classified as 0, because all other entries in the\\nfirst column of the confusion matrix are 0 (there are no false positives for class 0).\\nSome digits were confused with others, though—for example, the digit 2 (third row),\\nthree of which were classified as the digit 3 (fourth column). There was also one digit\\n3 that was classified as 2 (third column, fourth row) and one digit 8 that was classified\\nas 2 (thrid column, fourth row).\\nWith the classification_report function, we can compute the precision, recall,\\nand f-score for each class:\\nIn[65]:\\nprint(classification_report(y_test, pred))\\nOut[65]:\\n             precision    recall  f1-score   support\\n          0       1.00      1.00      1.00        37\\n          1       0.89      0.91      0.90        43\\n          2       0.95      0.93      0.94        44\\n          3       0.90      0.96      0.92        45\\n          4       0.97      1.00      0.99        38\\n          5       0.98      0.98      0.98        48\\n          6       0.96      1.00      0.98        52\\n          7       1.00      0.94      0.97        48\\n          8       0.93      0.90      0.91        48\\n          9       0.96      0.94      0.95        47\\navg / total       0.95      0.95      0.95       450\\n298 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 312}, page_content='Unsurprisingly, precision and recall are a perfect 1 for class 0, as there are no confu‐\\nsions with this class. For class 7, on the other hand, precision is 1 because no other\\nclass was mistakenly classified as 7, while for class 6, there are no false negatives, so\\nthe recall is 1. We can also see that the model has particular difficulties with classes 8\\nand 3.\\nThe most commonly used metric for imbalanced datasets in the multiclass setting is\\nthe multiclass version of the f-score. The idea behind the multiclass f-score is to com‐\\npute one binary f-score per class, with that class being the positive class and the other\\nclasses making up the negative classes. Then, these per-class f-scores are averaged\\nusing one of the following strategies:\\n• \"macro\" averaging computes the unweighted per-class f-scores. This gives equal\\nweight to all classes, no matter what their size is.\\n• \"weighted\" averaging computes the mean of the per-class f-scores, weighted by\\ntheir support. This is what is reported in the classification report.\\n• \"micro\" averaging computes the total number of false positives, false negatives,\\nand true positives over all classes, and then computes precision, recall, and f-\\nscore using these counts.\\nIf you care about each sample equally much, it is recommended to use the \"micro\"\\naverage f1-score; if you care about each class equally much, it is recommended to use\\nthe \"macro\" average f1-score:\\nIn[66]:\\nprint(\"Micro average f1 score: {:.3f}\".format\\n       (f1_score(y_test, pred, average=\"micro\")))\\nprint(\"Macro average f1 score: {:.3f}\".format\\n       (f1_score(y_test, pred, average=\"macro\")))\\nOut[66]:\\nMicro average f1 score: 0.953\\nMacro average f1 score: 0.954\\nRegression Metrics\\nEvaluation for regression can be done in similar detail as we did for classification—\\nfor example, by analyzing overpredicting the target versus underpredicting the target.\\nHowever, in most applications we’ve seen, using the default R2 used in the score\\nmethod of all regressors is enough. Sometimes business decisions are made on the\\nbasis of mean squared error or mean absolute error, which might give incentive to\\ntune models using these metrics. In general, though, we have found R2 to be a more\\nintuitive metric to evaluate regression models.\\nEvaluation Metrics and Scoring \\n| \\n299'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 313}, page_content='Using Evaluation Metrics in Model Selection\\nWe have discussed many evaluation methods in detail, and how to apply them given\\nthe ground truth and a model. However, we often want to use metrics like AUC in\\nmodel selection using GridSearchCV or cross_val_score. Luckily scikit-learn\\nprovides a very simple way to achieve this, via the scoring argument that can be used\\nin both GridSearchCV and cross_val_score. You can simply provide a string\\ndescribing the evaluation metric you want to use. Say, for example, we want to evalu‐\\nate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC\\nscore. Changing the score from the default (accuracy) to AUC can be done by provid‐\\ning \"roc_auc\" as the scoring parameter:\\nIn[67]:\\n# default scoring for classification is accuracy\\nprint(\"Default scoring: {}\".format(\\n    cross_val_score(SVC(), digits.data, digits.target == 9)))\\n# providing scoring=\"accuracy\" doesn\\'t change the results\\nexplicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\\n                                     scoring=\"accuracy\")\\nprint(\"Explicit accuracy scoring: {}\".format(explicit_accuracy))\\nroc_auc =  cross_val_score(SVC(), digits.data, digits.target == 9,\\n                           scoring=\"roc_auc\")\\nprint(\"AUC scoring: {}\".format(roc_auc))\\nOut[67]:\\nDefault scoring: [ 0.9  0.9  0.9]\\nExplicit accuracy scoring: [ 0.9  0.9  0.9]\\nAUC scoring: [ 0.994  0.99   0.996]\\nSimilarly, we can change the metric used to pick the best parameters in Grid\\nSearchCV:\\nIn[68]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    digits.data, digits.target == 9, random_state=0)\\n# we provide a somewhat bad grid to illustrate the point:\\nparam_grid = {\\'gamma\\': [0.0001, 0.01, 0.1, 1, 10]}\\n# using the default scoring of accuracy:\\ngrid = GridSearchCV(SVC(), param_grid=param_grid)\\ngrid.fit(X_train, y_train)\\nprint(\"Grid-Search with accuracy\")\\nprint(\"Best parameters:\", grid.best_params_)\\nprint(\"Best cross-validation score (accuracy)): {:.3f}\".format(grid.best_score_))\\nprint(\"Test set AUC: {:.3f}\".format(\\n    roc_auc_score(y_test, grid.decision_function(X_test))))\\nprint(\"Test set accuracy: {:.3f}\".format(grid.score(X_test, y_test)))\\n300 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 314}, page_content='6 Finding a higher-accuracy solution using AUC is likely a consequence of accuracy being a bad measure of\\nmodel performance on imbalanced data.\\nOut[68]:\\nGrid-Search with accuracy\\nBest parameters: {\\'gamma\\': 0.0001}\\nBest cross-validation score (accuracy)): 0.970\\nTest set AUC: 0.992\\nTest set accuracy: 0.973\\nIn[69]:\\n# using AUC scoring instead:\\ngrid = GridSearchCV(SVC(), param_grid=param_grid, scoring=\"roc_auc\")\\ngrid.fit(X_train, y_train)\\nprint(\"\\\\nGrid-Search with AUC\")\\nprint(\"Best parameters:\", grid.best_params_)\\nprint(\"Best cross-validation score (AUC): {:.3f}\".format(grid.best_score_))\\nprint(\"Test set AUC: {:.3f}\".format(\\n    roc_auc_score(y_test, grid.decision_function(X_test))))\\nprint(\"Test set accuracy: {:.3f}\".format(grid.score(X_test, y_test)))\\nOut[69]:\\nGrid-Search with AUC\\nBest parameters: {\\'gamma\\': 0.01}\\nBest cross-validation score (AUC): 0.997\\nTest set AUC: 1.000\\nTest set accuracy: 1.000\\nWhen using accuracy, the parameter gamma=0.0001 is selected, while gamma=0.01 is\\nselected when using AUC. The cross-validation accuracy is consistent with the test set\\naccuracy in both cases. However, using AUC found a better parameter setting in\\nterms of AUC and even in terms of accuracy.6\\nThe most important values for the scoring parameter for classification are accuracy\\n(the default); roc_auc for the area under the ROC curve; average_precision for the\\narea under the precision-recall curve; f1, f1_macro, f1_micro, and f1_weighted for\\nthe binary f1-score and the different weighted variants. For regression, the most com‐\\nmonly used values are r2 for the R2 score, mean_squared_error for mean squared\\nerror, and mean_absolute_error for mean absolute error. You can find a full list of\\nsupported arguments in the documentation or by looking at the SCORER dictionary\\ndefined in the metrics.scorer module:\\nEvaluation Metrics and Scoring \\n| \\n301'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 315}, page_content='7 We highly recommend Foster Provost and Tom Fawcett’s book Data Science for Business (O’Reilly) for more\\ninformation on this topic.\\nIn[70]:\\nfrom sklearn.metrics.scorer import SCORERS\\nprint(\"Available scorers:\\\\n{}\".format(sorted(SCORERS.keys())))\\nOut[70]:\\nAvailable scorers:\\n[\\'accuracy\\', \\'adjusted_rand_score\\', \\'average_precision\\', \\'f1\\', \\'f1_macro\\',\\n \\'f1_micro\\', \\'f1_samples\\', \\'f1_weighted\\', \\'log_loss\\', \\'mean_absolute_error\\',\\n \\'mean_squared_error\\', \\'median_absolute_error\\', \\'precision\\', \\'precision_macro\\',\\n \\'precision_micro\\', \\'precision_samples\\', \\'precision_weighted\\', \\'r2\\', \\'recall\\',\\n \\'recall_macro\\', \\'recall_micro\\', \\'recall_samples\\', \\'recall_weighted\\', \\'roc_auc\\']\\nSummary and Outlook\\nIn this chapter we discussed cross-validation, grid search, and evaluation metrics, the\\ncornerstones of evaluating and improving machine learning algorithms. The tools\\ndescribed in this chapter, together with the algorithms described in Chapters 2 and 3,\\nare the bread and butter of every machine learning practitioner.\\nThere are two particular points that we made in this chapter that warrant repeating,\\nbecause they are often overlooked by new practitioners. The first has to do with\\ncross-validation. Cross-validation or the use of a test set allow us to evaluate a\\nmachine learning model as it will perform in the future. However, if we use the test\\nset or cross-validation to select a model or select model parameters, we “use up” the\\ntest data, and using the same data to evaluate how well our model will do in the future\\nwill lead to overly optimistic estimates. We therefore need to resort to a split into\\ntraining data for model building, validation data for model and parameter selection,\\nand test data for model evaluation. Instead of a simple split, we can replace each of\\nthese splits with cross-validation. The most commonly used form (as described ear‐\\nlier) is a training/test split for evaluation, and using cross-validation on the training\\nset for model and parameter selection.\\nThe second point has to do with the importance of the evaluation metric or scoring\\nfunction used for model selection and model evaluation. The theory of how to make\\nbusiness decisions from the predictions of a machine learning model is somewhat\\nbeyond the scope of this book.7 However, it is rarely the case that the end goal of a\\nmachine learning task is building a model with a high accuracy. Make sure that the\\nmetric you choose to evaluate and select a model for is a good stand-in for what the\\nmodel will actually be used for. In reality, classification problems rarely have balanced\\nclasses, and often false positives and false negatives have very different consequences.\\n302 \\n| \\nChapter 5: Model Evaluation and Improvement'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 316}, page_content='Make sure you understand what these consequences are, and pick an evaluation met‐\\nric accordingly.\\nThe model evaluation and selection techniques we have described so far are the most\\nimportant tools in a data scientist’s toolbox. Grid search and cross-validation as we’ve\\ndescribed them in this chapter can only be applied to a single supervised model. We\\nhave seen before, however, that many models require preprocessing, and that in some\\napplications, like the face recognition example in Chapter 3, extracting a different\\nrepresentation of the data can be useful. In the next chapter, we will introduce the\\nPipeline class, which allows us to use grid search and cross-validation on these com‐\\nplex chains of algorithms.\\nSummary and Outlook \\n| \\n303'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 317}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 318}, page_content='CHAPTER 6\\nAlgorithm Chains and Pipelines\\nFor many machine learning algorithms, the particular representation of the data that\\nyou provide is very important, as we discussed in Chapter 4. This starts with scaling\\nthe data and combining features by hand and goes all the way to learning features\\nusing unsupervised machine learning, as we saw in Chapter 3. Consequently, most\\nmachine learning applications require not only the application of a single algorithm,\\nbut the chaining together of many different processing steps and machine learning\\nmodels. In this chapter, we will cover how to use the Pipeline class to simplify the\\nprocess of building chains of transformations and models. In particular, we will see\\nhow we can combine Pipeline and GridSearchCV to search over parameters for all\\nprocessing steps at once.\\nAs an example of the importance of chaining models, we noticed that we can greatly\\nimprove the performance of a kernel SVM on the cancer dataset by using the Min\\nMaxScaler for preprocessing. Here’s code for splitting the data, computing the mini‐\\nmum and maximum, scaling the data, and training the SVM:\\nIn[1]:\\nfrom sklearn.svm import SVC\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import MinMaxScaler\\n# load and split the data\\ncancer = load_breast_cancer()\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\n# compute minimum and maximum on the training data\\nscaler = MinMaxScaler().fit(X_train)\\n305'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 319}, page_content='In[2]:\\n# rescale the training data\\nX_train_scaled = scaler.transform(X_train)\\nsvm = SVC()\\n# learn an SVM on the scaled training data\\nsvm.fit(X_train_scaled, y_train)\\n# scale the test data and score the scaled data\\nX_test_scaled = scaler.transform(X_test)\\nprint(\"Test score: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\\nOut[2]:\\nTest score: 0.95\\nParameter Selection with Preprocessing\\nNow let’s say we want to find better parameters for SVC using GridSearchCV, as dis‐\\ncussed in Chapter 5. How should we go about doing this? A naive approach might\\nlook like this:\\nIn[3]:\\nfrom sklearn.model_selection import GridSearchCV\\n# for illustration purposes only, don\\'t use this code!\\nparam_grid = {\\'C\\': [0.001, 0.01, 0.1, 1, 10, 100],\\n              \\'gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\\ngrid.fit(X_train_scaled, y_train)\\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\\nprint(\"Best set score: {:.2f}\".format(grid.score(X_test_scaled, y_test)))\\nprint(\"Best parameters: \", grid.best_params_)\\nOut[3]:\\nBest cross-validation accuracy: 0.98\\nBest set score: 0.97\\nBest parameters:  {\\'gamma\\': 1, \\'C\\': 1}\\nHere, we ran the grid search over the parameters of SVC using the scaled data. How‐\\never, there is a subtle catch in what we just did. When scaling the data, we used all the\\ndata in the training set to find out how to train it. We then use the scaled training data\\nto run our grid search using cross-validation. For each split in the cross-validation,\\nsome part of the original training set will be declared the training part of the split,\\nand some the test part of the split. The test part is used to measure what new data will\\nlook like to a model trained on the training part. However, we already used the infor‐\\nmation contained in the test part of the split, when scaling the data. Remember that\\nthe test part in each split in the cross-validation is part of the training set, and we\\nused the information from the entire training set to find the right scaling of the data.\\n306 \\n| \\nChapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 320}, page_content='This is fundamentally different from how new data looks to the model. If we observe\\nnew data (say, in form of our test set), this data will not have been used to scale the\\ntraining data, and it might have a different minimum and maximum than the train‐\\ning data. The following example (Figure 6-1) shows how the data processing during\\ncross-validation and the final evaluation differ:\\nIn[4]:\\nmglearn.plots.plot_improper_processing()\\nFigure 6-1. Data usage when preprocessing outside the cross-validation loop\\nSo, the splits in the cross-validation no longer correctly mirror how new data will\\nlook to the modeling process. We already leaked information from these parts of the\\ndata into our modeling process. This will lead to overly optimistic results during\\ncross-validation, and possibly the selection of suboptimal parameters.\\nTo get around this problem, the splitting of the dataset during cross-validation should\\nbe done before doing any preprocessing. Any process that extracts knowledge from the\\ndataset should only ever be applied to the training portion of the dataset, so any\\ncross-validation should be the “outermost loop” in your processing.\\nTo achieve this in scikit-learn with the cross_val_score function and the Grid\\nSearchCV function, we can use the Pipeline class. The Pipeline class is a class that\\nallows “gluing” together multiple processing steps into a single scikit-learn estima‐\\nParameter Selection with Preprocessing \\n| \\n307'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 321}, page_content='1 With one exception: the name can’t contain a double underscore, __.\\ntor. The Pipeline class itself has fit, predict, and score methods and behaves just\\nlike any other model in scikit-learn. The most common use case of the Pipeline\\nclass is in chaining preprocessing steps (like scaling of the data) together with a\\nsupervised model like a classifier.\\nBuilding Pipelines\\nLet’s look at how we can use the Pipeline class to express the workflow for training\\nan SVM after scaling the data with MinMaxScaler (for now without the grid search).\\nFirst, we build a pipeline object by providing it with a list of steps. Each step is a tuple\\ncontaining a name (any string of your choosing1) and an instance of an estimator:\\nIn[5]:\\nfrom sklearn.pipeline import Pipeline\\npipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])\\nHere, we created two steps: the first, called \"scaler\", is an instance of MinMaxScaler,\\nand the second, called \"svm\", is an instance of SVC. Now, we can fit the pipeline, like\\nany other scikit-learn estimator:\\nIn[6]:\\npipe.fit(X_train, y_train)\\nHere, pipe.fit first calls fit on the first step (the scaler), then transforms the train‐\\ning data using the scaler, and finally fits the SVM with the scaled data. To evaluate on\\nthe test data, we simply call pipe.score:\\nIn[7]:\\nprint(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))\\nOut[7]:\\nTest score: 0.95\\nCalling the score method on the pipeline first transforms the test data using the\\nscaler, and then calls the score method on the SVM using the scaled test data. As you\\ncan see, the result is identical to the one we got from the code at the beginning of the\\nchapter, when doing the transformations by hand. Using the pipeline, we reduced the\\ncode needed for our “preprocessing + classification” process. The main benefit of\\nusing the pipeline, however, is that we can now use this single estimator in\\ncross_val_score or GridSearchCV.\\n308 \\n| \\nChapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 322}, page_content='Using Pipelines in Grid Searches\\nUsing a pipeline in a grid search works the same way as using any other estimator. We\\ndefine a parameter grid to search over, and construct a GridSearchCV from the pipe‐\\nline and the parameter grid. When specifying the parameter grid, there is a slight\\nchange, though. We need to specify for each parameter which step of the pipeline it\\nbelongs to. Both parameters that we want to adjust, C and gamma, are parameters of\\nSVC, the second step. We gave this step the name \"svm\". The syntax to define a param‐\\neter grid for a pipeline is to specify for each parameter the step name, followed by __\\n(a double underscore), followed by the parameter name. To search over the C param‐\\neter of SVC we therefore have to use \"svm__C\" as the key in the parameter grid dictio‐\\nnary, and similarly for gamma:\\nIn[8]:\\nparam_grid = {\\'svm__C\\': [0.001, 0.01, 0.1, 1, 10, 100],\\n              \\'svm__gamma\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\nWith this parameter grid we can use GridSearchCV as usual:\\nIn[9]:\\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\\nprint(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\\nprint(\"Best parameters: {}\".format(grid.best_params_))\\nOut[9]:\\nBest cross-validation accuracy: 0.98\\nTest set score: 0.97\\nBest parameters: {\\'svm__C\\': 1, \\'svm__gamma\\': 1}\\nIn contrast to the grid search we did before, now for each split in the cross-validation,\\nthe MinMaxScaler is refit with only the training splits and no information is leaked\\nfrom the test split into the parameter search. Compare this (Figure 6-2) with\\nFigure 6-1 earlier in this chapter:\\nIn[10]:\\nmglearn.plots.plot_proper_processing()\\nUsing Pipelines in Grid Searches \\n| \\n309'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 323}, page_content='Figure 6-2. Data usage when preprocessing inside the cross-validation loop with a\\npipeline\\nThe impact of leaking information in the cross-validation varies depending on the\\nnature of the preprocessing step. Estimating the scale of the data using the test fold\\nusually doesn’t have a terrible impact, while using the test fold in feature extraction\\nand feature selection can lead to substantial differences in outcomes.\\nIllustrating Information Leakage\\nA great example of leaking information in cross-validation is given in Hastie, Tibshir‐\\nani, and Friedman’s book The Elements of Statistical Learning, and we reproduce an\\nadapted version here. Let’s consider a synthetic regression task with 100 samples and\\n1,000 features that are sampled independently from a Gaussian distribution. We also\\nsample the response from a Gaussian distribution:\\nIn[11]:\\nrnd = np.random.RandomState(seed=0)\\nX = rnd.normal(size=(100, 10000))\\ny = rnd.normal(size=(100,))\\nGiven the way we created the dataset, there is no relation between the data, X, and the\\ntarget, y (they are independent), so it should not be possible to learn anything from\\nthis dataset. We will now do the following. First, select the most informative of the 10\\nfeatures using SelectPercentile feature selection, and then we evaluate a Ridge\\nregressor using cross-validation:\\n310 \\n| \\nChapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 324}, page_content='In[12]:\\nfrom sklearn.feature_selection import SelectPercentile, f_regression\\nselect = SelectPercentile(score_func=f_regression, percentile=5).fit(X, y)\\nX_selected = select.transform(X)\\nprint(\"X_selected.shape: {}\".format(X_selected.shape))\\nOut[12]:\\nX_selected.shape: (100, 500)\\nIn[13]:\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.linear_model import Ridge\\nprint(\"Cross-validation accuracy (cv only on ridge): {:.2f}\".format(\\n      np.mean(cross_val_score(Ridge(), X_selected, y, cv=5))))\\nOut[13]:\\nCross-validation accuracy (cv only on ridge): 0.91\\nThe mean R2 computed by cross-validation is 0.91, indicating a very good model.\\nThis clearly cannot be right, as our data is entirely random. What happened here is\\nthat our feature selection picked out some features among the 10,000 random features\\nthat are (by chance) very well correlated with the target. Because we fit the feature\\nselection outside of the cross-validation, it could find features that are correlated both\\non the training and the test folds. The information we leaked from the test folds was\\nvery informative, leading to highly unrealistic results. Let’s compare this to a proper\\ncross-validation using a pipeline:\\nIn[14]:\\npipe = Pipeline([(\"select\", SelectPercentile(score_func=f_regression,\\n                                             percentile=5)),\\n                 (\"ridge\", Ridge())])\\nprint(\"Cross-validation accuracy (pipeline): {:.2f}\".format(\\n      np.mean(cross_val_score(pipe, X, y, cv=5))))\\nOut[14]:\\nCross-validation accuracy (pipeline): -0.25\\nThis time, we get a negative R2 score, indicating a very poor model. Using the pipe‐\\nline, the feature selection is now inside the cross-validation loop. This means features\\ncan only be selected using the training folds of the data, not the test fold. The feature\\nselection finds features that are correlated with the target on the training set, but\\nbecause the data is entirely random, these features are not correlated with the target\\non the test set. In this example, rectifying the data leakage issue in the feature selec‐\\ntion makes the difference between concluding that a model works very well and con‐\\ncluding that a model works not at all.\\nUsing Pipelines in Grid Searches \\n| \\n311'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 325}, page_content='2 Or just fit_transform.\\nThe General Pipeline Interface\\nThe Pipeline class is not restricted to preprocessing and classification, but can in\\nfact join any number of estimators together. For example, you could build a pipeline\\ncontaining feature extraction, feature selection, scaling, and classification, for a total\\nof four steps. Similarly, the last step could be regression or clustering instead of classi‐\\nfication.\\nThe only requirement for estimators in a pipeline is that all but the last step need to\\nhave a transform method, so they can produce a new representation of the data that\\ncan be used in the next step.\\nInternally, during the call to Pipeline.fit, the pipeline calls fit and then transform\\non each step in turn,2 with the input given by the output of the transform method of\\nthe previous step. For the last step in the pipeline, just fit is called.\\nBrushing over some finer details, this is implemented as follows. Remember that pipe\\nline.steps is a list of tuples, so pipeline.steps[0][1] is the first estimator, pipe\\nline.steps[1][1] is the second estimator, and so on:\\nIn[15]:\\ndef fit(self, X, y):\\n    X_transformed = X\\n    for name, estimator in self.steps[:-1]:\\n        # iterate over all but the final step\\n        # fit and transform the data\\n        X_transformed = estimator.fit_transform(X_transformed, y)\\n    # fit the last step\\n    self.steps[-1][1].fit(X_transformed, y)\\n    return self\\nWhen predicting using Pipeline, we similarly transform the data using all but the\\nlast step, and then call predict on the last step:\\nIn[16]:\\ndef predict(self, X):\\n    X_transformed = X\\n    for step in self.steps[:-1]:\\n        # iterate over all but the final step\\n        # transform the data\\n        X_transformed = step[1].transform(X_transformed)\\n    # fit the last step\\n    return self.steps[-1][1].predict(X_transformed)\\n312 \\n| \\nChapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 326}, page_content='The process is illustrated in Figure 6-3 for two transformers, T1 and T2, and a\\nclassifier (called Classifier).\\nFigure 6-3. Overview of the pipeline training and prediction process\\nThe pipeline is actually even more general than this. There is no requirement for the\\nlast step in a pipeline to have a predict function, and we could create a pipeline just\\ncontaining, for example, a scaler and PCA. Then, because the last step (PCA) has a\\ntransform method, we could call transform on the pipeline to get the output of\\nPCA.transform applied to the data that was processed by the previous step. The last\\nstep of a pipeline is only required to have a fit method.\\nConvenient Pipeline Creation with make_pipeline\\nCreating a pipeline using the syntax described earlier is sometimes a bit cumbersome,\\nand we often don’t need user-specified names for each step. There is a convenience\\nfunction, make_pipeline, that will create a pipeline for us and automatically name\\neach step based on its class. The syntax for make_pipeline is as follows:\\nIn[17]:\\nfrom sklearn.pipeline import make_pipeline\\n# standard syntax\\npipe_long = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\\n# abbreviated syntax\\npipe_short = make_pipeline(MinMaxScaler(), SVC(C=100))\\nThe General Pipeline Interface \\n| \\n313'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 327}, page_content='The pipeline objects pipe_long and pipe_short do exactly the same thing, but\\npipe_short has steps that were automatically named. We can see the names of the\\nsteps by looking at the steps attribute:\\nIn[18]:\\nprint(\"Pipeline steps:\\\\n{}\".format(pipe_short.steps))\\nOut[18]:\\nPipeline steps:\\n[(\\'minmaxscaler\\', MinMaxScaler(copy=True, feature_range=(0, 1))),\\n (\\'svc\\', SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\\n \\n     decision_function_shape=None, degree=3, gamma=\\'auto\\',\\n             kernel=\\'rbf\\', max_iter=-1, probability=False,\\n             random_state=None, shrinking=True, tol=0.001,\\n             verbose=False))]\\nThe steps are named minmaxscaler and svc. In general, the step names are just low‐\\nercase versions of the class names. If multiple steps have the same class, a number is\\nappended:\\nIn[19]:\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.decomposition import PCA\\npipe = make_pipeline(StandardScaler(), PCA(n_components=2), StandardScaler())\\nprint(\"Pipeline steps:\\\\n{}\".format(pipe.steps))\\nOut[19]:\\nPipeline steps:\\n[(\\'standardscaler-1\\', StandardScaler(copy=True, with_mean=True, with_std=True)),\\n (\\'pca\\', PCA(copy=True, iterated_power=4, n_components=2, random_state=None,\\n             svd_solver=\\'auto\\', tol=0.0, whiten=False)),\\n (\\'standardscaler-2\\', StandardScaler(copy=True, with_mean=True, with_std=True))]\\nAs you can see, the first StandardScaler step was named standardscaler-1 and the\\nsecond standardscaler-2. However, in such settings it might be better to use the\\nPipeline construction with explicit names, to give more semantic names to each\\nstep.\\nAccessing Step Attributes\\nOften you will want to inspect attributes of one of the steps of the pipeline—say, the\\ncoefficients of a linear model or the components extracted by PCA. The easiest way to\\naccess the steps in a pipeline is via the named_steps attribute, which is a dictionary\\nfrom the step names to the estimators:\\n314 \\n| \\nChapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 328}, page_content='In[20]:\\n# fit the pipeline defined before to the cancer dataset\\npipe.fit(cancer.data)\\n# extract the first two principal components from the \"pca\" step\\ncomponents = pipe.named_steps[\"pca\"].components_\\nprint(\"components.shape: {}\".format(components.shape))\\nOut[20]:\\ncomponents.shape: (2, 30)\\nAccessing Attributes in a Grid-Searched Pipeline\\nAs we discussed earlier in this chapter, one of the main reasons to use pipelines is for\\ndoing grid searches. A common task is to access some of the steps of a pipeline inside\\na grid search. Let’s grid search a LogisticRegression classifier on the cancer dataset,\\nusing Pipeline and StandardScaler to scale the data before passing it to the Logisti\\ncRegression classifier. First we create a pipeline using the make_pipeline function:\\nIn[21]:\\nfrom sklearn.linear_model import LogisticRegression\\npipe = make_pipeline(StandardScaler(), LogisticRegression())\\nNext, we create a parameter grid. As explained in Chapter 2, the regularization\\nparameter to tune for LogisticRegression is the parameter C. We use a logarithmic\\ngrid for this parameter, searching between 0.01 and 100. Because we used the\\nmake_pipeline function, the name of the LogisticRegression step in the pipeline is\\nthe lowercased class name, logisticregression. To tune the parameter C, we there‐\\nfore have to specify a parameter grid for logisticregression__C:\\nIn[22]:\\nparam_grid = {\\'logisticregression__C\\': [0.01, 0.1, 1, 10, 100]}\\nAs usual, we split the cancer dataset into training and test sets, and fit a grid search:\\nIn[23]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=4)\\ngrid = GridSearchCV(pipe, param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nSo how do we access the coefficients of the best LogisticRegression model that was\\nfound by GridSearchCV? From Chapter 5 we know that the best model found by\\nGridSearchCV, trained on all the training data, is stored in grid.best_estimator_:\\nThe General Pipeline Interface \\n| \\n315'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 329}, page_content='In[24]:\\nprint(\"Best estimator:\\\\n{}\".format(grid.best_estimator_))\\nOut[24]:\\nBest estimator:\\nPipeline(steps=[\\n    (\\'standardscaler\\', StandardScaler(copy=True, with_mean=True, with_std=True)),\\n    (\\'logisticregression\\', LogisticRegression(C=0.1, class_weight=None,\\n    dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100,\\n    multi_class=\\'ovr\\', n_jobs=1, penalty=\\'l2\\', random_state=None,\\n    solver=\\'liblinear\\', tol=0.0001, verbose=0, warm_start=False))])\\nThis best_estimator_ in our case is a pipeline with two steps, standardscaler and\\nlogisticregression. To access the logisticregression step, we can use the\\nnamed_steps attribute of the pipeline, as explained earlier:\\nIn[25]:\\nprint(\"Logistic regression step:\\\\n{}\".format(\\n      grid.best_estimator_.named_steps[\"logisticregression\"]))\\nOut[25]:\\nLogistic regression step:\\nLogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\\n                  intercept_scaling=1, max_iter=100, multi_class=\\'ovr\\', n_jobs=1,\\n                  penalty=\\'l2\\', random_state=None, solver=\\'liblinear\\', tol=0.0001,\\n                  verbose=0, warm_start=False)\\nNow that we have the trained LogisticRegression instance, we can access the coeffi‐\\ncients (weights) associated with each input feature:\\nIn[26]:\\nprint(\"Logistic regression coefficients:\\\\n{}\".format(\\n      grid.best_estimator_.named_steps[\"logisticregression\"].coef_))\\nOut[26]:\\nLogistic regression coefficients:\\n[[-0.389 -0.375 -0.376 -0.396 -0.115  0.017 -0.355 -0.39  -0.058  0.209\\n  -0.495 -0.004 -0.371 -0.383 -0.045  0.198  0.004 -0.049  0.21   0.224\\n  -0.547 -0.525 -0.499 -0.515 -0.393 -0.123 -0.388 -0.417 -0.325 -0.139]]\\nThis might be a somewhat lengthy expression, but often it comes in handy in under‐\\nstanding your models.\\n316 \\n| \\nChapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 330}, page_content='Grid-Searching Preprocessing Steps and Model\\nParameters\\nUsing pipelines, we can encapsulate all the processing steps in our machine learning\\nworkflow in a single scikit-learn estimator. Another benefit of doing this is that we\\ncan now adjust the parameters of the preprocessing using the outcome of a supervised\\ntask like regression or classification. In previous chapters, we used polynomial fea‐\\ntures on the boston dataset before applying the ridge regressor. Let’s model that using\\na pipeline instead. The pipeline contains three steps—scaling the data, computing\\npolynomial features, and ridge regression:\\nIn[27]:\\nfrom sklearn.datasets import load_boston\\nboston = load_boston()\\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\\n                                                    random_state=0)\\nfrom sklearn.preprocessing import PolynomialFeatures\\npipe = make_pipeline(\\n    StandardScaler(),\\n    PolynomialFeatures(),\\n    Ridge())\\nHow do we know which degrees of polynomials to choose, or whether to choose any\\npolynomials or interactions at all? Ideally we want to select the degree parameter\\nbased on the outcome of the classification. Using our pipeline, we can search over the\\ndegree parameter together with the parameter alpha of Ridge. To do this, we define a\\nparam_grid that contains both, appropriately prefixed by the step names:\\nIn[28]:\\nparam_grid = {\\'polynomialfeatures__degree\\': [1, 2, 3],\\n              \\'ridge__alpha\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\nNow we can run our grid search again:\\nIn[29]:\\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\\ngrid.fit(X_train, y_train)\\nWe can visualize the outcome of the cross-validation using a heat map (Figure 6-4), as\\nwe did in Chapter 5:\\nIn[30]:\\nplt.matshow(grid.cv_results_[\\'mean_test_score\\'].reshape(3, -1),\\n            vmin=0, cmap=\"viridis\")\\nplt.xlabel(\"ridge__alpha\")\\nplt.ylabel(\"polynomialfeatures__degree\")\\nGrid-Searching Preprocessing Steps and Model Parameters \\n| \\n317'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 331}, page_content='plt.xticks(range(len(param_grid[\\'ridge__alpha\\'])), param_grid[\\'ridge__alpha\\'])\\nplt.yticks(range(len(param_grid[\\'polynomialfeatures__degree\\'])),\\n           param_grid[\\'polynomialfeatures__degree\\'])\\nplt.colorbar()\\nFigure 6-4. Heat map of mean cross-validation score as a function of the degree of the\\npolynomial features and alpha parameter of Ridge\\nLooking at the results produced by the cross-validation, we can see that using polyno‐\\nmials of degree two helps, but that degree-three polynomials are much worse than\\neither degree one or two. This is reflected in the best parameters that were found:\\nIn[31]:\\nprint(\"Best parameters: {}\".format(grid.best_params_))\\nOut[31]:\\nBest parameters: {\\'polynomialfeatures__degree\\': 2, \\'ridge__alpha\\': 10}\\nWhich lead to the following score:\\nIn[32]:\\nprint(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))\\nOut[32]:\\nTest-set score: 0.77\\nLet’s run a grid search without polynomial features for comparison:\\nIn[33]:\\nparam_grid = {\\'ridge__alpha\\': [0.001, 0.01, 0.1, 1, 10, 100]}\\npipe = make_pipeline(StandardScaler(), Ridge())\\ngrid = GridSearchCV(pipe, param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nprint(\"Score without poly features: {:.2f}\".format(grid.score(X_test, y_test)))\\n318 \\n| \\nChapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 332}, page_content=\"Out[33]:\\nScore without poly features: 0.63\\nAs we would expect looking at the grid search results visualized in Figure 6-4, using\\nno polynomial features leads to decidedly worse results.\\nSearching over preprocessing parameters together with model parameters is a very\\npowerful strategy. However, keep in mind that GridSearchCV tries all possible combi‐\\nnations of the specified parameters. Therefore, adding more parameters to your grid\\nexponentially increases the number of models that need to be built.\\nGrid-Searching Which Model To Use\\nYou can even go further in combining GridSearchCV and Pipeline: it is also possible\\nto search over the actual steps being performed in the pipeline (say whether to use\\nStandardScaler or MinMaxScaler). This leads to an even bigger search space and\\nshould be considered carefully. Trying all possible solutions is usually not a viable\\nmachine learning strategy. However, here is an example comparing a RandomForest\\nClassifier and an SVC on the iris dataset. We know that the SVC might need the\\ndata to be scaled, so we also search over whether to use StandardScaler or no pre‐\\nprocessing. For the RandomForestClassifier, we know that no preprocessing is nec‐\\nessary. We start by defining the pipeline. Here, we explicitly name the steps. We want\\ntwo steps, one for the preprocessing and then a classifier. We can instantiate this\\nusing SVC and StandardScaler:\\nIn[34]:\\npipe = Pipeline([('preprocessing', StandardScaler()), ('classifier', SVC())])\\nNow we can define the parameter_grid to search over. We want the classifier to\\nbe either RandomForestClassifier or SVC. Because they have different parameters to\\ntune, and need different preprocessing, we can make use of the list of search grids we\\ndiscussed in “Search over spaces that are not grids” on page 271. To assign an estima‐\\ntor to a step, we use the name of the step as the parameter name. When we wanted to\\nskip a step in the pipeline (for example, because we don’t need preprocessing for the\\nRandomForest), we can set that step to None:\\nIn[35]:\\nfrom sklearn.ensemble import RandomForestClassifier\\nparam_grid = [\\n    {'classifier': [SVC()], 'preprocessing': [StandardScaler(), None],\\n     'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\\n     'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]},\\n    {'classifier': [RandomForestClassifier(n_estimators=100)],\\n     'preprocessing': [None], 'classifier__max_features': [1, 2, 3]}]\\nGrid-Searching Which Model To Use \\n| \\n319\"),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 333}, page_content='Now we can instantiate and run the grid search as usual, here on the cancer dataset:\\nIn[36]:\\nX_train, X_test, y_train, y_test = train_test_split(\\n    cancer.data, cancer.target, random_state=0)\\ngrid = GridSearchCV(pipe, param_grid, cv=5)\\ngrid.fit(X_train, y_train)\\nprint(\"Best params:\\\\n{}\\\\n\".format(grid.best_params_))\\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\\nprint(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))\\nOut[36]:\\nBest params:\\n{\\'classifier\\':\\n SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\\n     decision_function_shape=None, degree=3, gamma=0.01, kernel=\\'rbf\\',\\n     max_iter=-1, probability=False, random_state=None, shrinking=True,\\n     tol=0.001, verbose=False),\\n \\'preprocessing\\':\\n StandardScaler(copy=True, with_mean=True, with_std=True),\\n \\'classifier__C\\': 10, \\'classifier__gamma\\': 0.01}\\nBest cross-validation score: 0.99\\nTest-set score: 0.98\\nThe outcome of the grid search is that SVC with StandardScaler preprocessing, C=10,\\nand gamma=0.01 gave the best result.\\nSummary and Outlook\\nIn this chapter we introduced the Pipeline class, a general-purpose tool to chain\\ntogether multiple processing steps in a machine learning workflow. Real-world appli‐\\ncations of machine learning rarely involve an isolated use of a model, and instead are\\na sequence of processing steps. Using pipelines allows us to encapsulate multiple steps\\ninto a single Python object that adheres to the familiar scikit-learn interface of fit,\\npredict, and transform. In particular when doing model evaluation using cross-\\nvalidation and parameter selection using grid search, using the Pipeline class to cap‐\\nture all the processing steps is essential for proper evaluation. The Pipeline class also\\nallows writing more succinct code, and reduces the likelihood of mistakes that can\\nhappen when building processing chains without the pipeline class (like forgetting\\nto apply all transformers on the test set, or not applying them in the right order).\\nChoosing the right combination of feature extraction, preprocessing, and models is\\nsomewhat of an art, and often requires some trial and error. However, using pipe‐\\nlines, this “trying out” of many different processing steps is quite simple. When\\n320 \\n| \\nChapter 6: Algorithm Chains and Pipelines'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 334}, page_content='experimenting, be careful not to overcomplicate your processes, and make sure to\\nevaluate whether every component you are including in your model is necessary.\\nWith this chapter, we have completed our survey of general-purpose tools and algo‐\\nrithms provided by scikit-learn. You now possess all the required skills and know\\nthe necessary mechanisms to apply machine learning in practice. In the next chapter,\\nwe will dive in more detail into one particular type of data that is commonly seen in\\npractice, and that requires some special expertise to handle correctly: text data.\\nSummary and Outlook \\n| \\n321'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 335}, page_content=''),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 336}, page_content='CHAPTER 7\\nWorking with Text Data\\nIn Chapter 4, we talked about two kinds of features that can represent properties of\\nthe data: continuous features that describe a quantity, and categorical features that are\\nitems from a fixed list. There is a third kind of feature that can be found in many\\napplications, which is text. For example, if we want to classify an email message as\\neither a legitimate email or spam, the content of the email will certainly contain\\nimportant information for this classification task. Or maybe we want to learn about\\nthe opinion of a politician on the topic of immigration. Here, that individual’s\\nspeeches or tweets might provide useful information. In customer service, we often\\nwant to find out if a message is a complaint or an inquiry. We can use the subject line\\nand content of a message to automatically determine the customer’s intent, which\\nallows us to send the message to the appropriate department, or even send a fully\\nautomatic reply.\\nText data is usually represented as strings, made up of characters. In any of the exam‐\\nples just given, the length of the text data will vary. This feature is clearly very differ‐\\nent from the numeric features that we’ve discussed so far, and we will need to process\\nthe data before we can apply our machine learning algorithms to it.\\nTypes of Data Represented as Strings\\nBefore we dive into the processing steps that go into representing text data for\\nmachine learning, we want to briefly discuss different kinds of text data that you\\nmight encounter. Text is usually just a string in your dataset, but not all string features\\nshould be treated as text. A string feature can sometimes represent categorical vari‐\\nables, as we discussed in Chapter 5. There is no way to know how to treat a string\\nfeature before looking at the data.\\n323'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Optimization Shell 5.9.1.5 (http://www.pdf-tools.com)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-09-21T13:04:39+00:00', 'source': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'file_path': 'D:\\\\XXX.BADI BAAT CHEET.XXX\\\\GenAiPedia\\\\GenAIPed\\\\data\\\\Introduction-to-Machine-Learning-with-Python.pdf', 'total_pages': 392, 'format': 'PDF 1.6', 'title': 'Introduction to Machine Learning with Python', 'author': 'Andreas C. Müller and Sarah Guido', 'subject': '', 'keywords': '', 'moddate': '2020-08-19T07:09:16+02:00', 'trapped': '', 'modDate': \"D:20200819070916+02'00'\", 'creationDate': 'D:20160921130439Z', 'page': 337}, page_content='There are four kinds of string data you might see:\\n• Categorical data\\n• Free strings that can be semantically mapped to categories\\n• Structured string data\\n• Text data\\nCategorical data is data that comes from a fixed list. Say you collect data via a survey\\nwhere you ask people their favorite color, with a drop-down menu that allows them\\nto select from “red,” “green,” “blue,” “yellow,” “black,” “white,” “purple,” and “pink.”\\nThis will result in a dataset with exactly eight different possible values, which clearly\\nencode a categorical variable. You can check whether this is the case for your data by\\neyeballing it (if you see very many different strings it is unlikely that this is a categori‐\\ncal variable) and confirm it by computing the unique values over the dataset, and\\npossibly a histogram over how often each appears. You also might want to check\\nwhether each variable actually corresponds to a category that makes sense for your\\napplication. Maybe halfway through the existence of your survey, someone found that\\n“black” was misspelled as “blak” and subsequently fixed the survey. As a result, your\\ndataset contains both “blak” and “black,” which correspond to the same semantic\\nmeaning and should be consolidated.\\nNow imagine instead of providing a drop-down menu, you provide a text field for the\\nusers to provide their own favorite colors. Many people might respond with a color\\nname like “black” or “blue.” Others might make typographical errors, use different\\nspellings like “gray” and “grey,” or use more evocative and specific names like “mid‐\\nnight blue.” You will also have some very strange entries. Some good examples come\\nfrom the xkcd Color Survey, where people had to name colors and came up with\\nnames like “velociraptor cloaka” and “my dentist’s office orange. I still remember his\\ndandruff slowly wafting into my gaping yaw,” which are hard to map to colors auto‐\\nmatically (or at all). The responses you can obtain from a text field belong to the sec‐\\nond category in the list, free strings that can be semantically mapped to categories. It\\nwill probably be best to encode this data as a categorical variable, where you can\\nselect the categories either by using the most common entries, or by defining cate‐\\ngories that will capture responses in a way that makes sense for your application. You\\nmight then have some categories for standard colors, maybe a category “multicol‐\\nored” for people that gave answers like “green and red stripes,” and an “other” cate‐\\ngory for things that cannot be encoded otherwise. This kind of preprocessing of\\nstrings can take a lot of manual effort and is not easily automated. If you are in a posi‐\\ntion where you can influence data collection, we highly recommend avoiding man‐\\nually entered values for concepts that are better captured using categorical variables.\\nOften, manually entered values do not correspond to fixed categories, but still have\\nsome underlying structure, like addresses, names of places or people, dates, telephone\\n324 \\n| \\nChapter 7: Working with Text Data'),\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be5116de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1658\n"
     ]
    }
   ],
   "source": [
    "print(len(extracted_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0e9a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def filter_to_minimal(docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Given a list of Document objects, return a new list containing\n",
    "    only `page_content` and the `source` field in metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    minimal_docs: List[Document] = []\n",
    "    for doc in docs:  # <- use doc instead of docs\n",
    "        src = doc.metadata.get(\"source\")\n",
    "        minimal_docs.append(\n",
    "            Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={'source': src}  # fixed typo\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return minimal_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a89c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# splitting into smaller chunks\n",
    "\n",
    "def tect_splitter(minimal_docs):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "\n",
    "    text_chunks=text_splitter.split_documents(minimal_docs)\n",
    "    return text_chunks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fa45f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks = text_split(minimal_docs)\n",
    "# print(f\"Number of chunks: {len(text_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba7ed90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fakee\\AppData\\Local\\Temp\\ipykernel_4636\\634226631.py:12: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding model loaded on: CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def download_embedding():\n",
    "    \"\"\"\n",
    "    Download and return HuggingFace embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={'device': device}\n",
    "    )\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_embedding()\n",
    "print(\"✅ Embedding model loaded on:\", \"GPU\" if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "297f8e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.02205723710358143,\n",
       " 0.03135428950190544,\n",
       " 0.04234999418258667,\n",
       " 0.026110902428627014,\n",
       " -0.06519313901662827,\n",
       " -0.07004161924123764,\n",
       " 0.05430891364812851,\n",
       " -0.0016129856230691075,\n",
       " -0.06259436160326004,\n",
       " 0.03530614450573921,\n",
       " 0.027306079864501953,\n",
       " 0.05138075724244118,\n",
       " 0.09750932455062866,\n",
       " -0.06328082084655762,\n",
       " 0.008590328507125378,\n",
       " 0.022969570010900497,\n",
       " 0.06379096210002899,\n",
       " -0.026898536831140518,\n",
       " -0.13075022399425507,\n",
       " 0.03945808485150337,\n",
       " -0.04759358614683151,\n",
       " 0.051643915474414825,\n",
       " -0.008419306017458439,\n",
       " 0.0008068254101090133,\n",
       " -0.025930233299732208,\n",
       " -0.011951657943427563,\n",
       " -0.015885256230831146,\n",
       " 0.015778228640556335,\n",
       " 0.04706123098731041,\n",
       " -0.07030076533555984,\n",
       " 0.050214845687150955,\n",
       " 0.020775893703103065,\n",
       " 0.13571423292160034,\n",
       " 0.023629644885659218,\n",
       " 0.019857900217175484,\n",
       " 0.03593343496322632,\n",
       " -0.046726249158382416,\n",
       " -0.06226654350757599,\n",
       " -0.013844880275428295,\n",
       " -0.012370163574814796,\n",
       " -0.0317610539495945,\n",
       " -0.09367705881595612,\n",
       " -0.034933824092149734,\n",
       " -0.02574366331100464,\n",
       " 0.07390405982732773,\n",
       " -0.053186845034360886,\n",
       " -0.04518059641122818,\n",
       " 0.018814638257026672,\n",
       " 0.05634775385260582,\n",
       " -0.014028830453753471,\n",
       " -0.11054079234600067,\n",
       " -0.07068169862031937,\n",
       " -0.0013920937199145555,\n",
       " 0.016634445637464523,\n",
       " 0.05543273314833641,\n",
       " 0.011519731022417545,\n",
       " -0.023982210084795952,\n",
       " -0.039766136556863785,\n",
       " 0.01683732494711876,\n",
       " -0.08122079819440842,\n",
       " -0.020979799330234528,\n",
       " 0.04538748785853386,\n",
       " -0.055857110768556595,\n",
       " -0.0056115067563951015,\n",
       " 0.035526104271411896,\n",
       " -0.07962455600500107,\n",
       " -0.09304643422365189,\n",
       " -0.008659346029162407,\n",
       " -0.0012974727433174849,\n",
       " -0.0832023099064827,\n",
       " -0.026384664699435234,\n",
       " 0.06257691234350204,\n",
       " 0.01121602114289999,\n",
       " -0.003329504979774356,\n",
       " -0.034746140241622925,\n",
       " 0.04141344130039215,\n",
       " 0.0605878084897995,\n",
       " -0.03956429660320282,\n",
       " 0.022355929017066956,\n",
       " -0.042227908968925476,\n",
       " 0.046184904873371124,\n",
       " -0.07538673281669617,\n",
       " -0.021479390561580658,\n",
       " 0.013061363250017166,\n",
       " 0.004331756383180618,\n",
       " -0.009552577510476112,\n",
       " 0.012786935083568096,\n",
       " -0.036933474242687225,\n",
       " -0.005514406133443117,\n",
       " 0.022030198946595192,\n",
       " -0.12408144026994705,\n",
       " -0.008702156133949757,\n",
       " 0.02675602026283741,\n",
       " -0.03626164048910141,\n",
       " -0.11687617748975754,\n",
       " -0.06071846932172775,\n",
       " 0.033843688666820526,\n",
       " -0.013682645745575428,\n",
       " -0.09602653235197067,\n",
       " 0.22963358461856842,\n",
       " 0.009348662570118904,\n",
       " 0.07820979505777359,\n",
       " 0.015045699663460255,\n",
       " 0.05755985528230667,\n",
       " 0.01020764745771885,\n",
       " 0.0025226969737559557,\n",
       " -0.044642120599746704,\n",
       " 0.030379103496670723,\n",
       " -0.03968381509184837,\n",
       " 0.03330163657665253,\n",
       " -0.008976927027106285,\n",
       " -0.0213785283267498,\n",
       " -0.013625765219330788,\n",
       " -0.012874490581452847,\n",
       " 0.07796627283096313,\n",
       " -0.013226363807916641,\n",
       " -0.06876374781131744,\n",
       " 0.016280189156532288,\n",
       " 0.0367538183927536,\n",
       " 0.004795299377292395,\n",
       " 0.00507265655323863,\n",
       " 0.059669192880392075,\n",
       " 0.023314325138926506,\n",
       " 0.021185416728258133,\n",
       " -0.1063300147652626,\n",
       " -0.03833455219864845,\n",
       " 0.07211504876613617,\n",
       " -4.522456273373555e-33,\n",
       " 0.027335142716765404,\n",
       " 0.0020472118631005287,\n",
       " -0.009808731265366077,\n",
       " 0.15850844979286194,\n",
       " -0.009016240015625954,\n",
       " -0.0033776401542127132,\n",
       " -0.03989366814494133,\n",
       " -0.052505407482385635,\n",
       " -0.0006156283197924495,\n",
       " 0.01677430234849453,\n",
       " 0.037250127643346786,\n",
       " 0.006282258313149214,\n",
       " -0.042243920266628265,\n",
       " 0.020294172689318657,\n",
       " 0.03365323320031166,\n",
       " 0.027241883799433708,\n",
       " -0.046183280646800995,\n",
       " 0.01020265743136406,\n",
       " -0.058962561190128326,\n",
       " 0.058991577476263046,\n",
       " -0.04121154174208641,\n",
       " 0.016267046332359314,\n",
       " 0.007167506497353315,\n",
       " 0.08636107295751572,\n",
       " 0.017405234277248383,\n",
       " -0.007778078317642212,\n",
       " 0.041081856936216354,\n",
       " -0.05575437843799591,\n",
       " -0.013084162026643753,\n",
       " 0.02492188662290573,\n",
       " 0.02722764201462269,\n",
       " -0.030635034665465355,\n",
       " 0.09964340925216675,\n",
       " 0.04451429843902588,\n",
       " -0.031742487102746964,\n",
       " -0.029156355187296867,\n",
       " 0.038977593183517456,\n",
       " -0.04803934693336487,\n",
       " -0.03566892445087433,\n",
       " -0.011000942438840866,\n",
       " -0.07750045508146286,\n",
       " 0.03693181648850441,\n",
       " 0.04813605919480324,\n",
       " -0.06909150630235672,\n",
       " 0.055146414786577225,\n",
       " 0.016942068934440613,\n",
       " 0.02962631918489933,\n",
       " 0.06599096953868866,\n",
       " 0.03579479083418846,\n",
       " 0.04946454241871834,\n",
       " 0.05101867765188217,\n",
       " 0.01875029131770134,\n",
       " -0.09286917001008987,\n",
       " 0.06615084409713745,\n",
       " -0.04218549653887749,\n",
       " 0.015501974150538445,\n",
       " -0.0022310204803943634,\n",
       " -0.013314560055732727,\n",
       " 0.048017702996730804,\n",
       " 0.08069080859422684,\n",
       " 0.030310658738017082,\n",
       " 0.08850878477096558,\n",
       " -0.01820780523121357,\n",
       " 0.027470117434859276,\n",
       " 0.008387885987758636,\n",
       " -0.06184953451156616,\n",
       " 0.0520973838865757,\n",
       " 0.06743858009576797,\n",
       " 0.026618726551532745,\n",
       " 0.001351247075945139,\n",
       " -0.014477846212685108,\n",
       " -0.03522709012031555,\n",
       " -0.03518245369195938,\n",
       " 0.026843100786209106,\n",
       " 0.018795721232891083,\n",
       " 0.041659291833639145,\n",
       " 0.09347138553857803,\n",
       " -0.03851080685853958,\n",
       " 0.0067323618568480015,\n",
       " -0.05565562844276428,\n",
       " 0.014049987308681011,\n",
       " -0.02385706640779972,\n",
       " -0.03314581885933876,\n",
       " -0.012032240629196167,\n",
       " 0.036135248839855194,\n",
       " -0.00030938739655539393,\n",
       " -0.051017243415117264,\n",
       " -0.08993197232484818,\n",
       " 0.04211454093456268,\n",
       " 0.015342808328568935,\n",
       " -0.0322791151702404,\n",
       " 0.07115723937749863,\n",
       " 0.06273623555898666,\n",
       " -0.021076837554574013,\n",
       " -0.06244686618447304,\n",
       " 3.661936735880053e-33,\n",
       " 0.10509847849607468,\n",
       " -0.004707911517471075,\n",
       " -0.07333016395568848,\n",
       " 0.01847505196928978,\n",
       " -0.09632518142461777,\n",
       " -0.02275468036532402,\n",
       " -0.015285484492778778,\n",
       " 0.1631201207637787,\n",
       " -0.07286746054887772,\n",
       " 0.09933967143297195,\n",
       " 0.032615821808576584,\n",
       " 0.038013700395822525,\n",
       " 0.07582204788923264,\n",
       " 0.018397372215986252,\n",
       " 0.0682143047451973,\n",
       " -0.07261015474796295,\n",
       " 0.12584620714187622,\n",
       " 0.03559095039963722,\n",
       " 0.025969643145799637,\n",
       " 0.04967903718352318,\n",
       " -0.047376543283462524,\n",
       " -0.012329989112913609,\n",
       " -0.07172983884811401,\n",
       " -0.003069873433560133,\n",
       " 0.030923547223210335,\n",
       " -0.002194629982113838,\n",
       " -0.003967700060456991,\n",
       " 0.042894888669252396,\n",
       " -0.11592283099889755,\n",
       " -0.002380000427365303,\n",
       " 0.07403351366519928,\n",
       " 0.05381366237998009,\n",
       " -0.03758934512734413,\n",
       " 0.026907125487923622,\n",
       " 0.005883782636374235,\n",
       " 0.05761600658297539,\n",
       " -0.013321510516107082,\n",
       " -0.007190285250544548,\n",
       " -0.03515922278165817,\n",
       " -0.020705193281173706,\n",
       " -0.11105144023895264,\n",
       " 0.025323253124952316,\n",
       " -0.0730958878993988,\n",
       " 0.03548255190253258,\n",
       " -0.056926045566797256,\n",
       " -0.029925676062703133,\n",
       " -0.045105330646038055,\n",
       " -0.012686741538345814,\n",
       " -0.029009487479925156,\n",
       " 0.028855634853243828,\n",
       " -0.06583082675933838,\n",
       " 0.006223728880286217,\n",
       " 0.007840684615075588,\n",
       " 0.01629818230867386,\n",
       " -0.05923839285969734,\n",
       " -0.004182525910437107,\n",
       " -0.06295741349458694,\n",
       " -0.06519576162099838,\n",
       " -0.003802071325480938,\n",
       " -0.011146867647767067,\n",
       " 0.02419794164597988,\n",
       " 0.05301505699753761,\n",
       " 0.01267341710627079,\n",
       " 0.0928298681974411,\n",
       " 0.014117647893726826,\n",
       " 0.04737703129649162,\n",
       " -0.0403350293636322,\n",
       " -0.023019105195999146,\n",
       " 0.0180230550467968,\n",
       " -0.015020160004496574,\n",
       " 0.011018970981240273,\n",
       " 0.05470914766192436,\n",
       " 0.02371108904480934,\n",
       " 0.026044102385640144,\n",
       " -0.06312958896160126,\n",
       " 0.01522545050829649,\n",
       " -0.08650799840688705,\n",
       " -0.03997374698519707,\n",
       " 0.0015629325062036514,\n",
       " -0.0511799082159996,\n",
       " 0.000962629506830126,\n",
       " 0.0018843327416107059,\n",
       " -0.019877413287758827,\n",
       " -0.0498027466237545,\n",
       " -0.010613597929477692,\n",
       " -0.002417338080704212,\n",
       " 0.032544903457164764,\n",
       " 0.025539038702845573,\n",
       " -0.007947270758450031,\n",
       " 0.016176559031009674,\n",
       " 0.0265226848423481,\n",
       " -0.021186694502830505,\n",
       " 0.008496362715959549,\n",
       " 0.03192000836133957,\n",
       " -0.027117790654301643,\n",
       " -1.5282614285183627e-08,\n",
       " -0.06411872059106827,\n",
       " 0.02464344911277294,\n",
       " 0.04626180976629257,\n",
       " 0.048596564680337906,\n",
       " 0.007819435559213161,\n",
       " -0.012928787618875504,\n",
       " -0.04803771898150444,\n",
       " -0.08486431837081909,\n",
       " -0.030247414484620094,\n",
       " -0.03762543946504593,\n",
       " 0.07040376216173172,\n",
       " 0.04994654282927513,\n",
       " -0.12290773540735245,\n",
       " 0.007145001087337732,\n",
       " 0.03754075616598129,\n",
       " 0.042475491762161255,\n",
       " 0.00904129259288311,\n",
       " 0.024457957595586777,\n",
       " -0.030379517003893852,\n",
       " -0.052715010941028595,\n",
       " 0.0039764465764164925,\n",
       " 0.05427493900060654,\n",
       " -0.012069986201822758,\n",
       " -0.0035730248782783747,\n",
       " 0.021069228649139404,\n",
       " -0.007885019294917583,\n",
       " -0.06972014158964157,\n",
       " 0.013524859212338924,\n",
       " 0.027061408385634422,\n",
       " -0.017136594280600548,\n",
       " 0.008757298812270164,\n",
       " 0.1335127204656601,\n",
       " -0.06976009160280228,\n",
       " -0.03137251362204552,\n",
       " -0.009788607247173786,\n",
       " -0.008997020311653614,\n",
       " -0.017270853742957115,\n",
       " 0.032768554985523224,\n",
       " 0.047551192343235016,\n",
       " -0.04580565169453621,\n",
       " -0.046248145401477814,\n",
       " 0.08598405867815018,\n",
       " -0.00576346181333065,\n",
       " -0.12718035280704498,\n",
       " 0.028943462297320366,\n",
       " 0.04704803600907326,\n",
       " 0.03756965696811676,\n",
       " -0.1049632802605629,\n",
       " 0.015016702003777027,\n",
       " -0.07220996916294098,\n",
       " -0.025340640917420387,\n",
       " 0.08254174888134003,\n",
       " 0.09899785369634628,\n",
       " 0.06218205764889717,\n",
       " 0.05866355076432228,\n",
       " 0.06878487020730972,\n",
       " 0.057139765471220016,\n",
       " 0.09557656943798065,\n",
       " -0.03990267962217331,\n",
       " -0.038208723068237305,\n",
       " 0.015516477636992931,\n",
       " -0.03052833303809166,\n",
       " 0.07461880147457123,\n",
       " 0.05132466182112694]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector=embeddings.embed_query('hello word')\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "426f40e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "print(len(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a332c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4403e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\n",
    "GROQ_API_KEY=os.getenv(\"GROQ_API\")\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"]=PINECONE_API_KEY\n",
    "os.environ[\"GROQ_API_KEY\"]=GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d29477c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "PINECONE_API_KEY=PINECONE_API_KEY\n",
    "\n",
    "\n",
    "pc=Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d3ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAiPed_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
